<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>YC Note</title><link href="YCNote/" rel="alternate"></link><link href="YCNote/feeds/all.atom.xml" rel="self"></link><id>YCNote/</id><updated>2018-04-14T12:00:00+08:00</updated><entry><title>物件導向武功秘笈（3）：內功篇 — 物件導向指導原則SOLID</title><link href="YCNote/introduction-object-oriented-programming_3.html" rel="alternate"></link><published>2018-04-14T12:00:00+08:00</published><updated>2018-04-14T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2018-04-14:YCNote/introduction-object-oriented-programming_3.html</id><summary type="html">&lt;h3&gt;物件導向怎麼用才能成就好的程式碼？&lt;/h3&gt;
&lt;p&gt;一個好的工具，也要配合對於工具的理解，才能發揮效用。&lt;a href="http://www.ycc.idv.tw/YCNote/post/48"&gt;在上一回中&lt;/a&gt;，我們完整介紹了Java和Python的物件導向實現方式，我們講到了「封裝」、「繼承」、「多型」等等物件導向的特色，也講了「抽象類別」、「接口」等抽象化的方法，不過我並沒有告訴大家該怎麼用這些工具？使用這些工具是不是有什麼樣的法則？&lt;/p&gt;
&lt;p&gt;在接下來的這一篇，我將會介紹物件導向的使用方式，我會提到物件導向著名的六大法則：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;單一職責原理&lt;/li&gt;
&lt;li&gt;開閉原理&lt;/li&gt;
&lt;li&gt;里氏替換原則&lt;/li&gt;
&lt;li&gt;迪米特法則&lt;/li&gt;
&lt;li&gt;依賴倒置原則&lt;/li&gt;
&lt;li&gt;接口分隔原則&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在這之前我們先來介紹描述類別關係的UML類別圖。&lt;/p&gt;
&lt;h3&gt;UML類別圖&lt;/h3&gt;
&lt;p&gt;開始介紹各種原則之前，先來介紹UML類別圖，UML全名稱為Unified Modeling Language，是一種使用圖形來描繪軟體工程架構的方法，這邊準備介紹的是它的類別圖，這個工具有助於我們快速的了解物件與物件之間的關係。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;類別(Class): -代表&lt;code&gt;private&lt;/code&gt;，+代表&lt;code&gt;public&lt;/code&gt;，#代表&lt;code&gt;protected&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Class" src="https://www.ycc.idv.tw/media/SOLID_Introduction/Class.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;抽象類別(Abstract Class)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="AbstractClass" src="https://www.ycc.idv.tw/media/SOLID_Introduction/AbstractClass.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;接口(Interface)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Interface" src="https://www.ycc.idv.tw/media/SOLID_Introduction/Interface.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;繼承關係 …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h3&gt;物件導向怎麼用才能成就好的程式碼？&lt;/h3&gt;
&lt;p&gt;一個好的工具，也要配合對於工具的理解，才能發揮效用。&lt;a href="http://www.ycc.idv.tw/YCNote/post/48"&gt;在上一回中&lt;/a&gt;，我們完整介紹了Java和Python的物件導向實現方式，我們講到了「封裝」、「繼承」、「多型」等等物件導向的特色，也講了「抽象類別」、「接口」等抽象化的方法，不過我並沒有告訴大家該怎麼用這些工具？使用這些工具是不是有什麼樣的法則？&lt;/p&gt;
&lt;p&gt;在接下來的這一篇，我將會介紹物件導向的使用方式，我會提到物件導向著名的六大法則：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;單一職責原理&lt;/li&gt;
&lt;li&gt;開閉原理&lt;/li&gt;
&lt;li&gt;里氏替換原則&lt;/li&gt;
&lt;li&gt;迪米特法則&lt;/li&gt;
&lt;li&gt;依賴倒置原則&lt;/li&gt;
&lt;li&gt;接口分隔原則&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在這之前我們先來介紹描述類別關係的UML類別圖。&lt;/p&gt;
&lt;h3&gt;UML類別圖&lt;/h3&gt;
&lt;p&gt;開始介紹各種原則之前，先來介紹UML類別圖，UML全名稱為Unified Modeling Language，是一種使用圖形來描繪軟體工程架構的方法，這邊準備介紹的是它的類別圖，這個工具有助於我們快速的了解物件與物件之間的關係。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;類別(Class): -代表&lt;code&gt;private&lt;/code&gt;，+代表&lt;code&gt;public&lt;/code&gt;，#代表&lt;code&gt;protected&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Class" src="https://www.ycc.idv.tw/media/SOLID_Introduction/Class.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;抽象類別(Abstract Class)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="AbstractClass" src="https://www.ycc.idv.tw/media/SOLID_Introduction/AbstractClass.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;接口(Interface)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Interface" src="https://www.ycc.idv.tw/media/SOLID_Introduction/Interface.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;繼承關係(Inheritance)和抽象類、接口實現&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Inheritance" src="https://www.ycc.idv.tw/media/SOLID_Introduction/Inheritance.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;關聯關係(Association)：A類中使用B類當作「成員變數」，但是A和B並沒有「擁有」的關係，只能說是「有個」的關係，就稱為：A關聯到B，英文為"has-a"的關係。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Associatione" src="https://www.ycc.idv.tw/media/SOLID_Introduction/Association.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;聚合關係(Aggregation)：A類中使用B類當作「成員變數」，而且A和B有一個弱的「擁有」關係，A包含B，但B不是A的一部分，拔掉B，A依然能存在，就稱為：A聚合到B，英文為"owns-a"關係。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Aggregation" src="https://www.ycc.idv.tw/media/SOLID_Introduction/Aggregation.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合成（組合）關係(Composition)：A類中使用B類當作「成員變數」，而且A和B有一個強的「擁有」關係，B是A的組成的一部分，拔掉B，A就不完整，就稱為：A合成到B，英文為"is-part-of"關係。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Composition" src="https://www.ycc.idv.tw/media/SOLID_Introduction/Composition.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;依賴關係(Dependency)：A類中使用到B類，但僅僅是弱連結，譬如：B類作為A類方法的參數、B類作為A類的局域變數、A類調用B類的靜態方法、B類作為A類方法的回傳值，就稱為：A依賴B，英文為"uses-a"的關係。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Dependency" src="https://www.ycc.idv.tw/media/SOLID_Introduction/Dependency.png"&gt;&lt;/p&gt;
&lt;h3&gt;單一職責原則(Single Responsibility Principle, SRP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定義：There should never be more than one reason for a class to change.（一個類別中不要有多於一個以上的變化理由）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;簡單的說，就是一個類別中不要做超過一件事，要去切分直到不能再分割為止，如此一來可以提高內聚性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;乍看之下，這樣的原則很容易實現，但是魔鬼藏在細節裡，我們常常會沒注意到其實還可以繼續的切分。舉個例子，假設我想設計一個電話的接口，我可能是這樣設計的&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="phone_1" src="https://www.ycc.idv.tw/media/SOLID_Introduction/phone_1.png"&gt;&lt;/p&gt;
&lt;p&gt;乍看之下沒有問題，一個電話擁有撥號、掛號、數據傳送和接收，但是等等！連接的過程和數據的傳輸其實是兩個職責啊！它們之間沒有強烈的關聯性，完全是可以分開處理的，因此這個配置不符合「單一職責原則」，可以繼續切分下去，修改如下。&lt;/p&gt;
&lt;p&gt;&lt;img alt="phone_2" src="https://www.ycc.idv.tw/media/SOLID_Introduction/phone_2.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;「單一職責原則」原文指的是類別的單一職責，但是務實上，類別如果切分到如此程度，程式碼會變得細碎不堪，這違反了程式碼的「可讀性」，所以我們一般只要求「接口必須保持單一原則」，而類別去套用接口，類別就盡量達成少的職責就好。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;開閉原則(Open-Closed Principle, OCP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定義：Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification.（軟體中的實體，例如：類、模組、函數等等，都必須對延伸開放，但對修改封閉）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對延伸開放：實體在因應新的改變時，必須是可以靈活擴充的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對修改封閉：實體一旦完成，就盡量不要再去修改它了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;綜合以上兩點，我們可以總結出：實體本身的內聚性要高，可以讓我們未來不需要再做修改，單一職責可以做到增強內聚性；實體間的耦合性要低，所以實體像是積木一樣可以因應各種需求去任意組合、擴充。所以「開閉原則」只是進一步的把「低耦合高內聚」再說的更清楚一點，實現「開閉原則」將有利於單元測試、提高維護和擴充能力。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;里氏替換原則(Liskov Subsititution Principle, LSP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定義：What is wanted here is something like the following substitution property: If for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T.（簡言之：子類對象能夠替換其父類對象，使用父類方法而不會有問題）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;「里氏替換原則」用於規範繼承，子類繼承自父類的方法是保有彈性可以覆寫(Overriding)和多載(Overloading)的，但是應該怎麼做，程式碼才不會髒掉？「里氏替換原則」告訴我們一個簡單的法則，就是先寫一段父類的執行代碼，然後把父類替換成子類，然後再跑跑看能不能正常執行，如果正常執行代表這個繼承關係是健康的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;為什麼要這樣檢查？之前我們提過繼承主要是為了要避免Repeat Yourself而生，我們找出各種類別共享的屬性和方法，把它獨立出來，然後大家再一起繼承自它，所以我們要盡可能的避免父類出現不是共享的性質。也就是說在理想情況下「父類必須等於子類們的交集」，所以「父類必定是任一子類的子集合」，因此「使用子類來執行父類是不應該有問題的」，這就是「里氏替換原則」。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;為了遵循「里氏替換原則」，則子類必須完全實現父類的方法。如果子類不能完整地實現父類的方法，或者父類的某些方法在子類中已經發生了「畸變」，則建議斷開父子繼承關係，採用依賴、聚集、組合等關係替代。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;有了「里氏替換原則」，我們終於可以談談一個上一章沒提到的重要問題：什麼情況可以做繼承？有一些書籍會告訴你，繼承為"is-a"的關係，例如：瑪爾濟斯(B) is-a 狗(A)，所以瑪爾濟斯(B)可以繼承狗(A)，乍看之下沒問題，但這樣的說法存在缺陷，舉個例子，假設今天我先有了類別&lt;code&gt;Retangle&lt;/code&gt;，也就是長方形，然後我想要弄一個新的類別&lt;code&gt;Square&lt;/code&gt;，也就是正方形，我可以讓&lt;code&gt;Square&lt;/code&gt;繼承自&lt;code&gt;Retangle&lt;/code&gt;嗎？我們用"is-a"來檢視：正方形是一個長方形？答案是Yes，但是「里氏替換原則」持相反意見，來看一下，&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="square_1" src="https://www.ycc.idv.tw/media/SOLID_Introduction/square_1.png"&gt;&lt;/p&gt;
&lt;p&gt;依照「里氏替換原則」，&lt;code&gt;Square&lt;/code&gt;不能繼承自&lt;code&gt;Retangle&lt;/code&gt;，因為&lt;code&gt;Square&lt;/code&gt;只需要&lt;code&gt;width&lt;/code&gt;的成員變數，而&lt;code&gt;Retangle&lt;/code&gt;則需要&lt;code&gt;width&lt;/code&gt;和&lt;code&gt;height&lt;/code&gt;兩個成員變數，當我們將子類&lt;code&gt;Square&lt;/code&gt;放到父類&lt;code&gt;Retangle&lt;/code&gt;的方法中，因為缺少&lt;code&gt;height&lt;/code&gt;變數，必然會出錯，所以違反「里氏替換原則」，因此這兩類不適合作為「繼承」關係。我們可以這樣改善，讓&lt;code&gt;Square&lt;/code&gt;應用&lt;code&gt;Retangle&lt;/code&gt;來幫忙計算，使用「關聯」關係取代「繼承」關係。&lt;/p&gt;
&lt;p&gt;&lt;img alt="square_2" src="https://www.ycc.idv.tw/media/SOLID_Introduction/square_2.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下面這一張集合圖是我自創的，圖中清楚的指出「繼承」中的父類和子類應該是什麼樣的關係。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Inheritance Principle.jpeg" src="https://www.ycc.idv.tw/media/SOLID_Introduction/inheritance_principle.jpeg"&gt;&lt;/p&gt;
&lt;h3&gt;迪米特法則(Law of Demeter, LoD)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;又稱為「最少知識原則」&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;定義：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each unit should have only limited knowledge about other units: only units "closely" related to the current unit.&lt;/li&gt;
&lt;li&gt;Each unit should only talk to its friends; don't talk to strangers.&lt;/li&gt;
&lt;li&gt;Only talk to your immediate friends.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;「朋友」的定義：對於類別C的所有方法M而言，在M的方法中僅能訪問以下物件的方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;self&lt;/code&gt;，類別C自身&lt;/li&gt;
&lt;li&gt;M的輸入參數&lt;/li&gt;
&lt;li&gt;C的成員變數&lt;/li&gt;
&lt;li&gt;M的輸出物件&lt;/li&gt;
&lt;li&gt;全域變數的物件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;白話總結：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;僅能訪問那些類別出現在自身、成員變數、方法的輸入和輸出參數中的方法。&lt;/li&gt;
&lt;li&gt;減少類別的對外方法，將沒必要對外公布的方法隱藏起來。 &lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;（詳解）僅能訪問那些類別出現在自身、成員變數、方法的輸入和輸出參數中的方法。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;-- 例子: 假設今天一名老師給了學生名條想叫班長幫忙點名。&lt;/p&gt;
&lt;p&gt;錯誤示範：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Student&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#friends: None&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Leader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#friends: Student&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;countStudents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;student_list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Total number of students is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;student_list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Teacher&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#friends: Leader&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;command&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;student_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;student_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Student&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;#`Student` is not a friend&lt;/span&gt;
        &lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;countStudents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;student_list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;teacher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Teacher&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;leader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Leader&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;name_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;D&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;E&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;teacher&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="teacher-leader-student_1" src="https://www.ycc.idv.tw/media/SOLID_Introduction/teacher-leader-student_1.png"&gt;&lt;/p&gt;
&lt;p&gt;以上程式違反「迪米特法則」，因為在類別&lt;code&gt;Teacher&lt;/code&gt;的方法&lt;code&gt;command&lt;/code&gt;中訪問了不是朋友的&lt;code&gt;Student&lt;/code&gt;，這會使得&lt;code&gt;Teacher&lt;/code&gt;和&lt;code&gt;Student&lt;/code&gt;會產生不必要的耦合，我們可以將創造&lt;code&gt;student_list&lt;/code&gt;的權責轉移到&lt;code&gt;Leader&lt;/code&gt;上，如此一來就可以斷開&lt;code&gt;Teacher&lt;/code&gt;和&lt;code&gt;Student&lt;/code&gt;的耦合。&lt;/p&gt;
&lt;p&gt;正確示範：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Student&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#friends: None&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Leader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#friends: Student&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;giveNameList&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;student_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;student_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Student&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__student_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;student_list&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;countStudents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Total number of students is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__student_list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Teacher&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#friends: Leader&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;command&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;giveNameList&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;countStudents&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;teacher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Teacher&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;leader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Leader&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;name_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;D&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;E&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;teacher&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img alt="teacher-leader-student_2" src="https://www.ycc.idv.tw/media/SOLID_Introduction/teacher-leader-student_2.png"&gt;&lt;/p&gt;
&lt;p&gt;-- Why it works? &lt;/p&gt;
&lt;p&gt;先來想想「朋友」有什麼共通之處？其實它們都是類別本身無法斷開耦合的物件，既然無法斷開耦合，何不運用到底，運用這些「朋友」來完成任務，不要再去增加其他的耦合性，也同時幫助提升類別的內聚性，這就是「迪米特法則」真正想做的事。&lt;/p&gt;
&lt;p&gt;以這樣的方式去寫程式，也可以避免寫出像是&lt;code&gt;A.getB().getC()&lt;/code&gt;的程式碼（A和C不是朋友），這樣冗長的程式碼不僅增加了無益的耦合，也讓程式變得不利於可讀性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（詳解）減少類別的對外方法，將沒必要對外公布的方法隱藏起來。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;-- 例子: 安裝程式。&lt;/p&gt;
&lt;p&gt;錯誤範例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Wizard&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# 3 public methods&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;first&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install first step of wizard at mode&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;second&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install second step of wizard at mode &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;third&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install third step of wizard&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Install&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;second&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;third&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;有太多沒必要對外公布的細節了，依照「迪米特法則」，我們應該將盡量減少對外公布的資訊，把不必要公布的細節私有化。&lt;/p&gt;
&lt;p&gt;正確範例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Wizard&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# only 1 public method&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__first&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__second&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__third&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__first&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install first step of wizard&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__second&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install second step of wizard at mode &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__third&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install third step of wizard&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Install&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;依賴倒置原則(Dependence Inversion Principle, DIP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定義：High level modules should not depend upon low level modules. Both should depend upon abstractions. Abstractions should not depend upon details. Details should depend upon abstractions.（高層次模組不應該依賴低層次模組，兩者都應該依賴抽象。而抽象不應該依賴細節，反之細節應該要依賴抽象。）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;範例：假設我們成立一家玩具車公司，開始著手設計我們的第一款車款A，設計的架構圖如下&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="toycar_1" src="https://www.ycc.idv.tw/media/SOLID_Introduction/toycar_1.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ToyCarA&lt;/code&gt;是我們的實體車子，可以使用&lt;code&gt;setRPM&lt;/code&gt;來設定在幾秒之後到達什麼轉速，而控制他的是電腦模擬的虛擬車&lt;code&gt;VirtualCar&lt;/code&gt;，虛擬車提供方法&lt;code&gt;setSpeeed&lt;/code&gt;可以設定車速，當然！模擬的車速要和真實車速吻合，還需配合適當的調控實體車的轉速，所以&lt;code&gt;VirtualCarA&lt;/code&gt;關聯到&lt;code&gt;ToyCarA&lt;/code&gt;去做控制。然後我們公司會提供一個控制器&lt;code&gt;ControllerA&lt;/code&gt;來控制車子，控制器上的搖桿分為五級，讓使用者可以控制速度，使用&lt;code&gt;controlBarLevel&lt;/code&gt;方法根據級數去控制&lt;code&gt;VirtualCarA&lt;/code&gt;的速度。&lt;/p&gt;
&lt;p&gt;檢驗一下這個設計圖，它違反「依賴倒置原則」，&lt;code&gt;ControllerA&lt;/code&gt;依賴&lt;code&gt;VirtualCarA&lt;/code&gt;，&lt;code&gt;VirtualCarA&lt;/code&gt;依賴&lt;code&gt;ToyCarA&lt;/code&gt;，這些都是實體類別，高層次依賴了低層次。不過，公司的車子A還是賣得很好，沒有什麼大礙。&lt;/p&gt;
&lt;p&gt;終於有一天災難降臨了，市場出現了比車子A馬力更強大的玩具車，我們公司如果不趕緊採用新的馬達推出新的車款，就會失去競爭力，我們需要採用新的玩具車&lt;code&gt;ToyCarB&lt;/code&gt;，它擁有更好的馬達，我們需要為因應高速度而推出九級分級的搖桿，新的控制器&lt;code&gt;ControllerB&lt;/code&gt;，結果回頭一看原本設計圖，完了！所有的A系列的程式碼都耦合在一起了，核心程式&lt;code&gt;VirtualCarA&lt;/code&gt;原可以不需要大改，就建造出&lt;code&gt;VirtualCarB&lt;/code&gt;的，但是現在程式碼全部耦合在一起，它已經變得不可擴張了。&lt;/p&gt;
&lt;p&gt;如果我們一開始就依照「依賴倒置原則」，我們來看看擴張會有多容易&lt;/p&gt;
&lt;p&gt;&lt;img alt="toycar_2" src="https://www.ycc.idv.tw/media/SOLID_Introduction/toycar_2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;VirtualCar&lt;/code&gt;中的很多方法都可以在B車款上再重用，大大的減少重新開發的成本。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;依賴倒置原則又稱為「面向接口原則」，這裡的接口應該想的更廣義一點，不侷限在interface上，我認為只要藉由抽象化將架構擬定出來的這些抽象單元都可以稱作接口，「廣義的接口」可以是指&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;客戶端和業務邏輯的分離介面&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;物件的開放方法&lt;/li&gt;
&lt;li&gt;抽象類別&lt;/li&gt;
&lt;li&gt;定義行為的interface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我們不讓作為實現的類別彼此依賴，而是使用接口將抽象架構擬定好，再讓類別去依賴接口實現目標。&lt;/p&gt;
&lt;h3&gt;接口分隔原則(Interface Segregation Principle, ISP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定義：Clients should not be forced to depend uponn interfaces that they don't use. The dependency of one class to another one should depend on the smallest possible interface.（客戶類不應該被強迫依賴那些它不需要的接口，類別間的彼此依賴應該建立在盡可能小的接口上）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;這裡說的接口同樣的是剛剛所說的「廣義接口」，可以是客戶端和業務邏輯的分離介面、物件的開放方法、抽象類別和Interface。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;接口分隔原則建議我們要讓這些廣義接口盡可能的細切，但在實務上，切的過細會導致程式碼非常零碎難以閱讀，所以YC的建議是切到遵守「單一職責原理」就足夠了，與剛剛的建議一致，Interface一定要遵守「單一職責原理」，但是類別就盡力而為吧！&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;範例：剛剛玩具車公司的設計圖其實還是不夠好，如果今天公司想要開發新的車款C，添加新「方向盤」的功能，你會發現夢魘又再次的降臨，抽象類別&lt;code&gt;ToyCar&lt;/code&gt;、&lt;code&gt;VirtualCar&lt;/code&gt;、&lt;code&gt;Controller&lt;/code&gt;都需要改變，而且就算真的把「方向盤」的相關方法添加上去，抽象類別也會開始出現多於一的職責，所以我們用Interface來重新改寫架構，如下圖所示。不難看出，控制馬達、控制速度、控制搖桿的行為是彼此依賴的，我們可以將他們的行為由Interface獨立拉出並相互依賴。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="toycar_3" src="https://www.ycc.idv.tw/media/SOLID_Introduction/toycar_3.png"&gt;&lt;/p&gt;
&lt;p&gt;如此一來，當我們想要開發新的車款C，添加新「方向盤」的功能，也能輕鬆的擴充，如下所示。&lt;code&gt;ToyCarPlus&lt;/code&gt;、&lt;code&gt;VirtualCarPlus&lt;/code&gt;、&lt;code&gt;ControllerPlus&lt;/code&gt;是我們實作C車款的抽象類別，它現在可以直接套用&lt;code&gt;IMotor&lt;/code&gt;、&lt;code&gt;ISpeed&lt;/code&gt;、&lt;code&gt;IControlBar&lt;/code&gt;的Interface。&lt;/p&gt;
&lt;p&gt;&lt;img alt="toycar_4" src="https://www.ycc.idv.tw/media/SOLID_Introduction/toycar_4.png"&gt;&lt;/p&gt;
&lt;h3&gt;總結：物件導向的指導原則—SOLID&lt;/h3&gt;
&lt;p&gt;上面介紹的六大原理：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Single Responsibility Principl&lt;/li&gt;
&lt;li&gt;Open-Closed Principle&lt;/li&gt;
&lt;li&gt;Liskov Subsititution Principle&lt;/li&gt;
&lt;li&gt;Law of Demeter&lt;/li&gt;
&lt;li&gt;Interface Segregation Principle&lt;/li&gt;
&lt;li&gt;Dependence Inversion Principle&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;剛剛好組成SOLID這個單字，所以又被統稱SOLID原則。&lt;/p&gt;
&lt;p&gt;事實上，這些原則所要達到的目的，不外乎就是我們&lt;a href="http://www.ycc.idv.tw/YCNote/post/47"&gt;第一篇&lt;/a&gt;當中所介紹的好的程式碼特性：「正常執行」、「穩健」、「不重複撰寫」、「可讀性」、「可擴展」，或者是「低耦合、高內聚」，所以寫程式時如果能時時注意，說不定你也可以自己領會這六大法則。&lt;/p&gt;
&lt;p&gt;我來快速的總結這六大法則告訴我們的事：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在開發程式的初期，先定義好抽象架構，也就是廣義的接口，徹底的使客戶端與業務邏輯分離，將「行為」定義成Interface，將「類別的泛化」定義成Abstract Class。&lt;/li&gt;
&lt;li&gt;所有的實體類別都依賴於抽象，細節依賴於抽象。&lt;/li&gt;
&lt;li&gt;每個單元盡量達到：單一權責、對延伸開放但對修改封閉、盡可能少的對外方法。&lt;/li&gt;
&lt;li&gt;牽涉「繼承」，必須要問自己：子類可以替換父類執行嗎？父類是不是為子類的交集？&lt;/li&gt;
&lt;li&gt;類別中的方法僅能訪問那些類別出現在自身、成員變數、方法的輸入和輸出參數中的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如此一來，我們心中就有一個準則去使用物件導向。&lt;/p&gt;
&lt;p&gt;在一般情形下，這三篇的內容應該就足夠讓你寫出好的程式碼，但是實際面上使用仍然會碰到許多問題，於是乎有人將問題整理並總結出一些套路，這就是「設計模式」，我們以後再來談談吧！今天就先到這。&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.tenlong.com.tw/products/9789866761799"&gt;大話設計模式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tenlong.com.tw/products/9787111437871"&gt;設計模式之禪&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="軟體設計"></category></entry><entry><title>物件導向武功秘笈（2）：招式篇 — Python與Java的物件導向編程介紹</title><link href="YCNote/introduction-object-oriented-programming_2.html" rel="alternate"></link><published>2018-04-10T12:00:00+08:00</published><updated>2018-04-10T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2018-04-10:YCNote/introduction-object-oriented-programming_2.html</id><summary type="html">&lt;h3&gt;物件導向編程&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://www.ycc.idv.tw/YCNote/post/47"&gt;在上一章當中&lt;/a&gt;，我們藉由好的程式碼的特性：「正常執行」、「穩健」、「不重複撰寫」、「可讀性」、「可擴展」，自然而然引出物件導向的概念。在這一章當中YC會接續介紹完整的物件導向要如何實現，包括物件導向三大特性：封裝、繼承和多型。&lt;/p&gt;
&lt;p&gt;在本章我會採用兩種語言交叉作說明，一種是靜態型別的語言Java，另一種是動態型別的語言Python，這兩種語言都是可以實現物件導向的語言，而所謂型別的動態與靜態可以用一個簡單的方法來區分：型別檢查(Type Checking)發生在什麼時候？像Java這類的靜態型別語言，它的型別檢查是在編譯時期(Compile Time)完成的，而像是Python這類的動態型別語言，它的型別檢查則是在執行時期(Runtime)才去做，所以Python可以不事先宣告變數型別，這點使得Python在開發上方便許多。&lt;/p&gt;
&lt;p&gt;雖然Python和Java都是支援物件導向的語言，但在使用上有很大的差異，首先，因為Python的動態型別，所以有些物件導向的性質對它來說就不是那麼重要，另外，因為Python追求簡潔，簡化了相當多的東西，所以很多的使用方法不同於傳統的物件導向，需要認識到這些差異才可以讓你使用Python的物件導向不會顯得很彆扭。Java是一套對物件導向支援非常完整的語言，而Python是一套易於快速開發的語言，使用兩種語言說明物件導向是為了讓讀者更能了解物件導向的本質，而非語言本身。&lt;/p&gt;
&lt;p&gt;本篇採用『&lt;a href="https://www.tenlong.com.tw/products/9789866761799"&gt;大話設計模式&lt;/a&gt;』書中的物件導向篇範例 …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;物件導向編程&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://www.ycc.idv.tw/YCNote/post/47"&gt;在上一章當中&lt;/a&gt;，我們藉由好的程式碼的特性：「正常執行」、「穩健」、「不重複撰寫」、「可讀性」、「可擴展」，自然而然引出物件導向的概念。在這一章當中YC會接續介紹完整的物件導向要如何實現，包括物件導向三大特性：封裝、繼承和多型。&lt;/p&gt;
&lt;p&gt;在本章我會採用兩種語言交叉作說明，一種是靜態型別的語言Java，另一種是動態型別的語言Python，這兩種語言都是可以實現物件導向的語言，而所謂型別的動態與靜態可以用一個簡單的方法來區分：型別檢查(Type Checking)發生在什麼時候？像Java這類的靜態型別語言，它的型別檢查是在編譯時期(Compile Time)完成的，而像是Python這類的動態型別語言，它的型別檢查則是在執行時期(Runtime)才去做，所以Python可以不事先宣告變數型別，這點使得Python在開發上方便許多。&lt;/p&gt;
&lt;p&gt;雖然Python和Java都是支援物件導向的語言，但在使用上有很大的差異，首先，因為Python的動態型別，所以有些物件導向的性質對它來說就不是那麼重要，另外，因為Python追求簡潔，簡化了相當多的東西，所以很多的使用方法不同於傳統的物件導向，需要認識到這些差異才可以讓你使用Python的物件導向不會顯得很彆扭。Java是一套對物件導向支援非常完整的語言，而Python是一套易於快速開發的語言，使用兩種語言說明物件導向是為了讓讀者更能了解物件導向的本質，而非語言本身。&lt;/p&gt;
&lt;p&gt;本篇採用『&lt;a href="https://www.tenlong.com.tw/products/9789866761799"&gt;大話設計模式&lt;/a&gt;』書中的物件導向篇範例。&lt;/p&gt;
&lt;h3&gt;類別(Class)與物件(Object)&lt;/h3&gt;
&lt;p&gt;首先來看物件導向的基本組成，類別(Class)與物件(Object)。
&lt;em&gt; 類別：建立物件的藍圖，描述所建立的物件共同的屬性和方法。
&lt;/em&gt; 物件：一個自我包含的實體，物件包括屬性（Properties）和方法（Methods），屬性就是需要記憶的資訊，方法就是物件能夠提供的服務。&lt;/p&gt;
&lt;p&gt;舉個例子，我想要創造一隻有名字的貓，她有喵喵叫的能力，在Java中可以寫成&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;  &lt;span class="c1"&gt;//{1}&lt;/span&gt;
    &lt;span class="kd"&gt;private&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;//{2}&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//{3}&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;//{4}&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//{5}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. meow~&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;//{6}&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Test&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//{7}&lt;/span&gt;
        &lt;span class="n"&gt;Cat&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//{8}&lt;/span&gt;
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt; &lt;span class="c1"&gt;//{9}&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="c1"&gt;// output:&lt;/span&gt;
&lt;span class="c1"&gt;// My name is May. meow~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} 建構一個&lt;code&gt;Cat&lt;/code&gt;的類別，類別不是物件，類別只是物件的藍圖。&lt;/p&gt;
&lt;p&gt;{2} 建立一個私有變數&lt;code&gt;name&lt;/code&gt;，用來代表貓的名字，我們使用&lt;code&gt;private&lt;/code&gt;的修飾詞讓它是私有的，也就是說外部環境沒辦法去讀取到這個變數，只有物件內部才可以讀取的到&lt;/p&gt;
&lt;p&gt;{3} 提供建造方法(constructor)來初始化這一個物件，初始化需要&lt;code&gt;name&lt;/code&gt;的參數。&lt;/p&gt;
&lt;p&gt;{4} 在初始化的過程中，我們會將從外部讀取的&lt;code&gt;name&lt;/code&gt;存入私有變數&lt;code&gt;this.name&lt;/code&gt;裡，在Java裡頭，如果外部變數名稱與本地變數名稱相同，需要使用&lt;code&gt;this&lt;/code&gt;來特別區分。&lt;/p&gt;
&lt;p&gt;{5} 創造一個公開的類別方法&lt;code&gt;shout()&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;{6} 使用私有變數&lt;code&gt;name&lt;/code&gt;讓貓可以自我介紹，再發出喵喵叫的聲音。&lt;/p&gt;
&lt;p&gt;{7} Java只要遇到&lt;code&gt;main&lt;/code&gt;就會去執行，方法&lt;code&gt;main&lt;/code&gt;具有靜態方法的修飾詞&lt;code&gt;static&lt;/code&gt;，也就是說&lt;code&gt;Test&lt;/code&gt;不需要被實體化也能執行&lt;code&gt;main&lt;/code&gt;這個方法。&lt;/p&gt;
&lt;p&gt;{8} 使用&lt;code&gt;new&lt;/code&gt;來創造一個物件，在創造的過程會執行初始化，所以必須放入初始化需要的參數&lt;code&gt;name&lt;/code&gt;，所以上面的新的物件有了&lt;code&gt;"May"&lt;/code&gt;的名字。&lt;/p&gt;
&lt;p&gt;{9} 接下來使用&lt;code&gt;cat.shout()&lt;/code&gt;去執行喵喵叫的動作，這個方法會回傳字串，再利用&lt;code&gt;System.out.println&lt;/code&gt;的方法將字串顯示出來。注意！在物件導向的習慣中，會用&lt;code&gt;.&lt;/code&gt;來表示在那物件中的方法或屬性，所以&lt;code&gt;cat.shout()&lt;/code&gt;就是執行在物件&lt;code&gt;cat&lt;/code&gt;中的方法&lt;code&gt;shout()&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;再來看Python怎麼表示，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#{1}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{2}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="c1"&gt;#{3}&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{4}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;. meow~&amp;quot;&lt;/span&gt; &lt;span class="c1"&gt;#{5}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;#{6}&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="c1"&gt;#{7}&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#{8}&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# output:&lt;/span&gt;
&lt;span class="c1"&gt;# My name is May. meow~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} 建構&lt;code&gt;Cat&lt;/code&gt;的類別，這是Python3的表示方法，如果是使用Python2.7的話，要寫成&lt;code&gt;class Cat(object):&lt;/code&gt;才可以。&lt;/p&gt;
&lt;p&gt;{2} Python的初始化方法，Python在初始化之前會先自行執行&lt;code&gt;__new__&lt;/code&gt;的方法，這個過程會產生一個新的物件，也就是實體化，而這個新的物件會以第一個參數的方法被帶入&lt;code&gt;__init__&lt;/code&gt;的方法裡進行初始化，我們通常會命名這個變數為&lt;code&gt;self&lt;/code&gt;，這裡的&lt;code&gt;self&lt;/code&gt;已經是個物件而不是類別，那初始化的過程需要引入外部資訊&lt;code&gt;name&lt;/code&gt;的參數來進行命名，所以第二個參數就要設&lt;code&gt;name&lt;/code&gt;，記住喔！第一個參數是Python自動產生的，不是由外部帶入的，所以外部只要給&lt;code&gt;name&lt;/code&gt;一個參數就足夠了。&lt;/p&gt;
&lt;p&gt;{3} 創造一個私有本地變數&lt;code&gt;__name&lt;/code&gt;來將&lt;code&gt;name&lt;/code&gt;存入，在Python當中以雙底線&lt;code&gt;__&lt;/code&gt;開頭的變數會被視為是「私有的」，效果和Java的&lt;code&gt;private&lt;/code&gt;接近，不過Python並沒有這麼嚴格禁止外部去讀取私有變數，所以需要配合工程師的自我規範。&lt;/p&gt;
&lt;p&gt;{4} 類別方法&lt;code&gt;shout()&lt;/code&gt;，只要你不是靜態的類別方法，Python都會自動幫你帶入物件資訊當作第一個參數，通常命名為&lt;code&gt;self&lt;/code&gt;，那為什麼靜態方法沒有自動帶入，因為靜態方法不用實體化，所以根本不擁有物件的資訊。&lt;/p&gt;
&lt;p&gt;{5} 使用到本地的&lt;code&gt;self.__name&lt;/code&gt;變數&lt;/p&gt;
&lt;p&gt;{6} 創造一個物件，在創造的過程會執行初始化，所以必須放入初始化需要的參數&lt;code&gt;name&lt;/code&gt;，所以上面的新的物件有了&lt;code&gt;"May"&lt;/code&gt;的名字。&lt;/p&gt;
&lt;p&gt;{7} 接下來使用&lt;code&gt;cat.shout()&lt;/code&gt;去執行喵喵叫的動作，這個方法會回傳字串，再利用&lt;code&gt;print&lt;/code&gt;的方法將字串顯示出來。注意！在物件導向的習慣中，會用&lt;code&gt;.&lt;/code&gt;來表示在那物件中的方法或屬性，所以&lt;code&gt;cat.shout()&lt;/code&gt;就是執行在物件&lt;code&gt;cat&lt;/code&gt;中的方法&lt;code&gt;shout()&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;{8} 在Python程式執行時，它的&lt;code&gt;__name__&lt;/code&gt;會是&lt;code&gt;"__main__"&lt;/code&gt;，也就是說會去執行這個&lt;code&gt;if&lt;/code&gt;判斷式下面的程式。&lt;/p&gt;
&lt;h3&gt;方法多載（Method Overloading）&lt;/h3&gt;
&lt;p&gt;物件導向允許「使用不同的參數形式去實現同一個方法」，這就稱之為方法多載，這個方法涵蓋一般方法和構造初始化方法。&lt;/p&gt;
&lt;p&gt;來延伸剛剛的貓的例子，假設今天我們允許用戶不去設定貓咪的名字，而程式會預先給貓咪No-Name的預設值，所以我們需要另外一個初始化方法是不用貓咪名字的參數形式。&lt;/p&gt;
&lt;p&gt;Java的實現程式碼如下所示，如此一來只要碰到沒有參數的形式，程式會給予&lt;code&gt;"No-Name"&lt;/code&gt;的名字去當作貓咪的名字，並進行初始化。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;  
    &lt;span class="kd"&gt;private&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; 

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; 
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; 
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="c1"&gt;// method overloading&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// given &amp;quot;No-Name&amp;quot; as its name&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; 
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. meow~&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; 
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Test&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; 
        &lt;span class="n"&gt;Cat&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt; &lt;span class="c1"&gt;// no-argumant format&lt;/span&gt;
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="c1"&gt;// output:&lt;/span&gt;
&lt;span class="c1"&gt;// My name is No-Name. meow~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;但在Python當中，不允許這種「相同方法名稱，卻又不同參數形式」，Python採用其他的方式來產生同樣的方法多載效果，如以下所示，我們可以看到Python使用default方法來實現多載，只要我們不給予&lt;code&gt;name&lt;/code&gt;，它的default就是&lt;code&gt;"No-Name"&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;# name&amp;#39;s default is &amp;quot;No-Name&amp;quot;&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; 

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; 
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;. meow~&amp;quot;&lt;/span&gt; 

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# no-argument format&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; 

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# output:&lt;/span&gt;
&lt;span class="c1"&gt;# My name is No-Name. meow~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;物件導向三大特性—封裝(Encapsulation)&lt;/h3&gt;
&lt;p&gt;還記得「低耦合，高內聚」的原則嗎？為了符合這原則，每個物件都要盡可能的去包含需要用到的屬性和方法，並且使得外部不能以不合理的方法去影響物件，這就稱之為「封裝」。&lt;/p&gt;
&lt;p&gt;我們來看看上次的成果，我們就用這個例子來說明「封裝」。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Calculation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="c1"&gt;#{1}&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{2}&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;invalid positive integer: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{3}&lt;/span&gt;
        &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
            &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maxsize&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findLCM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;lcm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} 使用私有變數，讓外部不能任意的改變&lt;code&gt;self.__name&lt;/code&gt;變數，在這個例子當中，如果&lt;code&gt;self.__name&lt;/code&gt;被任意改變，它將會逃過&lt;code&gt;__checkPositiveInteger&lt;/code&gt;的檢查。
{2}&amp;amp;{3} 不需要由外部讀取的方法，就盡量讓它是私有的。&lt;/p&gt;
&lt;p&gt;再回到貓咪的這個例子，如果我們想要可以調控「叫聲次數」的話，可以這樣實現。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;private&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kd"&gt;private&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;//{1}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;getShoutNum&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;//{2}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setShoutNum&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//{3}&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="k"&gt;throw&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;lang&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;IllegalArgumentException&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; 
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;meow~ &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Test&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;Cat&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;setShoutNum&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//{4}&lt;/span&gt;
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="c1"&gt;// output:&lt;/span&gt;
&lt;span class="c1"&gt;// My name is May. meow~ meow~ meow~ meow~ meow~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} 設置私有變數&lt;code&gt;shout_num&lt;/code&gt;來決定叫聲次數。&lt;/p&gt;
&lt;p&gt;{2} 為了做到封裝，外部不能直接去讀取&lt;code&gt;shout_num&lt;/code&gt;，而是經由&lt;code&gt;getShoutNum&lt;/code&gt;的外部方法來得到叫聲次數，這個&lt;code&gt;getXXX&lt;/code&gt;的形式在物件導向裡頭被稱為Getter。&lt;/p&gt;
&lt;p&gt;{3} 為了做到封裝，外部不能直接去改變&lt;code&gt;shout_num&lt;/code&gt;，而是經由&lt;code&gt;setShoutNum&lt;/code&gt;的外部方法來以合理的方法改變叫聲次數，在這個方法中，我們會先檢查再設定，如果是直接的改變&lt;code&gt;shout_num&lt;/code&gt;將會少了這一份檢查，這個&lt;code&gt;setXXX&lt;/code&gt;的形式在物件導向裡頭被稱為Setter。&lt;/p&gt;
&lt;p&gt;{4} 使用Setter方法由外部來改變叫聲次數。&lt;/p&gt;
&lt;p&gt;在Python中，Getter和Setter被簡化了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;  &lt;span class="c1"&gt;#{1}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__shout_num&lt;/span&gt;

    &lt;span class="nd"&gt;@shoutNum.setter&lt;/span&gt;  &lt;span class="c1"&gt;#{2}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__shout_num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;meow~ &amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;  &lt;span class="c1"&gt;#{3}&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# output:&lt;/span&gt;
&lt;span class="c1"&gt;# My name is May. meow~ meow~ meow~ meow~ meow~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} Python使用Decorator&lt;code&gt;@property&lt;/code&gt;來創造Getter，一旦加上了&lt;code&gt;@property&lt;/code&gt;，當下的函數方法就會變成一種性質。&lt;/p&gt;
&lt;p&gt;{2} 再使用&lt;code&gt;shout_num.setter&lt;/code&gt;來替&lt;code&gt;shout_num&lt;/code&gt;這個特性加上Setter。&lt;/p&gt;
&lt;p&gt;{3} 然後我們就可以以像是修改一般變數的方式來修改&lt;code&gt;shout_num&lt;/code&gt;，但實際上&lt;code&gt;shout_num&lt;/code&gt;是有被封裝的，如此一來就可以更為簡潔，不用去寫&lt;code&gt;getXX&lt;/code&gt;和&lt;code&gt;setXXX&lt;/code&gt;等囉唆的寫法。&lt;/p&gt;
&lt;h3&gt;物件導向三大特性—繼承(Inheritance)&lt;/h3&gt;
&lt;p&gt;還記得「Don't Repeat Yourself」原則嗎？物件導向同樣提供了這個選項，「繼承」可以讓子類擁有父類的屬性和方法，避免不必要的重寫，但同時也會增加父類和子類之間的耦合，所以使用時要去評估它影響了多少耦合性。&lt;/p&gt;
&lt;p&gt;子類可以先繼承父類的屬性和方法，再去新增屬於子類自己的屬性和方法，甚至還可以去覆寫父類的方法，這稱之為方法覆寫(Method Overriding)，有了這些方法，子類可以在不重複撰寫父類方法的情況下，去增加自己的特色和自己的功能。&lt;/p&gt;
&lt;p&gt;依循著剛剛的例子，如果我們今天想要增加狗的類別，但是又不想重複撰寫相同的部分，所以我們可以選擇創造動物的類別，再讓貓和狗繼承自動物。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;//{1}&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;getShoutNum&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setShoutNum&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="k"&gt;throw&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;lang&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;IllegalArgumentException&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt; &lt;span class="kd"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="kd"&gt;super&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//{2}&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="kd"&gt;super&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;meow~ &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt; &lt;span class="kd"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;woof~ &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} &lt;code&gt;protected&lt;/code&gt;的效果和&lt;code&gt;private&lt;/code&gt;一樣，讓外部無法讀取到內部的私有化，但是&lt;code&gt;private&lt;/code&gt;無法被「繼承」，而&lt;code&gt;protected&lt;/code&gt;可以被繼承，所以如果希望可以被繼承的私有變數或方法，就使用&lt;code&gt;protected&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;{2} &lt;code&gt;super&lt;/code&gt;指的是父類，這裡我們使用父類來做初始化，事實上這邊可以不用再初始化一次，子類本身就會繼承父類的初始化方法，所以可以像&lt;code&gt;Dog&lt;/code&gt;一樣省略不寫。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="c1"&gt;#{1}&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;
    &lt;span class="nd"&gt;@shout_num.setter&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No_Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;#{2}&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;meow~ &amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;woof~ &amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} Python的&lt;code&gt;protected&lt;/code&gt;使用單底線&lt;code&gt;_&lt;/code&gt;開頭表示。&lt;/p&gt;
&lt;p&gt;{2} &lt;code&gt;super&lt;/code&gt;指的是父類，這裡我們使用父類來做初始化，事實上這邊可以不用再初始化一次，子類本身就會繼承父類的初始化方法，所以可以像&lt;code&gt;Dog&lt;/code&gt;一樣省略不寫。&lt;/p&gt;
&lt;h3&gt;抽象化：抽象類別(Abstract Class)、抽象方法(Abstract Method)和接口(Interface)&lt;/h3&gt;
&lt;p&gt;事實上，剛剛使用&lt;code&gt;Animal&lt;/code&gt;的方法並不是很正確，我們將&lt;code&gt;Animal&lt;/code&gt;當作一個類別處理，所以&lt;code&gt;Animal&lt;/code&gt;其實是可以被實例化的，但是&lt;code&gt;Animal&lt;/code&gt;根本沒有什麼有用的方法，它必須被繼承後再添加方法才有用處，所以我們其實可以把&lt;code&gt;Animal&lt;/code&gt;抽象化，將&lt;code&gt;Animal&lt;/code&gt;視為抽象類別，其中擁有一些方法需要在子類實現的，稱為抽象方法，我們直接看怎麼做。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;

&lt;span class="kd"&gt;abstract&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//{1}&lt;/span&gt;
    &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;getShoutNum&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setShoutNum&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="k"&gt;throw&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;lang&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;IllegalArgumentException&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//{2}&lt;/span&gt;
        &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;getShoutSound&lt;/span&gt;&lt;span class="o"&gt;()+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;//{3}&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="kd"&gt;abstract&lt;/span&gt; &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt; &lt;span class="c1"&gt;//{4}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt; &lt;span class="kd"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//{5}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;meow~&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; 
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt; &lt;span class="kd"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;woof~&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} 使用&lt;code&gt;abstract class&lt;/code&gt;修飾詞來創建抽象類別，只有在抽象類別中才可以擁有抽象方法，抽象類別不能直接被實例化。&lt;/p&gt;
&lt;p&gt;{2} 抽象類別也可以有一般的具體方法。&lt;/p&gt;
&lt;p&gt;{3} 這裡使用的&lt;code&gt;getShoutSound()&lt;/code&gt;方法要等在子類才會被實現。&lt;/p&gt;
&lt;p&gt;{4} 使用&lt;code&gt;abstract&lt;/code&gt;設置抽象方法，繼承自抽象類別的子類必須要完全實現所有的抽象方法。&lt;/p&gt;
&lt;p&gt;{5} 實現抽象方法。&lt;/p&gt;
&lt;p&gt;在Python中沒有原生的抽象類別和方法，必須&lt;code&gt;import abc&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;abc&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ABC&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{1}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;
    &lt;span class="nd"&gt;@shout_num.setter&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;  
    &lt;span class="nd"&gt;@abc.abstractmethod&lt;/span&gt;  &lt;span class="c1"&gt;#{2}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{3}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;meow~&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;woof~&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} 繼承&lt;code&gt;abc.ABC&lt;/code&gt;來建立抽象類別。&lt;/p&gt;
&lt;p&gt;{2} 使用&lt;code&gt;@abc.abstractmethod&lt;/code&gt;來建立抽象方法。&lt;/p&gt;
&lt;p&gt;{3} 實現抽象方法。&lt;/p&gt;
&lt;p&gt;還有一種類型抽象化的更為徹底，稱之為「接口」，「接口」上的所有方法都是抽象未實現的，「接口」不能擁有任何具體的方法。雖然「接口」很像是完全抽象化的「抽象類別」，也確實可以利用「抽象類別」來創造「接口」，但是兩者的意義是不同的，「抽象類別」是從子類中發現共通的東西，而泛化出現的，但是「接口」可以根本不預先知道子類是什麼，而僅僅事先定義行為本身，換句話說，「抽象類別」是類別的抽象化，而「接口」則是行為的抽象化。&lt;/p&gt;
&lt;p&gt;例如，我想要讓某些動物擁有「飛」的能力，這是一個行為，而不會事先知道它會套用到哪一個類別上面。&lt;/p&gt;
&lt;p&gt;在Java之中只允許單一繼承，但是卻可以有多個「接口」；而Python沒有現成的「接口」可以使用，我們必須使用「抽象方法」來創造「接口」，所以開發者要謹記「接口」的限制：不能有任何的具體方法，因為Python允許多重繼承，所以就可以直接將模擬「接口」的抽象類別直接疊加上去。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;

&lt;span class="kd"&gt;interface&lt;/span&gt; &lt;span class="nc"&gt;IFly&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//{1}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;flyTo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//{2}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;FlyingCat&lt;/span&gt; &lt;span class="kd"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt; &lt;span class="kd"&gt;implements&lt;/span&gt; &lt;span class="n"&gt;IFly&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//{3}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;flyTo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;//{4}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;()+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; I&amp;#39;m going to fly to &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;.&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Test&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;FlyingCat&lt;/span&gt; &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;FlyingCat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;flyTo&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Taiwan&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="c1"&gt;// output: &lt;/span&gt;
&lt;span class="c1"&gt;// My name is May. meow~ meow~ meow~ I&amp;#39;m going to fly to Taiwan.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} 使用&lt;code&gt;interface&lt;/code&gt;創建「接口」，我們習慣會使用開頭大寫I來表示Interface(接口)。&lt;/p&gt;
&lt;p&gt;{2} 「接口」定義未實現的抽象方法&lt;code&gt;flyTo&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;{3} &lt;code&gt;FlyingCat&lt;/code&gt;繼承自&lt;code&gt;Cat&lt;/code&gt;並且裝上&lt;code&gt;IFly&lt;/code&gt;的「接口」。&lt;/p&gt;
&lt;p&gt;{4} 必須實現「接口」上所有的抽象方法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;abc&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;IFly&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ABC&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{1}&lt;/span&gt;
    &lt;span class="nd"&gt;@abc.abstractmethod&lt;/span&gt; &lt;span class="c1"&gt;#{2}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;flyTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;FlyingCat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;IFly&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{3}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;flyTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; I&amp;#39;m going to fly to &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FlyingCat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fly&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Taiwan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# output: &lt;/span&gt;
&lt;span class="c1"&gt;# My name is May. meow~ meow~ meow~ I&amp;#39;m going to fly to Taiwan.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} 使用抽象類別來創造「接口」。&lt;/p&gt;
&lt;p&gt;{2} 要注意！「接口」裡頭不能有具體方法。&lt;/p&gt;
&lt;p&gt;{3} 直接使用多重繼承，將&lt;code&gt;IFly&lt;/code&gt;安裝上去。&lt;/p&gt;
&lt;h3&gt;物件導向三大特性—多型(Polymorphism)&lt;/h3&gt;
&lt;p&gt;最後，來講講物件導向的最後一個特性，那就是「多型」。「多型」的涵義是指「子類可以以父類的身分出現」，而因為是以父類的角色出現，所以只能執行父類擁有的方法，也就是只能執行這些子類共同泛化分享的方法，當然不同的子類實現後的效果會不一樣，不然使用「多型」的意義就不大了，至於子類自己的特殊方法則不可以使用「多型」去執行。&lt;/p&gt;
&lt;p&gt;直接來看範例，假設今天我要邀請三隻貓貓狗狗來參加叫聲比賽，分別請他們叫個幾聲來聽聽，此時就需要使用到「多型」的方法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;

&lt;span class="kd"&gt;abstract&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; 
    &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;getShoutNum&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setShoutNum&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="k"&gt;throw&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;lang&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;IllegalArgumentException&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; 
        &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++){&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;getShoutSound&lt;/span&gt;&lt;span class="o"&gt;()+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; 
        &lt;span class="o"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="kd"&gt;abstract&lt;/span&gt; &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt; 
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt; &lt;span class="kd"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; 
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;meow~&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt; 
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt; &lt;span class="kd"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;protected&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;woof~&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ShoutGame&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kd"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;arrayAnimal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;];&lt;/span&gt; &lt;span class="c1"&gt;//{1}&lt;/span&gt;
        &lt;span class="c1"&gt;// polymorphism&lt;/span&gt;
        &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;//{2}&lt;/span&gt;
        &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Dog&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Linda&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Joy&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt; &lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;shout&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt; &lt;span class="c1"&gt;//{3}&lt;/span&gt;
        &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="c1"&gt;// output: &lt;/span&gt;
&lt;span class="c1"&gt;// My name is May. meow~ meow~ meow~&lt;/span&gt;
&lt;span class="c1"&gt;// My name is Linda. woof~ woof~ woof~&lt;/span&gt;
&lt;span class="c1"&gt;// My name is Joy. meow~ meow~ meow~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} 創建&lt;code&gt;Animal&lt;/code&gt;的Array，Animal是抽象類別不能實體化，這裡預定要放的是它的繼承實現類別。&lt;/p&gt;
&lt;p&gt;{2} 將&lt;code&gt;Cat&lt;/code&gt;和&lt;code&gt;Dog&lt;/code&gt;任意放到&lt;code&gt;Animal&lt;/code&gt;的Array是可以的，此時就套用「多型」，不管是&lt;code&gt;Cat&lt;/code&gt;和&lt;code&gt;Dog&lt;/code&gt;都是以&lt;code&gt;Animal&lt;/code&gt;的形式出現，只能執行&lt;code&gt;Animal&lt;/code&gt;有的方法。&lt;/p&gt;
&lt;p&gt;{3} &lt;code&gt;shout()&lt;/code&gt;是父類&lt;code&gt;Animal&lt;/code&gt;有的方法，可以被執行。&lt;/p&gt;
&lt;p&gt;而在Python當中，「多型」就沒這麼重要了，因為Python具有「鴨子型別」（Duck Typing），什麼是「鴨子型別」呢？有一句話道出它的意義：「當看到一隻鳥走起來像鴨子、游泳起來像鴨子、叫起來也像鴨子，那麼這隻鳥就可以被稱為鴨子」，因為Python是動態型別的語言，變數型態不需要事先宣告，所以一個變數具有彈性可以放入任意型別，直到出現不合適的使用方法，才會報錯，所以在Python中變數可以任意放入不同的類別的物件，只要確保這些類別都具有這些變數所需要用到的方法，就可以了，這不正是接近「多型」的概念。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;### Python3.4&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;abc&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;abc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ABC&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; 
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;
    &lt;span class="nd"&gt;@shout_num.setter&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;  
    &lt;span class="nd"&gt;@abc.abstractmethod&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;meow~&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;woof~&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# Shout Game&lt;/span&gt;
    &lt;span class="n"&gt;arrayAnimal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="c1"&gt;#{1}&lt;/span&gt;
    &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; 
    &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Linda&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Joy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;animal&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;TypeError&lt;/span&gt; &lt;span class="c1"&gt;#not necessary #{2}&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# output: &lt;/span&gt;
&lt;span class="c1"&gt;# My name is May. meow~ meow~ meow~&lt;/span&gt;
&lt;span class="c1"&gt;# My name is Linda. woof~ woof~ woof~&lt;/span&gt;
&lt;span class="c1"&gt;# My name is Joy. meow~ meow~ meow~&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;{1} 要存入多型的List不需要特別處理。&lt;/p&gt;
&lt;p&gt;{2} 可以檢查是不是繼承自&lt;code&gt;Animal&lt;/code&gt;，以確保多型的嚴格定義，但這個過程是非必要的。&lt;/p&gt;
&lt;h3&gt;總結：物件導向使用方法&lt;/h3&gt;
&lt;p&gt;好！我們花了好大的力氣，終於了解如何在Java和Python中使用物件導向。從一開始的「類別」和「物件」講起，再來談到物件導向的三大特性：「封裝」、「繼承」和「多型」，還談到方法可以「多載」也可以「覆寫」，以及一些抽象化的東西，包括「抽象類別」、「抽象方法」和「接口」。&lt;/p&gt;
&lt;p&gt;但是等等！你知道該怎麼運用這些技巧嗎？沒錯，僅僅是了解這些招式不足以讓你寫出卓越的程式碼，你還需要了解如何使用，就像是外功招式還得配合內功才可以是一套完整的功夫，否則只是半吊子而已，我們將在下一章節中來帶大家了解如何去使用這些招式。&lt;/p&gt;</content><category term="軟體設計"></category></entry><entry><title>物件導向武功秘笈（1）：認知篇 — 什麼是好的程式？</title><link href="YCNote/introduction-object-oriented-programming_1.html" rel="alternate"></link><published>2018-04-05T12:00:00+08:00</published><updated>2018-04-05T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2018-04-05:YCNote/introduction-object-oriented-programming_1.html</id><summary type="html">&lt;h3&gt;物件導向為何重要？&lt;/h3&gt;
&lt;p&gt;我相信很多朋友一定像YC我一樣，想要學某個程式語言，就去買那個程式語言的簡介書籍，然後一章一章的唸下去，這種書通常會先教變數怎麼設定？然後再教if、while、for、function等程式邏輯。&lt;/p&gt;
&lt;p&gt;那如果你學的是「物件導向」的語言，譬如：Java、C++、Python，接下來的章節就會開始介紹「類別」、「物件」等等難懂的東西，然後就會陷入一種霧煞煞的狀態，然後心中就會出現一種聲音：為什麼寫個程式你跟我扯什麼「物件」？我原本用前面所學的方法就可以完成所有事情啦！為何要把事情弄的這麼複雜？這東西到底有什麼好處啊？&lt;/p&gt;
&lt;p&gt;YC一開始也是充滿著疑惑，然後一知半解的就把這些定義記在心中，然後天真的認為「物件導向」只是讓程式比較整齊的方法罷了！直到後來學了資料結構與演算法，然後又學了一點設計模式，然後又有過幾個大型軟體開發的經驗，一路走過才漸漸的了解「物件導向」是怎麼一回事？&lt;/p&gt;
&lt;p&gt;所以我打算把這些收穫用三篇文章來說明，好讓讀者們可以少走一點冤枉路，在第一篇中，也就是本篇，我會帶大家認識好的程式是長什麼樣子的，它擁有什麼樣的特點，有了正確的認知，除了可以讓我們避免寫出糟糕的程式之外，我們也才能漸漸的認識到「物件導向」為何重要 …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;物件導向為何重要？&lt;/h3&gt;
&lt;p&gt;我相信很多朋友一定像YC我一樣，想要學某個程式語言，就去買那個程式語言的簡介書籍，然後一章一章的唸下去，這種書通常會先教變數怎麼設定？然後再教if、while、for、function等程式邏輯。&lt;/p&gt;
&lt;p&gt;那如果你學的是「物件導向」的語言，譬如：Java、C++、Python，接下來的章節就會開始介紹「類別」、「物件」等等難懂的東西，然後就會陷入一種霧煞煞的狀態，然後心中就會出現一種聲音：為什麼寫個程式你跟我扯什麼「物件」？我原本用前面所學的方法就可以完成所有事情啦！為何要把事情弄的這麼複雜？這東西到底有什麼好處啊？&lt;/p&gt;
&lt;p&gt;YC一開始也是充滿著疑惑，然後一知半解的就把這些定義記在心中，然後天真的認為「物件導向」只是讓程式比較整齊的方法罷了！直到後來學了資料結構與演算法，然後又學了一點設計模式，然後又有過幾個大型軟體開發的經驗，一路走過才漸漸的了解「物件導向」是怎麼一回事？&lt;/p&gt;
&lt;p&gt;所以我打算把這些收穫用三篇文章來說明，好讓讀者們可以少走一點冤枉路，在第一篇中，也就是本篇，我會帶大家認識好的程式是長什麼樣子的，它擁有什麼樣的特點，有了正確的認知，除了可以讓我們避免寫出糟糕的程式之外，我們也才能漸漸的認識到「物件導向」為何重要。&lt;/p&gt;
&lt;h3&gt;程式的好壞？&lt;/h3&gt;
&lt;p&gt;一開始，我們必須要對程式培養出鑑賞能力，我曾經聽過電視上有一位歌唱老師說過：「好的歌手必須先練他的聽力」，我覺得相同的，一個好的Programmer要先培養出對於程式的鑑賞能力。&lt;/p&gt;
&lt;p&gt;首先，一個好的程式當然要「能正常執行」，要能滿足客戶的需求，這是基本款，所以一般而言我們會使用很多的測試去看看程式是否可以正常運作，我們會找一些一般的條件來測試，我們也會找一些合法但是位於極端條件的例子，也就是邊界條件（Edge Case）來測試，或者找一些不合法的例子試試程式是否可以排除錯誤條件。&lt;/p&gt;
&lt;p&gt;測試可以即早的發現Bug，即早的治療，如果真的發現有Bug的話，接下來就是去找出Bug的源頭，這可就相當的困難，這裡想像一下喔！如果你的程式總共有1000行，而當你測試時發現有Bug，那想從這麼多行當中找出Bug的來源是相當困難的，所以好的方法是這樣的，先將一個大任務分解成為幾個小任務，然後完成這幾個小任務後，逐一的進行測試，稱之為「單元測試」，最後在將這些測試完成的小任務組合成為大任務，然後再做最後的總測試，這麼一來就可以避免在大範圍中找尋Bug，又可以做到對程式從裡到外的完整測試以達到程式「能正常執行」的目的。&lt;/p&gt;
&lt;p&gt;這裡提出一個問題讓大家思考，究竟要使用什麼方法去解析問題？讓我們可以有條理的拆解出小「單元」，來組合出最後的目標，有沒有一個系統化的思考方法？&lt;/p&gt;
&lt;p&gt;第二點，一個好的程式必須是「穩健的」(Robust)，程式原本能用的功能，不會因為更新、不會因為添加新功能，就出現錯誤！要做到這一點，除了剛剛說的「單元」拆分以外，還要讓「單元」和「單元」之間不會有太多的彼此影響，這麼一來在原先的功能所調用的「單元」不被動到的前提下，我還可以新增新的功能，才能做到「穩健的」特質。&lt;/p&gt;
&lt;p&gt;第三點，一個好的程式必須具備「不重複撰寫」的特性，有一句經典的法則叫做「Don't Repeat Yourself」，不要去重複寫已經寫過的程式碼，如果是重複需要用到的「單元」我們就把它獨立出來，讓其他程式去調用它，對於工程師來說，「不重複撰寫」意味著可以少寫一點程式碼，增加開發的速度，更重要的是，調用公享的程式碼可以讓程式更有邏輯，更具一致性，能夠減少出錯的可能性。&lt;/p&gt;
&lt;p&gt;第四點，好的程式要具有「可讀性」，軟體開發常常是長時間、多人合作、龐大的程式碼，如果程式碼沒有具備清晰的邏輯、沒有在該註解的部分寫清楚、沒有一個統一的規範，這樣的開發終就會陷入泥坑，永遠解不完的Bug會不斷的出現，解了一個又產生一個，永無止盡的輪迴，而且最慘的是完全不清楚真正的源頭在哪裡，這可是軟體工程師的夢魘啊！&lt;/p&gt;
&lt;p&gt;第五點，一個好的程式要具備「可擴展」，工程師最討厭的一句話應該就是客戶說：「我突然想到我還需要XXX功能，這只是在這邊再多一點而已，應該不難吧！」呵呵～通常「這多一點」就要大大的修改整個程式碼，弄不好還可能把原本的功能給搞壞，所以工程師應該在設計的一開始就考慮到會有什麼潛在需要更改的部分，而先採取因應措施，好讓程式易於擴展，好讓自己不會因此而加班！&lt;/p&gt;
&lt;h3&gt;低耦合、高內聚&lt;/h3&gt;
&lt;p&gt;再重複一次，一個好的程式要具備「正常執行」、「穩健」、「不重複撰寫」、「可讀性」、「可擴展」的特性，請將這些原則記在心裡，隨時的檢視自己的程式是不是有違反這些規則。&lt;/p&gt;
&lt;p&gt;而剛剛我們有了一個大致的想法：將任務分成幾個小的「單元」是一個很好的策略，而為了讓程式「穩健」，這些「單元」之間不能有太多的相依性；但是站在另外一個角度看，為了讓程式「不重複撰寫」，我們需要讓一個「單元」使用另外一個「單元」，好讓工程師可以做到「Don't Repeat Yourself」，如此一來則是增加了「單元」間的相依性，這兩者是一個Trade-off。&lt;/p&gt;
&lt;p&gt;有關「單元」的相依性有兩個重要術語—耦合性(Coupling)和聚合性(Cohesion)，耦合性指的是「單元」和「單元」之間資訊或參數依賴的程度，所以我們要追求「低耦合」。聚合性指的是「單元」內使用到自身資訊或參數的程度，所以我們要追求「高內聚」，通常「低耦合」都會伴隨著「高內聚」。&lt;/p&gt;
&lt;h3&gt;程式碼精練之旅&lt;/h3&gt;
&lt;p&gt;來看個例子，假設今天我想要實現一個求最大公因數的計算機，使用&lt;strong&gt;Python&lt;/strong&gt;隨便寫一段程式碼可能是這樣的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;str_numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer A: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;str_numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer B: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;    

    &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Greatest Common Factor: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;好！那接下來用剛剛的規則來檢視看看這個程式，第一點，有沒有「可正常執行」？上述的例子，沒有考慮到一些Edge Case，當輸入的值不是正整數，必須要報錯，所以我們將程式修改一下。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;str_numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer A: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;str_numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer B: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;invalid positive integer: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numA&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;invalid positive integer: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numB&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;    

    &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Greatest Common Factor: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;再來檢查一下是不是具有「不重複撰寫」的特性？也就是Don't Repeat Yourself，顯然是沒有遵守，&lt;code&gt;numA&lt;/code&gt;和&lt;code&gt;numB&lt;/code&gt;處理方法幾乎一模一樣，這會造成程式碼很冗長，來稍做修改。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;invalid positive integer: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;str_numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer A: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;str_numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer B: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Greatest Common Factor: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;接下來來檢查一下「穩健度」和「可擴展」，也就是程式是否符合：低耦合、高內聚，其實上面的程式碼有一個大問題，客戶端邏輯和業務邏輯混為一談，客戶端邏輯就是實現功能的部分，而業務邏輯就是實作的細節，所以上面的程式碼把所有的實作的細節全部攤在客戶端，這是相當不好的，這會造成不易更改，因此我們將程式作單元的拆分，讓業務邏輯和客戶端邏輯相分離，讓不直接實現客戶端的程式碼可以隱藏起來，減少客戶端和業務邏輯的耦合。然後順道加入求取最小公倍數的功能。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;invalid positive integer: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
        &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maxsize&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findLCM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;lcm&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;str_numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer A: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;str_numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer B: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;numB&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;    
    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;findLCM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Greatest Common Factor: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Lowest Common Multiple: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lcm&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;如此一來程式碼就看起來乾淨很多，function和function之間的耦合性被降低了，而function本身的內聚性提高了，程式碼達到了低耦合、高內聚，但是似乎還可以更好。&lt;/p&gt;
&lt;h3&gt;形塑出物件導向&lt;/h3&gt;
&lt;p&gt;剛剛我們已經完成了一個看起來很乾淨的程式碼了，但是其實還可以更好，在這裡我們就必須形塑出物件導向，才有辦法再前進一步。&lt;/p&gt;
&lt;p&gt;剛剛的程式碼當中的&lt;code&gt;checkPositiveInteger(num)&lt;/code&gt;, &lt;code&gt;primeFactorize(num)&lt;/code&gt;, &lt;code&gt;findGCF(nums)&lt;/code&gt;, &lt;code&gt;findLCM(nums)&lt;/code&gt;函數其實都是實現同一個目標—因式計算，但卻是被寫成一個一個獨立的函數，這裡的內聚性還可以再更好。&lt;/p&gt;
&lt;p&gt;而且&lt;code&gt;checkPositiveInteger(num)&lt;/code&gt;, &lt;code&gt;primeFactorize(num)&lt;/code&gt;並不是用來實現主要的目的，而只是實現目的過程中，為了避免重複而產生的，這樣寫很容易讓人不清楚什麼是重要的函數，而什麼只是中繼的函數，這裡的「可讀性」應該還可以再提升。&lt;/p&gt;
&lt;p&gt;輸入的數字&lt;code&gt;nums&lt;/code&gt;對於&lt;code&gt;findGCF&lt;/code&gt;和&lt;code&gt;findLCM&lt;/code&gt;，應該是一模一樣的，有沒有一個方法可以讓&lt;code&gt;nums&lt;/code&gt;避免重複呢？以增強「不要重複撰寫」的原則。&lt;/p&gt;
&lt;p&gt;要擁有以上的功能，我們需要一個「物件」，這個「物件」能夠保有屬於它的變數，才可以儲存&lt;code&gt;nums&lt;/code&gt;等參數，變數可以是對外公布的，也可以是私有的。另外,這個「對象」擁有屬於它的函數方法，而方法一樣可以是對外公布的，也可以是私有的，所以我們可以公布&lt;code&gt;findGCF(nums)&lt;/code&gt;, &lt;code&gt;findLCM(nums)&lt;/code&gt;，而私有化
&lt;code&gt;checkPositiveInteger(num)&lt;/code&gt;, &lt;code&gt;primeFactorize(num)&lt;/code&gt;。我們使用「藍圖」去建構「物件」的模版，再由「藍圖」配合不同的輸入參數去生成一個一個獨立的「物件」，以因應不同的狀況。&lt;/p&gt;
&lt;p&gt;這就是物件導向！&lt;/p&gt;
&lt;p&gt;接下來，我將上面程式碼引入物件導向改寫如下。（看不懂～沒關係！未來會詳述）&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Calculation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;invalid positive integer: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
            &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maxsize&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findLCM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;lcm&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;str_numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer A: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;str_numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer B: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;numB&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;calc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Calculation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findLCM&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Greatest Common Factor: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Lowest Common Multiple: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lcm&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;總結：程式碼鑑賞能力&lt;/h3&gt;
&lt;p&gt;本章YC帶大家建立一種品味，像是藝術評論家一樣，我們學會了如何鑑賞好的程式碼，我們提到了好的程式碼須要符合「正常執行」、「穩健」、「不重複撰寫」、「可讀性」、「可擴展」的特性，並且提到我們要追求低耦合、高內聚，但是「不重複撰寫」的這個原則會和低耦合相互違和，所以工程師要小心拿捏！有了鑑賞能力，我們開始精練我們的程式，而自然而然就可以引出物件導向的概念。當然，物件導向不只如此啦！我們下章就會看到物件導向還有什麼花拳繡腿。&lt;/p&gt;</content><category term="軟體設計"></category></entry><entry><title>自私的基因：基因觀點下的天擇</title><link href="YCNote/the-selfish-gene.html" rel="alternate"></link><published>2018-02-03T12:00:00+08:00</published><updated>2018-02-03T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2018-02-03:YCNote/the-selfish-gene.html</id><summary type="html">&lt;h3&gt;物競天擇？&lt;/h3&gt;
&lt;p&gt;『自私的基因』是當代相當重要的一本書，它在生物學上的地位等同於在物理學上的『時間簡史』。作者理察·道金斯（Richard Dawkins）是一個跨領域的通才，曾獲得動物學學士學位、文學碩士、哲學博士以及科學博士，我認為也是因為這樣的跨領域學習，才能讓他完成這樣一本創作俱佳的好書。&lt;/p&gt;
&lt;p&gt;達爾文的天擇說提出後，生物學開始有了一個思考的脈絡來描述生物的演化，天擇說告訴我們「物競天擇，適者生存、不適者淘汰」，地球上目前存在的物種是多年來環境盲目篩選後的結果，但我們會發現如果以物種為單位來說明天擇會有一些說不清的地方。&lt;/p&gt;
&lt;p&gt;舉個例子，物種的利他行為，例如許多小型鳥類遇到老鷹時會發出警訊給同伴，通知同伴趕快逃跑，這毫無疑問的是一種利他行為，發出警訊將會使自己暴露在危險之中，而換取到的是其他同伴的安全，針對這樣的利他行為，「群體選擇」理論會說生物會因演化而做出對種族有利的事，作者認為這是一種謬誤，生物間應該無法輕易的區分種族，就算可以，那我們又如何去劃分層次，要從界、門、綱、目、科、屬、種哪個層面下手去有意識的幫助自己的種族呢？以物種為單位的天擇說僅能說明生物本身的自利行為，但是解釋不清楚利他行為如何形成。&lt;/p&gt;
&lt;p&gt;作者道金斯受到魏斯曼（A. Weismann）的學說 …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;物競天擇？&lt;/h3&gt;
&lt;p&gt;『自私的基因』是當代相當重要的一本書，它在生物學上的地位等同於在物理學上的『時間簡史』。作者理察·道金斯（Richard Dawkins）是一個跨領域的通才，曾獲得動物學學士學位、文學碩士、哲學博士以及科學博士，我認為也是因為這樣的跨領域學習，才能讓他完成這樣一本創作俱佳的好書。&lt;/p&gt;
&lt;p&gt;達爾文的天擇說提出後，生物學開始有了一個思考的脈絡來描述生物的演化，天擇說告訴我們「物競天擇，適者生存、不適者淘汰」，地球上目前存在的物種是多年來環境盲目篩選後的結果，但我們會發現如果以物種為單位來說明天擇會有一些說不清的地方。&lt;/p&gt;
&lt;p&gt;舉個例子，物種的利他行為，例如許多小型鳥類遇到老鷹時會發出警訊給同伴，通知同伴趕快逃跑，這毫無疑問的是一種利他行為，發出警訊將會使自己暴露在危險之中，而換取到的是其他同伴的安全，針對這樣的利他行為，「群體選擇」理論會說生物會因演化而做出對種族有利的事，作者認為這是一種謬誤，生物間應該無法輕易的區分種族，就算可以，那我們又如何去劃分層次，要從界、門、綱、目、科、屬、種哪個層面下手去有意識的幫助自己的種族呢？以物種為單位的天擇說僅能說明生物本身的自利行為，但是解釋不清楚利他行為如何形成。&lt;/p&gt;
&lt;p&gt;作者道金斯受到魏斯曼（A. Weismann）的學說「生殖細胞的延續性」所啟蒙，提出了天擇利己主義的基本單位，既非種，也非群體，亦非個體，而是「基因」（Gene）這個遺傳的基本單位。而也正是基因的自私行為才造就個體的利他行為，基因傾向於保全與自己基因相似的個體，以獲得基因本身的延續，基因是自私的，所以本書的書名才會叫做『自私的基因』。&lt;/p&gt;
&lt;h3&gt;生命源自於複製&lt;/h3&gt;
&lt;p&gt;為了說清楚基因觀點下的演化，我們回到四十億年前的地球，那時的地球海洋已經形成，稱之為太古混湯（primeval soup），海洋中存在著大量的基本化合物，譬如：水、二氧化碳、甲烷和氮。這些基本化合物會因為化學作用，而可能有機會合併形成更大、更複雜的化合物，在那個時候大的有機化合物因為沒有存在細菌可以分解它，所以是可以被完整保存下來的，直到有一天出現了一種可以自我複製的化合物，它就開始大量形成相同的化學結構，而且方便的是太古混湯擁有大量的垂手可得的建材，所以可以大量的複製出更多的複製者，這樣的複製就開始擴張開來。&lt;/p&gt;
&lt;p&gt;當然像這類的複製者不只一種，太古混湯中夾雜許多可以自我複製的化合物種類，而且就連複製者在複製的過程當中都可能會出錯而產生出其他形式的複製者，當複製者越來越多，造成太古混湯開始缺乏素材而無法供養所有的複製者，競爭就開始了，天擇就開始了！而能存活的複製者必須符合長命、生產力大、複製正確度高等特性，否則將無法在歷史的長河中留下。&lt;/p&gt;
&lt;p&gt;競爭只會隨著時間日益增加而不會減少，為了存活（不是有意識的，而是天擇的結果）複製者開始改良自身，增加自身的穩定性，瓦解對手的穩定度，譬如：形成蛋白質保護自己、形成可以分解對手分子鍵結的能力以得到更多的建材，為了生存，複製者走向發展出更精細的、更複雜的求生機器。&lt;/p&gt;
&lt;p&gt;四十億年過去了，這些複製者在今日被稱之為基因，多個基因組成DNA，DNA可以轉譯成蛋白質分子來打造出求生機器，求生機器是承載基因的「工具」，承載數千、數萬個基因，每具肉體都是基因精巧的共同傑作，想要區分這個基因和另一個基因的貢獻，幾乎是做不到的。&lt;/p&gt;
&lt;p&gt;書中作者用划船來作比喻，划船人員就像是基因，而這艘船就像是個體，當個體無法繁殖，這組團隊就像是輸了比賽就會被淘汰，而獲勝的團隊將可能在未來與其他的基因合作繼續的比賽下去，這艘船的每個船員都有適合他的位置，可能有船首、舵手或尾槳，把人放在適合的位置才可以發揮最好的實力，那麼最後存活下來的物種就是擁有好基因彼此之間合作無間的成果。&lt;/p&gt;
&lt;h3&gt;基因的代理人—神經網絡&lt;/h3&gt;
&lt;p&gt;動物和植物不同之處，動物擁有快速的行為，譬如遇到敵人就要拔腿就跑、看到獵物就要採取攻擊，這一些快速的行為不可能由基因直接控制，因為基因的影響速度太慢了，可能需要幾個月的時間，所以基因依造它們的需求打造出了神經網絡，然後神經網絡就接管了個體每時每刻的行為，才能因應各種環境的變化去做出相應的行為，使得個體得以生存，幫助基因可以繼續繁衍下去，如果用電腦來比喻的話，圍棋程式AlphaGo在比賽之前是由人類打造而成的，但是在真正比賽時，它會依照當下狀況去自行下決定，基因也是一樣的，基因先打造好神經網絡，接下來個體的行為就由這些神經網絡來代理了。&lt;/p&gt;
&lt;h3&gt;再談利他行為&lt;/h3&gt;
&lt;p&gt;好的基因產生的神經網絡所造就的動物行為，是要可以幫助基因複製自身的，基因是自私的，但不是有意識的，而是天擇造就了自私的基因得以存活下來，也正是這一層關係，物種的利他行為仍然是基因自私的結果。&lt;/p&gt;
&lt;p&gt;像是一開始舉例的，小鳥會發出叫聲來提醒其他同伴，這樣的行為對個體是不利的，但是犧牲的個體可以保全其他相似的基因存活，這個基因就會延續下去。在人類社會，父母親給予子女的愛是無私的，但是這還是基因自私的結果，對子女無私的基因傾向讓子女更容易活下去，所以這樣的基因存活下來，相反的會拋下子女的基因，如果不能配合嬰兒時期可以自行生存的基因，這些子女也就會提早夭折，這樣的基因就會消失在歷史的長河裡頭。&lt;/p&gt;
&lt;p&gt;因此，所有的生物行為都只有一個目的—讓基因複製下去，而環境會做出審判，作者還進一步的使用賽局理論來解釋哪些行為會是有利的、哪些行為會造成不利，譬如說某個族群50%的個體不喜歡起衝突而50%的個體喜歡追殺到底，想當然爾，如果這個種族在競爭食物時，喜歡追殺到底的個體比較有利，所以不喜歡起衝突的個體會數量減少，所以接下來可能是存在20%的個體不喜歡起衝突而80%的個體喜歡追殺到底，因此喜歡追殺到底的個體很容易遇到一樣喜歡追殺到底的個體，這些喜歡衝突的個體可能會弄的兩拜俱傷，而不喜歡衝突的個體可能使用逃跑的方式躲避反而是存活下來了，此時就會出現數量上的反轉，最後會形成一種平衡，稱為「演化穩定策略」（evolutionary stable strategy, ESS），那當然不會有全然不喜歡衝突和喜歡衝突的個體，所以這樣的平衡是反應在基因上面的，基因在編寫神經網絡的時候就會擬定一個對它最有利的穩定策略，當這個策略奏效，這樣的基因就得以存活。&lt;/p&gt;
&lt;p&gt;這個部分讓我想起一個遊戲叫做&lt;a href="https://audreyt.github.io/trust-zh-TW/"&gt;信任的演化&lt;/a&gt;，這個遊戲是一個欺騙和合作的遊戲，當雙方都彼此合作時，會出現雙贏，而一方欺騙的情況下，欺騙者得利，另一方損失，如果雙方都欺騙則雙方都得不到好處。遊戲中有多個角色採取不同的策略，其中：「紅嬰仔」會一直信任對方，「黑到底」則會一直欺騙，「模仿貓」則是一開始採取合作的態度，當對方欺騙時在下一回合欺騙他，當對方合作時在下一回合就與他合作。當我們將這些角色丟進去進行多次的賽局，失敗者淘汰，成功者複製，在多次賽局之後你會發現「模仿貓」會勝出，利用這個遊戲你就能了解有限度的利他行為是如何勝出的，而基因所產生行為也就是在這樣多次的賽局下修正成有限度的利他行為。&lt;/p&gt;
&lt;h3&gt;文化的複製者—迷因（Meme）&lt;/h3&gt;
&lt;p&gt;基因為了自己的利益編寫出了神經網絡來幫助自己存活下去，但基因萬萬沒想到它所開發的東西居然會反過來對它不利！&lt;/p&gt;
&lt;p&gt;人類的基因產生了非常厲害的人腦，的確是有助於基因的繁衍，你看地球上的人口數量就知道了，但是大腦複雜到一個程度，就產生了思想，人類發展出了語言作溝通，文化得以使用語言或文字傳承下去，這在定義上也是一種複製者，這種文化的複製因子，作者將它稱之為迷因（Meme）。&lt;/p&gt;
&lt;p&gt;迷因（Meme）也是彼此競爭著，它們競爭的是你有限的大腦，如果你的大腦有資本主義，可能就容不下共產主義，而迷因也會不斷的改善自己，讓它們可以贏得競爭，然後傳播出去並大量的被複製。而有些時候迷因的競爭對手可能不只是其他迷因，也有可能會和基因作對，譬如說使用保險套的迷因，正阻止了基因的複製，激烈的宗教信仰的迷因，使人走向自殺式攻擊，同樣也是不利於基因的繁衍。&lt;/p&gt;
&lt;p&gt;這讓我想起最近人工智慧的興起，很多人對人工智慧開始產生擔憂，擔心機器人會反撲人類，如果依照上述的觀點看，這個可能是有機會發生的，程式設計師就像是基因，為了我們自身的目的去創造人工智慧來為我們服務，但是如果人工智慧越來越聰明，而且它的運算速度又比人類快，我們會根本無法理解人工智慧在想什麼，但是它們也確實是幫助人類解決很多問題，所以就算無法理解也就繼續的使用著，直到有一天，這些人工智慧在運作中突變出一種可以複製自己的複製者，也許是一種病毒，而這樣的複製者又能快速在人工智慧溝通中被傳遞出去，它就有可能回頭做出不利於人類的事。&lt;/p&gt;
&lt;h3&gt;結語：複製與競爭&lt;/h3&gt;
&lt;p&gt;本書讓我們重新認識了演化論，有了可以自己複製自己的複製者，就會有擴張，擴張的結果就是資源的稀缺，因此就自然產生競爭，在競爭的過程中，成功的策略符合ESS使得複製者得以存活下來，而失敗的複製者則會被淘汰、無法複製下去，這就是天擇的適者生存。從這一層的思想讓我們對於演化有了更深的認識，如果是生物的演化這個複製者就是基因，如果是文化的演化這個複製者就是迷因，也許你能想到其他的複製者，歡迎在下面留言讓大家知道。&lt;/p&gt;</content></entry><entry><title>[吉他] 方大同-三人遊</title><link href="YCNote/uwarn-performance_1.html" rel="alternate"></link><published>2017-11-09T12:00:00+08:00</published><updated>2017-11-09T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-11-09:YCNote/uwarn-performance_1.html</id><summary type="html">&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;iframe width="100%" height="450" src="https://www.youtube.com/embed/8lwc81a9mAo"  frameborder="0" gesture="media" allowfullscreen&gt;&lt;/iframe&gt;</summary><content type="html">&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;iframe width="100%" height="450" src="https://www.youtube.com/embed/8lwc81a9mAo"  frameborder="0" gesture="media" allowfullscreen&gt;&lt;/iframe&gt;</content><category term="吉他"></category></entry><entry><title>[吉他] 光良-傷心地鐵</title><link href="YCNote/uwarn-performance_2.html" rel="alternate"></link><published>2017-11-08T12:00:00+08:00</published><updated>2017-11-08T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-11-08:YCNote/uwarn-performance_2.html</id><summary type="html">&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;iframe width="100%" height="450" src="https://www.youtube.com/embed/7KpOh8N-9Z0" frameborder="0" gesture="media" allowfullscreen&gt;&lt;/iframe&gt;</summary><content type="html">&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;iframe width="100%" height="450" src="https://www.youtube.com/embed/7KpOh8N-9Z0" frameborder="0" gesture="media" allowfullscreen&gt;&lt;/iframe&gt;</content><category term="吉他"></category></entry><entry><title>股票策略：移動停損法</title><link href="YCNote/stock-sell-point.html" rel="alternate"></link><published>2017-09-05T12:00:00+08:00</published><updated>2017-09-05T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-09-05:YCNote/stock-sell-point.html</id><summary type="html">&lt;p&gt;最近在研究股票，在網路上看到這篇文章&lt;/p&gt;
&lt;p&gt;https://m.mobile01.com/topicdetail.php?f=291&amp;amp;t=3065829&amp;amp;p=1&lt;/p&gt;
&lt;p&gt;覺得相當實用，在這邊跟大家分享，並且當作給自己參考的筆記。&lt;/p&gt;
&lt;p&gt;一般我們常常說玩股票要做好風險管理，最廣為人知的就是設停損點，假設今天我以20元買入一張股票，假如停損比率設10%好了，那我的停損點就是18元，股票一到這個價位就忍痛賣出，以做到風險管理，避免自己大賠。&lt;/p&gt;
&lt;p&gt;停損點的另外一個相反就是停利點，也就是當賺到某一個比例的金額時就獲利了結，假設一樣20元買入一張股票，設20%停利，也就是24元的時候賣出，設停利原本的目的是為了讓自己賣在高點，不過卻是有可能造成反效果。&lt;/p&gt;
&lt;p&gt;如果今天你幸運的買到一張飆股，一連兩天漲了兩根來到了24元，到了你設下的停利點，你不會真的就把它給賣了吧！正常人應該會等它漲到夠了，等它開始反轉時再考慮賣出。那究竟應該在反轉後跌多少才應該賣？這就變成了新的問題，總不能還是以停損點當作賣點吧！那你應該永遠賺不到錢～&lt;/p&gt;
&lt;p&gt;所以這篇的作者提供了一個方法，把停損點和停利點結合成為一點，而這一點會隨著股價上漲而移動上漲，所以下面的網友就稱這個叫做移動停損法。聽起來很神奇，其實概念很簡單 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;最近在研究股票，在網路上看到這篇文章&lt;/p&gt;
&lt;p&gt;https://m.mobile01.com/topicdetail.php?f=291&amp;amp;t=3065829&amp;amp;p=1&lt;/p&gt;
&lt;p&gt;覺得相當實用，在這邊跟大家分享，並且當作給自己參考的筆記。&lt;/p&gt;
&lt;p&gt;一般我們常常說玩股票要做好風險管理，最廣為人知的就是設停損點，假設今天我以20元買入一張股票，假如停損比率設10%好了，那我的停損點就是18元，股票一到這個價位就忍痛賣出，以做到風險管理，避免自己大賠。&lt;/p&gt;
&lt;p&gt;停損點的另外一個相反就是停利點，也就是當賺到某一個比例的金額時就獲利了結，假設一樣20元買入一張股票，設20%停利，也就是24元的時候賣出，設停利原本的目的是為了讓自己賣在高點，不過卻是有可能造成反效果。&lt;/p&gt;
&lt;p&gt;如果今天你幸運的買到一張飆股，一連兩天漲了兩根來到了24元，到了你設下的停利點，你不會真的就把它給賣了吧！正常人應該會等它漲到夠了，等它開始反轉時再考慮賣出。那究竟應該在反轉後跌多少才應該賣？這就變成了新的問題，總不能還是以停損點當作賣點吧！那你應該永遠賺不到錢～&lt;/p&gt;
&lt;p&gt;所以這篇的作者提供了一個方法，把停損點和停利點結合成為一點，而這一點會隨著股價上漲而移動上漲，所以下面的網友就稱這個叫做移動停損法。聽起來很神奇，其實概念很簡單，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;從買入當天開始算起，以過程的每天當中最高股價當作基準點，向下去設停損點，或是停利點，低於這點就當天賣，或隔天賣。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這就是我們剛剛說的，等到股價反轉後再來賣，而這個方法也可以順便做到停損，所以這點既是停損點也是停利點，但行為上都一樣是賣出，差別只在於你有沒有賺到錢囉！所以接下來我會以「賣出點」來稱這一點，避免大家被混淆。&lt;/p&gt;
&lt;p&gt;舉個例子好了，如果第一天我以50元買入一張股票，當天最高價位來到了51元，所以過程中最高價位目前是51元，假設我停損停利設10%，所以如果第二天股價低於51*90%=45.9元就賣出，45.9元就是目前的賣出點，假設未來都沒有價位超出51元，那賣出點永遠都在45.9元不會變。好，如果到了第二天，它飆上去了，收盤54元，最高價位曾經來到了55元，此時賣出點也跟著增加，以買股以來的最高價55元當作新的基準向下10％算出新的賣出點，也就是55*90%=49.5元，第三天氣勢依舊，開盤直接跳空漲停，最高價位來到了59.4元，所以賣出點目前是59.4*90%=53.45元，第四天開始反轉，微微下跌，最低價來到55元，最高價為57元，沒有低於賣出點所以不賣，此時賣出點也沒有改變，因為最高價沒有超過第三天，所以一樣是用第三天當作基準點向下算出賣出點53.45元，第五天終於稱不住了，股價跌到了53.45元，然後就當天賣出或隔天賣出，假如當天賣出的話，那就是53.45元，所以最後我的投報率是(53.45 - 50) / 50 = 6.9 %。&lt;/p&gt;
&lt;p&gt;那這個停損停利的比例該怎麼訂呢？在這個作者寫這篇文章的時候漲跌停百分比為7%，所以他設10%很合理，不會只有一天跌停就賣出了，而目前的漲跌停百分比為10%，所以我認為應該設把停損停利百分比設更高一點，建議在10~15%之間。&lt;/p&gt;
&lt;p&gt;那如果遇到除息的話，當然要將你口袋裡的股息從最高價上扣掉，在重新計算賣出點，這樣的賣出點才合理。&lt;/p&gt;
&lt;p&gt;以上的策略在一種情形下會賺錢，那就是你投資的股票漲的比跌的還多，當然這是一句廢話啦！這讓我想起之前在跟我朋友聊股票，我朋友說他的股票都賠錢，我就跟他開玩笑，股票不就是買在低點，然後賣在高點，你怎麼不照做，難怪會賠錢。移動停損法提供一種紀律讓你不會賠太多，但如果真的要獲利的話，還是得從基本面或技術面去分析股票，並在低價買進好股票，然後等它上漲。&lt;/p&gt;</content></entry><entry><title>Python玩數據 (3)：Numpy [2/2]</title><link href="YCNote/python-play-with-data_3.html" rel="alternate"></link><published>2017-05-06T12:00:00+08:00</published><updated>2017-05-06T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-05-06:YCNote/python-play-with-data_3.html</id><summary type="html">&lt;p&gt;在上一章節的討論，我們已經有了Numpy的基礎概念，在這一篇當中，我們會更深入的了解Numpy還有什麼進階的功能，包括：產生ndarray的多種方法、broadcast的概念以及ndarray的進階操作手法。&lt;/p&gt;
&lt;h2&gt;產生ndarray的其他方法&lt;/h2&gt;
&lt;p&gt;在上一章，ndarray的產生方法是由list產生的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Numpy還提供產生ndarray的其他方式，幫助我們更容易的產生ndarray，譬如，產生一個數列。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;1 …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;在上一章節的討論，我們已經有了Numpy的基礎概念，在這一篇當中，我們會更深入的了解Numpy還有什麼進階的功能，包括：產生ndarray的多種方法、broadcast的概念以及ndarray的進階操作手法。&lt;/p&gt;
&lt;h2&gt;產生ndarray的其他方法&lt;/h2&gt;
&lt;p&gt;在上一章，ndarray的產生方法是由list產生的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Numpy還提供產生ndarray的其他方式，幫助我們更容易的產生ndarray，譬如，產生一個數列。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;5.&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;5.5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;stop指的是停止的那點，那點是不包含在產生的數列的。&lt;/p&gt;
&lt;p&gt;1D的數列也可轉換成多維度的數列。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;另外還有一種可以產生連續數列的方法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;  &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;  &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.5&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.75&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;  &lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;這個函數是這樣的，0是起始值，2是最終值，這個最終值是包含在數列裡的，9是代表數列會有九個數字，所以它會自動從這區間找九個數字均勻分配。&lt;/p&gt;
&lt;p&gt;另外，也可以產生一個全部都零或一的數列，或是矩陣中的「單位矩陣」。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;# &amp;quot;eye&amp;quot; means &amp;quot;I&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;或者，你想要亂數產生也可以。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.14405468&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.2312139&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.79134702&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.18676625&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.95305253&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.44833768&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.87919535&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.69051727&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;如果你想要數列依照你給定的規則產生，就先定義好函數，然後再利用&lt;code&gt;fromfunction&lt;/code&gt;製造數列。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;func1&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromfunction&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;這麼一來，每個位置的值都是由我們所定義的函數所決定。如果你覺得那個&lt;code&gt;func1&lt;/code&gt;名稱很多餘，還有下面這個方法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromfunction&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;上面我使用了&lt;code&gt;lambda&lt;/code&gt;，這個東西稱之為『匿名函數』。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;和&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;something&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;上面這兩個函式是等價的，差異只在於，第一個函式是沒有名稱的，稱為匿名函數，第二種就是一般的函式，具有名稱。&lt;/p&gt;
&lt;h2&gt;Broadcasting&lt;/h2&gt;
&lt;p&gt;在上一章，我有提到一般的矩陣運算，在Numpy中是採用element-wise operation，也就是每個相應元素做運算，然後產生新的ndarray，這個前題是兩組要運算的ndarray他們的shape是相同的，那如果遇到shape不一致，Numpy會怎麼處理呢？實際上，Numpy會幫你把陣列給延伸展開，就像廣播(broadcasting)一樣的傳遞出去，這遵照所謂的broadcasting rules。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;      &lt;span class="c1"&gt;# element-wise plus&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;6.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;上面就是最普遍的兩個相同shape的矩陣作運算。&lt;/p&gt;
&lt;p&gt;那如果是這個情況呢？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;6.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;8.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;你會發現如果矩陣對一個單一元素作運算，其實就等同於這個單一元素對矩陣內的元素分別作運算，這個方式相當好理解，那如果是這樣呢？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;10.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;14.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;22.&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;    &lt;span class="c1"&gt;# [[ 2*5, 2*7, 2*11 ],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;15.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;21.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;33.&lt;/span&gt; &lt;span class="p"&gt;]])&lt;/span&gt;   &lt;span class="c1"&gt;#  [ 3*5, 3*7, 3*11 ]]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;讓我來分解解說一下broadcasting究竟做了什麼，broadcasting能自動填滿矩陣有一個大前提，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;參與運算的所有矩陣必須符合以下規則才能做broadcasting，所有矩陣的shape由axis＝-1開始對齊去比較彼此間的rank，所有矩陣的在每個axis下的rank必須符合以下兩種規則其中之一：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;所有rank為同一個值&lt;/li&gt;
&lt;li&gt;只能有一個矩陣rank為非0或1，其餘矩陣的rank都要為0或1&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面這個例子，在axis= -2之下，只有C矩陣rank具有非0或1的2，而D的rank則為1；在axis= -1之下，只有D矩陣rank具有非0或1的3，而C的rank則為1，因此這兩個陣列可以使用broadcasting rule來延伸。&lt;/p&gt;
&lt;p&gt;為什麼我們需要這樣的前提假設，原因是符合這樣的情況下，我們可以藉由重複的複製貼上來使得兩個或多個矩陣有一樣的shape，C矩陣shape為(2,1)，所以在axis= -2的方向上，重複貼3次就會產生出shape為(2,3)的矩陣；D矩陣的shape為(1,3)，所以在axis= -1的方向上，重複貼2次就會產生出(2,3)的矩陣，如此一來兩個矩陣都是(2,3)就可以作element-wise operation。&lt;/p&gt;
&lt;p&gt;逐步示範一下，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;C  = [[2],[3]] # shape = (2,1)
C&amp;#39; = [[2,2,2],[3,3,3]] # shape = (2,3)
D  = [[5,7,11]] # shape = (1,3)
D&amp;#39; = [[5,7,11],[5,7,11]] # shape = (2,3)
E = C&amp;#39; * D&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;以下這些都是同樣道理&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;A      (2d array):  5 x 4
B      (1d array):      1
Result (2d array):  5 x 4

A      (2d array):  5 x 4
B      (1d array):      4
Result (2d array):  5 x 4

A      (3d array):  15 x 3 x 5
B      (3d array):  15 x 1 x 5
Result (3d array):  15 x 3 x 5

A      (3d array):  15 x 3 x 5
B      (2d array):       3 x 5
Result (3d array):  15 x 3 x 5

A      (3d array):  2 x 3 x 4
B      (2d array):      3 x 1
Result (3d array):  2 x 3 x 4
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;我們再來看一下，如果維度不一樣是怎麼運作，譬如說2D碰上3D的，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;分解一下&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;F  = [[[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]],
      [[12, 13, 14, 15],
       [16, 17, 18, 19],
       [20, 21, 22, 23]]]  # shape = (2,3,4)
G = [[1],
     [2],
     [3]] # shape = (3,1)
G&amp;#39;= [[1,1,1,1],
     [2,2,2,2],
     [3,3,3,3]] # shape = (3,4)
G&amp;quot;= [[[1,1,1,1],
      [2,2,2,2],
      [3,3,3,3]],
     [[1,1,1,1],
      [2,2,2,2],
      [3,3,3,3]]] # shape = (2,3,4)

H = F + G&amp;quot;
   = [[[ 0+1,  1+1,  2+1,  3+1],
       [ 4+2,  5+2,  6+2,  7+2],
       [ 8+3,  9+3, 10+3, 11+3]],
      [[12+1, 13+1, 14+1, 15+1],
       [16+2, 17+2, 18+2, 19+2],
       [20+3, 21+3, 22+3, 23+3]]]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;那這樣的性質可以怎麼運用呢？舉個例子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Example:
請問平面上這些點(102.0, 203.0),(132.0, 193.0),(45.0, 155.0),(57.0, 173.0)，哪一點最接近(111.0,188.0)?
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;observation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;111.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;188.0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;codes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;102.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;203.0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;132.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;193.0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;45.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;155.0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;57.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;173.0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;codes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;observation&lt;/span&gt;  &lt;span class="c1"&gt;# broadcasting&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;diff&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;9.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;15.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;21.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;66.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;33.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;54.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;15.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;diff&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# distance&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;17.49285568&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;21.58703314&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;73.79024326&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;56.04462508&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;nearest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;nearest&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="c1"&gt;# ANS is (102.0, 203.0)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Slice and Fancy Indexing&lt;/h2&gt;
&lt;p&gt;最後，來看一下我們可以怎麼去切ndarray。在python內建語言中，常見的slice是這個樣子&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;那如果是維度再加一級呢？則是這個樣子&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;如果是ndarray，我們常常處理維度大於1的陣列，如果用這個方法來slice，就顯得非常麻煩，Numpy提供了一種比較直覺的方式來做slice。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;在中括號裡頭用逗點隔開來表示在各個axis上要取的位置，還可以填入一個陣列來取出一個範圍。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt;   &lt;span class="c1"&gt;# &amp;quot;:&amp;quot;代表全取，效果和 M[1,0:2]一樣&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="c1"&gt;# 寫成陣列也可以，效果和 M[1,0:2]一樣&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="c1"&gt;# 還可以做到在axis=0的方向上取範圍，這是list做不到的&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;我們也可以引入一個ndarray來做篩選，常見使用的是布林陣列。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;b是由一個邏輯運算產生，這個邏輯運算會對矩陣作element-wise operation，所以會得出一個大小相同的布林陣列。而我們可以將b引入N當作篩選器，把符合的給取出來。事實上還可以更強大的去將取出來的值改值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;上面我將取出來的值加倍，這樣的手法來取值改值會直接影響到原陣列，這是一個很重要的手法。&lt;/p&gt;
&lt;h2&gt;子彈總結&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;產生ndarray的其他方法：np.arange, np.linspace, np.zeros, np.ones, np.eye, np.random.random 和 np.fromfunction&lt;/li&gt;
&lt;li&gt;Broadcasting的前題：所有矩陣的shape由axis＝-1開始對齊去比較彼此間的rank，所有矩陣的在每個axis下的rank必須符合以下兩種規則其中之一：&lt;/li&gt;
&lt;li&gt;所有rank為同一個值&lt;/li&gt;
&lt;li&gt;只能有一個矩陣rank為非0或1，其餘矩陣的rank都要為0或1&lt;/li&gt;
&lt;li&gt;Slicing Method ( Ex: M[1,0:2] )&lt;/li&gt;
&lt;li&gt;布林陣列的取值賦值方法&lt;/li&gt;
&lt;/ul&gt;</content><category term="Python玩數據"></category></entry><entry><title>機器學習技法 學習筆記 (7)：Radial Basis Function Network與Matrix Factorization</title><link href="YCNote/ml-course-techniques_7.html" rel="alternate"></link><published>2017-04-22T12:00:00+08:00</published><updated>2017-04-22T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-04-22:YCNote/ml-course-techniques_7.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋Radial Basis Function (RBF) Network、K-Means、One-Hot Encoding和Matrix Factorization。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Radial Basis Function (RBF) Network&lt;/h3&gt;
&lt;p&gt;回顧一下Gaussian Kernel SVM，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;W = 𝚺&lt;sub&gt;n=sv&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;  &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;G&lt;sub&gt;SVM&lt;/sub&gt;   &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;= sign[WZ+b] &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;= sign{[𝚺&lt;sub&gt;n=sv&lt;/sub&gt;α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X)]+b} &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;⇒ G&lt;sub&gt;SVM&lt;/sub&gt; = sign{[𝚺&lt;sub&gt;n=sv&lt;/sub&gt;α&lt;sub&gt;n …&lt;/sub&gt;&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋Radial Basis Function (RBF) Network、K-Means、One-Hot Encoding和Matrix Factorization。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Radial Basis Function (RBF) Network&lt;/h3&gt;
&lt;p&gt;回顧一下Gaussian Kernel SVM，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;W = 𝚺&lt;sub&gt;n=sv&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;  &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;G&lt;sub&gt;SVM&lt;/sub&gt;   &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;= sign[WZ+b] &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;= sign{[𝚺&lt;sub&gt;n=sv&lt;/sub&gt;α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X)]+b} &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;⇒ G&lt;sub&gt;SVM&lt;/sub&gt; = sign{[𝚺&lt;sub&gt;n=sv&lt;/sub&gt;α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;exp(-γ|X-X&lt;sub&gt;n&lt;/sub&gt;|&lt;sup&gt;2&lt;/sup&gt;)]+b} &lt;br/&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;看到這個式子你想到了什麼？有沒有融會貫通的感覺，你同樣的可以把上面的式子看成是Aggregation，又或者是Network。&lt;/p&gt;
&lt;p&gt;先來定義一下RBF Function， 其實就是Gaussian Function，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RBF Function: RBF(X,X&lt;sub&gt;n&lt;/sub&gt;)=exp(-γ|X-X&lt;sub&gt;n&lt;/sub&gt;|&lt;sup&gt;2&lt;/sup&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以我們可以仿造SVM的形式來造一個Network，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;G=Output{[𝚺&lt;sub&gt;m&lt;/sub&gt; β&lt;sub&gt;m&lt;/sub&gt;RBF(X,μ&lt;sub&gt;m&lt;/sub&gt;)]+b}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;當Output為sign Function、β&lt;sub&gt;m&lt;/sub&gt;為α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;就回到特例SVM了。&lt;/p&gt;
&lt;p&gt;我們來細看這個式子傳遞的概念，RBF Network的第一層是先產生M組RBF(X,μ&lt;sub&gt;m&lt;/sub&gt;)，意味著以這M個位置μ&lt;sub&gt;m&lt;/sub&gt;當作中心點來評估各個X與它的相似程度，RBF是有評估相似度的味道，越接近μ&lt;sub&gt;m&lt;/sub&gt;的點，RBF越大，並隨著與μ&lt;sub&gt;m&lt;/sub&gt;距離變大，RBF的值也快速遞減，所以這M個μ&lt;sub&gt;m&lt;/sub&gt;是有象徵性的，越接近它你越受它的影響。&lt;/p&gt;
&lt;p&gt;決定了每一筆數據各是受哪些μ&lt;sub&gt;m&lt;/sub&gt;影響，接下來第二層是由這M個代表性的位置來進行投票決定最後的結果，這意味的不同的地方μ&lt;sub&gt;m&lt;/sub&gt;對最後結果也有不同的影響力，舉個例子，假設在SVM裡頭，某個μ&lt;sub&gt;m&lt;/sub&gt;如果它的y&lt;sub&gt;m&lt;/sub&gt;=+1，那它對最後的影響就會是正的；那如果某個μ&lt;sub&gt;m&lt;/sub&gt;的y&lt;sub&gt;m&lt;/sub&gt;=-1，那它對最後的影響就會是負的，所以一個點進來，先評估一下它和象徵性的幾個點μ&lt;sub&gt;m&lt;/sub&gt;的距離，如果相鄰幾點都是正的，這個點最後的結果就會是正的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="RBF Network" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_05.png"&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RBF Network在歷史上是Neural Network的一個分支，不過從上面的介紹你就會發現，它們的結構是有差異的，演算法也就不一樣。&lt;/p&gt;
&lt;p&gt;通常最佳化RBF Network做法是這樣的，我們會先用一些方法將μ&lt;sub&gt;m&lt;/sub&gt;決定，如果μ&lt;sub&gt;m&lt;/sub&gt;很懶惰的就直接使用所有的Training Data，總共有N個μ&lt;sub&gt;m&lt;/sub&gt;，這就叫做&lt;strong&gt;「Full RBF Network」&lt;/strong&gt;。&lt;strong&gt;我們也可以使用一些歸納的演算法找出代表資料群體的幾個象徵性的中心點，例如待會會介紹的K-Means的方法&lt;/strong&gt;，找出k個μ&lt;sub&gt;m&lt;/sub&gt;再做計算，這樣的RBF Network稱為&lt;strong&gt;「k Nearest Neighbor RBF Network」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;找到了μ&lt;sub&gt;m&lt;/sub&gt;就已經決定了所有的RBF Function，接下來就可以線性組合這些RBF Function，我們可以使用Regression的方法來求取β&lt;sub&gt;m&lt;/sub&gt;。&lt;/p&gt;
&lt;p&gt;而如果你使用「Full RBF Network」，你會發現做完Regression後E&lt;sub&gt;in&lt;/sub&gt;=0，這是典型的Overfitting，那這時你可能就要採用有Regularization的Regression啦！譬如說Ridge Regression之類的。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;K-Means&lt;/h3&gt;
&lt;p&gt;&lt;img alt="K-Means" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_06.png"&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;接下來來看怎麼用K-Means找到代表資料群體的幾個象徵性的中心點。&lt;/p&gt;
&lt;p&gt;首先，先決定要有幾個「中心點」，這裡假設我要有k個好了，接下來先隨機給這些「中心點」一個初始的位置，接下來根據數據的靠近程度開始歸類，如果一筆數據比較所有的「中心點」後發現離「中心點」A是最近的話，那這筆數據就歸「中心點」A了，就用這樣的規則把所有數據都做分類。&lt;/p&gt;
&lt;p&gt;分完類後，接下來平均每一個資料群體裡的數據座標找出新的代表這個群體的「中心點」，然後又拿這個新的「中心點」根據數據的靠近程度再歸類一次，如此循環多次，直到收斂為止。這樣的話，這k個「中心點」收斂後會各自佔據四方，並且代表某個群體的中心點。我們就可以找到代表性的k個點，並拿這些點做「k Nearest Neighbor RBF Network」。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;One-Hot Encoding&lt;/h3&gt;
&lt;p&gt;討論這麼久的ML，我們還沒有討論過假設遇到「類別」要怎麼處理！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;通常遇到類別的狀況，我們還是需要把它轉換成數值或向量來處理，常見的方法叫做One-Hot Encoding。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;舉個例子，如果要描述血型應該要怎麼做？我們可是無法拿字串下去Regression的啊～此時就需要One-Hot Encoding，假設血型有A, B, AB, O四種，我們可以這樣設定，&lt;/p&gt;
&lt;p&gt;A = [1, 0, 0, 0]&lt;sup&gt;T&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;B = [0, 1, 0, 0]&lt;sup&gt;T&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;AB = [0, 0, 1, 0]&lt;sup&gt;T&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;O = [0, 0, 0, 1]&lt;sup&gt;T&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;就是這麼簡單，這個動作就叫做One-Hot Encoding。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Matrix Factorization&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;那如果今天我的Input和Output都是類別，而我們想要讓機器自己去找到匹配Input和Output的機制，解決這個問題的方法稱之為Matrix Factorization。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Matrix Factorization精神上有點像是Autoencoder，Autoencoder找出隱含在Data裡的特性，而Matrix Factorization則是找出隱含的匹配關係。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;舉個例子，如果Netflix有了一堆用戶和他們曾看過的電影的資料，我們想要從中抽取出用戶與他愛看的電影之間的關係，所以這不單單只是匹配而已，單純匹配就只需要硬碟就做的到了，我們要做的是找出匹配的規律，並且用更少、更精簡的方式表示這個匹配關係，舉個例子，有可能有部分用戶會被歸納到愛看恐怖片的，並且同時這些客戶會被連結到具有恐怖元素的電影，我們預期Matrix Factorization會有自行歸納整理的能力。&lt;/p&gt;
&lt;p&gt;可以仿造Autoencoder來設計Matrix Factorization，而你會發現Activation Function只要使用線性就已經足夠了，因為對於One-Hot Encoding的類別來說，只有一條通道是有效的，這已經具有開關的味道了，所以我們不用在Activation Function上面再弄一道開關，所以採用Linear就足夠了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Matrix Factorization" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_07.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;因為是線性模型的緣故，我們可以很簡單的使用矩陣來描述，&lt;/p&gt;
&lt;p&gt;Hypothesis: h(X) = W&lt;sup&gt;T&lt;/sup&gt;VX&lt;/p&gt;
&lt;p&gt;而如果是某一用戶，則&lt;/p&gt;
&lt;p&gt;h(X&lt;sub&gt;n&lt;/sub&gt;) = W&lt;sup&gt;T&lt;/sup&gt;V&lt;sub&gt;n&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;對某個用戶而言與他匹配的電影是一個向量，上面紀錄了他看過的電影，假設我再指定一部電影m，此時W&lt;sub&gt;m&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;V&lt;sub&gt;n&lt;/sub&gt;就代表這個用戶有沒有看過這部電影。&lt;/p&gt;
&lt;p&gt;用這個方法來想問題，假設今天你把用戶和電影填成一個大的表格，或是矩陣，有交集的部分就打個勾，這個矩陣的每個元素表示成r&lt;sub&gt;nm&lt;/sub&gt;，有打勾的部分r&lt;sub&gt;nm&lt;/sub&gt;=1，沒打勾的部分r&lt;sub&gt;nm&lt;/sub&gt;=0，那我們做的轉換W和V最終就是為了讓&lt;/p&gt;
&lt;p&gt;W&lt;sub&gt;m&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;V&lt;sub&gt;n&lt;/sub&gt;≈r&lt;sub&gt;nm&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;為了評估匹配的好壞，我們定義Error Function為&lt;/p&gt;
&lt;p&gt;E&lt;sub&gt;in&lt;/sub&gt;({W&lt;sub&gt;m&lt;/sub&gt;},{V&lt;sub&gt;n&lt;/sub&gt;}) = (1/𝚺&lt;sub&gt;m&lt;/sub&gt; |D&lt;sub&gt;m&lt;/sub&gt;|)×𝚺&lt;sub&gt;n,m&lt;/sub&gt; (r&lt;sub&gt;nm&lt;/sub&gt;-W&lt;sub&gt;m&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;V&lt;sub&gt;n&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;最佳化Matrix Factorization有兩個演算法，一個是Alternating Least Squares，另外一個是SGD。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Alternating Least Squares for Matrix Factorization&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Alternating Least Squares for Matrix Factorization" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_08.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第一個方法是利用Linear Regression交互的優化W&lt;sub&gt;m&lt;/sub&gt;和V&lt;sub&gt;n&lt;/sub&gt;，我們的目標是使得W&lt;sub&gt;m&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;V&lt;sub&gt;n&lt;/sub&gt;=r&lt;sub&gt;nm&lt;/sub&gt;，這式子可以用兩個角度看，如果固定W&lt;sub&gt;m&lt;/sub&gt;，優化V&lt;sub&gt;n&lt;/sub&gt;，那就是線性擬合{V&lt;sub&gt;n&lt;/sub&gt;, r&lt;sub&gt;nm&lt;/sub&gt;}的問題；那如果固定V&lt;sub&gt;n&lt;/sub&gt;，優化W&lt;sub&gt;m&lt;/sub&gt;，這就是線性擬合{W&lt;sub&gt;m&lt;/sub&gt;, r&lt;sub&gt;nm&lt;/sub&gt;}的問題。&lt;strong&gt;因此，交替優化W&lt;sub&gt;m&lt;/sub&gt;和V&lt;sub&gt;n&lt;/sub&gt;就可以使得W&lt;sub&gt;m&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;V&lt;sub&gt;n&lt;/sub&gt;越來越接近r&lt;sub&gt;nm&lt;/sub&gt;了&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;SGD for Matrix Factorization&lt;/h3&gt;
&lt;p&gt;&lt;img alt="SGD for Matrix Factorization" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_09.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第二個方法則是老招—Gradient Descent，這裡採用隨機的版本SGD，所以過程中我們會隨意的從(n,m)中挑點，然後根據Error Measure&lt;/p&gt;
&lt;p&gt;E&lt;sub&gt;in&lt;/sub&gt;({W&lt;sub&gt;m&lt;/sub&gt;},{V&lt;sub&gt;n&lt;/sub&gt;}) = (1/𝚺&lt;sub&gt;m&lt;/sub&gt; |D&lt;sub&gt;m&lt;/sub&gt;|)×𝚺&lt;sub&gt;n,m&lt;/sub&gt; (r&lt;sub&gt;nm&lt;/sub&gt;-W&lt;sub&gt;m&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;V&lt;sub&gt;n&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;我們就可以得到更新W&lt;sub&gt;m&lt;/sub&gt;和V&lt;sub&gt;n&lt;/sub&gt;的方法，詳細的方法見上圖所示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目前，SGD方法是處理大型Matrix Factorization最流行的作法。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;本篇介紹類似Neural Network的兩種Network結構，分別為Radial Basis Function (RBF) Network和Matrix Factorization。&lt;/p&gt;
&lt;p&gt;在做RBF Network時，我們先找出幾個代表的中心，並評估一筆資料與這些中心的距離，再來再考慮不同中心對於答案的貢獻，加總起來可以預測這筆資料的答案，我們可以使用K-Means的方法來找出k點代表性的中心點來做RBF Network。&lt;/p&gt;
&lt;p&gt;Matrix Factorization和Autoencoder有點類似，Autoencoder目標在於找出隱含在Data裡的特性，而Matrix Factorization則是找出隱含的匹配關係，並且介紹了兩種Matrix Factorization的演算法：Alternating Least Squares和SGD方法。&lt;/p&gt;
&lt;p&gt;這系列的介紹文章，到這裡算是走到尾聲了，最後跟大家推薦一下老師的最後一堂課的投影片：&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/216_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/216_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;這個投影片裡頭林軒田教授用心的彙整了一整個學期的內容，很值得一看。&lt;/p&gt;</content><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)</title><link href="YCNote/ml-course-techniques_6.html" rel="alternate"></link><published>2017-04-17T12:00:00+08:00</published><updated>2017-04-17T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-04-17:YCNote/ml-course-techniques_6.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋神經網路(Neural Network, NN)、深度學習(Deep Learning, DL)、反向傳播算法(Backpropagation, BP)、Weight-elimination Regularizer、Early Stop、Autoencoder、Principal Component Analysis (PCA)。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;神經網路(Neural Network)&lt;/h3&gt;
&lt;p&gt;最後一個主題，我們要來講第三種「特徵轉換」— Extraction Models，其實就是現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，包括下圍棋的AlphaGo、Tesla的自動駕駛都是採用這一類的Machine Learning。&lt;/p&gt;
&lt;p&gt;Extraction Models的基本款就是廣為人知的「神經網路」(Neural Network)，它的特色是使用神經元來做非線性的特徵轉換，那如果具有多層神經元，就是做了多次的非線性特徵轉換，這就是所謂的「深度學習」(Deep …&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋神經網路(Neural Network, NN)、深度學習(Deep Learning, DL)、反向傳播算法(Backpropagation, BP)、Weight-elimination Regularizer、Early Stop、Autoencoder、Principal Component Analysis (PCA)。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;神經網路(Neural Network)&lt;/h3&gt;
&lt;p&gt;最後一個主題，我們要來講第三種「特徵轉換」— Extraction Models，其實就是現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，包括下圍棋的AlphaGo、Tesla的自動駕駛都是採用這一類的Machine Learning。&lt;/p&gt;
&lt;p&gt;Extraction Models的基本款就是廣為人知的「神經網路」(Neural Network)，它的特色是使用神經元來做非線性的特徵轉換，那如果具有多層神經元，就是做了多次的非線性特徵轉換，這就是所謂的「深度學習」(Deep Learning)。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neural Network" src="https://dl.dropbox.com/s/a7divvzh6mzfwvb/MachineLearningTechniques.016.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;上圖左側就是具有一層神經元的Neural Network，首先我們有一組特徵X，通常我們會加入一個維度X&lt;sub&gt;0&lt;/sub&gt;=1，這是為了可以讓結構變得更好看，未來可以與W&lt;sub&gt;0&lt;/sub&gt;相乘產生常數項。使用W來給予特徵X權重，最後總和的結果稱之為Score，s = W&lt;sub&gt;0&lt;/sub&gt;X&lt;sub&gt;0&lt;/sub&gt;+𝚺&lt;sub&gt;i=1&lt;/sub&gt;W&lt;sub&gt;i&lt;/sub&gt;X&lt;sub&gt;i&lt;/sub&gt; = 𝚺&lt;sub&gt;i=0&lt;/sub&gt;W&lt;sub&gt;i&lt;/sub&gt;X&lt;sub&gt;i&lt;/sub&gt;。&lt;/p&gt;
&lt;p&gt;這個Score會被輸入到一個Activation Function裡頭，&lt;strong&gt;Activation Function的用意就是開關&lt;/strong&gt;，當Score大於某個閥值，就打通線路讓這條路的貢獻可以繼續向後傳遞；當Score小於某個閥值，就關閉線路，所以Activation Function可以是Binary Function，但在實際操作之下不會使用像Binary Function這類不可以微分的Activation Function，所以我們會找具有相似特性但又可以微分的函數，例如tanh或者是ReLU這類比較接近開關效果的函數，經過Activation Function轉換後的輸出表示成g&lt;sub&gt;t&lt;/sub&gt; = σ(𝚺&lt;sub&gt;i&lt;/sub&gt;W&lt;sub&gt;i&lt;/sub&gt;X&lt;sub&gt;i&lt;/sub&gt;)，這個g&lt;sub&gt;t&lt;/sub&gt;就稱為神經元、σ為Activation Function、𝚺&lt;sub&gt;i&lt;/sub&gt; W&lt;sub&gt;i&lt;/sub&gt;X&lt;sub&gt;i&lt;/sub&gt;是Score。&lt;/p&gt;
&lt;p&gt;如果我們有多組權重W就能產生多組神經元g&lt;sub&gt;t&lt;/sub&gt;，然後最後把g&lt;sub&gt;t&lt;/sub&gt;做線性組合並使用Output Function h(x)來衡量出最後的答案，Output Function可以是Linear Classification的Binary Function h(x)=sign(x)，不過一樣的問題，它不可以微分，通常不會被使用，常見的是使用Linear Regression h(x)=x，或者Logistic Regression h(x)=Θ(x)來當作Output Function，最後的結果可以表示成 y=h(𝚺&lt;sub&gt;t&lt;/sub&gt;α&lt;sub&gt;t&lt;/sub&gt;g&lt;sub&gt;t&lt;/sub&gt;)，看到這個式子有沒有覺得很熟悉，它就像我們上一回講的Aggregation，將特徵X使用特徵轉換轉成使用g&lt;sub&gt;t&lt;/sub&gt;表示，再來組合這些g&lt;sub&gt;t&lt;/sub&gt;成為最後的Model，所以單層的Neural Network就使用到了Aggregation，它繼承了Aggregation的優點。&lt;/p&gt;
&lt;p&gt;有了這個Model的形式了，我們可以使用Gradient Descent的手法來做最佳化，這也就是為什麼要讓操作過程當中所使用的函數都可以微分的原因。Gradient Descent在Neural Network的領域裡面發展出一套方法稱為Backpropagation，我們待會會介紹。&lt;strong&gt;因此實現Backpropagation，我只要餵Data進去，Model就會去尋找可以描述這組Data的特徵轉換g&lt;sub&gt;t&lt;/sub&gt;，這就好像是可以從Data中萃取出隱含的Feature一樣，所以這類的Models才會被統稱為Extraction Models&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;深度學習(Deep Learning)&lt;/h3&gt;
&lt;p&gt;剛剛我們介紹了最基本款的Neural Network，那如果這個Neural Network有好幾層，我還會稱它為Deep Learning，所以基本上Deep Neural Network和Deep Learning是指同一件事，那為什麼會有兩個名字呢？其實是有歷史典故的。&lt;/p&gt;
&lt;p&gt;Neural Network的歷史相當悠久，早在1958年就有人提出以Perceptron當作Activation Function的單層Neural Network，大家也知道一層的Neural Network是不Powerful的，所以在1969年，就有人寫了論文叫做「perceptron has limitation」，從那時Neural Network的方法就很少人研究了。&lt;/p&gt;
&lt;p&gt;直到1980年代，有人開始使用多層的Neural Network，並在1989年，Yann LeCun博士等人就已經將反向傳播演算法(Backpropagation, BP)應用於Neural Network，當時Neural Network的架構已經和現在的Deep Learning很接近了，不過礙於當時的硬體設備計算力不足，Neural Network無法發揮功效，並且緊接的&lt;strong&gt;有人在1989年證明了只要使用一層Neural Network就可以代表任意函數，那為何還要Deep呢？&lt;/strong&gt;所以Deep Neural Network這方法就徹底黑掉了。&lt;/p&gt;
&lt;p&gt;一直到了最近，&lt;strong&gt;G. E. Hinton博士為了讓Deep Neural Network起死回生，重新給了它一個新名字「Deep Learning」&lt;/strong&gt;，再加上他在2006年提出的RBM初始化方法，這是一個非常複雜的方法，所以在學術界就造成了一股流行，雖然後來被證明RBM是沒有用的，不過卻因為很多人參與研究Deep Learning的關係，也找出了解決Deep Learning痛處的方法，&lt;strong&gt;2009年開始有人發現使用GPU可以大大的加速Deep Learning&lt;/strong&gt;，從這一刻起，Deep Learning就開始流行起來，直到去年的2016年3月，圍棋程式Alpha GO運用Deep Learning技術以4:1擊敗世界頂尖棋手李世乭，Deep Learning正式掀起了AI的狂潮。&lt;/p&gt;
&lt;p&gt;聽完這個故事我們知道改名字的重要性XDD，不過大家是否還有看到什麼關鍵，「使用一層Neural Network就可以代表任意函數，那為何還要Deep呢？」這句話，這不就否定了我們今天做的事情了嗎？的確，使用一層的Neural Network就可以形成任意函數，但這一層的神經元也同樣需要無窮多個才做的到。&lt;strong&gt;Deep Learning的學習方法和人有點類似，我們在學習一個艱深的理論時，會先單元式的針對幾個簡單的概念學習，然後在整合這些概念去理解更高層次的問題&lt;/strong&gt;，Deep Learning透過多層結構學習，雖然第一層的神經元沒有很多，能學到的也只是簡單的概念而已，不過第二層再重組這些簡單概念，第三層再用更高層次的方法看問題，所以同樣的問題使用一層Neural Network可能需要很多神經元才有辦法描述，但是Deep Learning卻可以使用更少的神經元做到一樣的效果。&lt;/p&gt;
&lt;p&gt;另外，Deep Learning還有一個很大好處，就是比較不容易Overfitting，你可以想像一下如果我們使用100個神經元來造一個單層Neural Network，跟使用100個神經元來造一個五層的Neural Network，哪一個比容易Overfitting，當然是單層的Neural Network，多層Neural Network是使用一個從簡單到複雜的抽取特徵方法，所以它比較不易受到雜訊的影響，&lt;strong&gt;Deep Learning在建立多層「模組化」的過程可以抑制對於雜訊的過度反應，這等於是一種Regularization&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;因此，Deep Learning中每一層當中做了Aggregation，在增加模型複雜度的同時，也因為平均的效果而做到截長補短，這具有Regularization的效果，並且在採用多層且瘦的結構也同時因為「模組化」而做到Regularization，這就不難想像Deep Learning為何如此強大。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;反向傳播算法(Backpropagation, BP)&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Neural Network" src="https://dl.dropbox.com/s/khbpmalswll787f/MachineLearningTechniques.017.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;我們接下來就來看一下Deep Learning的演算法—反向傳播法，我們來看要怎麼從Gradient Descent來推出這個算法。&lt;/p&gt;
&lt;p&gt;看一下上面的圖，我畫出了具有L層深的Deep Learning，每一層都有一個權重W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;，因此我們可以估計出每一層的Score s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;= 𝚺&lt;sub&gt;i&lt;/sub&gt; W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(ℓ-1)&lt;/sup&gt;，把Score s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;通過Activation Function，就可以得到下一層的Input，如此不斷的疊上去，直到最後一層L為Output Layer，Output最後的結果y，這裡我使用Linear Function來當作Output Function，這就是Deep Learning最簡單的架構。&lt;/p&gt;
&lt;p&gt;而我們需要Training的就是這些權重W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;，我們如何一步一步的更新W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;，使得它可以Fit數據呢？回想一下Gradient Descent的流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定義出Error函數&lt;/li&gt;
&lt;li&gt;Error函數讓我們可以去評估E&lt;sub&gt;in&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;算出它的梯度∇E&lt;sub&gt;in&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;朝著∇E&lt;sub&gt;in&lt;/sub&gt;的反方向更新參數W，而每次只跨出η大小的一步&lt;/li&gt;
&lt;li&gt;反覆的計算新參數W的梯度，並一再的更新參數W&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;假設使用平方誤差的話，Error函數在這邊就是&lt;/p&gt;
&lt;p&gt;L = (1/2) (y-ŷ)&lt;sup&gt;2&lt;/sup&gt;，&lt;/p&gt;
&lt;p&gt;因此我們的更新公式可以表示成&lt;/p&gt;
&lt;p&gt;W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt; ←  W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;-η×∂L/∂W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt; &lt;/p&gt;
&lt;p&gt;那我們要怎麼解這個式子呢？關鍵就在∂L/∂W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;這項要怎麼計算，這一項在Output Layer (ℓ=L)是很好計算的，&lt;/p&gt;
&lt;p&gt;∂L/∂W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;= {∂L/∂s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;}×{∂s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;/∂W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;}  (連鎖率)&lt;/p&gt;
&lt;p&gt;= {δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;}×{X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(L-1)&lt;/sup&gt;}&lt;/p&gt;
&lt;p&gt;上式當中我們使用了微分的連鎖率，並且令&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt; = ∂L/∂s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;這一項被稱為Backward Pass Term，而X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(L-1)&lt;/sup&gt;這項被稱為Forward Pass Term，所以L層權重的更新取決於Forward Pass Term和Backward Pass Term相乘δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;×X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(L-1)&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;我們先來看一下L層的Forward Pass Term要怎麼計算，X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(L-1)&lt;/sup&gt;這項是很容易求的，我們只要讓數據一路從0層傳遞上來就可以自然而然的得到X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(L-1)&lt;/sup&gt;的值，所以我們會稱X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(L-1)&lt;/sup&gt;這一項為Forward Pass Term，因為我們必須要往前傳遞才可以得到這個值。&lt;/p&gt;
&lt;p&gt;再來看一下L層的Backward Pass Term要怎麼計算，δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;一樣是很容易求得的，&lt;/p&gt;
&lt;p&gt;δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt; = ∂L/∂s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt; = ∂[(1/2) (y-ŷ)&lt;sup&gt;2&lt;/sup&gt;]/∂y = (y-ŷ)&lt;/p&gt;
&lt;p&gt;你會發現這一項的計算需要得到誤差的資訊，而誤差資訊要等到Forward的動作做完才有辦法得到，所以資訊的傳遞方向是從尾巴一路回到頭，是一個Backword的動作。&lt;/p&gt;
&lt;p&gt;因此，最後一層也是Output Layer的更新公式如下：&lt;/p&gt;
&lt;p&gt;W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt; ←  W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;-η×δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(L)&lt;/sup&gt;×X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(L-1)&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;權重的更新取決於Input和Error的影響，需要考慮Forward Pass Term和Backward Pass Term。&lt;/p&gt;
&lt;p&gt;那除了Output這一層以外的權重應該怎麼更新？來看一下(ℓ)層，&lt;/p&gt;
&lt;p&gt;∂L/∂W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;= {∂L/∂s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;}×{∂s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;/∂W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;} (連鎖率)&lt;/p&gt;
&lt;p&gt;= δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;×X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(ℓ-1)&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;一樣是Forward Pass Term和Backword Pass Term相乘，不過δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;這一項的計算有點技巧性，來看一下，&lt;/p&gt;
&lt;p&gt;δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;= ∂L/∂s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;= 𝚺&lt;sub&gt;k&lt;/sub&gt; {∂L/∂s&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;(ℓ+1)&lt;/sup&gt;}×{∂s&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;(ℓ+1)&lt;/sup&gt;/∂X&lt;sub&gt;jk&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;}×{∂X&lt;sub&gt;jk&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;/∂s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;} (連鎖率)&lt;/p&gt;
&lt;p&gt;= 𝚺&lt;sub&gt;k&lt;/sub&gt; {δ&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;(ℓ+1)&lt;/sup&gt;}×{W&lt;sub&gt;jk&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;}×{σ'(s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;)}&lt;/p&gt;
&lt;p&gt;W&lt;sub&gt;jk&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;和σ'(s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;)都是Forward之後就會得到的資訊，而δ&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;(ℓ+1)&lt;/sup&gt; 而是需要Backward才可以得到，我們已經知道δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ=L)&lt;/sup&gt;的值，就可以從δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ=L)&lt;/sup&gt;開始利用上面的公式，一路Backward把所有的δ&lt;sub&gt;j&lt;/sub&gt;都找齊。好！那現在我們已經找到了更新所有Weights的方法了。&lt;/p&gt;
&lt;p&gt;看一下上圖中的最下面的Flow，一開始我們Forward，把所有X和s都得到，到了Output Layer，我們得到了δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ=L)&lt;/sup&gt;，再Backward回去找出所有的δ，接下來就可以用Forward Pass Term和Backword Pass Term來Update所有的W了。&lt;/p&gt;
&lt;p&gt;總結一下，反向傳播算法(Backpropagation, BP)更新權重的方法為&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt; ←  W&lt;sub&gt;ij&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;-η×δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;×X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(ℓ-1)&lt;/sup&gt;  &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If output layer (ℓ=L), δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ=L)&lt;/sup&gt;=(y-ŷ)  &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If other layer, δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;= σ'(s&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;) × 𝚺&lt;sub&gt;k&lt;/sub&gt; δ&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;(ℓ+1)&lt;/sup&gt;×W&lt;sub&gt;jk&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;  &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;δ&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;為Backword Pass Term；X&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;(ℓ-1)&lt;/sup&gt;為Forward Pass Term。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Regularization in Deep Learning&lt;/h3&gt;
&lt;p&gt;那麼像是Deep Learning這麼複雜的Model，我們要怎麼避免Overfitting呢？有五個方法。&lt;/p&gt;
&lt;p&gt;第一個方法，就是我們剛剛提過的&lt;strong&gt;「設計Deep Neural Network的結構」&lt;/strong&gt;，藉由限縮一層當中的神經元來達到一種限制，做到Regularization。&lt;/p&gt;
&lt;p&gt;第二個方法是&lt;strong&gt;「限制W的大小」&lt;/strong&gt;，和標準Regularization作一樣的事情，我們將W的大小加進去Cost裡頭做Fitting，例如使用L2 Regularizer Ω(W)=𝚺(W&lt;sub&gt;jk&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;，但這樣使用有一個問題就是W並不是Sparse的，L2 Regularizer在抑制W的方法是，如果W的分量大的話就抑制多一點，如果分量小就抑制少一點（因為W&lt;sup&gt;2&lt;/sup&gt;微分為一次），所以最後會留下很多很小的分量，造成計算量大大增加，尤其像是Deep Learing這麼龐大的Model，這樣的Regularization顯然不夠好，L1 Regularizer顯然可以解決這個問題（因為在大部分位置微分為常數），但不幸的是它無法微分，所以就有了L2 Regularizer的衍生版本，&lt;/p&gt;
&lt;p&gt;Weight-elimination L2 regularizer: 𝚺[(W&lt;sub&gt;jk&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;]/[1+(W&lt;sub&gt;jk&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;)&lt;sup&gt;2&lt;/sup&gt;]&lt;/p&gt;
&lt;p&gt;這麼一來不管W大或小，它受到抑制的值大小接近的 (Weight-elimination L2 regularizer微分為 -1次方)，因此就可以使得部分W可以為0，大大便利於我們做計算。&lt;/p&gt;
&lt;p&gt;第三種方法是最常使用的&lt;strong&gt;「Early Stopping」&lt;/strong&gt;，所謂的Early Stopping就是，在做Backpropagation的過程去觀察Validation Data的Error有沒有脫離Training Data的Error太多，如果開始出現差異，我們就立刻停止計算，這樣就可以確保Model裡的參數沒有使得Model產生Overfitting，是一個很直接的作法。&lt;/p&gt;
&lt;p&gt;第四種方法是&lt;strong&gt;「Drop-out」&lt;/strong&gt;，在Deep Learing Fitting的過程中，隨機的關閉部分神經元，藉由這樣的作法使得Fitting的過程使用較少的神經元，並且使得結構是瘦長狀的，來達到Regularization。&lt;/p&gt;
&lt;p&gt;第五種方法是接下來會用更大篇幅介紹的&lt;strong&gt;「Denoising Autoencoder」&lt;/strong&gt;，在Deep Neural Network前面加入這樣的結構有助於抑制雜訊。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Autoencoder&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Regularization in Deep Learning" src="https://dl.dropbox.com/s/qnyy3uyyszq45yx/MachineLearningTechniques.018.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Neural Network針對不同需要發展出很多不同的型態，包括CNN, RNN，還有接下來要介紹的Autoencoder，&lt;strong&gt;Autoencoder是一種可以將資料重要資訊保留下來的Neural Network&lt;/strong&gt;，效果有點像是資料壓縮，在做資料壓縮時，會有一個稱為Encoder的方法可以將資料壓縮，那當然還要有另外一個方法將它還原回去，這方法稱為Decoder，壓縮的過程就是用更精簡的方式保存了資料。&lt;strong&gt;Autoencoder同樣的有Encoder和Decoder，不過它不像資料壓縮一樣可以百分之一百還原，不過特別之處是Autoencoder會試著從Data中自己學習出Encoder和Decoder，並盡量讓資料在壓縮完了可以還原回去原始數據&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;見上圖中Basic Autoencoder的部分，透過兩層的轉換，我們試著讓Input X可以完整還原回去，通常中間這一層會使用比較少的神經元，因為我們想要將資訊做壓縮，所以第一層的部分就是一個Encoder，而第二層則是Decoder，他們由權重W&lt;sub&gt;jk&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;決定，而在Training的過程，Autoencoder會試著找出最好的W&lt;sub&gt;jk&lt;/sub&gt;&lt;sup&gt;(ℓ)&lt;/sup&gt;來使得資訊可以盡量完整還原回去，這也代表Autoencoder可以自行找出了Encoder和Decoder。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Encoder這一段就是在做一個Demension Reduction&lt;/strong&gt;，Encoder轉換原本數據到一個新的空間，這個空間可以比原本Features描述的空間更能精準的描述這群數據，而中間這層Layer的數值就是新空間裡頭的座標，有些時候我們會用這個新空間來判斷每筆Data之間彼此的接近程度。&lt;/p&gt;
&lt;p&gt;我們也可以讓Encoder和Decoder可以設計的更複雜一點，所以你同樣的可以使用多層結構，稱之為Deep Autoencoder。另外，也有人使用Autoencoder的方法來Pre-train Deep Neural Network的各個權重。&lt;/p&gt;
&lt;p&gt;緊接著介紹兩種特殊的例子，第一個是Linear Autoencoder，我們把所有的Activation Function改成線性的，這個方法可以等效於待會要講的Principal Component Analysis (PCA)的方法，PCA是一個全然線性的方法，所以它的效力會比Autoencoder差一點。&lt;/p&gt;
&lt;p&gt;第二個是剛剛提到的Denoising Autoencoder，我們在原本Autoencoder的前面加了一道增加人工雜訊的流程，但是又要讓Autoencoder試著去還原出原來沒有加入雜訊的資訊，這麼一來&lt;strong&gt;我們將可以找到一個Autoencoder是可以消除雜訊的&lt;/strong&gt;，把這個Denoising Autoencoder加到正常Neural Network的前面，那這個Neural Network就擁有了抑制雜訊的功用，所以可以當作一種Regularization的方法。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Principal Component Analysis (PCA)&lt;/h3&gt;
&lt;p&gt;最後來講一下Principal Component Analysis (PCA)，它不太算是Deep Learning的範疇，不過它是一個傳統且重要的Dimension Reduction的方法，我們就來看一下。&lt;/p&gt;
&lt;p&gt;&lt;img alt="PCA" src="https://dl.dropbox.com/s/4ek9k9g8vrwnwia/MachineLearningTechniques.019.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;PCA的演算法是這樣的，第一步先求出資料Features的平均值，並且將各個Features減掉平均值，令為ζ，第二步求出由ζ&lt;sup&gt;T&lt;/sup&gt;ζ產生的矩陣的Eigenvalue和Eigenvector，第三步，從這些Eigenvalue和Eigenvector中挑選前面k個，並組成轉換矩陣W，而最終PCA的轉換就是Φ(x)=W&lt;sup&gt;T&lt;/sup&gt;(X-mean(X))，這個轉換做的就是Dimension Reduction，將數據降維到k維。&lt;/p&gt;
&lt;p&gt;PCA做的事是這樣的，每一個Eigenvector代表新空間裡頭的一個軸，而Eigenvalue代表站在這個軸上看資料的離散程度，當然我們如果可以描述每筆資料越分離，就代表這樣的描述方法越好，所以Eigenvalue越大的Eigenvector越是重要，&lt;strong&gt;所以取前面k個Eigenvector的用意是在降低維度的過程，還可以盡量的保持對數據的描述力，而且Eigenvector彼此是正交的，也就是說在新空間裡頭的每個軸是彼此垂直，彼此沒有Dependent的軸是最精簡的，所以PCA所做的Dimension Reduction一定是線性模型中最好、最有效率的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;另外，剛剛有提到的Linear Autoencode幾乎是等效於PCA，大家可以看上圖中的描述，這裡不多贅述，不過不同的是，Linear Autoencoder並沒有限制新空間軸必須是正交的特性，所以它的效率一定會比PCA來的差。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;這一篇當中，我們介紹了Neural Network，並且探討多層Neural Network—Deep Neural Network，也等同於Deep Learning，並且說明為什麼需要「Deep」，然後介紹Deep Learning最重要的演算法—反向傳播算法，接著介紹五種常用的Regularization的方法：設計Deep Neural Network的結構、限制W的大小、Early Stopping、Drop-out和Denoising Autoencoder。&lt;/p&gt;
&lt;p&gt;介紹完以上內容，我們就已經對於Deep Learning的全貌有了一些認識了，緊接著來看Deep Learning的特殊例子—Autoencoder，Autoencoder可以用來做Dimension Reduction，那既然提到了Dimension Reduction，那就不得不在講一下重要的線性方法PCA。&lt;/p&gt;
&lt;p&gt;那在下一回，我們會繼續探討Neural Network還有哪些特殊的分支。&lt;/p&gt;</content><category term="機器學習技法"></category></entry><entry><title>Python玩數據 (2)：Numpy [1/2]</title><link href="YCNote/python-play-with-data_2.html" rel="alternate"></link><published>2017-04-17T12:00:00+08:00</published><updated>2017-04-17T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-04-17:YCNote/python-play-with-data_2.html</id><summary type="html">&lt;p&gt;在上一次我們已經成功了安裝了IPython，這將會是我們這系列教學的主要舞台，而今天我要教大家在這個舞台上利用Numpy來做一些簡單的科學計算。&lt;/p&gt;
&lt;h2&gt;IPython&lt;/h2&gt;
&lt;p&gt;像上次一樣，打開IPython，緊接著把numpy和pandas載入，載入numpy之後我們習慣用&lt;code&gt;as&lt;/code&gt;將它縮寫為&lt;code&gt;np&lt;/code&gt;，pandas則縮寫為&lt;code&gt;pd&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="ipython" src="http://www.ycc.idv.tw/media/PlayDataWithPython/ipython.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;IPython是一個具有互動式介面的python執行介面，你可以一邊寫一邊理解目前的狀況，舉個例子&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="c1"&gt;# integer(整數)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;     &lt;span class="c1"&gt;# check variable a&lt;/span&gt;
&lt;span class="mi"&gt;12&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;在第一行中，我令變數a為12，而第二行只要把變數a直接key出來，我們就可以立刻查看變數裡頭有什麼內容，注意喔！在一般的python語言中，直接把變數key出來這件事是沒有意義的，這只有在IPython上才有的方便功能，&lt;strong&gt;有了這樣一個互動式的介面，讓我們在處理數據的時候可以隨時查看，目前數據的狀況&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;Python常見的資料型別&lt;/h2&gt;
&lt;p&gt;Python常見的資料型別有整數(integer)、浮點數(floating-point number)、字串(string)、串列(list)、序對(tuple)、字典(dictionary …&lt;/p&gt;</summary><content type="html">&lt;p&gt;在上一次我們已經成功了安裝了IPython，這將會是我們這系列教學的主要舞台，而今天我要教大家在這個舞台上利用Numpy來做一些簡單的科學計算。&lt;/p&gt;
&lt;h2&gt;IPython&lt;/h2&gt;
&lt;p&gt;像上次一樣，打開IPython，緊接著把numpy和pandas載入，載入numpy之後我們習慣用&lt;code&gt;as&lt;/code&gt;將它縮寫為&lt;code&gt;np&lt;/code&gt;，pandas則縮寫為&lt;code&gt;pd&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="ipython" src="http://www.ycc.idv.tw/media/PlayDataWithPython/ipython.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;IPython是一個具有互動式介面的python執行介面，你可以一邊寫一邊理解目前的狀況，舉個例子&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="c1"&gt;# integer(整數)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;     &lt;span class="c1"&gt;# check variable a&lt;/span&gt;
&lt;span class="mi"&gt;12&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;在第一行中，我令變數a為12，而第二行只要把變數a直接key出來，我們就可以立刻查看變數裡頭有什麼內容，注意喔！在一般的python語言中，直接把變數key出來這件事是沒有意義的，這只有在IPython上才有的方便功能，&lt;strong&gt;有了這樣一個互動式的介面，讓我們在處理數據的時候可以隨時查看，目前數據的狀況&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;Python常見的資料型別&lt;/h2&gt;
&lt;p&gt;Python常見的資料型別有整數(integer)、浮點數(floating-point number)、字串(string)、串列(list)、序對(tuple)、字典(dictionary)，可以使用&lt;code&gt;type()&lt;/code&gt;來查詢資料型別。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="c1"&gt;# integer&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;40.0&lt;/span&gt; &lt;span class="c1"&gt;# float, 必須有&amp;#39;.&amp;#39;&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;word&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;# string&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# list&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;6&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# tuple&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;# dictionary&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;str&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;list&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;dict&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;list和tuple裡面可以塞入任意的資料型別，甚至可以塞入另外一個list，或是自己定義的物件，list和tuple其實非常的相似，差異只在於tuple一旦決定了就不能在變更，但是list卻可以。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;new&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;new&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# 取出第一項(index=0)加一再設定回去第一項&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;new&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 刪除index=1的那項&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;new&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;new&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# fail&lt;/span&gt;
&lt;span class="ne"&gt;AttributeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tuple&amp;#39;&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt; &lt;span class="n"&gt;has&lt;/span&gt; &lt;span class="n"&gt;no&lt;/span&gt; &lt;span class="n"&gt;attribute&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;append&amp;#39;&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="c1"&gt;# fail&lt;/span&gt;
&lt;span class="ne"&gt;TypeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tuple&amp;#39;&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt; &lt;span class="n"&gt;does&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;support&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="n"&gt;assignment&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# fail&lt;/span&gt;
&lt;span class="ne"&gt;TypeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tuple&amp;#39;&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt; &lt;span class="n"&gt;doesn&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;t support item deletion&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;在python中，整數和浮點數可以作簡單的四則運算&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;6.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;
&lt;span class="mf"&gt;2.0&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;  &lt;span class="c1"&gt;# 指數&lt;/span&gt;
&lt;span class="mi"&gt;9&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;一群整數做完運算輸出是整數，一群浮點數做完運算輸出是浮點數，那假如整數和浮點數混雜的情形呢？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="c1"&gt;# 整數除整數，結果必定是整是，是整數0，而不是想像中的0.5，這種運算效果有點像求商&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;
&lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="c1"&gt;# 只要有任意浮點數出現，整數強迫轉為浮點數，然後再做運算，這才是我們要的結果&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;
&lt;span class="mf"&gt;9.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;所以在運算之前，你要想清楚你想要的目標是什麼？如果你有一個整數變數&lt;code&gt;someInt&lt;/code&gt;接下來要作浮點數運算，可以使用&lt;code&gt;float(someInt)&lt;/code&gt;強制先轉成浮點數再做接下來的運算，這樣比較不會犯錯。&lt;/p&gt;
&lt;p&gt;事實上，轉成浮點數這樣的自動轉換在python中是很少見的，python是屬於&lt;strong&gt;強型別語言，所以型別和型別之間有很強的區份性，常常不會自動轉換&lt;/strong&gt;，如果需要轉換必須要作額外的操作。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;# fail&lt;/span&gt;
&lt;span class="ne"&gt;TypeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;unsupported&lt;/span&gt; &lt;span class="n"&gt;operand&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;int&amp;#39;&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;str&amp;#39;&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 使用int()將字串轉成整數&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;常見的型別轉換函數有&lt;code&gt;int()&lt;/code&gt;, &lt;code&gt;float()&lt;/code&gt;, &lt;code&gt;str()&lt;/code&gt;, &lt;code&gt;list()&lt;/code&gt;, &lt;code&gt;tuple()&lt;/code&gt;。所以如果要對一個tuple做更改，可以先轉成list再做運算。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 在index為0的地方插入整數4&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Numpy的數學運算&lt;/h2&gt;
&lt;p&gt;在上一段我簡單介紹了python內建的運算，在大多數情況，內建的運算就已經足夠應付了，不過如果遇到複雜的運算，例如：三角函數、取最大最小值、exp、log、開根號、矩陣運算，我們就需要用到 Numpy    。&lt;/p&gt;
&lt;p&gt;首先先介紹Numpy的一些數學運算，Numpy的數學運算詳細&lt;a href="https://docs.scipy.org/doc/numpy/reference/routines.math.html"&gt;參考這&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;我這邊舉幾個比較常見的例子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c1"&gt;# 加總&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c1"&gt;# 最大值&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c1"&gt;# 最小值&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 求餘數&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 求sin&lt;/span&gt;
&lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# ln 和 e&lt;/span&gt;
&lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Numpy基礎元素：ndarray&lt;/h2&gt;
&lt;p&gt;Numpy最重要的元素就是ndarray，它是N-Dimensional Array的縮寫，在Numpy裡，dimesions被稱為axes，而axes的數量被稱為rank，axes是一個重要的概念，了解這個概念基本上就把Numpy搞懂一半以上了。&lt;/p&gt;
&lt;p&gt;先來建立一個簡單1D的ndarray&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;從外到內第一個遇到的中括號就是axis=0，往內就遞增上去，所以從1到2再到3，這個方向就叫做axis=0，Numpy大部分的運算都支援陣列的運算，經常你需要限制要在哪個axis方向上作運算，舉個例子&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# axis為None的時候則加總所有元素&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# fail 因為A只有一維&lt;/span&gt;
&lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;axis&amp;#39;&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;bounds&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;另外，也可以由內往外數，最內部的第一個中括後就是axis=-1，越外面就越負。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;剛來上面的例子可能看不出效果，再來就稍微有趣一點，我們來看看2D的ndarray&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;   &lt;span class="c1"&gt;# [1+4, 2+5, 3+6]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;    &lt;span class="c1"&gt;# [1+2+3, 4+5+6]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;有看懂axis怎麼運作嗎？最外面的中括號是axis=0，它包含[1,2,3]和[4,5,6]兩個元素，方向就是從[1,2,3]到[4,5,6]的方向，在這個方向上做sum，所以結果就會得到[1+4, 2+5, 3+6]。若是axis=1則是第二層中括號，也就是1到3和4到5的方向，所以結果會是[1+2+3, 4+5+6]。&lt;/p&gt;
&lt;p&gt;一樣從內而外也可以，如果axis=None或defalut情形下，則是對矩陣內所有元素作運算。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;21&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# same as above&lt;/span&gt;
&lt;span class="mi"&gt;21&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 和axis=1等價&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;相信大家已經有感覺了，那3D也是一樣道理的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]],[[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;    &lt;span class="c1"&gt;# [1+7,  2+8,  3+9 ]&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;   &lt;span class="c1"&gt;# [4+10, 5+11, 6+12]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;    &lt;span class="c1"&gt;# [1+4, 2+5, 3+6]&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;   &lt;span class="c1"&gt;# [7+10, 8+11, 9+12]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;        &lt;span class="c1"&gt;# [1+2+3, 4+5+6]&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;       &lt;span class="c1"&gt;# [7+8+9, 10+11+12]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;畫張圖可能比較好理解一點，在各個方向上加總的結果都不一樣。&lt;/p&gt;
&lt;p&gt;&lt;img alt="ndarray axis" src="http://www.ycc.idv.tw/media/PlayDataWithPython/ndarray_axis.png"&gt;&lt;/p&gt;
&lt;p&gt;同樣，axis的概念也可以用在矩陣的shape&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;(3, 2)&lt;/code&gt;這樣的shape我們就一點都不意外了，axis=0有三個元素，而axis=1有兩個元素。shape可以直接改，如果數量恰當的話就會自動重組。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;axis=0有兩個元素，axis=1有一個元素，axis=2有三個元素。&lt;/p&gt;
&lt;p&gt;同樣的概念也可以用在取出單一元素上。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;在axis=0上選第二個元素(1)，在axis=1上選第一個元素(0)，在axis=2上選第二個元素(1)，所以選出來的元素就是5啦！&lt;/p&gt;
&lt;p&gt;有了axis的概念，我們來看另外一個重要的概念—dtype。&lt;/p&gt;
&lt;p&gt;ndarray有其資料型別，這個資料型別就稱為dtype，有哪些內建的資料型別呢？我們可以透過numpy的內建資料來查看。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sctypes&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;complex&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;complex64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;complex128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;complex256&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;float&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float128&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;int&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;others&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;unicode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;void&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;uint&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint64&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;有複數、浮點數、整數，另外每個資料型別還可以由資料的儲存容量大小來區分，例如：numpy.int32就代表是容量為32bits的整數。我們可以在設置ndarray的時候事先強迫設成某資料型別。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;int32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;
&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;int32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t2&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;
&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Numpy的矩陣運算&lt;/h2&gt;
&lt;p&gt;有了ndarray就可以作矩陣的運算了，矩陣運算有兩種系統，一種是element-wise(元素方面) operation，一種是matrix operation。&lt;/p&gt;
&lt;p&gt;這樣講好像很抽象，我來解釋一下，element-wise operation就是每個元素獨立運算，例如，以下例子就是element-wise的相加。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;      &lt;span class="c1"&gt;# element-wise plus&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;6.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A和B矩陣中同樣位置的元素相加，再放到新的矩陣中，這一種操作就叫做element-wise operation。&lt;/p&gt;
&lt;p&gt;在numpy中如果沒有特別指定，所有的運算都是這類的運算，我們來看一下減、乘和除。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;      &lt;span class="c1"&gt;# element-wise minus&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;      &lt;span class="c1"&gt;# element-wise multiply&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;      &lt;span class="c1"&gt;# element-wise divide&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;那我如果想要作矩陣操作(matrix operation)呢？譬如說矩陣內積，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 矩陣內積&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;  &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;15.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;還有更多的矩陣操作，&lt;/p&gt;
&lt;p&gt;矩陣轉置&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="c1"&gt;# 矩陣轉置&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;反矩陣&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A_rev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 反矩陣&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A_rev&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;A_rev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;  &lt;span class="mf"&gt;1.00000000e+00&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;0.00000000e+00&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;  &lt;span class="mf"&gt;8.88178420e-16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;1.00000000e+00&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A和A的反矩陣內積為單位矩陣，你有注意到&lt;code&gt;8.88178420e-16&lt;/code&gt;這個奇怪的數字嗎？這是因為python在計算的過程有一些誤差的緣故，所以才會產生一個這麼小的數字，但基本上可以看作是0。&lt;/p&gt;
&lt;p&gt;另外矩陣跟矩陣間也可以合併。&lt;/p&gt;
&lt;p&gt;垂直方向合併&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;V&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;V&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;水平方向合併&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;當然也可以分割矩陣，&lt;/p&gt;
&lt;p&gt;垂直方向分割&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vsplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;V&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# 2代表切兩份&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;水平方向分割&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hsplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# 4代表切四份&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;子彈總結&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python常見的資料型別：整數(integer)、浮點數(floating-point number)、字串(string)、串列(list)、序對(tuple)、字典(dictionary)&lt;/li&gt;
&lt;li&gt;ndarray的axes概念很重要，這會決定函數操作的方式，例如：np.sum&lt;/li&gt;
&lt;li&gt;ndarray的資料型別(dtype)，例如：'float64', 'int64', 'string', ...&lt;/li&gt;
&lt;li&gt;numpy的矩陣運算有element-wise operation和matrix operation兩種&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Numpy的基礎概念我們已經有了，在下一篇當中會再更深入的了解Numpy還有什麼進階的功能，包括：產生ndarray的多種方法、broadcast的概念以及ndarray的進階操作手法。&lt;/p&gt;</content><category term="Python玩數據"></category></entry><entry><title>大數據 Big Data:A Revolution That Will Transform How We Live, Work, and Think</title><link href="YCNote/big-data-a-revolution.html" rel="alternate"></link><published>2017-04-07T12:00:00+08:00</published><updated>2017-04-07T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-04-07:YCNote/big-data-a-revolution.html</id><summary type="html">&lt;p&gt;&lt;img alt="cover" src="http://www.ycc.idv.tw/media/Reading/BigData_pic.jpg"&gt;&lt;/p&gt;
&lt;p&gt;最近，Big Data這個詞相當的紅，但是對於這個詞我們還是有很多的誤會，一個常見的疑問是，究竟多大才可以稱得上是大數據呢？事實上，我接下來要介紹的這本書告訴你，大數據多「大」不是重點，重點是你怎麼看待和處理數據。&lt;/p&gt;
&lt;p&gt;「&lt;a href="http://www.books.com.tw/products/0010587258"&gt;大數據&lt;/a&gt;」這本書分為三個部分，在第一個部分，作者為讀者介紹大數據的三大思維變革，包括：採用全體數據取代抽樣數據、容忍資料的混雜特性、「是什麼」比「為什麼」還重要，第二部分則在講述大數據如何改變了商業、市場和社會的本質。第三部分在探討大數據會對人類產生什麼不好的影響，而我們如何去避免。本篇我主要著墨於第一部分和第二部分。&lt;/p&gt;
&lt;h2&gt;樣本=總體&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;大數據是指不採用統計「隨機採樣」這樣的捷徑，而直接處理所有的數據。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在資料分析中，如果要研究的對象（母群體）非常的龐大、資料量非常大，我們通常會採取「隨機採樣」來處理，這條捷徑在處理特定問題非常成功，也因此它成為現代社會、現代測量領域的主要路數，但這方式存在著一些缺陷。&lt;/p&gt;
&lt;p&gt;「隨機採樣」的缺陷之一是無法瞭解更深層次的細節。在宏觀領域起作用的方法在微觀領域失去了作用。隨機採樣就像印象派的畫作一樣 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="cover" src="http://www.ycc.idv.tw/media/Reading/BigData_pic.jpg"&gt;&lt;/p&gt;
&lt;p&gt;最近，Big Data這個詞相當的紅，但是對於這個詞我們還是有很多的誤會，一個常見的疑問是，究竟多大才可以稱得上是大數據呢？事實上，我接下來要介紹的這本書告訴你，大數據多「大」不是重點，重點是你怎麼看待和處理數據。&lt;/p&gt;
&lt;p&gt;「&lt;a href="http://www.books.com.tw/products/0010587258"&gt;大數據&lt;/a&gt;」這本書分為三個部分，在第一個部分，作者為讀者介紹大數據的三大思維變革，包括：採用全體數據取代抽樣數據、容忍資料的混雜特性、「是什麼」比「為什麼」還重要，第二部分則在講述大數據如何改變了商業、市場和社會的本質。第三部分在探討大數據會對人類產生什麼不好的影響，而我們如何去避免。本篇我主要著墨於第一部分和第二部分。&lt;/p&gt;
&lt;h2&gt;樣本=總體&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;大數據是指不採用統計「隨機採樣」這樣的捷徑，而直接處理所有的數據。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在資料分析中，如果要研究的對象（母群體）非常的龐大、資料量非常大，我們通常會採取「隨機採樣」來處理，這條捷徑在處理特定問題非常成功，也因此它成為現代社會、現代測量領域的主要路數，但這方式存在著一些缺陷。&lt;/p&gt;
&lt;p&gt;「隨機採樣」的缺陷之一是無法瞭解更深層次的細節。在宏觀領域起作用的方法在微觀領域失去了作用。隨機採樣就像印象派的畫作一樣，遠看很不錯，可以看見整個整體趨勢，但是一旦聚焦於某一點，就會變得模糊不清。&lt;/p&gt;
&lt;p&gt;另外，「隨機採樣」還有一個缺陷是缺乏延展性，人們只能從採樣數據中得出事先設計好的問題的結果——千萬不要奢求採樣的數據還能回答你突然意識到的其他問題，也就是調查得出的數據不能夠重新分析以實現計劃之外的目的。&lt;/p&gt;
&lt;p&gt;不過，在目前這個技術和資訊爆炸的時代，我們訊息量的增長速度比世界經濟的增長速度快4倍，而電腦數據處理能力的增長速度則比世界經濟的增長速度快9倍，也因此我們有更充沛的資料和處理資料的能力，所以是時候應該丟棄以往的「隨機採樣」，而直接採用「樣本=總體」的方式。&lt;/p&gt;
&lt;p&gt;Xoom是一個專門從事跨境匯款業務的公司。2011年，它注意到用「發現卡」從新紐澤西州匯款的交易量比正常情況多一些，系統於是啟動警報。Xoom公司的CEO John Kunze(約翰·孔澤) 解釋說：「這個系統關注的是不應該出現的情況。」單獨來看，每筆交易都是合法的，但是事實證明這是一個犯罪集團在試圖詐騙。而要能發現異常的唯一方法是，需要檢查所有的數據，找出「隨機採樣」分析法所獲取不到的訊息。&lt;/p&gt;
&lt;p&gt;另外一個例子，Lytro相機，它把大數據運用到了基本的攝影中。與傳統相機只可以記錄一束光不同，Lytro相機可以記錄整個光場裡所有的光，可以達到1100萬束之多。具體生成什麼樣的照片則可以在拍攝之後再依照需要決定。用戶沒必要在一開始就聚焦，因為該相機可以捕捉到所有的數據，所以之後可以選擇聚焦圖像中的任一一點。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大數據所謂的「大」，並不是指數據量有多大，而是指如何處理數據的方法，直接處理「樣本=總體」，而非傳統的「隨機採樣」，我們將得到更多的細節，做更多的事。&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;允許不精確&lt;/h2&gt;
&lt;p&gt;對於採取隨機取樣的小數據而言，保證每筆資料的質量是相當重要的，為了使結果更加準確，很多科學家都致力於優化測量工具。不過，面對大數據的時候，我們可能增加不少不正確的資料，正因為我們無法逐一的檢查，甚至在資料的格式上也難以統一，因此大數據本身就具有混雜的特性。&lt;/p&gt;
&lt;p&gt;不過這混雜所造成的不準確也可以因為數據量大而彌補，事實上，&lt;strong&gt;大數據的簡單演算法比小數據的複雜演算法更為有效&lt;/strong&gt;，舉個例子，在冷戰時期，美國掌握了大量關於蘇聯的各種資料，但缺少翻譯這些資料的人手。所以，計算機翻譯也成了急需解決的問題。那個時候的科學家想藉由結合文法規則和字典來創造一個翻譯機器， 最後卻失敗了，他們發現機器翻譯不能只是讓電腦熟悉常用規則，還必須教會電腦處理「特殊的」語言情況。畢竟，翻譯不僅僅只是記憶和複述，也涉及選詞，而明確地教會電腦這些是非常困難的。&lt;/p&gt;
&lt;p&gt;時間拉回到現代，Google翻譯則採取另外一種方式，Google翻譯系統不由程式設計師直接告訴計算機要怎麼做，而是靠著資料來訓練計算機學習怎麼做，計算機會盡量吸收它能找到的所有翻譯文本，從各式各樣語言的公司網站上尋找對譯的文檔，還會去尋找聯合國和歐盟這些國際組織發佈的官方文件和報告的譯本，藉由這大量的數據去預測對譯詞語應該是什麼，&lt;strong&gt;然而儘管其輸入來源很混亂，但相較於其他翻譯系統而言，Google的翻譯質量相對而言還是最好的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;要想獲得大規模數據帶來的好處，混亂應該是一種標準途徑，而不應該去竭力避免，不過數據量一旦大，這些混亂所帶來的不精確將被彌補。&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;「是什麼」比「為什麼」還重要&lt;/h2&gt;
&lt;p&gt;大數據利用數值方法，他可以看到人類不容易看出來的相關性，兩件事雖然擁有相關性，但並不代表他們擁有因果關係，但是在大部分時間裡，相關性比因果關係更為重要。&lt;/p&gt;
&lt;p&gt;美國折扣零售商塔吉特（Target）使用大數據的相關性分析已經有很多年了。《紐約時報》的記者杜西格（Charles Duhigg）就在一份報道中闡述了塔吉特公司怎樣在完全不和准媽媽對話的前提下預測一個女性會在什麼時候懷孕。塔吉特公司注意到，資料上的婦女會在懷孕大概第三個月的時候買很多無香乳液。幾個月之後，她們會買一些營養品，比如鎂、鈣、鋅。公司最終找出了大概20多種關聯項目，這些關聯項目可以給顧客進行「懷孕趨勢」評分。杜西格在《習慣的力量》（The Power of Habit）一書中講到了接下來發生的事情。一天，一個男人衝進了一家位於明尼阿波利斯市郊的塔吉特商店，要求經理出來見他。他氣憤地說：「我女兒還是高中生，你們卻給她郵寄嬰兒服和嬰兒床的優惠券，你們是在鼓勵她懷孕嗎？」而當幾天後，經理打電話向這個男人致歉時，這個男人的語氣變得平和起來。他說：「我跟我的女兒談過了，她的預產期是8月份，是我完全沒有意識到這個事情的發生，應該說抱歉的人是我。」 &lt;/p&gt;
&lt;p&gt;在上述的例子，我們雖然不見得可以找出這20項關聯項和懷孕之間的因果關係，不過他們確實相關，所以我們可以用來預測。&lt;strong&gt;有些時候我們只需要知道「是什麼」就夠了，沒必要知道「為什麼」。&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;大數據時代的商業變革&lt;/h2&gt;
&lt;p&gt;Matthew Fontaine Maury是一位前途看好的美國海軍軍官，1839年，卻意外的出了車禍，使得他無法繼續在海上工作，不過危機就是轉機，在近三年的休養，美國海軍將他安排進辦公室，讓他負責修復陳舊的圖表和儀器，他在其中挖到了寶，那是一批航海日記，日記裡頭詳細的記載了特定時間在特定地點的風、水和天氣狀況，Maury意識到如果把這些資料整理起來，將會呈現一張全新的航海圖，這些數據將會比大家口耳相傳的經驗還有用，後來也證明Maury是對的，這資料幫助船長們省去了三分之一左右的航程，後來全世界第一條跨越大西洋的電報電纜也是建基在這個基礎之上。&lt;/p&gt;
&lt;p&gt;數據就像是一座鑽石礦，透過分析我們可以將其中的鑽石給掏出，事實上這金礦無所不在，數據可能藏於書籍或網路文本、數據可能藏於方位、數據可能藏於溝通網絡、數據可能藏於微型運動感測器，仔細留意，數據幾乎無所不在，什麼都可以量化，有了大數據的思維，我們不會再把世界看成只有單純是自然現象或是社會現象，我們會意識到世界的本質就是由眾多信息所構成的，而這會帶來的是一場商業上的變革。&lt;/p&gt;
&lt;p&gt;作者認為大數據時代，依照提供價值不同，分別會出現三類的大數據公司，第一種是擁有大量數據的公司，第二種是擁有技能挖掘數據的公司，最後一種是提供嶄新大數據思維的公司，能從數據中創造出意想不到的價值，第三種是作者最為推崇的，作者列了幾種數據創新的方法。&lt;/p&gt;
&lt;p&gt;作者提了五種數據創新方法，第一種是&lt;strong&gt;數據再利用&lt;/strong&gt;，有許多數據因為儲存成本低而被保存下來，不過沒有被充分的利用，數據科學家稱之為「數據墳場」，從這墳場中我們可以盜到很多的寶，就像Maury從航海日記撈出了許多有用的資訊一樣。&lt;/p&gt;
&lt;p&gt;第二種是&lt;strong&gt;數據間的整合&lt;/strong&gt;，丹麥同時擁有從1985年起的手機用戶數據庫和該國所有癌症患者的資訊，有人想到如果整合這兩者資訊，研究人員可以研究手機用戶是不是比非手機用戶顯示出更容易得癌症，最後，研究結果沒有發現這兩者存在著相關性。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;隨著大數據的出現，數據的總和比部分更有價值。當我們將多個數據集的總和重組在一起時，重組總和本身的價值也比單個總和更大&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第三種是&lt;strong&gt;具可擴張性的數據&lt;/strong&gt;，要使得數據可以一再的利用，我們必須在一開始就設計好他的可擴張性，也就是要盡可能的一次蒐集所有資料齊全，舉個知名的例子，Google街景拍攝，其備受爭議的街景汽車不僅僅拍攝房屋和道路的照片，他還同時採集了每個位置的GPS數據，甚至還加入了無線網路名稱的蒐集，一輛Google街景車每時每刻都在累積大量的各方面的數據，而這些資訊可能在目前用不到，不過未來的某天可能會用到，花一次的錢可以得到更多的好處。&lt;/p&gt;
&lt;p&gt;第四種是&lt;strong&gt;必須考慮數據的折舊&lt;/strong&gt;，譬如你在亞馬遜十年前買一本書的資訊，一定不會比昨天剛購買的資訊重要，所以資料還必須考慮它隨時間下降的重要程度。&lt;/p&gt;
&lt;p&gt;第五種是&lt;strong&gt;數據廢氣能回收再利用&lt;/strong&gt;，什麼是數據廢氣呢？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一個用來描述人們在網上留下的數字軌跡的藝術詞彙出現了，這就是「數據廢氣」。它是用戶在線交互的副產品，包括瀏覽了哪些頁面、停留了多久、滑鼠光標停留的位置、輸入了什麼信息等。許多公司因此對系統進行了設計，使自己能夠得到數據廢氣並循環利用，以改善現有的服務或開發新服務。 &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Google就是這方面的高手，例如錯誤拼寫校正，Google擁有世界上最完整的拼寫檢查器，基本上涵蓋了各種語言，而且Google幾乎免費的獲得這種能力，它依據每天處理的30億個錯誤拼寫的查詢，一個巧妙的反饋系統可以讓用戶告訴Google他其實是想輸入什麼字，當搜尋頁面頂部顯示「你要找的是不是：流行病學」時，如果是的話，你將會點選並讓Google了解你真正想查的字詞，原本輸入錯誤這樣的數據廢氣卻被巧妙的回收再利用來優化它的系統。&lt;/p&gt;
&lt;h2&gt;全息社會&lt;/h2&gt;
&lt;p&gt;大數據正在慢慢影響這個社會，包括我們的知識取得方式，包括我們的社交活動，甚至在未來會決定人類很多的決策，大至公司策略發展，小至個人理財規劃，確實，大數據和機器學習的引入可能會取代掉許多目前的工作，不過也同時會創造更多新的工作內容，讓人類可以盡情發揮潛能，把更多的精力放在創造之上，如果亨利·福特問大數據他的顧客想要的是什麼，大數據將會回答，「一匹更快的馬。」在全息社會中，包括創意、直覺、冒險精神和知識野心在內的人類特性的培養顯得尤為重要，人類的進步正是源自我們的獨創性。&lt;/p&gt;</content></entry><entry><title>機器學習技法 學習筆記 (5)：Boost Aggregation Models</title><link href="YCNote/ml-course-techniques_5.html" rel="alternate"></link><published>2017-04-02T12:00:00+08:00</published><updated>2017-04-02T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-04-02:YCNote/ml-course-techniques_5.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋AdaBoost (Adaptive Boost)、Gradient Boost、AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Boost的精髓&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們介紹的Aggregation Models都屬於沒有Boost的，不管是Bagging或Decision Tree都沒有要試著在Training的過程中改善Model，&lt;strong&gt;而這篇將要提到的Boost方法，則是在產生每個g&lt;sub&gt;t&lt;/sub&gt;時試圖讓Model整體更完善，更能發揮Aggregation Models中截長補短中的「補短」的效果，也就是說g&lt;sub&gt;t&lt;/sub&gt;可以彼此互補不足之處&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那實際上我們應該怎麼做才能實踐Boost呢？其實方法的道理早就透漏在上一回中的Bagging和Decision Tree裡頭了，不管是Bagging和Decision Tree都是使用變換Data來做到變異度，在這個方法下Model的架構可以本身是不變的，這帶來相當的便利性，而今天我們要講的Boost也同樣的利用「變換Data」來做到變異度，但不同的是Boost的過程中「變換Data」這件事是有目標性的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boost方法在「變換Data」時會試著去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的g&lt;sub&gt;t&lt;/sub&gt;可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善 …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋AdaBoost (Adaptive Boost)、Gradient Boost、AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Boost的精髓&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們介紹的Aggregation Models都屬於沒有Boost的，不管是Bagging或Decision Tree都沒有要試著在Training的過程中改善Model，&lt;strong&gt;而這篇將要提到的Boost方法，則是在產生每個g&lt;sub&gt;t&lt;/sub&gt;時試圖讓Model整體更完善，更能發揮Aggregation Models中截長補短中的「補短」的效果，也就是說g&lt;sub&gt;t&lt;/sub&gt;可以彼此互補不足之處&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那實際上我們應該怎麼做才能實踐Boost呢？其實方法的道理早就透漏在上一回中的Bagging和Decision Tree裡頭了，不管是Bagging和Decision Tree都是使用變換Data來做到變異度，在這個方法下Model的架構可以本身是不變的，這帶來相當的便利性，而今天我們要講的Boost也同樣的利用「變換Data」來做到變異度，但不同的是Boost的過程中「變換Data」這件事是有目標性的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boost方法在「變換Data」時會試著去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的g&lt;sub&gt;t&lt;/sub&gt;可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善，這就是Boost的主要精髓。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;AdaBoost (Adaptive Boost) for Classification&lt;/h3&gt;
&lt;p&gt;剛剛上一段的最後我已經揭露了Boost的真正精髓，拿這樣的概念來做分類問題，就是我們接下來要談的AdaBoost，全名稱為Adaptive Boost。&lt;/p&gt;
&lt;p&gt;在分類問題中我們怎麼做到「凸顯原先做錯的Data」？簡單的想法是這樣的，我們可以減少原本已經是正確分類的Data的數量，然後增加原本錯誤分類的Data的數量，&lt;strong&gt;增減Data的數量其實是等效於改變每筆Data的權重&lt;/strong&gt;，假如我們給每筆資料權重，要做的事是拉低正確分類Data的權重，而且拉高錯誤分類Data的權重。&lt;/p&gt;
&lt;p&gt;那我們應該要提升權重或降低權重到什麼程度才是OK的呢？換個方式思考，我們為什麼要去調整權重？目的其實是要去凸顯原先做錯的部分，降低原本做對的部分，也就是想&lt;strong&gt;藉由調整每筆Data的多寡或權重來做到「弭平原先的預測性」，最好可以讓原本的預測方法看起來是隨機分布&lt;/strong&gt;，也就是「錯誤率＝正確率」，讓它像是擲銅板一樣，沒有什麼預測能力。&lt;/p&gt;
&lt;p&gt;&lt;img alt="AdaBoost" src="https://dl.dropbox.com/s/n61vejw6rs6f9nm/MachineLearningTechniques.012.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;有了概念之後，我們來看實際應該要怎麼做？見上圖說明，首先我們需要先將Data權重u&lt;sup&gt;(1)&lt;/sup&gt;先初始化，接下來就可以開始找g&lt;sub&gt;t&lt;/sub&gt;了，我們使用任意一個分類問題的Model搭配上Data的權重，求得一組g&lt;sub&gt;t&lt;/sub&gt;，接下來計算這組g&lt;sub&gt;t&lt;/sub&gt;的&lt;strong&gt;「錯誤率」ε&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ε&lt;sub&gt;t&lt;/sub&gt;= 𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt; ⟦y&lt;sub&gt;n&lt;/sub&gt;≠g&lt;sub&gt;t&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;)⟧ / 𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有注意到考慮「錯誤率」ε&lt;sub&gt;t&lt;/sub&gt;的時候必須要評估u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt;，要記得會有Data權重是為了表示增加或減少原本的Data的數量，所以依照每筆Data的出現機會不同，會有不同的權重，也就會有對「錯誤率」不同的貢獻程度。&lt;/p&gt;
&lt;p&gt;那為了待會要對權重重新分配，我們先定義了β&lt;sub&gt;t&lt;/sub&gt;，在未來我會將錯誤的Data的權重乘上β&lt;sub&gt;t&lt;/sub&gt;，即u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t+1)&lt;/sup&gt;=u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt;×β&lt;sub&gt;t&lt;/sub&gt;，並且把正確的Data權重除以β&lt;sub&gt;t&lt;/sub&gt;，即u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t+1)&lt;/sup&gt;=u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt;/β&lt;sub&gt;t&lt;/sub&gt;，&lt;strong&gt;而期望的結果是重新分配的Dataset在g&lt;sub&gt;t&lt;/sub&gt;的預測下可以表現的像隨機的一樣，於是乎下一次使用這組Dataset訓練出來的g&lt;sub&gt;t+1&lt;/sub&gt;將會彌補g&lt;sub&gt;t&lt;/sub&gt;的不足&lt;/strong&gt;，根據這樣的原則我們來推一下β&lt;sub&gt;t&lt;/sub&gt;，&lt;/p&gt;
&lt;p&gt;𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t+1)&lt;/sup&gt; ⟦y&lt;sub&gt;n&lt;/sub&gt;≠g&lt;sub&gt;t&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;)⟧ / 𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t+1)&lt;/sup&gt;=1/2 (預測能力像隨機分布)&lt;/p&gt;
&lt;p&gt;⇒  𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t+1)&lt;/sup&gt; ⟦y&lt;sub&gt;n&lt;/sub&gt;≠g&lt;sub&gt;t&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;)⟧ = 𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t+1)&lt;/sup&gt; ⟦y&lt;sub&gt;n&lt;/sub&gt;=g&lt;sub&gt;t&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;)⟧ &lt;/p&gt;
&lt;p&gt;⇒  𝚺&lt;sub&gt;n&lt;/sub&gt; (u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt;×β&lt;sub&gt;t&lt;/sub&gt;)  ⟦y&lt;sub&gt;n&lt;/sub&gt;≠g&lt;sub&gt;t&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;)⟧ = 𝚺&lt;sub&gt;n&lt;/sub&gt; (u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt;/β&lt;sub&gt;t&lt;/sub&gt;) ⟦y&lt;sub&gt;n&lt;/sub&gt;=g&lt;sub&gt;t&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;)⟧ &lt;/p&gt;
&lt;p&gt;⇒  β&lt;sub&gt;t&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; = 𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt; ⟦y&lt;sub&gt;n&lt;/sub&gt;=g&lt;sub&gt;t&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;)⟧ / 𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt;  ⟦y&lt;sub&gt;n&lt;/sub&gt;≠g&lt;sub&gt;t&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;)⟧&lt;/p&gt;
&lt;p&gt;⇒  β&lt;sub&gt;t&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; = [𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt; ⟦y&lt;sub&gt;n&lt;/sub&gt;=g&lt;sub&gt;t&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;)⟧ /  𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt;]/ [𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt;  ⟦y&lt;sub&gt;n&lt;/sub&gt;≠g&lt;sub&gt;t&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;)⟧ / 𝚺&lt;sub&gt;n&lt;/sub&gt; u&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;(t)&lt;/sup&gt; ]&lt;/p&gt;
&lt;p&gt;⇒  &lt;strong&gt;β&lt;sub&gt;t&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; = 1-ε&lt;sub&gt;t&lt;/sub&gt; / ε&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以我們就可以利用這個β&lt;sub&gt;t&lt;/sub&gt;來更新我的Data權重，並且在多次迭代後，得到很多個g&lt;sub&gt;t&lt;/sub&gt;。而將來我們會把所有的g&lt;sub&gt;t&lt;/sub&gt;做線性組合，而我們希望&lt;strong&gt;「錯誤率」越低的g&lt;sub&gt;t&lt;/sub&gt;可以有更高的貢獻度α&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;，所以使用β&lt;sub&gt;t&lt;/sub&gt;緊接著計算「g&lt;sub&gt;t&lt;/sub&gt;的權重」α&lt;sub&gt;t&lt;/sub&gt;，定義為&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;α&lt;sub&gt;t&lt;/sub&gt; = ln(βt)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以當一個百分之一百可以完全預測的g&lt;sub&gt;t&lt;/sub&gt;出現時，它的ε&lt;sub&gt;t&lt;/sub&gt;=0，此時它的β&lt;sub&gt;t&lt;/sub&gt; →∞，同時α&lt;sub&gt;t&lt;/sub&gt; →∞，所以這樣的g&lt;sub&gt;t&lt;/sub&gt;會有完全的貢獻。&lt;/p&gt;
&lt;p&gt;如果一個預測效果很差的g&lt;sub&gt;t&lt;/sub&gt;出現，它的ε&lt;sub&gt;t&lt;/sub&gt;=1/2，此時它的β&lt;sub&gt;t&lt;/sub&gt;=1，同時α&lt;sub&gt;t&lt;/sub&gt;=0，所以這樣的g&lt;sub&gt;t&lt;/sub&gt;並沒有任何參考價值。&lt;/p&gt;
&lt;p&gt;那如果出現一個g&lt;sub&gt;t&lt;/sub&gt;它的ε&lt;sub&gt;t&lt;/sub&gt; &amp;gt; 1/2，那這樣的g&lt;sub&gt;t&lt;/sub&gt;並不能說它沒有用處，反而是一個很好的反指標，我們只需要反著看就好了，當ε&lt;sub&gt;t&lt;/sub&gt; &amp;gt; 1/2時，β&lt;sub&gt;t&lt;/sub&gt; &amp;lt; 1，所以α&lt;sub&gt;t&lt;/sub&gt; &amp;lt; 0，這樣的g&lt;sub&gt;t&lt;/sub&gt;具有逆向的貢獻。&lt;/p&gt;
&lt;p&gt;最後只要把這些訓練好的g&lt;sub&gt;t&lt;/sub&gt;乘上各自的α&lt;sub&gt;t&lt;/sub&gt;再加總起來，我們就完成了AdaBoost啦！&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Gradient Boost for Regression&lt;/h3&gt;
&lt;p&gt;剛剛我們講了AdaBoost，是個很神奇的方法，當我們做錯了，沒關係！從哪裡跌倒就從哪裡站起來，利用這種精神我們就可以做到Boost的效果，但美中不足的是上面的方法只能用在「分類問題」上，那如果我也想在「Regression問題」也做到Boost呢？這就是接下來要講的GradientBoost的方法。&lt;/p&gt;
&lt;p&gt;在課程中林軒田教授是從AdaBoost出發經過推導後，得到一個很像是Gradient Decent的式子，接下來將式子一般化成為可以使用任意Error Measure的形式，我稍微列一下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;GradientBoost: min&lt;sub&gt;η&lt;/sub&gt; min&lt;sub&gt;h&lt;/sub&gt; (1/N) 𝚺&lt;sub&gt;n&lt;/sub&gt; err[𝚺&lt;sub&gt;τ=1~t-1&lt;/sub&gt; α&lt;sub&gt;τ&lt;/sub&gt; g&lt;sub&gt;τ&lt;/sub&gt;(x&lt;sub&gt;n&lt;/sub&gt;) + η h(x&lt;sub&gt;n&lt;/sub&gt;), y&lt;sub&gt;n&lt;/sub&gt;]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們這邊會考慮err為平方誤差(s-y)&lt;sup&gt;2&lt;/sup&gt;的結果，詳細的推導這邊就不多加討論，可以到影片中學習，這裡我想要從我觀察出來的觀點，概念性的來看這個GradientBoost的方法。&lt;/p&gt;
&lt;p&gt;「從哪裡跌倒就從哪裡站起來」就是Boost的精神，所以今天你有一個Regression問題沒做好，&lt;strong&gt;留下了餘數Residual，怎麼辦？那我就把這個餘數當作另外一個Regression問題來做它&lt;/strong&gt;，再把這個結果附到先前的那個就好啦！如果第一次Regression後的Model是g&lt;sub&gt;1&lt;/sub&gt;(x)，那剩下的沒做好的餘數就應該是y(x)-g&lt;sub&gt;1&lt;/sub&gt;(x)，我們拿這個餘數下去在做一次Regression得到另外一個Model g&lt;sub&gt;2&lt;/sub&gt;(x)，此時合併這兩個結果的餘數就變成了y(x)-g&lt;sub&gt;1&lt;/sub&gt;(x)-g&lt;sub&gt;2&lt;/sub&gt;(x)，就可以使用這個餘數繼續做下去，最後組合所有的g&lt;sub&gt;t&lt;/sub&gt;(x)就會得到一個更好的Model。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gradient Boost" src="https://dl.dropbox.com/s/dy5xu7ifew5dfn4/MachineLearningTechniques.013.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;依循這樣的概念我們來看GradientBoost作法，如上圖，一開始我們先初始化每一筆Data的預測值s&lt;sub&gt;n&lt;/sub&gt;為0，再接下來開始產生g&lt;sub&gt;t&lt;/sub&gt;，我們先把Data的 y&lt;sub&gt;n&lt;/sub&gt; 減去每一筆Data當前的預測值s&lt;sub&gt;n&lt;/sub&gt;，就會產生餘數(y&lt;sub&gt;n&lt;/sub&gt;-s&lt;sub&gt;n&lt;/sub&gt;)，當然，在一開始s&lt;sub&gt;n&lt;/sub&gt;=0，所以y&lt;sub&gt;n&lt;/sub&gt;-s&lt;sub&gt;n&lt;/sub&gt;=y&lt;sub&gt;n&lt;/sub&gt;，等於是對原問題求解。&lt;/p&gt;
&lt;p&gt;接下來因為最後我們要線性組合g&lt;sub&gt;t&lt;/sub&gt;(x)，所以需要決定g&lt;sub&gt;t&lt;/sub&gt;(x)前面的係數α&lt;sub&gt;t&lt;/sub&gt;，也就是貢獻度，這個α&lt;sub&gt;t&lt;/sub&gt;的決定方式是去求解一個One-Variable-Linear-Regression (單變數線性迴歸)，目的是&lt;strong&gt;去縮放g&lt;sub&gt;t&lt;/sub&gt;(x)使得它更接近剛剛的餘數(y&lt;sub&gt;n&lt;/sub&gt;-s&lt;sub&gt;n&lt;/sub&gt;)，而找到這個縮放值就是α&lt;sub&gt;t&lt;/sub&gt;&lt;/strong&gt;。所以每一次g&lt;sub&gt;t&lt;/sub&gt;(x)的產生都是為了可以把G(x)描述的更好，最後G(x)=𝚺&lt;sub&gt;t&lt;/sub&gt; α&lt;sub&gt;t&lt;/sub&gt;g&lt;sub&gt;t&lt;/sub&gt;(x)。&lt;/p&gt;
&lt;p&gt;看到這裡有人一定會認為One-Variable-Linear-Regression求α&lt;sub&gt;t&lt;/sub&gt;這一步是多餘的，因為在一開始做{x&lt;sub&gt;n&lt;/sub&gt;,y&lt;sub&gt;n&lt;/sub&gt;-s&lt;sub&gt;n&lt;/sub&gt;}的Regression中我們已經最佳化過g&lt;sub&gt;t&lt;/sub&gt;(x)，那為什麼還要把g&lt;sub&gt;t&lt;/sub&gt;(x)乘上α&lt;sub&gt;t&lt;/sub&gt;再做同樣的事呢？α&lt;sub&gt;t&lt;/sub&gt;一定是1的啊！就像我一開始舉的例子一樣啊！其實問題就出在於你把g&lt;sub&gt;t&lt;/sub&gt;(x)理所當然的看成是線性模型，你才會覺得這一步是多餘的，如果g&lt;sub&gt;t&lt;/sub&gt;(x)不是線性的，求α&lt;sub&gt;t&lt;/sub&gt;就很重要的，因為你要使用線性組合來組出G(x)，但是你的g&lt;sub&gt;t&lt;/sub&gt;(x)不是線性的，所以你只好在外面再用線性模型來包裝一遍。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)&lt;/h3&gt;
&lt;p&gt;&lt;img alt="AdaBoosted and GrandientBoosted DTree" src="https://dl.dropbox.com/s/zm43nardbpkyr4n/MachineLearningTechniques.014.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;和Random Forest一樣，我們也可以將AdaBoost和GradientBoost套用到Decision Tree上面，&lt;strong&gt;如果是處理分類問題就使用AdaBoosted Decision Tree；那如果是處理Regression問題可以使用Gradient Boosted Decision Tree&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;但要特別注意的是，這邊的Decision Tree都必須是弱的，也就是Pruning過後的樹，如果直接使用完全長成的樹，你會發現在AdaBoosted Decision Tree中，因為ε&lt;sub&gt;t&lt;/sub&gt;=0所以α&lt;sub&gt;t&lt;/sub&gt;→∞；在Gradient Boosted Decision Tree中，y&lt;sub&gt;n&lt;/sub&gt;-s&lt;sub&gt;n&lt;/sub&gt;→0，因為錯誤出現的太少了，所以造成我們不能真正使用到Boost的效果，也就失去做Boost的意義了，&lt;strong&gt;因此在做AdaBoosted Decision Tree或Gradient Boosted Decision Tree時要使用「弱」一點的Decision Tree&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;這一篇當中，我們完整提了Boost的方法，Boost的精神就是從哪裡跌倒就從哪裡站起來，使用變換Data權重的手法去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的g&lt;sub&gt;t&lt;/sub&gt;可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善。&lt;/p&gt;
&lt;p&gt;我們提了兩種Boost的方法，如果是處理分類問題就使用AdaBoost；如果是處理Regression問題可以使用GradientBoost，而且這兩種方法都可以和Decision Tree做結合。&lt;/p&gt;
&lt;p&gt;以上兩回，我們已經完成了Aggregation Models了，接下來的下一回將要探討的就是現今很流行的類神經網路和深度學習等等。&lt;/p&gt;</content><category term="機器學習技法"></category></entry><entry><title>輕鬆談演算法的複雜度分界：什麼是P, NP, NP-Complete, NP-Hard問題</title><link href="YCNote/algorithm-complexity-theory.html" rel="alternate"></link><published>2017-03-30T12:00:00+08:00</published><updated>2017-03-30T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-03-30:YCNote/algorithm-complexity-theory.html</id><summary type="html">&lt;p&gt;在寫程式的時候，會聽到有人說這些問題是NP-Complete問題，或說這些是P問題，那這到底是什麼東西？其實這就是一套定義演算法複雜度的方法，今天我就想帶大家來聊聊這個艱澀但有趣的話題。&lt;/p&gt;
&lt;h3&gt;Turing Machine&lt;/h3&gt;
&lt;p&gt;我們先從 &lt;a href="https://en.wikipedia.org/wiki/Turing_machine"&gt;Turing machine&lt;/a&gt;（圖靈機）開始講起，Turing machine是現代電腦的基本雛型，是英國數學家圖靈（Alan Turing）於1936年提出的一種抽象計算模型，這個計算模型在猜想上可以「計算所有在演算法中可計算的問題」，也就是可以解決人類所有可解的問題，這個猜想稱之為 &lt;a href="https://en.wikipedia.org/wiki/Church–Turing_thesis"&gt;Church–Turing thesis&lt;/a&gt;（thesis代表假設或猜想），僅管目前還無法證明這個猜想，但是目前為止它幾乎完全被接受。&lt;/p&gt;
&lt;p&gt;簡單的談一下 Turing machine的基本架構，首先我們需要一個磁帶，這一條磁帶上面可以一格一格的填入一些 symbols，這可以是單純的 0/1 symbols 或者更多種類的 symbols，但這些 symbols 的數量必須是有限的，而這個symbols就可以當作我的輸入，接下來我需要一個讀寫頭，這個讀寫頭會在磁帶上讀取或寫入 symbol，或左右移動，另外這個讀寫頭存有一個 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;在寫程式的時候，會聽到有人說這些問題是NP-Complete問題，或說這些是P問題，那這到底是什麼東西？其實這就是一套定義演算法複雜度的方法，今天我就想帶大家來聊聊這個艱澀但有趣的話題。&lt;/p&gt;
&lt;h3&gt;Turing Machine&lt;/h3&gt;
&lt;p&gt;我們先從 &lt;a href="https://en.wikipedia.org/wiki/Turing_machine"&gt;Turing machine&lt;/a&gt;（圖靈機）開始講起，Turing machine是現代電腦的基本雛型，是英國數學家圖靈（Alan Turing）於1936年提出的一種抽象計算模型，這個計算模型在猜想上可以「計算所有在演算法中可計算的問題」，也就是可以解決人類所有可解的問題，這個猜想稱之為 &lt;a href="https://en.wikipedia.org/wiki/Church–Turing_thesis"&gt;Church–Turing thesis&lt;/a&gt;（thesis代表假設或猜想），僅管目前還無法證明這個猜想，但是目前為止它幾乎完全被接受。&lt;/p&gt;
&lt;p&gt;簡單的談一下 Turing machine的基本架構，首先我們需要一個磁帶，這一條磁帶上面可以一格一格的填入一些 symbols，這可以是單純的 0/1 symbols 或者更多種類的 symbols，但這些 symbols 的數量必須是有限的，而這個symbols就可以當作我的輸入，接下來我需要一個讀寫頭，這個讀寫頭會在磁帶上讀取或寫入 symbol，或左右移動，另外這個讀寫頭存有一個 state，這個 state 會隨著狀況改變，然後我就利用 symbol 和 state 來建立一個規則表，舉個例子，譬如說：初始的 state 是 q0，如果讀寫頭在 q0 的情況下讀到 symbol 0，就寫入 symbol 1，並且向右移動3格，並且改變 state q0 為 q1，... 等等，藉由規則來完成我們想做的運算，最後最重要的是它必須有一個 halt state 讓機器知道已經計算完畢了。Turing machine 不僅僅在理論上可以做任何的計算，而更有價值的是 Turing machine 的架構是有辦法用物理的方式來製造的，所以才會有現代電腦這玩意兒。&lt;/p&gt;
&lt;p&gt;說到電腦，更嚴謹地說，我們當今的電腦架構是比較接近 &lt;a href="https://en.wikipedia.org/wiki/Turing_machine"&gt;deterministic Turing machine&lt;/a&gt; (DTM)，和它對比的是 &lt;a href="https://en.wikipedia.org/wiki/Non-deterministic_Turing_machine"&gt;non-deterministic Turing machine&lt;/a&gt; (NTM)，我來好好的解釋一下，deterministic 的中文稱為決定性，所以 non-deterministic 就是非決定性，如果給予 Turing machine 某個 state 和某個 symbol 下它的下一步如果只有一種可能，那我們就稱它為 deterministic Turing machine (DTM)，所以上述的讀取頭每次就依照當下特定的 symbol 和 state 然後「決定」下一步應該要怎麼動作。&lt;/p&gt;
&lt;p&gt;但是 non-deterministic Turing machine (NTM) 就不拘於此，針對某個 state 和某個 symbol 它的下一步可能會有很多種，它會是一個分支，它可能同時要向右移3格，又同時要向左移動2格，所以你可以想像一下你的讀寫頭一分為二，然後再各自進行自己的任務，這個分支可以有無限多個，只要最後有某個分支到達 halt state，我們就解完問題了，這就是 non-deterministic Turing machine (NTM)。&lt;/p&gt;
&lt;p&gt;顯而易見的，DTM 只是 NTM 的特例，所以 NTM 比 DTM 擁有更快的計算速度，但這裡不要誤會喔！不管是 DTM 和 NTM 能解的問題是一樣多的，而且在數學上可以將 NTM 轉換成 DTM，只是它們解決相同問題所用到的時間複雜度不一樣，不過這就很關鍵。&lt;/p&gt;
&lt;h3&gt;時間複雜度&lt;/h3&gt;
&lt;p&gt;接下來，我要開始切入正題，我們來聊聊時間複雜度吧！什麼是時間複雜度呢？時間複雜度用來評估演算法需要花多少時間做計算，我們常用&lt;a href="https://zh.wikipedia.org/wiki/大O符号"&gt;大O符號&lt;/a&gt;來描述，代表的是一個漸進的函數數量級上界，舉個例子，假設我想要在一個有序的數列2,3,5,7,13,27中找到7的位置，最簡單的做法就是從第一個元素開始檢查起，如果不是元素7就再找下一個，直到找到為止，所以最差的情形就是我一路找直到了最後一個元素，如果數列有Ｎ個元素，我們最差的情形就是做了Ｎ次的比較，而每次做比較所花的時間是一個常數時間，因此這個演算法的上界將被 a×N 所界定，a為常數，所以這個演算法的時間複雜度為O(N)，再舉個稍微難一點的例子：&lt;a href="https://en.wikipedia.org/wiki/Subset_sum_problem"&gt;子集合加總問題&lt;/a&gt;，假設給予一組集合{−7, −3, −2, 5, 8}，然後問是否有一組子集合相加為0，怎麼做呢？最簡單的做法就是，窮舉出所有可能的子集合然後相加驗證是否剛好為0，假設集合中有Ｎ個元素，我會有2^N種的子集合，而且要加總最多Ｎ個元素，所以這個過程的時間複雜度為 O(N×(2^N))。特別提醒，以上的分析方式大致上是符合DTM和現代電腦的運作方式，一步接著一步做（step-by-step），NTM就不這麼分析問題，當然兩者看待同一個問題的時間複雜度就會不一樣。&lt;/p&gt;
&lt;p&gt;剛剛有提到 Turing machine 可以解所有演算法問題，那如果我製造一台機器符合 Turing machine或者是我購買一台電腦，是不是就可以躺著解所有的問題了，很可惜的，並不是的！我們剛剛有簡單的帶大家了解時間複雜度，我們知道每種演算法有其計算時間，子集合加總問題的時間複雜度為O(N×(2^N))，如果今天很單純的，我的元素只有1000個，這個數量不過分吧！但大家試著計算一下1000 ×(2^1000)就會發現這是一個天文數字，它大到縱使每個相加只需要0.00001秒，也需要遠遠超過地球年齡的時間才有辦法算完，因此這類問題就算是可解的，也並不實際，所以只有在一個數量級時間以下的問題我們才好應付，這個數量級被稱為 polynomial time（多項式時間），用大Ｏ表示為Ｏ(N^k)，剛剛上面提到的數列找元素問題，它得時間複雜度為O(N)，為 &lt;a href="https://zh.wikipedia.org/wiki/多項式時間"&gt;polynomial time&lt;/a&gt;，這是屬於好對付的問題，如果超過 polynomial time 的問題我們稱為 &lt;a href="https://en.wikipedia.org/wiki/Intractability_(complexity)"&gt;intractable&lt;/a&gt; problem (難解的問題)。&lt;/p&gt;
&lt;h3&gt;P＝NP？&lt;/h3&gt;
&lt;p&gt;如果有一群演算法用DTM來做計算所需時間是 polynomial time，那這類演算法或問題被稱為Ｐ問題，Ｐ就是 polynomial-time 的縮寫，另外如果有一群演算法用NTM來做計算所需時間是 polynomial time，那這類問題被稱為NP問題，NP是 non-deterministic polynomial-time 的縮寫，NP問題還有另一個數學上等價的判斷方法，從驗證解的難度來界定，如果用DTM來驗證一組解是否正確只需要 polynomial time，那這個問題就是一個NP問題，剛剛子集合加總問題，我們要驗證解是否正確很簡單也很快速，我們只要把解的數字加總起來看是不是為0就可以了，所以子集合加總問題是一個NP問題，但因為這個問題的時間複雜度為 O(N×(2^N))，所以它不是一個Ｐ問題，當然也許有一天可以找到一種演算法來解這個問題，並且只需要 polynomial time，那這個問題就是既是NP問題也是P問題，那麼這種演算法找得到嗎？這就牽扯出一樁數學懸案。&lt;/p&gt;
&lt;p&gt;在討論這個問題之前，我先補充一件事，剛剛我提到NP問題有兩種定義是等價的，一種是NTM可在 polynomial time 內解決的問題，另一種是問題的解有辦法在DTM polynomial time下被驗證，這兩種定義如何連結起來呢？我來粗略地說明一下，因為NTM有無窮多個分支讓我利用，那我就讓每個分支去窮舉每種可能的解，然後再驗證每個分支的解是否正確，而驗證的過程只需要 polynomial time，所以自然在NTM下我只需要 polynomial time 就可以將這個問題給解完，也因此它們是等價的。那也許大家還有一個疑問，有什麼問題是無法在 polynomial time 內驗證解的？我們稍稍的改一下子集合加總問題，改問「這集合之中最多有多少種子集合符合加總為0 ?」這時候如果我告訴你解是3個，你要怎麼驗證這個答案是對的，你會發現你幾乎還是需要再重新解同樣的問題才有辦法驗證，這種問題被稱為Co-NP問題（&lt;a href="https://zh.wikipedia.org/wiki/反NP"&gt;反NP問題&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;毋庸置疑的，NP問題必定包含P問題，在DTM之下為 polynomial time 可解決的，在NTM之下也必定是 polynomial time 可解決的，但是P問題會等價於NP問題嗎？（&lt;a href="https://en.wikipedia.org/wiki/P_versus_NP_problem"&gt;P=NP?&lt;/a&gt;）這個問題到目前為止還是數學界無法證明的問題，目前既不能證明P=NP也不能證明P≠NP，克雷數學研究所曾在2000年公布千禧年大獎七大難題，每解破一題的解答者，會頒發獎金100萬美元，裡面的其中一題就是P=NP?問題，那為什麼這個問題很重要呢？ 舉個例子，有一種我們現今常用的加密方法叫做RSA加密，它的概念非常的簡單，我們知道由兩個質數相乘的合數，只有用這兩個質數的其中一個才有辦法整除它，今天我拿一個由兩個大質數相乘的合數當作鑰匙孔，所以手上有鑰匙（其中一個質數）的人就可以開啟這個鎖（整除它），如果你想要暴力破解這個鎖是很困難的，你需要超過 polynomial time 的時間，但是你要驗證解是否正確是很容易的，根據上面的定義&lt;a href="https://zh.wikipedia.org/wiki/RSA加密演算法"&gt;RSA加密&lt;/a&gt;是一個NP問題，如果今天有人找到方式可以把NP問題當作P問題處理，也就是說他可以輕易地用現代的電腦去解開RSA加密，還有破解其他的加密方法，目前的加密方法幾乎都是NP問題，這一定會造成世界不少的動盪，不過也不僅僅只有壞處啦，只要確立了NP=P，我們可以拿來解很多我們現今無法解的難題，含括各領域：人工智慧、物理、醫學 ...，人類知識科技將大步的躍進。&lt;/p&gt;
&lt;h3&gt;NP-Complete 問題&lt;/h3&gt;
&lt;p&gt;當數學家試圖解決 NP=P? 問題時，導出了一個重要的概念— NP-Complete問題。1971年美國 Stephen A. Cook提出了&lt;a href="https://zh.wikipedia.org/wiki/Cook-Levin理論"&gt;Cook-Levin理論&lt;/a&gt;，這個數學理論指出任何一個NP裡面的問題都可以在 polynomial time 內，使用DTM，將之化約成「一個布林方程式是否存在解」的問題，這個被化約的問題又稱為布爾可滿足性問題（SAT），我們稱SAT問題為NP-Complete問題。&lt;/p&gt;
&lt;p&gt;只要滿足以下兩個條件的，我們都稱之為&lt;a href="https://en.wikipedia.org/wiki/NP-completeness"&gt;NP-Complete&lt;/a&gt;：1. 它本身是一個NP問題  2. 所有的NP問題都可以用DTM在 polynomial time 內化約成為它。&lt;/p&gt;
&lt;p&gt;這個概念非常強大，假設我證明了SAT是P問題，就等於今天我隨便拿到一個NP問題就可以在 polynomial time 內把問題轉換成SAT，然後再用 polynomial time 把SAT解掉，所以所有的NP問題都只是P問題了，也就是P=NP，因此NP-Complete問題就是解決 P=NP 的關鍵，如果可以證明NP-Complete問題為P問題，就可以間接證明P=NP。&lt;/p&gt;
&lt;p&gt;NP-Complete 問題不只有SAT一種，在Cook提出Cook-Levin理論的隔一年，1972年，Richard Karp將這個想法往前推進了一步，他證明了&lt;a href="https://zh.wikipedia.org/wiki/卡普的二十一個NP-完全問題"&gt;21個不同但都難解的組合數學與圖論問題為NP-Complete問題&lt;/a&gt;，一樣的其中的任何一種只要被證明為P問題，都可以間接證明P=NP，目前已經有更多問題被證明為NP-Complete 問題。&lt;/p&gt;
&lt;p&gt;大家可能還會看到一個名詞叫做&lt;a href="https://en.wikipedia.org/wiki/NP-hardness"&gt;NP-Hard&lt;/a&gt;，它的定義很好了解，只需要符合NP-Complete的第二個條件：所有的NP問題都可以用DTM在 polynomial time 內化約成為它，就被稱為NP-Hard 問題。所以NP-Complete問題是NP-Hard 問題的一種特例，NP-Hard 問題可以不必是NP問題，譬如停機問題就是一個NP-Hard 問題但不是一個NP問題。&lt;/p&gt;
&lt;h3&gt;後話&lt;/h3&gt;
&lt;p&gt;最後，以下面這張圖作個結尾，左圖是假設P≠NP被證明的情形，NP-Hard有兩個部分，一個部分它同時是個NP問題，另外一部分則不是，所謂的NP問題就是可以用NTM在 polynomial time內給解掉的問題，另外其解的驗證必定能用DTM在 polynomial time內完成，兩種定義是等價的，有一部分的NP問題是屬於P問題，這些問題大部分都是易解的，有另外一部分的NP問題為NP-Complete問題，這些問題被視為難解的問題，我們只能用逼進的方法盡量接近答案。&lt;/p&gt;
&lt;p&gt;右圖是假設P＝NP被證明的情形，此時NP-Complete問題已經被證明為P問題，利用NP-Complete問題的特性，我們可以化約所有NP問題為NP-Complete問題，在把這個NP-Complete問題用 polynomial time 解掉，所以P=NP=NP-Complete。&lt;/p&gt;
&lt;p&gt;事實上，目前科學界普遍相信P≠NP，所以遇到NP-Complete的問題，就直接標註這是一道難題，使用近似解吧！這是一個不怎麼樂觀的看法，難道說我們真的無法把這樣的難題給解決掉了嗎？也未必啦！仔細想想我們也許還有另外一個方法，只要我們創建一個NTM就可以把這些難題給解決掉啦！&lt;a href="https://en.m.wikipedia.org/wiki/BQP"&gt;不過連量子電腦都普遍不被認為是一個NTM&lt;/a&gt;（最後又回補了一槍）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="P_np_np-complete_np-hard" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/P_np_np-complete_np-hard.svg/800px-P_np_np-complete_np-hard.svg.png"&gt;&lt;/p&gt;</content><category term="軟體設計"></category></entry><entry><title>機器學習技法 學習筆記 (4)：Basic Aggregation Models</title><link href="YCNote/ml-course-techniques_4.html" rel="alternate"></link><published>2017-03-29T12:00:00+08:00</published><updated>2017-03-29T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-03-29:YCNote/ml-course-techniques_4.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋Blending、Bagging、Decision Tree和Random Forest。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;綜觀Aggregation Models&lt;/h3&gt;
&lt;p&gt;如果今天我有很多支的Model，我有辦法融合他們得到更好的效果嗎？&lt;/p&gt;
&lt;p&gt;這就是Aggregation Models的精髓，Aggregation Models藉由類似於投票的方法綜合各個子Models的結果得到效果更好的Model。換個角度看，你可以把整個體系看成一個新的Model，而原本這些子Models當作轉換過後的新Features，&lt;strong&gt;所以Aggregation Model裡頭做了「特徵轉換」，這個特徵轉換產生出許多有預測答案能力的Features，稱為Predictive Features，然後再綜合它們得到最後的Model&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Aggregation Models" src="https://dl.dropbox.com/s/ibdowsfjwy0z7zm/MachineLearningTechniques.007.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Aggregation Models可以分成兩大類，第一種的作法比較簡單，先Train出一個一個獨立的Predictive Features，然後在綜合它們，&lt;strong&gt;「集合」的動作是發生在得到Train好的Predictive Feature之後，這叫做「Blending Models」&lt;/strong&gt;；第二種作法則是，&lt;strong&gt;「集合」的動作和Training同步進行，這叫做「Aggregation-Learning Models」&lt;/strong&gt;，Aggregation-Learning Models有一個特殊的例子叫做Boost，翻開字典查Boost的意思是「促進」，在這邊的意義是&lt;strong&gt;假設在Training過程所產生的Predictive Feature朝著改善Model的方向前進就叫做Boost&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;從「集合 …&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋Blending、Bagging、Decision Tree和Random Forest。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;綜觀Aggregation Models&lt;/h3&gt;
&lt;p&gt;如果今天我有很多支的Model，我有辦法融合他們得到更好的效果嗎？&lt;/p&gt;
&lt;p&gt;這就是Aggregation Models的精髓，Aggregation Models藉由類似於投票的方法綜合各個子Models的結果得到效果更好的Model。換個角度看，你可以把整個體系看成一個新的Model，而原本這些子Models當作轉換過後的新Features，&lt;strong&gt;所以Aggregation Model裡頭做了「特徵轉換」，這個特徵轉換產生出許多有預測答案能力的Features，稱為Predictive Features，然後再綜合它們得到最後的Model&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Aggregation Models" src="https://dl.dropbox.com/s/ibdowsfjwy0z7zm/MachineLearningTechniques.007.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Aggregation Models可以分成兩大類，第一種的作法比較簡單，先Train出一個一個獨立的Predictive Features，然後在綜合它們，&lt;strong&gt;「集合」的動作是發生在得到Train好的Predictive Feature之後，這叫做「Blending Models」&lt;/strong&gt;；第二種作法則是，&lt;strong&gt;「集合」的動作和Training同步進行，這叫做「Aggregation-Learning Models」&lt;/strong&gt;，Aggregation-Learning Models有一個特殊的例子叫做Boost，翻開字典查Boost的意思是「促進」，在這邊的意義是&lt;strong&gt;假設在Training過程所產生的Predictive Feature朝著改善Model的方向前進就叫做Boost&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;從「集合」的方法上也可以進一步細分三種類型，有票票等值的&lt;strong&gt;「Uniform Aggregation Type」&lt;/strong&gt;，有給予Predictive Features不同權重的&lt;strong&gt;「Linear Aggregation Type」&lt;/strong&gt;，甚至還可以用條件或任意Model來分配Predictive Features，這叫做&lt;strong&gt;「Non-linear Aggregation Type」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以兩種類型、三種Aggregation Type，交互產生各類的Aggregation Models。有Blending的三種Aggregation Type，Aggregation-Learning的Uniform Type—Bagging，再加上Aggregation-Learning的Linear Type兩種—AdaBoost和GradientBoost，這兩種也亦是Boost的方法，AdaBoost負責處理Classification的問題，而GradientBoost則負責處理Regression的問題，最後介紹Aggregation-Learning的Non-Linear Type—Decision Tree。然後接著，使用Decision Tree結合其他方法再進一步的產生Random Forest、AdaBoost Decision Tree和GradientBoost Decision Tree。&lt;/p&gt;
&lt;p&gt;我將會分兩篇來介紹Aggregation Models，一篇介紹沒Boost的部分，就是今天這一篇，另外一篇則是來專攻有Boost的部分。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Blending&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Blending是泛指在Training結束之後得到幾個Predictive Features，然後再對這些Predictive Features做集合的方法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Blending" src="https://dl.dropbox.com/s/kotwynmp51p457q/MachineLearningTechniques.008.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;如上圖，基本流程是這樣的，一開始先把Data切成一部分拿來Training，另外一部分拿來Validation，這部份很重要，因為我們待會要利用Validation的Error來決定每筆Predictive Feature對Model的貢獻分配比重；接下來使用不同的方法來產生不同的Predictive Features g&lt;sub&gt;t&lt;/sub&gt;，來源可能是不同的Model形式、不同的參數變化、不同的隨機情形等等；有了各類的g&lt;sub&gt;t&lt;/sub&gt;之後，我們就可以選擇使用怎樣的方式來結合它們，如果是Uniform Combination，就直接平均所有g&lt;sub&gt;t&lt;/sub&gt;就可以了，那如果是Linear Combination，想當然爾就是使用線性模型來結合，那如果是Non-Linear Combination，你可以使用任意Model來描述也行；決定好結合方式了，也就同時決定了「特徵轉換」的方法，接下來出動Validation Data，使用這個「特徵轉換」來轉化Validation Data並且做Fitting，最後我們會找到一組解最佳的參數來確定結合的方法，如果是Uniform Combination是不需要這一步的，基本上你得到g&lt;sub&gt;t&lt;/sub&gt;就直接平均就得到結果了，而Linear Combination則是需要去找出α&lt;sub&gt;t&lt;/sub&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;在數學上可以證明Aggregation的效果會比單一一個g&lt;sub&gt;t&lt;/sub&gt;的描述的結果還好&lt;/strong&gt;，這很像是在做投票選舉，不同方法可能帶有不一樣的偏見，但是綜合所有意見之後可以找到共識，這個共識是具有較少偏見的，你可以想像偏見就像是Overfitting，&lt;strong&gt;所以Aggregation是具有像Regularizaiton一般抑制Overfitting的效果的&lt;/strong&gt;，但有些時候特別的看法不一定是偏見，也許這一個方法可以看出其他方法看不出來的規律，此時這個部分也不會被完全忽略掉，&lt;strong&gt;所以Aggregation也可以同時擁有像Feature Transform一樣的複雜度。因此Aggregation的方法可以同時增加Model複雜度又同時防止它Overfitting，這個效果是我們以前沒看過的，所以我們會說Aggregation具有截長補短的效果&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Bagging&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Bagging" src="https://dl.dropbox.com/s/ht7d8qs8p5744le/MachineLearningTechniques.009.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bagging是一種利用變換原本Data來造出不同g&lt;sub&gt;t&lt;/sub&gt;的簡單方法&lt;/strong&gt;，Bagging的全名稱為Bootstrap Aggregation，其中&lt;strong&gt;Bootstrap指的是「重新取樣原有Data產生新的Data，取樣的過程是均勻且可以重複取樣的」&lt;/strong&gt;，使用Bootstrap我們就可以從一組Data中生出多組Dataset，然後就可以使用這些Dataset來產生多組g&lt;sub&gt;t&lt;/sub&gt;，最後再Uniform Combination這些g&lt;sub&gt;t&lt;/sub&gt;，就完成了Bagging。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Decision Tree（決策樹）&lt;/h3&gt;
&lt;p&gt;接下來談Decision Tree這個重要的概念，Decision Tree其實就像是一個多層次的分類，每一次的分類會根據某一個Feature來當作依據判斷它應該繼續往哪一條路走，然後繼續使用可能是另外一個Feature來繼續細分下去。舉個例子好了，假設今天有一個自由式摔跤重量63公斤的女選手Ms. D要參加奧運，所以得透過奧運的分級制度分級，一開始可能根據比賽模式這個Feature下去分類，我查了一下有自由式和古典式兩種，所以Ms. D會被歸類到自由式，再來根據性別這個Feature下去分類，Ms. D是女選手所以分到女選手這一類，再繼續可能會根據體重來細分，體重在奧運分級共有8級，Ms. D可能就被分到62公斤级的那類，這樣的分類精神就是Decision Tree。&lt;/p&gt;
&lt;p&gt;所以，Decision Tree的優點是結果所提供的結構非常容易讓人了解，另外在演算法部分也很容易實現，而且因為具有以條件篩選的結構，所以其實很容易可以做到多類別分類。但是Decision Tree也有一些為人詬病的缺點，Decision Tree整體理論是缺乏基礎的，存在很多是前人的巧思，很多作法都是使用起來感覺效果不錯就延續下去了，目前並不了解背後的原因，也因此沒有一個代表性的演算法存在。&lt;/p&gt;
&lt;p&gt;在講Decision Tree操作方法之前應該要先來講一下Decision Stump，Decision Stump做的事其實就是上述中提到的對某個Feature做切分的這件事，&lt;strong&gt;可以想知Decision Stump是一個預測效果很差的Model，而Aggregation這些Decision Stump形成Decision Tree卻有很好的效果&lt;/strong&gt;，這就是Aggregation的威力。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Decision Tree" src="https://dl.dropbox.com/s/nafpnsu8icnazic/MachineLearningTechniques.010.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;見上圖，我們來看一下Decision Tree的流程，Decision Tree最為人所知的演算法是C&amp;amp;RT，C&amp;amp;RT是一整套的套件，我們今天只是提到它整套套件中的一種特例。Decision Tree產生的函式是這樣的，一開始先判斷進來的這筆資料還能不能繼續分支下去，在三個情況下，我們沒辦法繼續分支下去：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;數據Ɗ只剩一筆數據。&lt;/li&gt;
&lt;li&gt;這群數據Ɗ已經最佳化了，我們會說它的Impurity=0，這個時候我們不知道要從哪裡再切一刀。&lt;/li&gt;
&lt;li&gt;這群數據Ɗ的Feature X&lt;sub&gt;n&lt;/sub&gt;都完全相同。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;當無法再繼續分支下去時，會回傳一個g&lt;sub&gt;t&lt;/sub&gt;(x)=constant，這個常數是一個可以使得這個群體內E&lt;sub&gt;in&lt;/sub&gt;最小的數值，在分類問題中這個常數是{y&lt;sub&gt;n&lt;/sub&gt;}中佔多數的類別，在Regression問題中這個常數是{y&lt;sub&gt;n&lt;/sub&gt;}的平均值。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;大家應該會有點驚訝，Decision Tree也有辦法做Regression？其實是可以的，在分類問題中我們可以利用類別來做分類，在Regression問題我們可以利用一個切分數值來區分成兩群或多群，例如：以50當切分數值，大於50的一類，小於等於50的另外一類，當我們切的夠細夠多層的時候就是在做一個Regression問題了。&lt;/p&gt;
&lt;p&gt;那接下來來看假如還可以繼續分支下去應該要怎麼做，這邊假設我們只切一刀分為兩個區塊C=2，我們該根據怎樣的條件來切呢？我們剛剛其實有稍微提到，那就是Impurity，我們&lt;strong&gt;可以根據Impurity Function來衡量「一群資料的不相似程度」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;分類問題的Impurity Function有以下兩種：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Impurity(Ɗ) = (1/N) 𝚺&lt;sub&gt;n&lt;/sub&gt; ⟦y&lt;sub&gt;n&lt;/sub&gt;≠y*⟧，其中y*是Ɗ中佔多數的類別，這個衡量方法就直接的去數出錯誤答案的比例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gini Index: Impurity(Ɗ) = 1 - 𝚺&lt;sub&gt;k&lt;/sub&gt; [ 𝚺&lt;sub&gt;n&lt;/sub&gt;⟦y&lt;sub&gt;n&lt;/sub&gt;=k⟧  / N ]&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;，Gini Index是最為流行的作法，它不同於上一個作法，它是在評估所有的類別後才去計算Impurity。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而Regression問題有以下方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Impurity(Ɗ) = (1/N) 𝚺&lt;sub&gt;n&lt;/sub&gt; ( y&lt;sub&gt;n&lt;/sub&gt; - ȳ )&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;，其中ȳ代表的是{y&lt;sub&gt;n&lt;/sub&gt;}的平均值，式子中使用平方誤差來評估資料的離散程度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有了Impurity Function我們就有了指標，找出應該要使用哪個Feature、應該要怎麼切，才能使得Impurity Function總和最小，決定好這一刀後，接下來就從這一刀切下去，把Data一分為二，然後這兩組Data再各自去長出一棵Decision Tree，經過遞迴式的迭代，我們就可以得到一棵完整的Decision Tree了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Show C&amp;amp;RT" src="https://dl.dropbox.com/s/sy6xt51dcxfcmz4/MachineLearningTechniques.015.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;如果我們讓一棵樹完整的長成了，可以想到的後果想當然爾就是Overfitting，所以我們必須要做Regularization，&lt;strong&gt;Decision Tree常用的Regularization的方法是Pruning&lt;/strong&gt;，就是砍樹，我們將分支的數量Ω(G)加進去E&lt;sub&gt;in&lt;/sub&gt;中做為Regularization，所以我們問題變成是去找到 argmin E&lt;sub&gt;in&lt;/sub&gt;(G)+λΩ(G)，其中的λ可以利用Validation Data來做選擇，你會發現如果真正的要去找到argmin E&lt;sub&gt;in&lt;/sub&gt;(G)+λΩ(G)的最佳解，這問題會非常的困難，因為你必須要把所有的可能的樹都考慮進去，所以有一個替代方案，&lt;strong&gt;我們可以先將樹整棵長完，然後在一一的去合併分支，看哪兩個分支合併之後可以使E&lt;sub&gt;in&lt;/sub&gt;最小就先合併，使用這樣的作法逐步減少分支的數量&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;順道一提，C&amp;amp;RT可以產生許多替代方案，這些替代方案稱為Surrogate Branch，當有一筆Data缺乏某個Feature，我們仍然有辦法使用替代方案來做決策，這是C&amp;amp;RT的一個大大的優點。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Random Forest（隨機森林）&lt;/h3&gt;
&lt;p&gt;如果我拿Decision Tree來做Bagging這樣可以嗎？當然OK，Aggregation Model的精髓就是可以綜合子Model，那Decision Tree也可以是看成一個子Model，所以我們在做的就是Aggregation of Aggregation，&lt;strong&gt;這種拿Decision Tree來做Bagging的Model叫做Random Forest&lt;/strong&gt;，這個名字取的很生動，有很多棵數的地方就是森林啦！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Decision Tree和Bagging其實是有互補的作用&lt;/strong&gt;，Decision Tree這種演算法是「變異度」很高的，因為它不像SVM這類的演算法，會去評估與Data之間的距離，空出最大的距離來避免Overfitting，而Bagging正可以拿來減少「變異度」，消除雜訊，所以&lt;strong&gt;Random Forest會比Decision Tree更不易Overfitting&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Random Forest" src="https://dl.dropbox.com/s/sw72it5miiczjri/MachineLearningTechniques.011.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;見上圖，我們來看一下Random Forest的流程，一開始先做和Bagging裡頭一樣做的事Bootstrap，藉此來產生新的Dataset，另外為了讓我們隨機程度變得更高，我也對我們Features來做點變化，將它乘上一個亂數產生的P，如果P&lt;sub&gt;i&lt;/sub&gt;=0代表我們完全不取這個Feature，如果P&lt;sub&gt;i&lt;/sub&gt;=1代表我們完全取這個Feature，我們更可以以分數來代表我們對某個Feature的重視程度，這個手法叫做Random-subspace。接下來就是把弄的很亂的Dataset放進去長一顆Decision Tree，最後再把所有的Decision Tree平均就是Random Forest的結果。&lt;/p&gt;
&lt;p&gt;Random Forest發展出了一套獨特的Validation方法，我們知道Bootstrap的結果會造成有些Data取用而有些Data不使用，而取用的Data會拿來Training，這讓你想到什麼呢？沒錯，沒有用到的Data可以做Validation，我們可以拿那些沒有被取用的Data來評估Training的好壞，我們會稱那些沒被取用的Date叫做Out-of-Bag Data，而利用Out-of-Bag Data來Validation的Error，稱為Out-of-Bag Error，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Out-of-Bag Error E&lt;sub&gt;oob&lt;/sub&gt;=(1/N) 𝚺&lt;sub&gt;n&lt;/sub&gt; err(y&lt;sub&gt;n&lt;/sub&gt;, G&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;(x&lt;sub&gt;n&lt;/sub&gt;)) &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;G&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt;(x) = Average(沒有取用這筆Data的所有Models)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Out-of-Bag Error提供一個很方便的Self-validation的方法。&lt;/p&gt;
&lt;p&gt;在以前Linear Model中，權重W代表每筆Feature對Model的貢獻度，我們可以由W的分量大小來評估每個Feature的重要程度。Random Forest則是可以利用E&lt;sub&gt;oob&lt;/sub&gt;和Random-subspace來標示出每個Feature的重要程度，想法是這樣的，如果今天某一個Feature i 對Model很重要，所以說我只對Feature i 做Random-subspace，也就是只有P&lt;sub&gt;i&lt;/sub&gt;是隨機的，可以想知E&lt;sub&gt;oob&lt;/sub&gt;會大幅增加，因此利用這個想法我們可以用來定義Feature的重要程度，&lt;/p&gt;
&lt;p&gt;important(i) = E&lt;sub&gt;oob&lt;/sub&gt;(G) - E&lt;sub&gt;oob&lt;/sub&gt;(G with random-subspace at i)&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;在這一篇我們提了幾個基礎的Aggregation Models，從最簡單的Blending，Blending的方法本身不去產生子Model，而是使用兩階段學習，先自行挑選和訓練來產生很多的子Model，而Blending只在這些結果上做不同方式的結合。&lt;/p&gt;
&lt;p&gt;接下來，Learning-Aggregation的方法則化被動為主動，我們先提了Bagging，裡頭使用Bootstrap的技巧來造成資料的隨機性，利用這樣的變異來產生多個g&lt;sub&gt;t&lt;/sub&gt;，再接下來我講了Decision Tree，Decision Tree由多個Decision Stump組合而成，每個Decision Stump就是g&lt;sub&gt;t&lt;/sub&gt;，Decision Tree做的事就是，產生Decision Stump、切分Dataset、再產生Decision Stump...接續下去，最後綜合全部的Decision Stump成為Decision Tree。&lt;/p&gt;
&lt;p&gt;最後，我們結合Decision Tree和Bagging產生了Random Forest，利用彼此的互補，讓效果變得更好可以比單純Decision Tree更好。&lt;/p&gt;</content><category term="機器學習技法"></category></entry><entry><title>讀書手札：大腦解密手冊 The Brain: The Story of You</title><link href="YCNote/the-brain-the-story-of-you.html" rel="alternate"></link><published>2017-03-24T12:00:00+08:00</published><updated>2017-03-24T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-03-24:YCNote/the-brain-the-story-of-you.html</id><summary type="html">&lt;p&gt;&lt;a href="http://www.books.com.tw/products/0010738920"&gt;⟪大腦解密手冊⟫&lt;/a&gt;是一本非常易懂但又相當豐富的書，作者David Eagleman是一名美國的神經科學家，在這本書他嘗試拋開教科書的模式，改以輕鬆聊天的方式來聊神經科學，讓讀者可以很輕易的一探這神秘的大腦。&lt;/p&gt;
&lt;h2&gt;大腦的可塑性&lt;/h2&gt;
&lt;p&gt;人類在剛出生的一刻是非常脆弱的，不能走不能自己吃東西，完全需要依賴他人的照顧，相反的其他的哺乳類在出生的那刻就已經有謀生的能力了，斑馬寶寶出生不到45分鐘就可以奔跑，長頸鹿出生幾個小時就可以學會站立。表面看起來，這對人類生存似乎很不利，事實上卻提供人類大腦更多的彈性，&lt;strong&gt;人類大腦不像其他動物在出生的一開始就已經接好線路了，雖然長大成人的過程腦細胞數量並不會增加，但是突觸的連結卻會有天翻地覆的改變&lt;/strong&gt;，人腦建造過程可以長達25年，過程中會有50%的突觸會被修掉，就算是一個成人突觸連結還是每天不斷的更新，這種彈性使得人類可以比其他動物更能應付環境的各種變化。&lt;/p&gt;
&lt;p&gt;能夠看東西不僅僅需要眼睛，而主要還是靠著眼睛後面的大腦，Mike May在3歲時失明了，過了40年因為幹細胞治療重見光明，不過這個恢復正常的眼睛並沒有讓他恢復視力，雖然能夠看見東西，卻很難說出那是什麼，而且也不清楚這個東西是遠是近，視覺系統不像是照相機，把鏡頭修好了就可以正常使用了，他的大腦長達40年沒有接受光線給的訊號，一時之間是無法辨識視覺給的訊號，也就造成眼睛恢復正常了但是視力並沒有恢復。&lt;/p&gt;
&lt;p&gt;只有感官的訊號是沒有用的，我們還需要大腦去統合和理解這些訊號，大腦幫我們做了很多事，一個有趣的例子，你知道嗎？人在閱讀的時候眼球是不斷的跳動的，一秒鐘會跳4次左右，這種快速的運動稱為「眼球迅速移動」(saccade)，儘管眼球不停的跳動但我們卻可以看到一個穩定的畫面 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://www.books.com.tw/products/0010738920"&gt;⟪大腦解密手冊⟫&lt;/a&gt;是一本非常易懂但又相當豐富的書，作者David Eagleman是一名美國的神經科學家，在這本書他嘗試拋開教科書的模式，改以輕鬆聊天的方式來聊神經科學，讓讀者可以很輕易的一探這神秘的大腦。&lt;/p&gt;
&lt;h2&gt;大腦的可塑性&lt;/h2&gt;
&lt;p&gt;人類在剛出生的一刻是非常脆弱的，不能走不能自己吃東西，完全需要依賴他人的照顧，相反的其他的哺乳類在出生的那刻就已經有謀生的能力了，斑馬寶寶出生不到45分鐘就可以奔跑，長頸鹿出生幾個小時就可以學會站立。表面看起來，這對人類生存似乎很不利，事實上卻提供人類大腦更多的彈性，&lt;strong&gt;人類大腦不像其他動物在出生的一開始就已經接好線路了，雖然長大成人的過程腦細胞數量並不會增加，但是突觸的連結卻會有天翻地覆的改變&lt;/strong&gt;，人腦建造過程可以長達25年，過程中會有50%的突觸會被修掉，就算是一個成人突觸連結還是每天不斷的更新，這種彈性使得人類可以比其他動物更能應付環境的各種變化。&lt;/p&gt;
&lt;p&gt;能夠看東西不僅僅需要眼睛，而主要還是靠著眼睛後面的大腦，Mike May在3歲時失明了，過了40年因為幹細胞治療重見光明，不過這個恢復正常的眼睛並沒有讓他恢復視力，雖然能夠看見東西，卻很難說出那是什麼，而且也不清楚這個東西是遠是近，視覺系統不像是照相機，把鏡頭修好了就可以正常使用了，他的大腦長達40年沒有接受光線給的訊號，一時之間是無法辨識視覺給的訊號，也就造成眼睛恢復正常了但是視力並沒有恢復。&lt;/p&gt;
&lt;p&gt;只有感官的訊號是沒有用的，我們還需要大腦去統合和理解這些訊號，大腦幫我們做了很多事，一個有趣的例子，你知道嗎？人在閱讀的時候眼球是不斷的跳動的，一秒鐘會跳4次左右，這種快速的運動稱為「眼球迅速移動」(saccade)，儘管眼球不停的跳動但我們卻可以看到一個穩定的畫面，這是因為大腦存在一個內在模型(internal model)，他會先預測你將會看到什麼，然後視覺訊號才進來作驗證，我們體驗到的視覺很少依賴照進眼睛的光線，較多是依賴腦中既有的東西。大腦幫我們預先處理很多的東西，讓我們可以感受到一個穩定的世界。&lt;/p&gt;
&lt;p&gt;另外一個例子，我們對光線的反應時間大約是190毫秒，而我們對聲音的反應時間比光線快一點是160毫秒，但是我們卻不會感受到這種不同步，因為大腦給你的是一個延遲過的版本，將時間差給隱蔽起來。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;腦其實不在意輸入資訊的細節，他只關注如何有效率的在這個世界活動，並得到它需要的東西。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也因為如此，所以有了&lt;strong&gt;「感官替代」&lt;/strong&gt;這種新科技，也就是利用其他感官體驗來取代失去功能的感官，譬如說失去聽覺的患者，可以穿一個背心，這個背心會把聲音轉換成振動，因為大腦不管進來什麼樣的資料，它都會調整並盡量的處理，天生聽障者使用這個背心大概5天的時間就能正確的辨別別人說出來的字詞，從這裡就可以看出大腦是如何具有彈性。&lt;/p&gt;
&lt;h2&gt;意識與無意識&lt;/h2&gt;
&lt;p&gt;如果說意識決定一個人，倒不如說無意識（或稱潛意識）決定一個人，我們常常把idea的出現歸功於意識，但事實上在你意識到這個idea之前的幾個小時、甚至幾個月，無意識已經開始塑造這個idea，包括鞏固記憶、試驗新組合、評估後果，美國社會心理學家Brett Pelham和他的研究小組從統計結果發現，名字叫做Dennis或Denise的牙醫(dentist)，以及名字叫做Laura或Laurence的律師(Lawyer)特別多，可以說無意識在我們人生的重大決策中扮演相當重要的角色。&lt;/p&gt;
&lt;p&gt;那意識是怎麼形成的？這是一個有趣的哲學問題，意識這種東西有辦法從物質中產生嗎？而如果打造一個人工智慧讓你分不清他是機器人或者是真人，我們該說這樣的機器人有意識嗎？有人會說不！機器人只遵照預先設計好的程式執行，他們不具有意識，作者舉了一個臆想實驗「中文房間」，房間中的人按照詳細的說明書來處理中文符號，並把回答送出房間，這的確可以騙過母語人士，讓他們以為房間裡面的人懂中文，但房間裡的人根本不懂他在做什麼，那機器人是不是也是這樣的。&lt;/p&gt;
&lt;p&gt;不過作者比較傾向於支持另一個反面，認為腦中每個各別神經元並不清楚他們自己在做什麼，但是集體行為讓大腦產生了意識，他舉了螞蟻群的例子，每隻螞蟻只做一些簡單的事，不過一群螞蟻卻可以打造出相當複雜的系統，譬如：蟻窩&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一旦夠多的螞蟻聚集在一起，超生物就出現了，這種集體擁有的特性比個別基礎部分更精緻、複雜。這種現象稱為突現（emergence）; 當簡單的單元以適當的方式交互作用，產生更大的格局，這就是突現。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;每個神經元一輩子只負責回應訊號，他不知道現在在彈奏貝多芬，他不知道你的存在，他不知道你的意圖，不過因為突現他們共同產生了你的意識。  &lt;/p&gt;
&lt;p&gt;我們每天都有一大段時間是處於無意識的狀態，那就是睡眠，威斯康辛大學教授Giulio Tononi認為，我們在清醒的的時候皮質的不同區域間會跨區溝通，而睡眠的無意識則是缺乏跨區溝通，也就是說，在有意識的情況下，我們大腦的各個部分會爭執不休的對話，也就造成你可能會猶豫不決，相反的，在無意識的情況下，你將會進入自動導航模式，大腦根據你既有的突觸迴路來執行任務。&lt;/p&gt;
&lt;h2&gt;腦中的交戰網路&lt;/h2&gt;
&lt;p&gt;如果無意識是一輛直行的車子，意識可以說是這輛車子的方向盤，大腦這一部從衝突中打造出來的機器，每天要下成千上萬的決策。&lt;/p&gt;
&lt;p&gt;作者重新詮釋了Michael Sandel在正義課中提及著名的電車難題，假設今天你是一名鐵路維修工人，遇到一輛失控的火車，在既定的軌道上會撞死正在修理鐵路的5名工人，不過你剛好站在鐵軌控制桿前面，但是如果你扳動控制桿則會造成另一名工人被撞死，幾乎所有人都會扳動這個控制桿，因為1人死亡總比5人好，但換一個情境，如果今天你遇到這個失控火車的時候剛好站在火車上方的橋上，旁邊又剛好有一個胖子，如果你把這個胖子推下去，火車就會因此被他擋住，然後可以拯救5條人命，同樣是1換5的狀況，這個時候你就發現你開始猶豫了。&lt;/p&gt;
&lt;p&gt;作者認為，對腦來說，第一種情境只涉及單純的數學問題，此時活化的是腦中解決邏輯問題的區域，&lt;strong&gt;但在第二種情境，你必須去碰觸那個人，把他推下去，這會激起額外的網路&lt;/strong&gt;，也就是腦中與情緒相關的區域，所以這兩個網路，解決邏輯問題的和情緒相關的，會開始爭辯不休，因此你開始產生猶豫。&lt;/p&gt;
&lt;p&gt;這一種腦中衝突有時候會讓你做出錯誤的決策，今天明明要健身的，不過想想有一部影集還沒看完，好想知道接下來劇情會怎麼走下去，算了！明天再去健身好了，我相信大家常常有類似的經驗，雖然健身對你的好處是多於追劇的，但對於大腦而言純粹在大腦裡模擬的好處，比不上此時此刻真實感覺到的好處，所以作者建議用一種方式來克服這種意志力的不足—&lt;strong&gt;尤里西斯合約&lt;/strong&gt;，希臘神話中的尤里西斯是一名凱旋歸國的戰士，在回國的途中，他們會在經過一座小島，這座小島住著美麗的賽蓮女妖，據說會唱出美麗的歌聲迷惑水手們不自覺得把船開到礁岩去，所以需要用蜂蠟把耳朵塞住，但是尤里西斯實在是很想聽聽看女妖們的聲音，於是他就心生一計命令水手們把他綁在船桅上，其他人把耳朵塞住，到時候不管尤里西斯怎麼叫，怎麼崩潰，都不要理他，船正常開就好了&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;尤里西斯知道未來的自己沒資格作良好的決策，所以在頭腦清楚的時候就把事情安排妥當，以防止自己作錯事，於是「現在的你」和「未來的你」之間的這一種協議稱為「尤里西斯合約」&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;運用在健身的這個例子，你可以約朋友一起在固定的時間健身，讓未來的你沒有任何理由可以推遲。&lt;/p&gt;
&lt;h2&gt;科技將如何改變大腦的未來&lt;/h2&gt;
&lt;p&gt;這本書的最後，作者探討了大腦科技的未來。舉幾個我覺得有趣的例子，奧爾科生命延長基金會一直致力於一種技術，他們將死亡人的大腦冰封保存下來，以期盼有一天科技進步可以讓這大腦重新擁有年輕的身體，這樣這些被冰封的大腦將擁有並享受第二生命週期。&lt;/p&gt;
&lt;p&gt;不過有些人可能不太喜歡這種感覺，那還有另外一種延續生命的方法，稱為數位的不朽，如果大腦運算只單純是神經元之間的運算，我們將可以使用這個運算的概念來創造一個大腦，實際的作法是利用大腦切片去紀錄每個神經元的狀態，並用數位的方式保存下來，然後也可以透過模擬去讓這顆大腦重新活過來，但是這非常的困難，一般神經元有多達一萬條分枝，要繪製完整人類連結體的圖譜預計還需要十年的時間，目前連建立一個大鼠的大腦都做不到。&lt;/p&gt;</content></entry><entry><title>Python玩數據 (1)：安裝Python, IPython, Numpy, Pandas</title><link href="YCNote/python-play-with-data_1.html" rel="alternate"></link><published>2017-03-20T12:00:00+08:00</published><updated>2017-03-20T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-03-20:YCNote/python-play-with-data_1.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;相較於R，我比較喜歡在工作上使用python來作數據處理&lt;/strong&gt;，主要原因有四個，&lt;strong&gt;第一點，python是一個簡潔的語言&lt;/strong&gt;，讓我們可以在不寫註解的情況下還可以很容易的看出每一行code在做哪些事，這可以省去了不少時間在；&lt;strong&gt;第二點，python可以更容易的寫成物件導向編程&lt;/strong&gt;，物件導向編程可以讓code看起來更為直覺，而且更易於修改、重構或套用，如果是大型軟體開發的話，需要多人協作，此時物件導向便是絕對必要的；&lt;strong&gt;第三點，python是一個通用語言&lt;/strong&gt;，不僅僅只可以作資料處理而已，你可以用python寫一套視窗程式，或者當作網站的後台（這個網站就是建基在python上），如果要做一些平行運算也很容易，&lt;strong&gt;最後一點，也是相當重要的一點，目前常見的deep learning套件TensorFlow或Keras都是架構在python上面&lt;/strong&gt;，所以如果你的數據處理結束要作deep learning的話，直接用python處理是相當理想的。講了python這麼多優點，其實它是有一項缺點是不如R的，R是一個專為資料科學設計的語言，所以背後有強大的社群，也就是說能直接取得資料分析方法的套件會比python來的多，不過這方面在這幾年已經漸漸的改善了。&lt;/p&gt;
&lt;p&gt;講了這麼多python的強大，不過在這個系列我並不會著墨太多在python上，這個部分我會在其他的文章中分享，這系列文章主要聚焦在python的資料處理這部份，我會從基礎講起，讓不懂python的人也可以聽懂。  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;最困難的第一步：安裝&lt;/h2&gt;
&lt;p&gt;不要以為我在開玩笑，安裝往往是最困難的一步，有些時候安裝一些套件的時候，你必須要先行安裝另外幾個相依套件，如果程式在安裝的過程無法自己補足這些相依套件的話，你就得自己安裝，一般來說如果是python的套件的話 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;相較於R，我比較喜歡在工作上使用python來作數據處理&lt;/strong&gt;，主要原因有四個，&lt;strong&gt;第一點，python是一個簡潔的語言&lt;/strong&gt;，讓我們可以在不寫註解的情況下還可以很容易的看出每一行code在做哪些事，這可以省去了不少時間在；&lt;strong&gt;第二點，python可以更容易的寫成物件導向編程&lt;/strong&gt;，物件導向編程可以讓code看起來更為直覺，而且更易於修改、重構或套用，如果是大型軟體開發的話，需要多人協作，此時物件導向便是絕對必要的；&lt;strong&gt;第三點，python是一個通用語言&lt;/strong&gt;，不僅僅只可以作資料處理而已，你可以用python寫一套視窗程式，或者當作網站的後台（這個網站就是建基在python上），如果要做一些平行運算也很容易，&lt;strong&gt;最後一點，也是相當重要的一點，目前常見的deep learning套件TensorFlow或Keras都是架構在python上面&lt;/strong&gt;，所以如果你的數據處理結束要作deep learning的話，直接用python處理是相當理想的。講了python這麼多優點，其實它是有一項缺點是不如R的，R是一個專為資料科學設計的語言，所以背後有強大的社群，也就是說能直接取得資料分析方法的套件會比python來的多，不過這方面在這幾年已經漸漸的改善了。&lt;/p&gt;
&lt;p&gt;講了這麼多python的強大，不過在這個系列我並不會著墨太多在python上，這個部分我會在其他的文章中分享，這系列文章主要聚焦在python的資料處理這部份，我會從基礎講起，讓不懂python的人也可以聽懂。  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;最困難的第一步：安裝&lt;/h2&gt;
&lt;p&gt;不要以為我在開玩笑，安裝往往是最困難的一步，有些時候安裝一些套件的時候，你必須要先行安裝另外幾個相依套件，如果程式在安裝的過程無法自己補足這些相依套件的話，你就得自己安裝，一般來說如果是python的套件的話，你可以先用待會要介紹的&lt;code&gt;pip install&lt;/code&gt;來安裝，如果不幸在上面找不到的話，就只好上網Google了，另外有些時候安裝還會遇到bug，這個時候Google也同樣是你的好朋友，或者到&lt;a href="https://stackoverflow.com"&gt;Stack Overflow&lt;/a&gt; 上找答案（一個好的coder要培養自己上網找答案的能力），不過大家先不用擔心，以下我會帶大家一步一步的安裝。&lt;/p&gt;
&lt;p&gt;我們將會用到python 2.7版（你也可以選擇更新的版本，不會差距太大），以及他的套件IPython, Numpy和Pandas。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;Python2.7&lt;/h2&gt;
&lt;h3&gt;Mac&lt;/h3&gt;
&lt;p&gt;python2.7已經是內建的程式了！打開「終端機」，直接輸入&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ python2.7 -V

Python &lt;span class="m"&gt;2&lt;/span&gt;.7.13
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;就會顯示他的版本。&lt;/p&gt;
&lt;p&gt;如果沒有的話，或者你想要自己安裝一份的話，可以參考&lt;a href="https://stringpiggy.hpd.io/mac-osx-python3-dual-install/"&gt;這篇&lt;/a&gt;的說明，或者跟著我往下作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; 安裝 Xcode：打開你的App Store，搜尋Xcode並安裝。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; 安裝 &lt;a href="https://brew.sh"&gt;Homebrew&lt;/a&gt; 這個Mac上好用的套件管理，打開「終端機」，輸入&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ /usr/bin/ruby -e &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;

&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; This script will install:
/usr/local/bin/brew
/usr/local/share/doc/homebrew
/usr/local/share/man/man1/brew.1
/usr/local/share/zsh/site-functions/_brew
/usr/local/etc/bash_completion.d/brew
/usr/local/Homebrew

Press RETURN to &lt;span class="k"&gt;continue&lt;/span&gt; or any other key to abort
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(附註：我用 &lt;code&gt;＄&lt;/code&gt; 代表終端機的輸入起始字元，後面才是你需要輸入的指令)&lt;/p&gt;
&lt;p&gt;按下Enter就會開始安裝了。&lt;/p&gt;
&lt;p&gt;安裝完畢你就可以直接在「終端機」上使用它，我們試著搜尋python&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ brew search python

app-engine-python               boost-python@1.59               micropython                    
python-markdown                 wxpython
boost-python                    gst-python                      python ✔                       
python3 ✔                       zpython
homebrew/apache/mod_python            
Caskroom/cask/kk7ds-python-runtime          
Caskroom/cask/mysql-connector-python
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;因為我的電腦已經安裝了python2.7和python3.0，所以你會看到他們已經是打勾的狀態，我們的目標就是安裝「python」。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; 安裝python2.7：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ brew install python
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;安裝完畢後檔案會被放在底下這個路徑，你可以打開來看一下&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ open /usr/local/Cellar
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;應該就會看到python的資料夾了。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; 設定路徑 $PATH（不跟系統 Python 打架）&lt;/p&gt;
&lt;p&gt;這是什麼呢？當你輸入&lt;code&gt;brew&lt;/code&gt; , &lt;code&gt;open&lt;/code&gt; , &lt;code&gt;python2.7&lt;/code&gt; 這些指令到「終端機」上，為什麼「終端機」會認的了這些指令，原因就出在於這個PATH上，又稱為「環境變數」，我們把它叫出來看看&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$PATH&lt;/span&gt;
/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;你可以看到有五個路徑分別被字元 &lt;code&gt;:&lt;/code&gt; 隔開，由前到後分別為&lt;code&gt;/user/local/bin&lt;/code&gt;、&lt;code&gt;/usr/bin&lt;/code&gt;、&lt;code&gt;/bin&lt;/code&gt;、&lt;code&gt;/usr/sbin&lt;/code&gt;、&lt;code&gt;/sbin&lt;/code&gt;，這一些都是裝有執行檔的資料夾，今天你如果輸入某個指令，他就會從第一個資料夾下面開始找起，也就是&lt;code&gt;/user/local/bin&lt;/code&gt;，沒有找到再依序往下找，直到找不到為止，如果今天&lt;code&gt;/usr/bin&lt;/code&gt;底下有python，而你剛剛用brew安裝的另一個python放在&lt;code&gt;/user/local/bin&lt;/code&gt; 底下，在這個例子中，你會執行到的就是第一個路徑&lt;code&gt;/user/local/bin&lt;/code&gt; 下的python，那這也是我們要的結果，我們想要執行我們自己安裝的，而不是系統原有的。&lt;/p&gt;
&lt;p&gt;如果&lt;code&gt;/user/local/bin&lt;/code&gt;不是在第一個的話，就必須去修改PATH的順序。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo emacs /etc/paths
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;輸入密碼後，就會進入修改模式，然後開始修改順序，利用以下指令把&lt;code&gt;/user/local/bin&lt;/code&gt; 放到最上面&lt;/p&gt;
&lt;p&gt;control + k：把一行字剪下來&lt;/p&gt;
&lt;p&gt;control + y：把字貼上&lt;/p&gt;
&lt;p&gt;control + x + s：存檔&lt;/p&gt;
&lt;p&gt;control + x + c：關掉 emacs&lt;/p&gt;
&lt;p&gt;修改完成重開「終端機」，讓環境變數重載，在輸入一次 &lt;code&gt;echo $PATH&lt;/code&gt; 應該就可以看到修改後正確的環境變數了。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; 那就安裝完畢啦！最後檢查一下你下&lt;code&gt;python2.7&lt;/code&gt;的時候是不是來自於&lt;code&gt;/user/local/bin&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ which python2.7
/usr/local/bin/python2.7
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;看起來很正常，Great!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Ubuntu&lt;/h3&gt;
&lt;p&gt;請參考&lt;a href="https://tecadmin.net/install-python-2-7-on-ubuntu-and-linuxmint/"&gt;這篇&lt;/a&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; 先安裝一些相依套件&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo apt-get install build-essential checkinstall
$ sudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; 從網路上下載python2.7 source code&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; /usr/src
$ wget https://www.python.org/ftp/python/2.7.13/Python-2.7.13.tgz
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; 解壓縮並進去資料夾&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tar xzf Python-2.7.13.tgz
$ &lt;span class="nb"&gt;cd&lt;/span&gt; Python-2.7.13
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; 依環境配置並安裝&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ sudo ./configure
$ sudo make altinstall
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;make altinstall&lt;/code&gt; 是為了避免你去取代掉預設的python在/usr/bin/python。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Windows&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; 在這個&lt;a href="https://www.python.org/downloads/release/python-2713/"&gt;網站&lt;/a&gt;依照你的CPU架構下載安裝檔，並安裝。&lt;/p&gt;
&lt;p&gt;&lt;img alt="python_win_install_01" src="https://dl.dropboxusercontent.com/s/awu45hdpv0d2j64/python_win_install_01.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; 設定環境變數&lt;/p&gt;
&lt;p&gt;打開 控制台 &amp;gt; 系統及安全性 &amp;gt; 系統 &amp;gt; 進階系統設定 &amp;gt; 環境變數&lt;/p&gt;
&lt;p&gt;選Path，並按下 編輯，將&lt;code&gt;C:\Python27;C:\Python27＼Scripts&lt;/code&gt; 加到後面，並儲存。環境變數的說明請參考上面Mac安裝的第四步，原理是一樣的，不過在windows裡的區分的符號是&lt;code&gt;;&lt;/code&gt;不是&lt;code&gt;:&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;IPython, Numpy, Pandas&lt;/h2&gt;
&lt;h3&gt;Mac ＆Ubuntu&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; 安裝pip&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ curl &lt;span class="s2"&gt;&amp;quot;https://bootstrap.pypa.io/get-pip.py&amp;quot;&lt;/span&gt; -o &lt;span class="s2"&gt;&amp;quot;get-pip.py&amp;quot;&lt;/span&gt;
$ python2.7 get-pip.py
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; 安裝套件&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pip2.7 install ipython
$ pip2.7 install numpy
$ pip2.7 install pandas
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Windows&lt;/h3&gt;
&lt;p&gt;雖然不建議在windows下開發程式，不過我還是提供一個方法，讓你在接下的文章可以正常作操作。有一個好用的軟體—Anaconda，這個軟體不只可以在windows上使用，在linux和mac都有辦法使用。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;安裝windows版的Anaconda(python 2.7)：&lt;a href="https://www.continuum.io/downloads#windows"&gt;網址&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;安裝結束，就已經安裝好「IPython」的程式，直接打開就可以使用。&lt;/li&gt;
&lt;li&gt;安裝Numpy和Pandas：打開「Anaconda Prompt 」，輸入&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ conda install numpy
$ conda install pandas
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;開啟IPython&lt;/h2&gt;
&lt;p&gt;IPython將會是未來我們這系列會用的一個介面，只要能夠開啟它，我們今天就大功告成了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mac ＆Ubuntu:  在終端機輸入 &lt;code&gt;$ ipython2&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Windows：直接打開「IPython」程式&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;試著import numpy和pandas進來，如果都正常，就代表成功了！&lt;/p&gt;
&lt;p&gt;&lt;img alt="ipython" src="http://www.ycc.idv.tw/media/PlayDataWithPython/ipython.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Ctrl + D 就可以結束跳出啦～ 今天就到這～&lt;/p&gt;</content><category term="Python玩數據"></category></entry><entry><title>從《如何閱讀一本書》想像一種不同的知識呈現方法</title><link href="YCNote/how-to-read-books.html" rel="alternate"></link><published>2017-03-18T12:00:00+08:00</published><updated>2017-03-18T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-03-18:YCNote/how-to-read-books.html</id><summary type="html">&lt;p&gt;⟪如何閱讀一本書⟫是美國哲學家Mortimer Jerome Adler在1940年所著的一本教你如何讀書的書，雖然是70年前的書，不過卻是歷久彌新，隨便google都有好幾十篇的文章在講這本書，其中的一篇就是現在你讀的這一篇。&lt;/p&gt;
&lt;p&gt;書中作者教你讀書的策略，像在練功一樣，必須循序漸進的，掌握了一階技巧在前進下一階，作者把閱讀分為四個階段，至於是哪四階段呢？在這邊賣個關子，我待會會好好的解釋。&lt;/p&gt;
&lt;p&gt;這裡先打個岔，我們先來談談書籍紀錄的知識呈現方式。&lt;/p&gt;
&lt;h2&gt;書籍與網路的PK&lt;/h2&gt;
&lt;p&gt;現今科技革命當中，對我們影響最深的就屬網際網路的發展了，它是人類歷史上資訊傳播方式的大變革，人類技術發展史你可以簡單看成是一部人類如何延長自己的手腳及大腦的故事，斧頭被發明來延長補足手的不足，眼鏡被發明來補足眼睛的不足，書籍被發明來延長溝通，同樣的，網際網路也一樣的是延長溝通的發明，使得資訊傳遞更有效率。&lt;/p&gt;
&lt;p&gt;雖然書籍和網際網路一樣是資訊傳播的方式，不過看起來更為便利的網路並沒有使書籍被淘汰，網路上的知識仍然走不進學校，撇開使用體驗不談，網路知識的缺點之一是過於零碎化，書籍是作者整合資訊和自己的認知而成的體系，有一個比較完整的結構，不是網路上零碎知識的總和所能取代的。&lt;/p&gt;
&lt;p&gt;不過網路上的確有許多整合得相當好的資訊，但如果仔細看這些知識的傳播方式不脫書籍的那一套，甚至有些有名的部落客將他們的部落格文章整理成書，通常在轉換上也不會遇到太大的困難，因為這些資訊的呈現方式和書籍的呈現方式並無太大差異，等於是把書搬到網路上而已。&lt;/p&gt;
&lt;p&gt;不過這樣感覺白白浪費電腦或網路不同於書籍的彈性，譬如說超連結或者是動態回饋，不過一定有人會想起來，維基百科不就是一個好例子嗎？它利用超連結把眾多知識給串起來，讓它更有結構性，不同於書籍的好處是 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;⟪如何閱讀一本書⟫是美國哲學家Mortimer Jerome Adler在1940年所著的一本教你如何讀書的書，雖然是70年前的書，不過卻是歷久彌新，隨便google都有好幾十篇的文章在講這本書，其中的一篇就是現在你讀的這一篇。&lt;/p&gt;
&lt;p&gt;書中作者教你讀書的策略，像在練功一樣，必須循序漸進的，掌握了一階技巧在前進下一階，作者把閱讀分為四個階段，至於是哪四階段呢？在這邊賣個關子，我待會會好好的解釋。&lt;/p&gt;
&lt;p&gt;這裡先打個岔，我們先來談談書籍紀錄的知識呈現方式。&lt;/p&gt;
&lt;h2&gt;書籍與網路的PK&lt;/h2&gt;
&lt;p&gt;現今科技革命當中，對我們影響最深的就屬網際網路的發展了，它是人類歷史上資訊傳播方式的大變革，人類技術發展史你可以簡單看成是一部人類如何延長自己的手腳及大腦的故事，斧頭被發明來延長補足手的不足，眼鏡被發明來補足眼睛的不足，書籍被發明來延長溝通，同樣的，網際網路也一樣的是延長溝通的發明，使得資訊傳遞更有效率。&lt;/p&gt;
&lt;p&gt;雖然書籍和網際網路一樣是資訊傳播的方式，不過看起來更為便利的網路並沒有使書籍被淘汰，網路上的知識仍然走不進學校，撇開使用體驗不談，網路知識的缺點之一是過於零碎化，書籍是作者整合資訊和自己的認知而成的體系，有一個比較完整的結構，不是網路上零碎知識的總和所能取代的。&lt;/p&gt;
&lt;p&gt;不過網路上的確有許多整合得相當好的資訊，但如果仔細看這些知識的傳播方式不脫書籍的那一套，甚至有些有名的部落客將他們的部落格文章整理成書，通常在轉換上也不會遇到太大的困難，因為這些資訊的呈現方式和書籍的呈現方式並無太大差異，等於是把書搬到網路上而已。&lt;/p&gt;
&lt;p&gt;不過這樣感覺白白浪費電腦或網路不同於書籍的彈性，譬如說超連結或者是動態回饋，不過一定有人會想起來，維基百科不就是一個好例子嗎？它利用超連結把眾多知識給串起來，讓它更有結構性，不同於書籍的好處是，你可以在自己不懂的地方，延伸出去找到答案，延伸出去的頁面如果有不懂的還可以繼續向下延伸。&lt;/p&gt;
&lt;p&gt;但也因為可以一直延伸下去，如果要學習的東西本身很複雜和有很強的連貫性，往往你會迷失在一片頁面海裡，這就是維基百科式的知識呈現方式的弱點。反觀，書籍有作者帶著你走，把後面需要用到的知識在前面為你補足，連結起每個知識節點，還會告訴你哪些地方太困難可以先不要碰，建立起易於學習的體系，這是目前網路知識無法做到的。&lt;/p&gt;
&lt;p&gt;讀到這裡你一定會覺得我應該是要吹捧完書籍閱讀有多棒，然後順著下去講如何閱讀。不過，你猜錯了，&lt;strong&gt;我想做的是了解如何有效率的閱讀一本書，然後回過頭想網路的知識呈現方式可以有怎樣的改善，既然有一本教你如何閱讀一本書的書這麼暢銷，也就是代表閱讀一本書本身還是有所不足，說不定我們可以用電腦或網路來補足這樣的不足&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;閱讀的層次&lt;/h2&gt;
&lt;p&gt;這本書對我來說，除了重新審視多年來我的讀書方法外，另外也可以進一步探討我上述的疑問，想找尋更好的「知識體系呈現方式」，必須先了解如何有效地取得知識，才能進一步的構思更好的方式來幫助讀者學習知識，本書雖然是一本教你如何讀一本書的書，其實也是在談如何獲取知識的方法。&lt;/p&gt;
&lt;p&gt;作者將閱讀分為四個層次，分別是基礎閱讀(Elementary Reading)、檢視閱讀(Inspectional Reading)、分析閱讀(Analytical Reading)和主題閱讀(Comparative Reading)，基礎閱讀就是指識字能力，也就是閱讀的基礎能力，下一個層次是檢視閱讀，培養能在閱讀中主動地去思考作者想說的話，更進階是分析閱讀，看完一本書有辦法綜觀一本書的整體性和複雜性，並透徹的了解作者的疑問還有解答，最後研究所等級的主題閱讀，這個時候就不只是一本書的事了，你必須有可以比較好幾本書的能力，去學習一個領域的知識，成為那個領域的專家。&lt;/p&gt;
&lt;h2&gt;檢視閱讀&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;檢視閱讀的首要條件就是化被動為主動，主動去思考檢視一本書&lt;/strong&gt;，那該怎麼著手呢? 首先拿到書，先不要就直接一股腦的從第一頁讀起，就像是古代有經驗的將領一定在會戰前會看看對方擺怎樣的陣式，所以先看看書名，再看看序和索引，了解這本書大概在講什麼，然後針對自己有興趣或不了解的篇章去大略的看過，或者隨機東翻翻西翻翻，這個時候你就了解這本書大概在講什麼了。&lt;/p&gt;
&lt;p&gt;這個過程花不了多少時間，可能大概半小時到一個小時之間，不過這個方法可以讓你大致了解一本書的架構，接下來你就可以決定要不要看這本書，如果你在書店，就可以決定要不要買這本書，有些書並不值得花時間閱讀，或者有些書你只需要知道個大概就好了，雖然有些書是公認的好書，但可能不適合你或你不需要，那這一個小時的閱讀也就足夠啦!&lt;/p&gt;
&lt;p&gt;如果大略掃過一本書，你覺得這本書值得一讀，並且還有一些不懂的地方，那就開始完整的讀一遍吧! 在已經知道這本書大致架構的情況下，開始進行完整的閱讀會更為流暢，依循著架構你可以調配你閱讀的速度，簡單的地方你可以快速的讀過，較難的地方就把速度放慢，也就是說你可以依照自己理解來配速。&lt;/p&gt;
&lt;p&gt;那有什麼依循的標準嗎? 有的，但是一樣的你必須保持主動的思考，你可以藉由檢視以下四個要素來判斷是不是真的理解作者想說的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;現在作者在談什麼?&lt;/li&gt;
&lt;li&gt;作者細說了哪些東西?&lt;/li&gt;
&lt;li&gt;作者說得有道理嗎?&lt;/li&gt;
&lt;li&gt;這些跟我有什麼關係?   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;時時不斷的問這些問題，知識才有辦法進到你的腦袋中，在過程中你可能會劃一些線或做一些筆記，去輔助你達到這四個要素，這樣的閱讀層次稱為檢視閱讀，讀完了你能大概能了解作者想說的，有些簡單的書只需要做到檢視閱讀就夠了，但如果想理解更複雜的書就必須開始另一個層次—分析閱讀。&lt;/p&gt;
&lt;h2&gt;分析閱讀&lt;/h2&gt;
&lt;p&gt;分析閱讀必須用檢視閱讀的精神來完成三個步驟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一個步驟，讀者必須結構性的回答一本書再說什麼。&lt;/strong&gt;首先你要盡快的了解這本書應該歸於哪一類，是文學、戲劇、歷史、傳記，還是科學、數學、哲學、社會科學，是虛構的還是紀實的，是實用類型的還是理論類型的書，這通常可以從書名還有前言中看出來，不過有些書就很難以界定，譬如《飄》是愛情小說抑或是歷史故事。&lt;/p&gt;
&lt;p&gt;了解書的類別有助於你依照相應的分析方法來讀書，書中有詳列各種類型書籍的閱讀方式，在這邊不一一舉例，有興趣的人可以去翻翻這本書。了解一本書的類別就可以開始分析閱讀，你的目標應該放在做到了解書中的三個面向：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;整體性： 能用幾句話寫出這本書主要在說些什麼&lt;/li&gt;
&lt;li&gt;複雜性： 為這本書擬大綱說明書中的篇章架構&lt;/li&gt;
&lt;li&gt;作者的意圖：找出作者的問題和他的答案。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;第二個步驟，讀者必須細部的去詮釋一本書的內容。&lt;/strong&gt;閱讀完一本書你需要回答有哪一些keyword，而這些keyword在作者心中代表的是什麼意思，藉由文字間的連結來產生與作者相同的共識，有哪一些重要的句子當中隱含著重要的主旨。接下來主動出擊，重構作者論述來明白他的主張，也就是以自己的話寫出作者的主張，並且重新審視作者在提出的疑問中解決哪些問題? 還有那些問題沒有被解決?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第三個步驟 ，讀者必須有能力去評論一本書。&lt;/strong&gt;當我們已經客觀的了解作者的想法後，我們可以開始評論一本書，也許有一些論述你是認可的，而另外一些是你不認可的，而當你要指出作者的錯誤，可以從四個方向去著手：證明作者知識不足、知識錯誤、不合邏輯或分析與理由不完整。&lt;/p&gt;
&lt;p&gt;這三個步驟都需要搭配前面所提到的檢視閱讀，完成這三個步驟你對於一本書已經有了深入的了解了! 但如果你想要了解一個領域，或成為那領域的專家，那就不能只是讀一本書而已，每個作者的著眼點不同，觀點也相異，甚至是針鋒相對的，就算是同一個作者，也無法在一本書之中闡述他所有的想法，譬如：如果你看了亞當．斯密的《國富論》，卻沒有看他的《道德情操論》，你會覺得有一些地方似乎很沒有說服力。因此接下來要談的是最後一個層次—主題閱讀。&lt;/p&gt;
&lt;h2&gt;主題閱讀&lt;/h2&gt;
&lt;p&gt;開始主題閱讀前，你先要有一份書單，這份書單可以囊括你想要研究的主題，這份書單可以是從一些書裡的參考書單，或者是同一個作者的其他書，有了這份書單，要再進一步的建立起這些書彼此間的連結，最好還是有簡單的翻過這些書，了解這些書大概在講什麼，建立起這些書大致的連結。&lt;/p&gt;
&lt;p&gt;有了這個書單當作地圖，我們就可以開始主題閱讀了。主題閱讀分為五個步驟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一個步驟，找出你的書單中最重要的章節&lt;/strong&gt;，並且去讀這些章節，在強調一下喔！是章節喔！不是最重要的一本書喔！在主題閱讀中，我們的重點應該放在主題上而不是單一書籍上，雖然作者也覺得很難以做到，所以如果你對這個領域還不夠了解，還是先利用分析閱讀好好的K幾本書在說，有了基本認識再來做主題閱讀。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二步驟，是建立與書單上的作者們之間的共識&lt;/strong&gt;，如同之前在分析閱讀所提到的，找出重要的keyword，並且了解作者所表示這個keyword的含意，你才有辦法和作者產生共識，但現在書單上有很多位作者，每位作者也許所用的語言都不同，所以你需要建立起每個作者之間的連結。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第三步驟，有了字義上的共識後，我們就可以開始釐清每本書所討論的中心主旨。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第四步驟，藉由每本書的中心主旨，我們就可以界定他們在討論的議題&lt;/strong&gt;，哪一些是主要的議題，哪一些是次要的議題，哪一些議題作者彼此間有差不多的看法，而哪些議題是爭論不休的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最後一個步驟，把自己丟進去討論，開始評論、分析這一些議題的討論，然後提出自己的看法。&lt;/strong&gt; 事實上，主題閱讀有點像分析閱讀的擴大版，只是分析閱讀是只對一本書，而主題閱讀則是對一群書，方法上差異不大。&lt;/p&gt;
&lt;h2&gt;書籍與網路的第二回合PK&lt;/h2&gt;
&lt;p&gt;回到最初的問題，我們藉由這本書來了解書籍閱讀本身有什麼弱點，我們需要用怎樣的策略來使得書籍閱讀更有效率，而我們是否能夠使用網路或電腦去改善這些缺點，讓網路可以在下一輪PK中扳回一城。&lt;/p&gt;
&lt;p&gt;先總結一下這本書，這本書所告訴我們的閱讀法則，其實不外乎是三個面向，第一，主動去閱讀，能用自己的話來講一本書的內容，第二，與作者建立相同的共識，第三，針對作者討論的議題，加以評論，提出自己的看法。&lt;/p&gt;
&lt;p&gt;從這幾個面向，所以我腦中就有了一個未來知識傳遞的另一種可能。在讀者方面，我們可以用超連結的方式來主動去找尋我們想要了解的部分，就像是維基百科一樣，但你所閱讀的網頁本身應該有所結構，不然很可能會使得讀者迷失在其中。&lt;/p&gt;
&lt;p&gt;所以不像一般的網路資訊，我們需要一個作者來整合，但在作者寫作方面要有所改變，與電腦技術相結合，我們之所以需要學習這本書的這些閱讀技巧，很大的原因是因為書籍本身的結構和架構並不是那麼容易被理解，往往不懂的作者真正想說什麼，書籍沒有明寫出作者心中的keyword和主旨，作者必須很費力的分章節分段落的解釋他的脈絡，好的章節分法會讓書籍更容易閱讀，作者也要很小心的在使用一個keyword的時候要先向讀者解釋清楚，避免造成歧意。&lt;/p&gt;
&lt;p&gt;於是，一件很奇妙的事情發生了，作者必須要有一個架構，從這個架構出發，然後巧妙的打平寫成一段段的文字，然後讀者在從這一段段的文字，試著了解作者想要表達的架構，等於繞了一大圈，如果一開始作者在寫作的時候就有軟體輔助他，作者給軟體一個架構，作者依照脈絡寫作，而這脈絡可以由軟體完整的呈現在讀者面前，另外，keyword的解釋可以放在一個獨立的頁面，用超連結連接進來，讓讀者在不清楚這個keyword的時候有更多的說明可以看，並且軟體可以依照作者所給的結構和段落，連貫成一本書，作者只需要再審視一下有哪個地方不通順，稍作修改，一本書就完成了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有了這樣的軟體，可以輕鬆的讓作者把心中的結構給「存」進文件裡，並且很智慧化的「呈現」給讀者，作者與讀者之間理解的鴻溝將會被弭平。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外，如果讀者想要做主題閱讀，網路資訊更容易重構，你可以把不同的書籍談論相同的章節給連結起來，相互比較閱讀，甚至這個軟體還可以幫你預先整理一個脈絡，這個軟體比你更了解每一位作者心中的脈絡，所以他可以連結比你好，也許還可以幫你準備一個學習順序，就像是一個老師一樣。&lt;/p&gt;
&lt;p&gt;還有可以加入社群的力量，在閱讀的過程，可以針對不清楚的地方進行提問，又或者看看別人寫下的某段的註解或評論，在書中寫下自己的看法，也可以看見其他讀者的評論，此時你不只是一個人在讀書，而是和一群人一起讀書，思考的廣度也就更大了。&lt;strong&gt;如果用這樣的方式來傳遞知識，可以同時兼顧書籍的完整性，和網路知識的彈性，我想知識傳遞將會更有效率。&lt;/strong&gt;&lt;/p&gt;</content></entry><entry><title>機器學習技法 學習筆記 (3)：Kernel Regression</title><link href="YCNote/ml-course-techniques_3.html" rel="alternate"></link><published>2017-03-15T12:00:00+08:00</published><updated>2017-03-15T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-03-15:YCNote/ml-course-techniques_3.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋Probabilistic SVM、Kernel Logistic Regression、Kernel Ridge Regression、Support Vector Regression (SVR)。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在上一篇當中我們看到了Kernel Trick的強大，我們繼續運用這個數學工具在其他的Regression上看看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Soft-Margin SVM其實很像L2 Regularized Logistic Regression&lt;/h3&gt;
&lt;p&gt;上一篇中提到的Soft-Margin SVM其實很像&lt;a href="http://www.ycc.idv.tw/tag__筆記：機器學習基石/"&gt;《機器學習基石》&lt;/a&gt;裡頭提到的L2 Regularized Logistic Regression，如果你還記得的話，Logistic Regression是為了因應雜訊而給予每筆資料的描述賦予「機率」的性質，讓Model在看Data的時候不那麼的非黑及白，那時候有提到這叫做Soft Classification，而這個概念就非常接近於Soft-Margin的概念。&lt;/p&gt;
&lt;p&gt;從數學式來看會更清楚，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Soft-Margin SVM：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;min. (W&lt;sup&gt;T&lt;/sup&gt;W/2) + C×𝚺&lt;sub&gt;n&lt;/sub&gt; ξ&lt;sub&gt;n&lt;/sub&gt; s.t …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋Probabilistic SVM、Kernel Logistic Regression、Kernel Ridge Regression、Support Vector Regression (SVR)。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在上一篇當中我們看到了Kernel Trick的強大，我們繼續運用這個數學工具在其他的Regression上看看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Soft-Margin SVM其實很像L2 Regularized Logistic Regression&lt;/h3&gt;
&lt;p&gt;上一篇中提到的Soft-Margin SVM其實很像&lt;a href="http://www.ycc.idv.tw/tag__筆記：機器學習基石/"&gt;《機器學習基石》&lt;/a&gt;裡頭提到的L2 Regularized Logistic Regression，如果你還記得的話，Logistic Regression是為了因應雜訊而給予每筆資料的描述賦予「機率」的性質，讓Model在看Data的時候不那麼的非黑及白，那時候有提到這叫做Soft Classification，而這個概念就非常接近於Soft-Margin的概念。&lt;/p&gt;
&lt;p&gt;從數學式來看會更清楚，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Soft-Margin SVM：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;min. (W&lt;sup&gt;T&lt;/sup&gt;W/2) + C×𝚺&lt;sub&gt;n&lt;/sub&gt; ξ&lt;sub&gt;n&lt;/sub&gt; s.t. y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b) ≥ 1-ξ&lt;sub&gt;n&lt;/sub&gt;且ξ&lt;sub&gt;n&lt;/sub&gt; ≥ 0, n=1~N&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的式子中，可以將限制條件由max取代掉，轉換成下面的Unbounded的表示方法，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Soft-Margin SVM：&lt;br&gt;&lt;/p&gt;
&lt;p&gt;min. C×𝚺&lt;sub&gt;n&lt;/sub&gt; Err&lt;sub&gt;hinge,n&lt;/sub&gt; + (W&lt;sup&gt;T&lt;/sup&gt;W/2)&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其中，Err&lt;sub&gt;hinge,n&lt;/sub&gt;=max[0,1-y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b)]，稱之為Hinge Error Measure&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;接下來比較一下L2 Regularized Logistic Regression，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;L2 Regularized Logistic Regression：&lt;br&gt;&lt;/p&gt;
&lt;p&gt;min. (1/N)×𝚺&lt;sub&gt;n&lt;/sub&gt; Err&lt;sub&gt;ce,n&lt;/sub&gt; +  (λ/N)×W&lt;sup&gt;T&lt;/sup&gt;W&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;其中，Err&lt;sub&gt;ce,n&lt;/sub&gt;=ln[1+exp(-y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;))]，為Cross-Entropy Error Measure。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;你會發現Soft-Margin SVM和L2 Regularized Logistic Regression兩個式子的形式是很接近的，都有W&lt;sup&gt;T&lt;/sup&gt;W這一項，只是意義上不同，在Soft-Margin SVM裡頭W&lt;sup&gt;T&lt;/sup&gt;W所代表的是反比於空白區大小距離的函式，而在L2 Regularized Logistic Regression裡頭則是指Regularization。&lt;/p&gt;
&lt;p&gt;另外，我們來疊一下Err&lt;sub&gt;hinge,n&lt;/sub&gt;和Err&lt;sub&gt;ce,n&lt;/sub&gt;來看看這兩個函數像不像，&lt;/p&gt;
&lt;p&gt;&lt;img alt="compare:hinge and ce" src="https://dl.dropbox.com/s/qg2gyf8646cp3jh/MachineLearningTechniques.000_03.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Err&lt;sub&gt;hinge,n&lt;/sub&gt;和Err&lt;sub&gt;ce,n&lt;/sub&gt;是非常接近的，所以我們可以說做Soft-Margin SVM，很像是在做L2 Regularized Logistic Regression。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;雖然說Soft-Margin SVM和L2 Regularized Logistic Regression非常的像，但是我在做完Soft-Margin SVM後，仍然沒辦法像Logistic Regression一樣得到一個具有機率分布的Target Function，以下提供了兩種方法，第一種是間接的方法，使用兩階段學習來達成Logistic的效果；第二種是直接將L2 Regularized Logistic Regression加入有如Soft-Margin SVM的Kernel性質。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;使用SVM做Logistic Regression：Probabilistic SVM&lt;/h3&gt;
&lt;p&gt;要讓Soft-Margin SVM在最後呈現的Target Function時具有機率性質，最簡單的作法就是透過兩階段的學習來達成，第一階段先用Soft-Margin SVM去解出切分資料的平面，第二階段再將Logistic Function套在這個平面上，並做Fitting，最後我們就得到一個以Logistic Function表示的Target Function，這個稱之為Probabilistic SVM。實際操作方法如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;使用Soft-Margin SVM解出切平面W&lt;sub&gt;SVM&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;Z+b&lt;sub&gt;SVM&lt;/sub&gt;=0，並將所有Data進一步的轉換到 Z'&lt;sub&gt;n&lt;/sub&gt;=W&lt;sub&gt;SVM&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;Z(X&lt;sub&gt;n&lt;/sub&gt;)+b&lt;sub&gt;SVM&lt;/sub&gt;。&lt;/li&gt;
&lt;li&gt;接下來用轉換後的結果{Z'&lt;sub&gt;n&lt;/sub&gt;, y&lt;sub&gt;n&lt;/sub&gt;}做Logistic Regression得到係數A和B。&lt;/li&gt;
&lt;li&gt;最後的Target Function就是 g(x)=Θ(A∙(W&lt;sub&gt;SVM&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;Z(X&lt;sub&gt;n&lt;/sub&gt;)+b&lt;sub&gt;SVM&lt;/sub&gt;)+B)，Θ為Logistic Function。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的方法有一個缺點，就是如果B的值不接近0時，SVM的切平面就會和Logistic Regression的邊界就會不同，而且一個Model要Fitting兩次也相當的麻煩，以下還有另外一個可以達到一樣的具有機率性質的效果的方法—Kernel Logistic Regression。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Trick的真正精髓：Representer Theorem&lt;/h3&gt;
&lt;p&gt;在說明Kernel Logistic Regression之前我們先來複習一下Kernel的概念，並且從中將他的重要觀念萃取出來。&lt;/p&gt;
&lt;p&gt;再來看一眼我們怎麼解Kernel Soft-Margin SVM的，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kernel Soft-Margin SVM：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;在0 ≤ α&lt;sub&gt;n&lt;/sub&gt; ≤ C; 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt; = 0的限制條件下，求解min. [(1/2)𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;α&lt;sub&gt;m&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;m&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;]&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;得到α&lt;sub&gt;n&lt;/sub&gt;，然後&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;W = 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;b=y&lt;sub&gt;sv&lt;/sub&gt;-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;sv&lt;/sub&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其中W可以想成是由Z&lt;sub&gt;n&lt;/sub&gt;所組合而成的，而決定貢獻程度則反應在放在它前面的係數(α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;)，y&lt;sub&gt;n&lt;/sub&gt;決定貢獻的方向，α&lt;sub&gt;n&lt;/sub&gt;決定影響的程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;數學上，有個理論Representer Theorem可以告訴我們，所有的最佳化問題中，W的最佳解都是由Z&lt;sub&gt;n&lt;/sub&gt;所組合而成的，以線性代數的角度，就是W由Z&lt;sub&gt;n&lt;/sub&gt;所展開(span)，數學上表示成W*=𝚺&lt;sub&gt;n&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;這個性質為Kernel Trick提供了一個良好的基礎，每次我們只要遇到W*&lt;sup&gt;T&lt;/sup&gt;Z的部分，我們就可以使用Representer Theorem把問題轉換成W*&lt;sup&gt;T&lt;/sup&gt;Z=𝚺&lt;sub&gt;n&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;Z=𝚺&lt;sub&gt;n&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X)，就可以使用Kernel Function了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="kernel trick" src="https://dl.dropbox.com/s/zba8381572jub0r/MachineLearningTechniques.000_04.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上圖是老師在上課時列出來SVM、PLA和Logistic Regression的W的展開式，你會發現都可以表現成Representer Theorem的形式。&lt;/p&gt;
&lt;p&gt;有了這個概念，我們就可以把很多問題都利用Representer Theorem來轉換，並且套上Kernel Trick。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Logistic Regression&lt;/h3&gt;
&lt;p&gt;那我們有了Representer Theorem就可以直接來轉換L2 Regularized Logistic Regression，讓它有擁有Kernel的效果，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;L2 Regularized Logistic Regression：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;min. (1/N)×𝚺&lt;sub&gt;n&lt;/sub&gt; ln[1+exp(-y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;))] +  (λ/N)×W&lt;sup&gt;T&lt;/sup&gt;W&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;使用W*=𝚺&lt;sub&gt;n&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;代入得，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kernel Logistic Regression: &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;min. (1/N)×𝚺&lt;sub&gt;n&lt;/sub&gt; ln[ 1+exp(-y&lt;sub&gt;n&lt;/sub&gt;×𝚺&lt;sub&gt;n&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X)) ] +  (λ/N)×𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt;β&lt;sub&gt;m&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的式子可以使用Grandient Descent來求解β&lt;sub&gt;n&lt;/sub&gt;，進而得到W*=𝚺&lt;sub&gt;n&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;。而且在Kernel Function的幫助之下，我們更容易可以做到非常高次的特徵轉換。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Ridge Regression&lt;/h3&gt;
&lt;p&gt;同理，我們也可以把相同技巧套用到Ridge Regression，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ridge Regression：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;min. (1/N)×𝚺&lt;sub&gt;n&lt;/sub&gt; (y&lt;sub&gt;n&lt;/sub&gt;-W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt; +  (λ/N)×W&lt;sup&gt;T&lt;/sup&gt;W&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;使用W*=𝚺&lt;sub&gt;n&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;代入得，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kernel Ridge Regression：&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;min. (1/N)×𝚺&lt;sub&gt;n&lt;/sub&gt; (y&lt;sub&gt;n&lt;/sub&gt;-𝚺&lt;sub&gt;m&lt;/sub&gt; β&lt;sub&gt;m&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;))&lt;sup&gt;2&lt;/sup&gt; +  (λ/N)×𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt;β&lt;sub&gt;m&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的式子也可以使用Grandient Descent來求解β&lt;sub&gt;n&lt;/sub&gt;。&lt;/p&gt;
&lt;p&gt;另外，這個式子有辦法推出解析解，先把上式可以寫成矩陣形式，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kernel Ridge Regression：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;min. E&lt;sub&gt;aug&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;E&lt;sub&gt;aug&lt;/sub&gt;=(1/N)×(β&lt;sup&gt;T&lt;/sup&gt;K&lt;sup&gt;T&lt;/sup&gt;Kβ-2β&lt;sup&gt;T&lt;/sup&gt;K&lt;sup&gt;T&lt;/sup&gt;y+y&lt;sup&gt;T&lt;/sup&gt;y) +  (λ/N)×β&lt;sup&gt;T&lt;/sup&gt;Kβ)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以，由∇E&lt;sub&gt;aug&lt;/sub&gt;=0就可以得到最小值成立的條件為&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;β*=(λI+K)&lt;sup&gt;-1&lt;/sup&gt;y&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其實這個式子非常像之前在線性模型時使用的Pseudo-Inverse，&lt;/p&gt;
&lt;p&gt;Pseudo-Inverse：W=(X&lt;sup&gt;T&lt;/sup&gt;X)&lt;sup&gt;-1&lt;/sup&gt;X&lt;sup&gt;T&lt;/sup&gt;y&lt;/p&gt;
&lt;p&gt;不過現在更為強大了，可以求得非線性模型+Regularization下的解析解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我們可以使用Kernel Ridge Regression來做分類問題，稱之為Least-Squares SVM (LSSVM) 。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Support Vector Regression (SVR)&lt;/h3&gt;
&lt;p&gt;其實，不管是Kernel Logistic Regression還是Kernel Ridge Regression，這種直接套用Representer Theorem在Regression上的都有一個缺點。&lt;/p&gt;
&lt;p&gt;那就是它們的&lt;strong&gt;β&lt;sub&gt;n&lt;/sub&gt;並不確保大多數是0&lt;/strong&gt;，如果Data筆數非常多的話，這在計算上會是一種負荷。在之前我們討論Kernel SVM時有提到只有Support Vector的數據才會對Model最後的結果有所貢獻，Support Vector的α&lt;sub&gt;n&lt;/sub&gt;&amp;gt;0；而不是Support Vector的數據則沒有貢獻，Non-Support Vector的α&lt;sub&gt;n&lt;/sub&gt;=0。所以你可以想見的是，&lt;strong&gt;α&lt;sub&gt;n&lt;/sub&gt;大多數是0除了Support Vector外，我們稱這叫做「Sparse α&lt;sub&gt;n&lt;/sub&gt;」性質&lt;/strong&gt;，有這樣的性質可以大大的減少計算量。&lt;/p&gt;
&lt;p&gt;因此接下來我們打算&lt;strong&gt;讓Regression具有Support Vector的性質，稱之為Support Vector Regression (SVR)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="SVR" src="https://dl.dropbox.com/s/76wyl84tdhj9r7a/MachineLearningTechniques.006.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;見上圖說明，Support Vector Regression簡稱SVR，以往的Linear Regression是求一條擬合直線能使所有數據點到直線的Error最小，而現在我們賦予它Soft-Margin的能力，&lt;strong&gt;SVR將擬合直線向外擴張距離ε，在這個擴張的區域裡頭的數據點不去計算它的Error，只有在超出距離ε外的才去計算Error&lt;/strong&gt;，此時這個擬合直線有點像一條水管，水管外我們才計算Error，所以又稱之為Tube Regression。&lt;/p&gt;
&lt;p&gt;這個概念和Soft-Margin SVM有點像，都是在邊界給予犯錯的機會，不同的是Soft-Margin SVM因為是分類問題，所以不允許錯誤的數據超過界，所以評估Error的方向是向內的，而SVR是向外評估Error，超出水管之上的Error我們記作ξ&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋀&lt;/sup&gt;，低於水管之下的Error我們記作ξ&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋁&lt;/sup&gt;，&lt;strong&gt;所以SVR的目的就是在Regularization之下使得ξ&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋀&lt;/sup&gt;+ξ&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋁&lt;/sup&gt;最小，並且調整距離ε和C來決定對Error的容忍程度&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;這個問題同樣的可以化作Dual問題，問題變成只需要最佳化α&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋀&lt;/sup&gt;和α&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋁&lt;/sup&gt;，再使用最佳化後的α&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋀&lt;/sup&gt;和α&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋁&lt;/sup&gt;就可以得到W和b。其中W=𝚺&lt;sub&gt;n&lt;/sub&gt; (α&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋀&lt;/sup&gt;-α&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋁&lt;/sup&gt;) Z&lt;sub&gt;n&lt;/sub&gt;這式子裡頭隱含著Representer Theorem，每筆數據的貢獻程度β&lt;sub&gt;n&lt;/sub&gt;=(α&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋀&lt;/sup&gt;-α&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋁&lt;/sup&gt;)，&lt;strong&gt;因此在管子內的α&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋀&lt;/sup&gt;=0且α&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;⋁&lt;/sup&gt;=0，不會有所貢獻，這使得SVR具有Sparse的性質，可以大大的減少計算&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;這一篇中，我們一開始揭露了「Soft-Margin SVM其實很像L2 Regularized Logistic Regression」的這個現象，所以在SVM中最小化W&lt;sup&gt;T&lt;/sup&gt;W有點像是Regression中的Regularization，也因為形式上相當的接近，所以在SVM裡頭用到的數學技巧同樣的可以套到這些有Regularized的Regression上。&lt;/p&gt;
&lt;p&gt;然後，我們從Kernel Soft-Margin SVM中萃取出Kernel Trick的精華—Representer Theorem，最佳化的W可以由Data的Feature Z&lt;sub&gt;n&lt;/sub&gt;所組成，記作W*=𝚺&lt;sub&gt;n&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;，這提供了Kernel Trick背後的實踐基礎，接下來我們就開始運用Representer Theorem在L2 Regularized Logistic Regression和Ridge Regression上，讓這些Regression可以輕易的做非線性特徵轉換。&lt;/p&gt;
&lt;p&gt;最後，我們指出了直接套用Representer Theorem在Regression上的缺點就是參數並不Sparse，所以造成計算量大大增加。因此Support Vector Regression (SVR)參照Soft-Margin SVM的形式重新設計Regression，並且使用Dual Transformation和Kernel Function來轉化問題，最後SVR就具有Sparse的特性了。&lt;/p&gt;
&lt;p&gt;上一篇跟這一篇，談的是「Kernel Models」，在這樣的形式下我們可以讓我們的「特徵轉化」變得更為複雜，甚至是無窮多次方還是做得到的。下一篇，我們會進到另外一個主題—Aggregation Models。&lt;/p&gt;</content><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (2)：Support Vector Machine (SVM)</title><link href="YCNote/ml-course-techniques_2.html" rel="alternate"></link><published>2017-02-20T12:00:00+08:00</published><updated>2017-02-20T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-02-20:YCNote/ml-course-techniques_2.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋Hard-Margin Support Vector Machine (SVM)、Kernel Function、Kernel Hard-Margin SVM、Soft-Margin SVM、Kernel Soft-Margin SVM、拉格朗日乘子法（Lagrange Multiplier）、Lagrangian Dual Problem。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在&lt;a href="http://www.ycc.idv.tw/YCNote/post/29"&gt;上一篇文章&lt;/a&gt;當中，我們掃過了《機器學習技法》 將會包含的內容，今天我們正式來看SVM。&lt;/p&gt;
&lt;p&gt;如果我想要使用無窮次高次方的非線性轉換加入我的Model，可以做到嗎？上一篇，我告訴大家，只要使用Dual Transformation加上Kernel Function等數學技巧就可以做到，我們今天就來看一下這是怎麼一回事。&lt;/p&gt;
&lt;p&gt;本篇文章分為兩個部分，第一部分我盡量不牽扯太多數學計算，而將數學證明放在第二個部分，數學證明的部分非常複雜，但我並不打算把它們忽略掉，因為這些數學計算是相當重要的，它所帶來的方法和概念是可以重複使用的，也有助於你了解和創造其他演算法，所以有心想要成為專家的你請耐心的把後半段的數學看完。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Hard-Margin Support Vector Machine (SVM …&lt;/h3&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;本篇內容涵蓋Hard-Margin Support Vector Machine (SVM)、Kernel Function、Kernel Hard-Margin SVM、Soft-Margin SVM、Kernel Soft-Margin SVM、拉格朗日乘子法（Lagrange Multiplier）、Lagrangian Dual Problem。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在&lt;a href="http://www.ycc.idv.tw/YCNote/post/29"&gt;上一篇文章&lt;/a&gt;當中，我們掃過了《機器學習技法》 將會包含的內容，今天我們正式來看SVM。&lt;/p&gt;
&lt;p&gt;如果我想要使用無窮次高次方的非線性轉換加入我的Model，可以做到嗎？上一篇，我告訴大家，只要使用Dual Transformation加上Kernel Function等數學技巧就可以做到，我們今天就來看一下這是怎麼一回事。&lt;/p&gt;
&lt;p&gt;本篇文章分為兩個部分，第一部分我盡量不牽扯太多數學計算，而將數學證明放在第二個部分，數學證明的部分非常複雜，但我並不打算把它們忽略掉，因為這些數學計算是相當重要的，它所帶來的方法和概念是可以重複使用的，也有助於你了解和創造其他演算法，所以有心想要成為專家的你請耐心的把後半段的數學看完。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Hard-Margin Support Vector Machine (SVM)&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Hard-Margin SVM" src="https://dl.dropbox.com/s/tknka2p5a7qcqcn/MachineLearningTechniques.001.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;回到我們最熟悉的二元分類問題，如果問題的答案是線性可分的話，我們可以找到一條直線把兩類Data給切開來，而在以前PLA的方法，切在哪裡其實是沒辦法決定的，PLA只能幫你找到可以分開兩類的一刀，但不能幫你把這刀切的更好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我們希望這個切開兩類的邊界可以離兩類Data越遠越好，讓邊界到Data有一個較大的空白區，這就是Hard-Margin SVM做的事&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;我們先來看一下如何計算切平面到任意Data的距離，首先我先假設切平面的方程式為&lt;/p&gt;
&lt;p&gt;W&lt;sup&gt;T&lt;/sup&gt;X+b = 0 (切平面)&lt;/p&gt;
&lt;p&gt;回想一下高中數學，這個平面的法向量是W，垂直於平面，所以垂直於平面的單位法向量是 W/|W|，今天如果我有一點Data Point落在X，另外在平面上任意再找一點X&lt;sub&gt;0&lt;/sub&gt;，從X&lt;sub&gt;0&lt;/sub&gt;到X的向量表示為X-X&lt;sub&gt;0&lt;/sub&gt;，這個向量如果投影到單位法向量上，這個向量的大小正是Data Point到平面的最短距離，表示成&lt;/p&gt;
&lt;p&gt;d = |W・(X - X&lt;sub&gt;0&lt;/sub&gt;)| / |W|&lt;/p&gt;
&lt;p&gt;X&lt;sub&gt;0&lt;/sub&gt;符合切平面的方程式W&lt;sup&gt;T&lt;/sup&gt;X&lt;sub&gt;0&lt;/sub&gt;+b = 0代入，得&lt;/p&gt;
&lt;p&gt;d = |W・X + b| / |W|&lt;/p&gt;
&lt;p&gt;所以假如我有一群線性可分的二元分類Data，這個切平面我希望可以離兩類Data越遠越好，所以我會有一段全部都沒有Data的空白區，這邊假設這個空白區的邊界為&lt;/p&gt;
&lt;p&gt;W&lt;sup&gt;T&lt;/sup&gt;X+b = ±1&lt;/p&gt;
&lt;p&gt;這個假設是可以做到的，因為我們可以以比例去調整W和b來達到縮放的效果，而不會影響切平面W&lt;sup&gt;T&lt;/sup&gt;X+b = 0 。從上面的距離公式，我們知道在這個假設之下，空白區邊界距離切平面為&lt;/p&gt;
&lt;p&gt;margin = 1 / |W|&lt;/p&gt;
&lt;p&gt;而剛好落在這空白區邊界的Data會符合以下方程式&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;X&lt;sub&gt;n&lt;/sub&gt;+b) = 1 (Support Vector)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;y&lt;sub&gt;n&lt;/sub&gt;的正負剛好和(W&lt;sup&gt;T&lt;/sup&gt;X&lt;sub&gt;n&lt;/sub&gt;+b)相抵消，&lt;strong&gt;這些落在空白區邊界的Data被稱為Support Vector，就字面上的意義就像是空白區由這一些數據給「撐」起來，而切平面只由這些Support Vector的數據點所決定，和其他的數據點無關&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果考慮所有Data的話，應該要滿足&lt;/p&gt;
&lt;p&gt;y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;X&lt;sub&gt;n&lt;/sub&gt;+b) ≥ 1 (All Data)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;綜合上述，Hard-Margin SVM的目標就是，在符合y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;X&lt;sub&gt;n&lt;/sub&gt;+b) ≥ 1 , n=1~N的條件下，求Margin (1 / |W|)最大的情形，也可以等價於求 (W&lt;sup&gt;T&lt;/sup&gt;W/2) 最小的情形，這個問題有辦法使用QP Solver來求解，詳見&lt;a href="https://en.wikipedia.org/wiki/Quadratic_programming"&gt;這裡&lt;/a&gt;，我就不多加介紹這個數學工具。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Function&lt;/h3&gt;
&lt;p&gt;Kernel Function是最終可以讓我們有無限多次方特徵的數學工具，但這個工具非常容易理解。&lt;/p&gt;
&lt;p&gt;假設考慮一個非線性轉換，將X空間轉換到Z空間，那如果我需要計算轉換過的兩個新Features相乘Z&lt;sub&gt;n&lt;/sub&gt;(X&lt;sub&gt;n&lt;/sub&gt;)×Z&lt;sub&gt;m&lt;/sub&gt;(X&lt;sub&gt;m&lt;/sub&gt;)，我有辦法&lt;strong&gt;不需要先做特徵轉換再相乘&lt;/strong&gt;，而是直接使用原有的Features X&lt;sub&gt;n&lt;/sub&gt;和X&lt;sub&gt;m&lt;/sub&gt;求出Z&lt;sub&gt;n&lt;/sub&gt;(X&lt;sub&gt;n&lt;/sub&gt;)×Z&lt;sub&gt;m&lt;/sub&gt;(X&lt;sub&gt;m&lt;/sub&gt;)的最後結果？這種情形數學可以表示成K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)=Z&lt;sub&gt;n&lt;/sub&gt;(X&lt;sub&gt;n&lt;/sub&gt;)×Z&lt;sub&gt;m&lt;/sub&gt;(X&lt;sub&gt;m&lt;/sub&gt;)，這個函式就叫Kernel Function。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果有了Kernel Function這樣的數學工具，就可以簡化和優化因為「特徵轉換」所帶來的複雜計算。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我列出以下幾種Kernel Function：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Polynomial Kernel：K&lt;sub&gt;Q&lt;/sub&gt;(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)=(ζ+γ X&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;X&lt;sub&gt;m&lt;/sub&gt;)&lt;sup&gt;Q&lt;/sup&gt;等價於 「Q次方非線性轉換後的兩個新特徵相乘」。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Guassian Kernel：K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)=exp(-γ|X&lt;sub&gt;n&lt;/sub&gt;-X&lt;sub&gt;m&lt;/sub&gt;|&lt;sup&gt;2&lt;/sup&gt;)等價於 「無窮次方非線性轉換後的兩個新特徵相乘」。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此有了Guassian Kernel的幫忙，我們完全不需要管特徵轉換有多複雜，我們可以直接使用原有的Features 來計算「無窮次方的非線性轉換」。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最後給予Kernel Function一個物理解釋，Kernel Function說穿了就是兩個向量轉換到Z空間後的「內積」，「內積」可以約略想成是「相似程度」，當兩個向量同向，內積是正的，相似度高，但當兩個向量反向，內積是負的，相似度極低，所以你會發現Guassian Kernel在X&lt;sub&gt;n&lt;/sub&gt;=X&lt;sub&gt;m&lt;/sub&gt;會出現最大值，因為代表這兩個位置相似度極高。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Hard-Margin SVM&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Kernel Hard-Margin SVM" src="https://dl.dropbox.com/s/dpyh8stjm665zxd/MachineLearningTechniques.002.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;那我們如何使用Kernel Function來使得Hard-Margin SVM更厲害呢？我們必須額外引入另外的數學工具，包括：Lagrange Multiplier和Lagrange Dual Problem，才有辦法把Kernel Function用上，不過這部份的數學有一些複雜，我將這部份的證明放在後面的附錄上，這邊就直接從結果講起。&lt;/p&gt;
&lt;p&gt;Kernel Hard-Margin SVM的公式是，在α&lt;sub&gt;n&lt;/sub&gt;  ≥ 0; 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt; = 0的限制條件下，求解α&lt;sub&gt;n&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;使得 [(1/2)𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;α&lt;sub&gt;m&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;m&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;]為最小值，&lt;/p&gt;
&lt;p&gt;其中K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)就是Kernel Function，由你的特徵轉換方式來決定，這個問題一樣可以使用QP Solver來求解。&lt;/p&gt;
&lt;p&gt;當我們已經有了每筆數據點的α&lt;sub&gt;n&lt;/sub&gt;了，接下來可以利用α&lt;sub&gt;n&lt;/sub&gt;求出切平面的W和b，在那之前來看一下α&lt;sub&gt;n&lt;/sub&gt;的意義，&lt;strong&gt;α&lt;sub&gt;n&lt;/sub&gt;可以看作是某個數據點對切平面的貢獻程度，α&lt;sub&gt;n&lt;/sub&gt;=0的這些數據點為非Support Vector，而α&lt;sub&gt;n&lt;/sub&gt;&amp;gt;0的這些數據點是Support Vector，所以對切平面有貢獻的只有Support Vector而已&lt;/strong&gt;，這和剛剛的結論相同。因此，W和b可由Support Vector決定，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;W = 𝚺&lt;sub&gt;n=sv&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;b=y&lt;sub&gt;sv&lt;/sub&gt;-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;sv&lt;/sub&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;最後提一個非常重要的概念，是什麼原因讓我們不需要管特徵轉換的複雜度？以往我們的作法是這樣的，我們有每筆Data的Features，接下來對每筆Data做特徵轉換，然後在用特徵轉換後的新Features去Train線性模型，這麼一來如果特徵轉換的次方非常高的話，計算的複雜度就會全落在特徵轉換上。&lt;strong&gt;所以我們巧妙的使用數學工具，讓我們可以單單使用Data的Labels來做優化，而將複雜的特徵轉換利用Kernel Function的方式「嵌入」到優化的過程裡頭，此時計算量就只與Data數量有關，所以可以完全不管特徵轉換所帶來的複雜度&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Hard-Margin SVM: 無窮次方的特徵轉換效果如何?&lt;/h3&gt;
&lt;p&gt;終於我們可以使用無窮次方的特徵轉換了，只要使用Kernel Hard-Margin SVM搭配上Guassian Kernel：K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)=exp(-γ|X&lt;sub&gt;n&lt;/sub&gt;-X&lt;sub&gt;m&lt;/sub&gt;|&lt;sup&gt;2&lt;/sup&gt;)就可以辦到，下圖是模擬的結果，是不是看起來很強大，隨著γ的不同會有不一樣的切分方法，&lt;strong&gt;你會發現γ越大時看起來的結果越接近Overfitting，所以必須小心挑選γ的大小。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Guassian Kernel in Hard-Margin SVM" src="https://dl.dropbox.com/s/nzl29z7z8cveefx/MachineLearningTechniques.000_01.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/203_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/203_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Soft-Margin SVM&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Soft-Margin SVM" src="https://dl.dropbox.com/s/ipimu7we3zd8vho/MachineLearningTechniques.003.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;剛剛Hard-Margin SVM會很容易Overfitting的原因在於它的機制無法&lt;strong&gt;容忍雜訊&lt;/strong&gt;，所以接下來要講的Soft-Margin SVM可以容忍部份的Data違反規則，讓它們可以超出空白區的邊界。&lt;/p&gt;
&lt;p&gt;見上圖，可以發現我們稍微修改了Hard-Margin SVM，加入了參數ξ&lt;sub&gt;n&lt;/sub&gt;，ξ&lt;sub&gt;n&lt;/sub&gt;代表錯誤的Data離空白區邊界有多遠，而我們將ξ&lt;sub&gt;n&lt;/sub&gt;的總和加進去Cost裡面，在優化的過程中將使違反的狀況不會太多和離邊界太遠，&lt;strong&gt;而參數C負責控制ξ&lt;sub&gt;n&lt;/sub&gt;總和的影響程度，如果C很大，代表不大能容忍雜訊；如果C很小，則代表對雜訊的容忍很寬鬆&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;因此我們現在有兩種Support Vector，一種是剛好落在空白區邊界的，稱為Free Support Vector；另外一種是違反規則並超出空白區的，稱為Bounded Support Vector，切平面一樣是由這些Support Vector所決定。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Soft-Margin SVM&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Kernel Soft-Margin SVM" src="https://dl.dropbox.com/s/opndal9c0nhbo9p/MachineLearningTechniques.004.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;接下來同樣的對Soft-Margin SVM做數學上Lagrange Multiplier和Lagrange Dual Problem的轉換，再將Kernel Function用上，一樣的，我將這部份的證明放在後面的附錄上，這邊就直接從結果講起。&lt;/p&gt;
&lt;p&gt;Kernel Soft-Margin SVM的公式是，在0 ≤ &lt;strong&gt;α&lt;sub&gt;n&lt;/sub&gt; ≤ C&lt;/strong&gt;; 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt; = 0的限制條件下，求解α&lt;sub&gt;n&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;使得 [(1/2)𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;α&lt;sub&gt;m&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;m&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;]為最小值，&lt;/p&gt;
&lt;p&gt;你會發現和Kernel Hard-Margin SVM唯一只差在α&lt;sub&gt;n&lt;/sub&gt;被C所限制。&lt;/p&gt;
&lt;p&gt;當我們已經有了每筆數據點的α&lt;sub&gt;n&lt;/sub&gt;了，接下來可以利用α&lt;sub&gt;n&lt;/sub&gt;求出切平面的W和b，α&lt;sub&gt;n&lt;/sub&gt;一樣的可以看作是某個數據點對切平面的貢獻程度，α&lt;sub&gt;n&lt;/sub&gt;=0的這些數據點為非Support Vector，而α&lt;sub&gt;n&lt;/sub&gt;&amp;gt;0的這些數據點是Support Vector，可以進一步細分，α&lt;sub&gt;n&lt;/sub&gt; &amp;lt; C為Free Support Vector，而α&lt;sub&gt;n&lt;/sub&gt;＝C為Bounded Support Vector。相同的，W和b可由Support Vector (Free Support Vector和Bounded Support Vector)決定，跟Kernel Hard-Margin SVM公式一模一樣&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;W = 𝚺&lt;sub&gt;n=sv&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;b=y&lt;sub&gt;sv&lt;/sub&gt;-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;sv&lt;/sub&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Soft-Margin SVM: 容忍雜訊的無窮次方特徵轉換&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Guassian Kernel in Soft-Margin SVM" src="https://dl.dropbox.com/s/aw9v5e2tr9onqfy/MachineLearningTechniques.000_02.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/204_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/204_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;來看看Kernel Soft-Margin SVM搭配上Guassian Kernel的效果如何，上圖是模擬的結果，我們會發現有部分Data違反分類規則，所以Soft-Margin SVM確實可以容忍雜訊，而且C越小，容忍雜訊的能力越強，所以要特別注意C的選取，如果沒有選好還是可能造成Overfitting的。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;在這一篇當中，我們介紹了Hard-Margin SVM和Soft-Margin SVM，並且成功的利用數學工具將問題轉換成，可以單單使用Data的Labels來做優化，而將複雜的特徵轉換利用Kernel Function的方式「嵌入」到優化的過程裡頭，此時計算量就只與Data數量有關，所以可以完全不管特徵轉換所帶來的複雜度，因此利用Guassian Kernel就可以做到「無窮多次的特徵轉換」了。最後再次強調數學的部分非常重要，它提供的方法和概念是可以重複使用的，而這部份的數學是少不了的，所以有興趣的可以繼續往下看下去。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;[進階] 拉格朗日乘子法（Lagrange Multiplier）&lt;/h3&gt;
&lt;p&gt;如果是物理系學生修過古典力學，應該對這個數學工具不陌生。&lt;strong&gt;Lagrange Multiplier是用在有限制條件之下的求極值問題&lt;/strong&gt;，步驟如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;問題：在限制 g&lt;sub&gt;i&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) = 0, i=1~k  之下，求 f(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) 的極值&lt;/li&gt;
&lt;li&gt;假設Lagrange Function：   L(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;,λ&lt;sub&gt;i&lt;/sub&gt;) = f(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) + 𝚺&lt;sub&gt;i&lt;/sub&gt; λ&lt;sub&gt;i&lt;/sub&gt; × g&lt;sub&gt;i&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;)&lt;/li&gt;
&lt;li&gt;聯立方程式求解：&lt;/li&gt;
&lt;li&gt;找L的極值：∇L = 0  [Stationarity Condition]&lt;/li&gt;
&lt;li&gt;g&lt;sub&gt;i&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) = 0, i=1~k  [Primal Feasibility Condition]&lt;/li&gt;
&lt;li&gt;求解以上聯立方程式得到最佳解 x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上面的聯立方程式不難理解，Primal Feasibility Condition就是我們的限制式，然後Stationarity Condition就是求極值的方法，非常直觀，滿足上面的式子我們就可以在限制上面找極值。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;上面是一般的Lagrange Multiplier，只有考慮到限制式是等式的情形，假如限制條件是不等式呢？我們來看一下加強版的Lagrange Multiplier：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;問題：在限制 g&lt;sub&gt;i&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) = 0, i=1~k 且  h&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) ≤ 0, j=1~r 之下，求 f(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) 的極值&lt;/li&gt;
&lt;li&gt;假設Lagrange Function：   L(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;, λ&lt;sub&gt;i&lt;/sub&gt;,μ&lt;sub&gt;j&lt;/sub&gt;) = f(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) + 𝚺&lt;sub&gt;i&lt;/sub&gt; λ&lt;sub&gt;i&lt;/sub&gt; × g&lt;sub&gt;i&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) + 𝚺&lt;sub&gt;j&lt;/sub&gt; μ&lt;sub&gt;j&lt;/sub&gt; × h&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;)&lt;/li&gt;
&lt;li&gt;聯立方程式求解：&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;找L的極值：∇L = 0  [Stationarity Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;g&lt;sub&gt;i&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) = 0, i=1~k 且 h&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) ≤ 0, j=1~r  [Primal Feasibility Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;μ&lt;sub&gt;j&lt;/sub&gt;  × h&lt;sub&gt;j&lt;/sub&gt; (x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) = 0, j=1~r  [Complementary Slackness Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;求L的最小值時 μ&lt;sub&gt;j&lt;/sub&gt; ≥ 0, j=1~r；求L的最大值時 μ&lt;sub&gt;j&lt;/sub&gt; ≤ 0, j=1~r [Dual Feasibility Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;以上的條件包括Stationarity、Primal Feasibility、Complementary Slackness、Dual Feasibility通稱 KKT (Karush-Kuhn-Tucker) Conditions&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;加強版的Lagrange Multiplier和一般版的一樣有Stationarity Condition和Primal Feasibility Condition。唯一增加的是Complementary Slackness Condition和Dual Feasibility Condition。&lt;/p&gt;
&lt;p&gt;先來講一下Complementary Slackness Condition怎麼來的，我們來考慮不等式條件h&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) ≤ 0，會有兩個情形發生，一個是壓到邊界，也就是h&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) = 0，這個時候問題就回到一般版的Lagrange Multiplier，此時μ&lt;sub&gt;j&lt;/sub&gt;和λ&lt;sub&gt;i&lt;/sub&gt;效果是一樣的，μ&lt;sub&gt;j&lt;/sub&gt;可以是任意值；另外一種情況是我沒壓到邊界，也就是h&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) &amp;lt; 0，這個時候我可以把這個限制看作不存，最簡易的方法就是令μ&lt;sub&gt;j&lt;/sub&gt;=0，他在L(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;, λ&lt;sub&gt;i&lt;/sub&gt;,μ&lt;sub&gt;j&lt;/sub&gt;) 中就不參與作用了。&lt;strong&gt;所以綜合壓到邊界和不壓到兩種情況，我們可以寫出一個有開關效果的方程式 μ&lt;sub&gt;j&lt;/sub&gt; × h&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) = 0，這就是Complementary Slackness Condition。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外一個是Dual Feasibility Condition，這個限制一樣是在不等式條件才會發生，μ&lt;sub&gt;j&lt;/sub&gt;的正負號取決於L是要求最大還是求最小值，稍微解釋一下，找極值我們用∇L = 0這個式子來求，代入Lagrange Function後得∇L = ∇f +𝚺&lt;sub&gt;i&lt;/sub&gt;λ&lt;sub&gt;i&lt;/sub&gt;×∇g&lt;sub&gt;i&lt;/sub&gt;+𝚺&lt;sub&gt;j&lt;/sub&gt;μ&lt;sub&gt;j&lt;/sub&gt;×∇h&lt;sub&gt;j&lt;/sub&gt;=0，先定性來看，假設不計∇g&lt;sub&gt;i&lt;/sub&gt;的影響，當最後解落在h ≤ 0的邊界上時∇f＝- μ×∇h，因為h ≤ 0的關係，所以∇h是朝向可行區的外面，如果今天是求f的極小值，那們∇f應當朝著可行區才合理，如果不是的話則可行區內部有更小更佳的解，所以求極小值時μ ≥ 0；如果是求f的極大值，那∇f應當朝著可行區的外面，所以μ ≤ 0，這個條件待會會用在對偶問題上面。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;其實我們之前在《機器學習基石》裡的Regularization有偷用了Lagrange Multiplier的產物。&lt;/p&gt;
&lt;p&gt;Regularization將W的長度限制在一個範圍，表示成&lt;/p&gt;
&lt;p&gt;|W|&lt;sup&gt;2&lt;/sup&gt; ≤ C&lt;/p&gt;
&lt;p&gt;在這個條件下我們要找E&lt;sub&gt;in&lt;/sub&gt;的極小值，使用加強版的Lagrange Multiplier：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;問題：在限制  |W|&lt;sup&gt;2&lt;/sup&gt; - C ≤ 0 之下，求 E&lt;sub&gt;in&lt;/sub&gt; 的極小值&lt;/li&gt;
&lt;li&gt;假設Lagrange Function：   L = E&lt;sub&gt;in&lt;/sub&gt; + μ × ( |W|&lt;sup&gt;2&lt;/sup&gt; - C)&lt;/li&gt;
&lt;li&gt;聯立方程式求解：&lt;/li&gt;
&lt;li&gt;𝞉L / 𝞉W = 𝞉E&lt;sub&gt;in&lt;/sub&gt; / 𝞉W + 2μ × |W| = 0  [Stationarity Condition]&lt;/li&gt;
&lt;li&gt;|W|&lt;sup&gt;2&lt;/sup&gt; - C ≤ 0  [Primal Feasibility Condition]&lt;/li&gt;
&lt;li&gt;μ × ( C - |W|&lt;sup&gt;2&lt;/sup&gt; ) = 0  [Complementary Slackness Condition]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stationarity Condition的結果就是Regularization的結果了，可以&lt;a href="http://www.ycc.idv.tw/YCNote/post/28"&gt;回去參照一下&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;[進階] Lagrangian Dual Problem&lt;/h3&gt;
&lt;p&gt;接下來來講對偶問題，這個部分很難，我也是反覆在網路上看了很多篇介紹才弄懂，推薦大家看&lt;a href="http://www.eng.newcastle.edu.au/eecs/cdsc/books/cce/Slides/Duality.pdf"&gt;這一篇&lt;/a&gt;，這篇介紹的很清楚，應該會對大家理解Lagrangian Dual有幫助。&lt;/p&gt;
&lt;p&gt;來考慮一下待會會用到的求極小值問題，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在限制 g&lt;sub&gt;i&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) = 0, i=1~k 且  h&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) ≤ 0, j=1~r 之下，求 f(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) 的極小值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果我們利用剛剛的解法，稱之為Lagrangian Primal Problem。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;而這個問題可以等效轉換成Lagrangian Dual Problem，利用以下關係式&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Minimum Problem ≡ min. L  ≡ min. [max.&lt;sub&gt;μ ≥ 0&lt;/sub&gt; L] ≥ max.&lt;sub&gt;μ ≥ 0&lt;/sub&gt; [min. L(μ)]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我們在將原本min. L 換成min. [max.&lt;sub&gt;μ ≥ 0&lt;/sub&gt; L] 是不影響結果的，因為我們剛剛分析過了在求最小值時μ ≥ 0是合理的，相反的如果μ &amp;lt; 0，則求max.&lt;sub&gt;μ ≥ 0&lt;/sub&gt; L時會產生無限大的結果，接下來就是交換min.和max.的部分，數學上可以證明min. [max.&lt;sub&gt;μ ≥ 0&lt;/sub&gt; L] ≥ max.&lt;sub&gt;μ ≥ 0&lt;/sub&gt; [min. L(μ)]這樣的關係，我們就稱左式轉到右式為Dual轉換。&lt;/p&gt;
&lt;p&gt;而上面式子右側的求法，我們可以先求出Θ(λ&lt;sub&gt;i&lt;/sub&gt;,μ&lt;sub&gt;j&lt;/sub&gt;) = given λ&lt;sub&gt;i&lt;/sub&gt;,μ&lt;sub&gt;j&lt;/sub&gt; to find min. L(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;, λ&lt;sub&gt;i&lt;/sub&gt;,μ&lt;sub&gt;j&lt;/sub&gt;) ，作法是使用∇L = 0所產生符合極值的參數代入L(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;, λ&lt;sub&gt;i&lt;/sub&gt;,μ&lt;sub&gt;j&lt;/sub&gt;)，換成以λ&lt;sub&gt;i&lt;/sub&gt;,μ&lt;sub&gt;j&lt;/sub&gt;表示的Θ(λ&lt;sub&gt;i&lt;/sub&gt;,μ&lt;sub&gt;j&lt;/sub&gt;)。然後，再求Θ(λ&lt;sub&gt;i&lt;/sub&gt;,μ&lt;sub&gt;j&lt;/sub&gt;)的最大值，就可以了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;經過Dual轉換後，我們將原本在x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;的問題轉換到λ&lt;sub&gt;i&lt;/sub&gt;,μ&lt;sub&gt;j&lt;/sub&gt;的空間上。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;這個轉換我們可以使用下面的圖來解釋，&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lagrangian Dual Geometric Interpretation" src="https://dl.dropbox.com/s/xbham8glnwivfzz/MachineLearningTechniques.005.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;我們先不管g(x)的部分只看f(x)和h(x)的部分，假設所有的Data x映射到f(x)和h(x)會產生一塊區域G。&lt;/p&gt;
&lt;p&gt;在Primal Problem中我們可以很容易的找出h&lt;sub&gt;j&lt;/sub&gt;(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) ≤ 0的限制之下f(x&lt;sub&gt;1&lt;/sub&gt;,x&lt;sub&gt;2&lt;/sub&gt;, … , x&lt;sub&gt;n&lt;/sub&gt;) 的最小值，見上圖左側。&lt;/p&gt;
&lt;p&gt;見上圖中間，Dual Problem採取另外一個方法，它先去找&lt;/p&gt;
&lt;p&gt;Θ(μ) = given μ to find min. L(x,μ)，其中 L(x,μ) = f(x)+μh(x)。&lt;/p&gt;
&lt;p&gt;f(x)+μh(x)=α在圖中的平面上是一條直線，而f(x)+μh(x)的值也就是α也正好是它的「截距」，所以在給定μ後要最小化f(x)+μh(x)的方法，就等效於固定直線斜率最小化截距，所以最後這個直線就必須要切於G才能使得截距最小，所以我們得到一條切於G且斜率(-μ)的直線， 因此我們就順利的得到Θ(μ)的關係式了，接下來我要找出Θ(μ)的最大值，所以就必須往上推，這個時候你就發現答案和前面Primal Problem答案一模一樣，這種最佳化答案相同的情況稱為「Strong Duality」，而最佳化答案不相同的情況就叫做「Weak Duality」，見上圖右側，在這種G的形狀下，就會產生最佳化答案不相同的情況。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;[進階] Hard-Margin SVM Dual + Kernel Function = Kernel Hard-Margin SVM&lt;/h3&gt;
&lt;p&gt;那我們現在可以正式的把Lagrangian Dual的東西放到Hard-Margin SVM上面。&lt;/p&gt;
&lt;p&gt;回想一下Hard-Margin SVM的問題是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;X&lt;sub&gt;n&lt;/sub&gt;+b) ≥ 1 , n=1~N的條件下，求(W&lt;sup&gt;T&lt;/sup&gt;W/2) 最小的情形。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;那如果加上非線性轉換，從X空間轉到Z空間，則問題變成&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b) ≥ 1 , n=1~N的條件下，求(W&lt;sup&gt;T&lt;/sup&gt;W/2) 最小的情形。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以我們可以使用Lagrangian Multiplier來解決問題，依以下步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;假設Lagrange Function：   L(W,b,α) = (W&lt;sup&gt;T&lt;/sup&gt;W/2) +  𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt; × [1-y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b)]&lt;/li&gt;
&lt;li&gt;考慮Primal Feasibility、Complementary Slackness、Dual Feasibility的限制&lt;/li&gt;
&lt;li&gt;Primal Feasibility Condition：1-y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b) ≤ 0 [式1-1]&lt;/li&gt;
&lt;li&gt;Complementary Slackness Condition：α&lt;sub&gt;n&lt;/sub&gt;  × [1-y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b)] = 0 [式1-2]&lt;/li&gt;
&lt;li&gt;Dual Feasibility Condition：α&lt;sub&gt;n&lt;/sub&gt;  ≥ 0 [式1-3]&lt;/li&gt;
&lt;li&gt;先求出Θ(α) = given α to find min. L(W,b,α)&lt;/li&gt;
&lt;li&gt;𝞉L / 𝞉b = - 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt; = 0 [式1-4]&lt;/li&gt;
&lt;li&gt;𝞉L / 𝞉W&lt;sub&gt;n&lt;/sub&gt; =  |W|- 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt; = 0，y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;應該和W同向，所以
     W = 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt; [式1-5]&lt;/li&gt;
&lt;li&gt;因此L(W,b,α)只要滿足[式1-4]和[式1-5]就代表是極小值了&lt;/li&gt;
&lt;li&gt;所以[式1-4]和[式1-5]代入得Θ(α,β) = (-1/2)𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;α&lt;sub&gt;m&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;m&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;m&lt;/sub&gt;+𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;求Θ(α)極大值&lt;/li&gt;
&lt;li&gt;max.[Θ(α)]＝min.[-Θ(α)]=min.[(1/2)𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;α&lt;sub&gt;m&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;m&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;m&lt;/sub&gt;-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;] —[式1-6]&lt;/li&gt;
&lt;li&gt;綜合上述[式1-3]、[式1-4]、[式1-6]並改寫成Kernel的形式得，min. [(1/2)𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;α&lt;sub&gt;m&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;m&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;], s.t. α&lt;sub&gt;n&lt;/sub&gt; ≥ 0 ;  𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt; = 0，使用QP Solver可以求出 α&lt;sub&gt;n&lt;/sub&gt;。&lt;/li&gt;
&lt;li&gt;可以用α&lt;sub&gt;n&lt;/sub&gt;來求W和b&lt;/li&gt;
&lt;li&gt;α&lt;sub&gt;n&lt;/sub&gt;涵義：觀察[式1-2]可得 (1) α&lt;sub&gt;n&lt;/sub&gt; = 0 為Non-Support Vector； (2) α&lt;sub&gt;n&lt;/sub&gt; &amp;gt; 0 代表y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b)=1，為Support Vector。&lt;/li&gt;
&lt;li&gt;由[式1-5]得，W = 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;，從式子中你會發現對W有貢獻的只有Support Vector (α&lt;sub&gt;n&lt;/sub&gt;&amp;gt;0)。&lt;/li&gt;
&lt;li&gt;假設在某個Support Vector(α&lt;sub&gt;n&lt;/sub&gt;&amp;gt;0)上，由[式1-2]可推得，b=y&lt;sub&gt;sv&lt;/sub&gt;-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;sv&lt;/sub&gt;)  (at Support Vector)。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;[進階] Soft-Margin SVM Dual + Kernel Function = Kernel Soft-Margin SVM&lt;/h3&gt;
&lt;p&gt;考慮Soft-Margin SVM和特徵轉換：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b) ≥ 1-ξ&lt;sub&gt;n&lt;/sub&gt;且ξ&lt;sub&gt;n&lt;/sub&gt; ≥ 0, n=1~N的條件下，求(W&lt;sup&gt;T&lt;/sup&gt;W/2) + C 𝚺&lt;sub&gt;n&lt;/sub&gt; ξ&lt;sub&gt;n&lt;/sub&gt;最小的情形。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以我們可以使用Lagrangian Dual Problem來解決問題，依以下步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;假設Lagrange Function：   L(W,b,ξ,α,β) = (W&lt;sup&gt;T&lt;/sup&gt;W/2) + C 𝚺&lt;sub&gt;n&lt;/sub&gt; ξ&lt;sub&gt;n&lt;/sub&gt; +  𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt; × [1-ξ&lt;sub&gt;n&lt;/sub&gt;-y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b)] + 𝚺&lt;sub&gt;n&lt;/sub&gt; β&lt;sub&gt;n&lt;/sub&gt; × [-ξ&lt;sub&gt;n&lt;/sub&gt;]&lt;/li&gt;
&lt;li&gt;考慮Primal Feasibility、Complementary Slackness、Dual Feasibility的限制&lt;/li&gt;
&lt;li&gt;Primal Feasibility Condition：1-ξ&lt;sub&gt;n&lt;/sub&gt;-y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b) ≤ 0 [式2-1]；-ξ&lt;sub&gt;n&lt;/sub&gt; ≤ 0 [式2-2]&lt;/li&gt;
&lt;li&gt;Complementary Slackness Condition：α&lt;sub&gt;n&lt;/sub&gt;  × [1-ξ&lt;sub&gt;n&lt;/sub&gt;-y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b)] = 0 [式2-3]；β&lt;sub&gt;n&lt;/sub&gt; × [-ξ&lt;sub&gt;n&lt;/sub&gt;] = 0 [式2-4]&lt;/li&gt;
&lt;li&gt;Dual Feasibility Condition：α&lt;sub&gt;n&lt;/sub&gt;  ≥ 0 [式2-5]；β&lt;sub&gt;n&lt;/sub&gt;  ≥ 0 [式2-6]&lt;/li&gt;
&lt;li&gt;先求出Θ(α,β) = given α,β to find min. L(W,b,ξ,α,β)&lt;/li&gt;
&lt;li&gt;𝞉L / 𝞉b = - 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt; = 0 [式2-7]&lt;/li&gt;
&lt;li&gt;𝞉L / 𝞉W&lt;sub&gt;n&lt;/sub&gt; =  |W|- 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt; = 0，y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;應該和W同向，所以
     W = 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt; [式2-8]&lt;/li&gt;
&lt;li&gt;𝞉L / 𝞉ξ&lt;sub&gt;n&lt;/sub&gt; = C - α&lt;sub&gt;n&lt;/sub&gt; - β&lt;sub&gt;n&lt;/sub&gt; = 0 [式2-9]&lt;/li&gt;
&lt;li&gt;因此L(W,b,ξ,α,β)只要滿足[式2-7]、[式2-8]和[式2-9]就代表是極小值了&lt;/li&gt;
&lt;li&gt;所以[式2-7]、[式2-8]和[式2-9]代入得Θ(α,β) = (-1/2)𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;α&lt;sub&gt;m&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;m&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;m&lt;/sub&gt;+𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;求Θ(α,β)極大值&lt;/li&gt;
&lt;li&gt;max.[Θ(α,β)]＝min.[-Θ(α,β)]=min.[(1/2)𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;α&lt;sub&gt;m&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;m&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;m&lt;/sub&gt;-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;] —[式2-10]&lt;/li&gt;
&lt;li&gt;綜合上述[式2-5]、[式2-6]、[式2-9]、[式2-10]並改寫成Kernel的形式得，min. [(1/2)𝚺&lt;sub&gt;n&lt;/sub&gt;𝚺&lt;sub&gt;m&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;α&lt;sub&gt;m&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;m&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;m&lt;/sub&gt;)-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;], s.t. 0 ≤ α&lt;sub&gt;n&lt;/sub&gt; ≤ C;  𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt; = 0，使用QP Solver可以求出 α&lt;sub&gt;n&lt;/sub&gt;。&lt;/li&gt;
&lt;li&gt;可以用α&lt;sub&gt;n&lt;/sub&gt;來求W和b&lt;/li&gt;
&lt;li&gt;α&lt;sub&gt;n&lt;/sub&gt;涵義：觀察[式2-3]和[式2-4]可得 (1) α&lt;sub&gt;n&lt;/sub&gt; = 0 為Non-Support Vector； (2) 0 &amp;lt; α&lt;sub&gt;n&lt;/sub&gt; &amp;lt; C 代表y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b)=1，為Free Support Vector；(3) α&lt;sub&gt;n&lt;/sub&gt; = C 代表y&lt;sub&gt;n&lt;/sub&gt;×(W&lt;sup&gt;T&lt;/sup&gt;Z&lt;sub&gt;n&lt;/sub&gt;+b)=1-ξ&lt;sub&gt;n&lt;/sub&gt;，為Bounded Support Vector。&lt;/li&gt;
&lt;li&gt;由[式2-8]得，W = 𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;Z&lt;sub&gt;n&lt;/sub&gt;，從式子中你會發現對W有貢獻的只有Support Vector (α&lt;sub&gt;n&lt;/sub&gt;&amp;gt;0)。&lt;/li&gt;
&lt;li&gt;假設在某個Support Vector(α&lt;sub&gt;n&lt;/sub&gt;&amp;gt;0且β&lt;sub&gt;n&lt;/sub&gt;&amp;gt;0)上，由[式2-3]和[式2-4]可推得，b=y&lt;sub&gt;sv&lt;/sub&gt;-𝚺&lt;sub&gt;n&lt;/sub&gt; α&lt;sub&gt;n&lt;/sub&gt;y&lt;sub&gt;n&lt;/sub&gt;K(X&lt;sub&gt;n&lt;/sub&gt;,X&lt;sub&gt;sv&lt;/sub&gt;)  (at Support Vector)。&lt;/li&gt;
&lt;/ol&gt;</content><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (1)：我們將會學到什麼? 先見林再來見樹</title><link href="YCNote/ml-course-techniques_1.html" rel="alternate"></link><published>2017-01-12T12:00:00+08:00</published><updated>2017-01-12T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2017-01-12:YCNote/ml-course-techniques_1.html</id><summary type="html">&lt;p&gt;在之前四篇文章中，我總結了台大教授林軒田在Coursera上的《機器學習基石》16堂課程，我覺得這是機器學習初學很重要的基礎課程，接下來我要接續更進階的課程。&lt;/p&gt;
&lt;p&gt;林軒田教授的機器學習是兩學期的課，第一學期是《機器學習基石》，第二學期就是接下來這個系列要講的《機器學習技法》，這兩堂課程是有相當大的銜接關係的，所以如果想看這系列的文章，請先看&lt;a href="http://www.ycc.idv.tw/tag__筆記：機器學習基石/"&gt;這四篇《機器學習基石》的介紹&lt;/a&gt;或者&lt;a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations"&gt;直接到Coursera上學習&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;《機器學習技法》課程影片可以到老師的Youtube [ &lt;a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2"&gt;https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&lt;/a&gt; ]上收看，投影片可以到老師的個人網站上下載 [ &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/&lt;/a&gt; ]。&lt;/p&gt;
&lt;p&gt;以前，我曾經和實驗室的英國學長聊英國的教育方法，然後我驚人的發現，他的學校在大一就已經學過量子場論（物理上很難的學科XDD）了，我就很好奇量子場論不是需要很深厚的數學基礎嗎？大一是要怎麼教啊？他告訴我，他們大一就會完整走過物理的各大領域，不過是用非常概念的方式來學習 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;在之前四篇文章中，我總結了台大教授林軒田在Coursera上的《機器學習基石》16堂課程，我覺得這是機器學習初學很重要的基礎課程，接下來我要接續更進階的課程。&lt;/p&gt;
&lt;p&gt;林軒田教授的機器學習是兩學期的課，第一學期是《機器學習基石》，第二學期就是接下來這個系列要講的《機器學習技法》，這兩堂課程是有相當大的銜接關係的，所以如果想看這系列的文章，請先看&lt;a href="http://www.ycc.idv.tw/tag__筆記：機器學習基石/"&gt;這四篇《機器學習基石》的介紹&lt;/a&gt;或者&lt;a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations"&gt;直接到Coursera上學習&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;《機器學習技法》課程影片可以到老師的Youtube [ &lt;a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2"&gt;https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&lt;/a&gt; ]上收看，投影片可以到老師的個人網站上下載 [ &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/&lt;/a&gt; ]。&lt;/p&gt;
&lt;p&gt;以前，我曾經和實驗室的英國學長聊英國的教育方法，然後我驚人的發現，他的學校在大一就已經學過量子場論（物理上很難的學科XDD）了，我就很好奇量子場論不是需要很深厚的數學基礎嗎？大一是要怎麼教啊？他告訴我，他們大一就會完整走過物理的各大領域，不過是用非常概念的方式來學習，不牽涉到太困難的數學，但這概念的一系列課程卻是四年大學中相當重要的基礎，讓他在開始學細節前就可以知道這些東西未來會用在哪裡？產生了連結讓學習更有效率。&lt;/p&gt;
&lt;p&gt;所以，《機器學習技法》中會介紹很多厲害的機器學習的方法，但這一篇我不直接進去看每個方法的細節，我想帶大家坐著直升機來先看看這遊樂園中有哪些遊樂設施，先來見林再來見樹，會更容易了解。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;有什麼特徵可以使用？&lt;/h3&gt;
&lt;p&gt;在之前《機器學習基石》中，我們講到了Features（特徵）的選擇，&lt;strong&gt;Features（特徵）就是我的Model描述Data的方法，也可以說是影響Data的變數&lt;/strong&gt;，那在之前我們講過Features（特徵）的選擇可以是線性的，那也可以使用「特徵轉換」來產生非線性。&lt;/p&gt;
&lt;p&gt;在這系列文章，我們會看到更多種類的Features，可以分為三類：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Embedding Numerous Features（嵌入大量特徵）&lt;/li&gt;
&lt;li&gt;Combining Predictive Features（綜合預測結果的特徵）&lt;/li&gt;
&lt;li&gt;Distilling Implicit Features（抽取隱含特性的特徵）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我已經盡力用我的理解翻譯上面的英文，哈！&lt;/p&gt;
&lt;p&gt;這些不同種類的Features就會造成不同的Models，這些Models分別是&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Embedding Numerous Features ：Kernel Models（Kernel模型）&lt;/li&gt;
&lt;li&gt;Combining Predictive Features：Aggregation Models（集合模型）&lt;/li&gt;
&lt;li&gt;Distilling Implicit Features：Extraction Models（萃取模型）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;讓我們依序來看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Embedding Numerous Features ：Kernel Models&lt;/h3&gt;
&lt;p&gt;還記得《機器學習基石》中，我們講了哪些Model嗎？我們一開始講了二元分類問題，然後提出了Perceptron Learning Algorithm (PLA)來解決這個問題（&lt;a href="http://www.ycc.idv.tw/YCNote/post/25"&gt;詳見《機器學習基石》第一篇&lt;/a&gt;），如果數據是線性可分的話，我們就可以使用PLA劃分出一條邊界來區分兩種種類。&lt;/p&gt;
&lt;p&gt;接下來提到我們可以使用Regression的方法來做二元分類問題，其中Logistic Regression考慮了雜訊造成每個Label的出現呈機率分布，給予一個較為寬鬆的區分方法，我們會稱PLA為Hard Classification，而Logistic Regression為Soft Classification。（&lt;a href="http://www.ycc.idv.tw/YCNote/post/27"&gt;詳見《機器學習基石》第三篇&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;最後，我們引入「特徵轉換」將我們原本的線性區分推到非線性區分，讓我的Model有更大的複雜度，也因為如此，我們需要使用Regularization和Validation來避免 Overfitting。（&lt;a href="http://www.ycc.idv.tw/YCNote/post/28"&gt;詳見《機器學習基石》第四篇&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;那如果我想要使用無窮個高次方的非線性Features來當作我的Model，可以做到嗎？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;來看一下之前我們做特徵轉換怎麼做的？其實我們沒有多做什麼功夫，我們只是把高次項先產生出來，然後在把這每一項當作線性模型的Features去處理，我們就用線性模型的方法產生了非線性的效果。&lt;/p&gt;
&lt;p&gt;那如果非線性項目的個數無窮多個，顯然這種方法就做不了了啊！&lt;/p&gt;
&lt;p&gt;不過，數學總是會拯救我們，&lt;strong&gt;我們可以使用Dual Transformation加上Kernel Function的技巧，帶我們走捷徑，直接用解析解讓我們得出答案，繞過要考慮無窮多個Features後再處理的窘境。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第一堂課「Linear Support Vector Machine」中，提出Hard-Margin Support Vector Machine (SVM)的架構，他和PLA非常相近，屬於Hard Classification，不同的是Hard-Margin SVM還會讓這個切分的邊界落在最佳的位置上。&lt;/p&gt;
&lt;p&gt;第二堂課 「Dual Support Vector Machine」中，我們開始使用Dual Transformation，把大部分與Data中Features有關的計算，取代成計算與Data中Labels有關的計算，讓我們朝不需要計算Features邁進一步，但是因為有另外一部分還是需要計算Features，所以一樣的我們還是無法讓Features有無窮多個。&lt;/p&gt;
&lt;p&gt;第三堂課「Kernel Support Vector Machine」中，我們引入Kernel Function來幫助我們，現在真的可以不需去列出所有Features也能算出答案，所以我們就可以讓Features有無窮多項，但也因為Model太過複雜，我們不得不去面對Overfitting的問題。&lt;/p&gt;
&lt;p&gt;第四堂課「Soft-Margin Support Vector Machine」中，提出Soft-Margin SVM，它是一種Soft Classification，讓我們可以允許部分錯誤發生，並且同樣的使用Dual Transformation加上Kernel Function的技巧，來讓我可以使用無窮多項的Features，而且因為Soft-Margin SVM可以允許錯誤，也就是對雜訊有容忍度，因此可以幫助我們抑制Overfitting的發生。&lt;/p&gt;
&lt;p&gt;第五堂課「Kernel Logistic Regression」中，我們將Kernel的方法引入Logistic Regression當中來用不同於Soft-Margin SVM的方式做二元分類。&lt;/p&gt;
&lt;p&gt;第六堂課「Support Vector Regression」中，會介紹如何使用Kernel Model來做各類Regression的問題。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這6堂課，主要做的事是把《機器學習基石》裡面學到的東西，全部引入數學工具讓Model的Features可以擴展到無窮多項，產生更強大的Kernel Model。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Combining Predictive Features：Aggregation Models&lt;/h3&gt;
&lt;p&gt;那如果今天我有很多支的Model，我有辦法融合他們得到更好的效果嗎？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這就是Aggregation Models的精髓，Aggregation Models藉由類似於投票的方法綜合各個子Models的結果得到效果更好的Model。換個角度看，你可以把整個體系看成一個新的Model，而原本這些子Models當作轉換過後的新Features，所以Aggregation Model裡頭做了「特徵轉換」，這個轉換產生出許多有預測答案能力的Features，稱為Predictive Features，然後再綜合它們。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Aggregation Models可以分成兩大類，第一種的作法比較簡單，先Train出一個一個獨立的Predictive Features，然後在綜合它們，&lt;strong&gt;「集合」的動作是發生在得到Train好的Predictive Feature之後，這叫做「Blending Models」&lt;/strong&gt;；第二種作法則是，&lt;strong&gt;「集合」的動作和Training同步進行，這叫做「Aggregation-Learning Models」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;從「集合」的方法上也可以進一步細分三種類型，有票票等值的&lt;strong&gt;「Uniform Aggregation Type」&lt;/strong&gt;，有給予Predictive Features不同權重的&lt;strong&gt;「Linear Aggregation Type」&lt;/strong&gt;，甚至還可以用條件或任意Model來分配Predictive Features，這叫做&lt;strong&gt;「Non-linear Aggregation Type」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以兩種類型、三種Aggregation Type，交互產生六種Aggregation Models。&lt;/p&gt;
&lt;p&gt;第七堂課「Bootstrip Aggregation」中，一開始介紹Blending Models的三種Aggregation Type，第一種是直接平均所有的Predictive Features，第二種則是藉由每個Predictive Feature的預測能力，使用線性模型去調配它們的權重，第三種則是使用任意模型分配權重。接著又介紹了Aggregation-Learning Models的Uniform Aggregation Type，稱之為Bagging，它的特點在於它可以利用變換Dataset來造出很多個Predictive Features，並接著做Aggregation。&lt;/p&gt;
&lt;p&gt;第八堂課「Adaptive Boosting」中，介紹Aggregation-Learning Models的Linear Aggregation Type，稱之為AdaBoost，它的特點在於它可以使得每個Predictive Features彼此間可以截長補短。&lt;/p&gt;
&lt;p&gt;第九堂課「Decision Tree」中，介紹Aggregation-Learning Models的Non-linear Aggregation Type，稱之為Decision Tree。&lt;/p&gt;
&lt;p&gt;第十堂課「Random Forest」中，使用Bagging來做Decision Tree，這叫做Random Forest。&lt;/p&gt;
&lt;p&gt;第十一堂課「Gradient Boosted Decision Tree」中，會介紹AdaBoost的Regression版本稱為GradientBoost，並且運用AdaBoost和GradientBoost在Decision Tree上面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這5堂課，我們將會介紹Aggregation Models，引入綜合、集合Predictive Feature的概念來使我們造出更好的Model。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Distilling Implicit Features：Extraction Models&lt;/h3&gt;
&lt;p&gt;那最後這個部分則是介紹現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，在這裡我們通稱Extraction Models。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extraction Models的特色在於它「特徵轉換」的方法，使用一層一層神經元來做非線性的特徵轉換，如果具有多層神經元，那就是做了多次的非線性特徵轉換，這就是「深度學習」，藉由Data機器會自行學習出這每一層的特徵轉換，找出隱含的Features。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第十二堂課「Neural Network」中，介紹Neural Network，並介紹Neural Network的演算法—Back-Propagation（反向傳遞法），在概念上Gradient Descent就是Back-Propagation的源頭，另外介紹避免Overfitting的方法—Early Stopping。&lt;/p&gt;
&lt;p&gt;第十三堂課「Deep Learning」中，開始介紹「深度學習」，考慮多層神經元的Neural Network就叫做Deep Learning，我們會探討如何在Deep Learning中加入Regularization，並介紹一種叫做Auto-encoder的特殊Deep Learning方法。&lt;/p&gt;
&lt;p&gt;第十四堂課「Radial Basis Function Network」中，介紹Radial Basis Function (RBF) Network，並且介紹K-means等非監督分類法。&lt;/p&gt;
&lt;p&gt;第十五堂課「Matrix Factorization」中，我們會探討類別的匹配問題，例如：我想要知道用戶喜歡看什麼電影，而我的Data只有用戶的ID和電影的編號。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這4堂課，我們將會介紹Extraction Model，使用神經元的概念來萃取出Data中的Features。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;後話&lt;/h3&gt;
&lt;p&gt;最後總結一下《機器學習技法》會講哪些東西？我們會講具有三種不同「特徵轉換」方式的Models。&lt;strong&gt;Kernel Model的「特徵轉換」是將非線性Features擴張到無窮多個；Aggregation Model的「特徵轉換」是產生出有預測能力的Features；Extraction Model的「特徵轉換」是利用神經元的方式來做到萃取出隱含的資訊。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;跟《機器學習基石》不一樣的地方，《機器學習技法》中介紹更厲害的「特徵轉換」來產生更厲害的Model，不過因為會有Overfitting的狀況，所以我們還需要介紹相應的配套措施。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在未來一系列的文章，我會帶大家一一的來看這些內容，不過和之前一樣，我不會以課堂當作單位來講，而是以單元式的方式，而且我主要的目的是去點出概念，並盡可能的不去牽涉太多的數學計算，但是數學計算的部分是很重要的，這會影響到你真正的實作，數學的部份可以去看林軒田老師的影片或投影片，裡頭都有很詳細的介紹。&lt;/p&gt;</content><category term="機器學習技法"></category></entry><entry><title>機器學習基石 學習筆記 (4)：機器可以怎麼學得更好?</title><link href="YCNote/ml-course-foundations_4.html" rel="alternate"></link><published>2016-09-18T12:00:00+08:00</published><updated>2016-09-18T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2016-09-18:YCNote/ml-course-foundations_4.html</id><summary type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回中，我們已經了解了機器學習基本的操作該怎麼做。而這一篇中，我們來看&lt;strong&gt;機器可以怎麼學得更好?&lt;/strong&gt; 基本上有三招：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），我們來看看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Feature Transformation（特徵轉換）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="https://dl.dropbox.com/s/vutayryjaw27ckp/MachineLearningFoundations.013.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;在上一回當中我們講了很多的線性模型，大家有沒有懷疑說，數據呈現的方式一定可以用線性描述嗎？我的答案是通常線性描述會表現不錯，但不是絕對，&lt;strong&gt;那我們怎麼用非線性的方法來描述我們的數據，這邊提供一個方法叫做「非線性轉換」，或者又稱為「特徵轉換」（還記得變數x又可以稱為特徵Features）&lt;/strong&gt;，聽起來有點困難齁～其實不會啦！&lt;/p&gt;
&lt;p&gt;假設今天你的Data分布是圓圈狀的分布，顯而易見的你很難用一條線去區分他們，那我們應該怎麼做呢？假設今天有一個轉換可以把這個圓圈狀分布的空間轉換到另外一個空間，在這個新的空間可以做到線性可分，這樣的問題不就解決了嗎，我們會做線性可分的問題啊！&lt;/p&gt;
&lt;p&gt;這個轉換就叫做「非線性轉換」，那這個轉換要怎麼得到呢？可以用人為定義，譬如你知道這個空間的分布狀況是圓圈分布，記作 &lt;/p&gt;
&lt;p&gt;H(x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;) = sign …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回中，我們已經了解了機器學習基本的操作該怎麼做。而這一篇中，我們來看&lt;strong&gt;機器可以怎麼學得更好?&lt;/strong&gt; 基本上有三招：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），我們來看看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Feature Transformation（特徵轉換）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="https://dl.dropbox.com/s/vutayryjaw27ckp/MachineLearningFoundations.013.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;在上一回當中我們講了很多的線性模型，大家有沒有懷疑說，數據呈現的方式一定可以用線性描述嗎？我的答案是通常線性描述會表現不錯，但不是絕對，&lt;strong&gt;那我們怎麼用非線性的方法來描述我們的數據，這邊提供一個方法叫做「非線性轉換」，或者又稱為「特徵轉換」（還記得變數x又可以稱為特徵Features）&lt;/strong&gt;，聽起來有點困難齁～其實不會啦！&lt;/p&gt;
&lt;p&gt;假設今天你的Data分布是圓圈狀的分布，顯而易見的你很難用一條線去區分他們，那我們應該怎麼做呢？假設今天有一個轉換可以把這個圓圈狀分布的空間轉換到另外一個空間，在這個新的空間可以做到線性可分，這樣的問題不就解決了嗎，我們會做線性可分的問題啊！&lt;/p&gt;
&lt;p&gt;這個轉換就叫做「非線性轉換」，那這個轉換要怎麼得到呢？可以用人為定義，譬如你知道這個空間的分布狀況是圓圈分布，記作 &lt;/p&gt;
&lt;p&gt;H(x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;) = sign(-A*x&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;-B*x&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;+C)&lt;/p&gt;
&lt;p&gt;，那只要做一件事我就可以把它轉換成線性可描述的，令 z&lt;sub&gt;1&lt;/sub&gt;=-x&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;; z&lt;sub&gt;2&lt;/sub&gt;=-x&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;，所以問題就變成&lt;/p&gt;
&lt;p&gt;H(z&lt;sub&gt;1&lt;/sub&gt;, z&lt;sub&gt;2&lt;/sub&gt;) = sign(A*z&lt;sub&gt;1&lt;/sub&gt;+B*z&lt;sub&gt;2&lt;/sub&gt;+C)&lt;/p&gt;
&lt;p&gt;此時這個問題就變成一個線性問題啦！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;藉由人為觀察數據並給予適當的特徵轉換是特徵工程（Feature Engineering）中一件重要的事。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但如果我們需要去人為定義這個「非線性轉換」，這就很弱啦！我們當然希望機器可以自行從Data中學習到這個轉換，作法是這樣的，我們先把變數x做個變化和擴充，讓它們互相的相乘創造出高次項，再把這些項等價的放到Linear Model裡，所以我們就用了線性的作法來做到Non-linear Model，而因為有權重W在非線性項前面的關係，所以機器會針對Data自行去調配非線性項，這效果就等同於機器自行學習到「非線性轉換」。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;機器自己學習特徵轉換的這個概念應該是現今ML最重要的概念之一，最近很夯的深度學習甚至不只做一次性的特徵轉換，而是做了多層的特徵轉換，而這些轉換都是機器自動從Data中學來的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特徵轉換讓ML變得很強大，但要特別注意，因為我們增加了非線性項，所以等於是增加了模型的複雜度，這麼做的確可以壓低E&lt;sub&gt;in&lt;/sub&gt;沒有錯，但也可能使得E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;不再成立，也就是Overfitting，所以建議要逐步的增加非線性項，從低次方的項開始加起，避免Overfitting。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Overfitting&lt;/h3&gt;
&lt;p&gt;Overfitting是一個大怪獸，在學習怎麼對付牠之前，我們先來好好的了解牠！&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overfitting" src="https://dl.dropbox.com/s/jet3ocknucywtlz/MachineLearningFoundations.000.03.png"&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上面這張圖用很簡單的方法說明了Overfitting是怎麼一回事，假設藍色的線是Target，也就是我們抽樣的母群體，因為雜訊的關係，抽樣出來的點可能會稍微偏離Target，而如果這個時候我們用二次式來描述這些抽樣出來的Data（上圖中的左側）會發現E&lt;sub&gt;in&lt;/sub&gt;不能壓到0，所以這個時候可能有人想說加進去更高次項來試試看（上圖中的右側），此時會發現E&lt;sub&gt;in&lt;/sub&gt;=0，所有數據都可以被完整描述了，但是你會發現Fit的曲線已經完全偏離了Target，反而是使用低次項還描述的比較好，所以結論是&lt;strong&gt;如果我們把「隨機雜訊」（Stochastic Noise）Fit進去Model裡面就會因此產生Overfitting&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overfitting2" src="https://dl.dropbox.com/s/wzv2dxk0m310wuy/MachineLearningFoundations.000.04.png"&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;但可別以為沒有「隨機雜訊」鬧場就不會出現Overfitting，上圖假設一個沒有「隨機雜訊」的情形，但是Target Function的複雜度很高（上圖右側），當我們從中採樣一些Data來進行Fitting，如上圖左側，我們分別使用2次和10次來做Fitting，這個時候你會發現雖然2次和10次都和Target曲線差很遠，但是小次方的還是Fit的比較好一點，造成Overfitting的原因是因為當Target很複雜的情況下，如果採樣的數據不大，根本無法反應Target本身，所以就算使用了和Target一樣複雜的Model，也只是在瞎猜而已。&lt;strong&gt;這種因為Target本身的複雜度所帶來的雜訊，我們稱為「決定性雜訊」(Deterministic Noise)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Noise" src="https://dl.dropbox.com/s/vgur2f9qjmonlm0/MachineLearningFoundations.000.05.png"&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們來看一下「隨機雜訊」（Stochastic Noise）和「決定性雜訊」（Deterministic Noise）怎麼造成Overfitting的，上圖中的兩張漸層圖表示的是Overfitting的程度，越接近紅色代表Overfitting越嚴重；反之，越接近藍色則Overfitting越輕微。左邊的漸層圖是考慮「隨機誤差」的影響，右邊的漸層圖則是考慮「決定性雜訊」的影響。從這兩張圖我們可以觀察出下面四點，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data數量N越少，越容易Overfitting&lt;/li&gt;
&lt;li&gt;「隨機雜訊」越多，越容易Overfitting&lt;/li&gt;
&lt;li&gt;「決定性雜訊」越多，越容易Overfitting&lt;/li&gt;
&lt;li&gt;Model本身越複雜，越容易Overfitting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;那有什麼方法可以防止Overfitting嗎？有的，有一些之前提過，而有一些我接下來會講，我們來看一下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;從簡單的模型開始做起，從低次模型開始做起，在慢慢加入高次項&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提升資料的正確性：Data Cleaning/Pruning（資料清洗）將錯誤的Data修正或刪除&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Hinting（製造資料），使用合理的方法擴增原有的資料，例如：在圖形辨識問題中，可以用平移和旋轉來擴增出更多Data&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regularization（正規化）：限制權重W的大小以控制高次的影響。&lt;/strong&gt;（接下來會詳述...）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Validation（驗證）：將部分Data保留不進去Fitting，然後用這個Validation Data來檢驗Overfitting的程度。&lt;/strong&gt;（接下來會詳述...）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Regularization（正規化）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="regularation" src="https://dl.dropbox.com/s/3aulwfr8gj2pr14/MachineLearningFoundations.014.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;剛剛我們提到了Overfitting所造成的影響很大一部分是因為Model複雜度所造成的，但是為了可以把E&lt;sub&gt;in&lt;/sub&gt;給壓下去，我們又的確需要去增加高次項，所以依照建議需要從低次項開始慢慢的加，這樣感覺很麻煩啊！&lt;strong&gt;有沒有辦法讓機器自己去限制高次項的出現呢？有的，這就是Regularization（正規化）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;還記得剛剛在講「特徵轉換」時，有提到一點，ML有辦法自行學習「特徵轉換」的關鍵是因為高次項前面有一個可調控的權重，而機器會針對Data來調整權重大小，那其實就是等價於機器自己學習到了「特徵轉換」，同理可知，&lt;strong&gt;我們只要限制權重W的大小就等同於限制了機器無所忌憚的使用高次項&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;經數學證明，&lt;strong&gt;限制權重W的大小可以等價於在E&lt;sub&gt;in&lt;/sub&gt;上面加上「W大小的平方」乘上定值λ，λ越大代表W大小限制越緊；λ越小代表W大小限制越鬆&lt;/strong&gt;，這也非常容易想像，訓練Model的方法是去降低E&lt;sub&gt;in&lt;/sub&gt;，但是如果使用了大的W，就會使得E&lt;sub&gt;in&lt;/sub&gt;增大，自然而然在訓練的過程中，機器會去尋找小一點的W，也就等同於限制了W的大小。&lt;/p&gt;
&lt;p&gt;見上圖左側，我們修改了Gradient Descent讓它受到Regularization的限制。&lt;/p&gt;
&lt;p&gt;而上圖左側下方，顯示了在λ增大的同時，限制W的大小會越來越緊，所以Fitting的結果從原本的Overfitting變成Underfitting。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Underfitting所代表的是Model本身的複雜度不足以使得E&lt;sub&gt;in&lt;/sub&gt;減小，如果你經過Validation（待會會講）後發現沒有Overfitting的現象，但是你的E&lt;sub&gt;in&lt;/sub&gt;始終壓不下來，那就有可能是Underfitting，那你可以考慮增加Model複雜度或者放寬Regularization。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regularizer的選擇常見的有兩種L2和L1，L2使用「W大小的平方」，L1則使用「W大小的絕對值」。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;當Linear Regression使用Regularization限制，統計上有一個名稱稱為Ridge Regression，你可以使用Gradient Descent來做，又或者使用解析解的方法。&lt;/p&gt;
&lt;p&gt;最後提一個Regularization的細節，你會發現因為高次項是彼此兩兩相乘的結果，所以項目的個數會隨著次方增加而增加，這麼一來在做Regularization時可能會過度懲罰高次項，因此，我們可以將Feature轉換成Legendre Polynomials來避免這個問題。&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Validation（驗證）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="validation" src="https://dl.dropbox.com/s/ytuv7ns8s39ocvd/MachineLearningFoundations.015.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;講了這麼多Overfitting，但到底要怎麼去量化Overfitting呢？Overfitting就是E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;不成立，但是E&lt;sub&gt;out&lt;/sub&gt;我們不會知道啊！因為我們不會知道Target Function是什麼，那該怎麼得到量化Overfitting的值呢？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有一個方法叫做Validation可以拿來量化Overfitting的值，這個方法是先將採樣的數據做分離，一部分將會拿來做Model Fitting（Model Training），另外一部分保留起來評估訓練完畢的Model，因為保留的這一部分源自於母群體，而且又沒有被Model給看過，所以它可以很客觀的反應出E&lt;sub&gt;out&lt;/sub&gt;的大小。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我們的Model和Algorithm從以前講到現在已經是越來越複雜了，來複習一下Model和Algorithm受哪些參數影響，Algorithm的選擇就有很多了，包括：PLA、Linear Regression、Logistic Regression；Learning Rate η也需要去選擇大小決定學習速率；Feature Transformation中Feature的決定和次方大小的決定；Regularization也有L2、L1 Regularizer的選擇；還有Regularization的λ值也必須被決定。&lt;/p&gt;
&lt;p&gt;這些條件彼此交互搭配會產生很多組的Model，那該如何挑選Model呢？我們就可以使用Validation來當作一個依據來選擇Model，選擇出E&lt;sub&gt;val&lt;/sub&gt;最小的Model，如上圖所示。&lt;/p&gt;
&lt;p&gt;另外實作上有一些方法：Leave-One-Out Cross Validation和V-Fold Cross Validation，他們的精髓就是保留k筆Data當作未來Validation用，另外一些拿下去Train Model，然後再用這k筆去評估並得到E&lt;sub&gt;val&lt;/sub&gt;1，還沒結束，為了讓E&lt;sub&gt;val&lt;/sub&gt;盡可能的正確，所以我們會在把Data作一個迴轉，這次使用另外一組k組Data來Validation，其餘的再拿去Train Model，然後在評估出E&lt;sub&gt;val&lt;/sub&gt;2， … 以此類推，當轉完一輪之後，在把這些E&lt;sub&gt;val&lt;/sub&gt;1, E&lt;sub&gt;val&lt;/sub&gt;2, ...做平均得到一個較為精確E&lt;sub&gt;val&lt;/sub&gt;。那Leave-One-Out Cross Validation顧名思義就是k=1，但這樣做要付出的代價就是計算量太大了，所以V-Fold Cross Validation則使用k=V來做。實務上，我常常做Validation時根本不會去Cross它們，我大都只是保留一部分的Data來驗證而已，給大家參考。&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;總結&lt;/h3&gt;
&lt;p&gt;來到了這四篇有關於林軒田教授機器學習基石學習筆記的尾聲了，讓我們重溫看看我們學會了什麼？&lt;/p&gt;
&lt;p&gt;一開始我帶大家初探ML的基本架構，建立Model、使用Data訓練、最後達到描述Target Function的目的，也帶大家認識各種機器學習的類型。&lt;/p&gt;
&lt;p&gt;接下來，我們用理論告訴大家，ML是不是真的可以做到，那在什麼時候可以做到？要符合哪些條件？我們知道要有好的Model，VC Dimension越小越好，也就是可調控的參數越少越好，才會使得E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;成立；要有足夠的Data；要有好的Learning Algorithm能把E&lt;sub&gt;in&lt;/sub&gt;壓低，這三種條件成立後，如此一來Model在描述訓練數據很好的同時也可以很好的去預測母群體，但我們發現E&lt;sub&gt;in&lt;/sub&gt;壓低和可調控的參數越少越好兩者是Trade-off，所以我們必須取適當的VC Dimension。&lt;/p&gt;
&lt;p&gt;再接下來我們開始看實際上ML該怎麼做，引入相當重要的Learning Algorithm，也就是Gradient Descent，並且說明了Linear Regression和Logistic Regression，而且還可以使用這兩種Regression來做分類問題。&lt;/p&gt;
&lt;p&gt;那最後就真正亮出ML的三大絕招啦：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），Feature Transformation使得Model更為強大，所以E&lt;sub&gt;in&lt;/sub&gt;更能夠壓低，但是為了避免Overfitting我們必須去限制它，Regularization可以限制高次項的貢獻，另外，Validation可以量化Overfitting的程度，有了這個我們就可以去選出體質健康而且E&lt;sub&gt;in&lt;/sub&gt;又小的Model。&lt;/p&gt;
&lt;p&gt;機器學習基石的這些概念都很重要，往後如果你開始學習其他的ML技巧，例如：深度學習，這些知識都是你強大的基礎，所以多看幾次吧！&lt;/p&gt;</content><category term="機器學習基石"></category></entry><entry><title>About Me</title><link href="YCNote/about-me.html" rel="alternate"></link><published>2016-08-17T12:00:00+08:00</published><updated>2016-08-17T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2016-08-17:YCNote/about-me.html</id><summary type="html">&lt;p&gt;&lt;img src="http://www.ycc.idv.tw/media/Me/my_picture.png"; width="150"&gt;&lt;/img&gt;&lt;/p&gt;
&lt;h2&gt;YC Chen 陳宜昌&lt;/h2&gt;
&lt;p&gt;你好，我是宜昌，我從小就對這個世界充滿著好奇心，喜歡接觸新的事物，喜歡思考並鑽研知識，物理本科生的我，身體裡永遠住著那追根究柢的科學家，因此我習慣於追溯原理來認識世界。&lt;/p&gt;
&lt;p&gt;現在我正在學習Machine Learning，希望成為這領域的專家，為此目標，開啟了我的自學旅程，從基礎電腦科學到演算法，從Machine Learning理論學習到實務操作，一步一腳印的積累，而寫作正是自我檢驗的最好方法，因此這個網站我會與大家分享一些我學到的東西，希望你們會喜歡，有任何疑問都可以在文章下面留言，或者我也很樂意你寫信與我討論。&lt;/p&gt;
&lt;h2&gt;Contact Me&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Email:&lt;/strong&gt;    &lt;a href="mailto:ycc.tw.email@gmail.com" target="blank"&gt;ycc.tw.email@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt;    &lt;a href="https://github.com/GitYCC" target="blank"&gt;https://github.com/GitYCC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Website:&lt;/strong&gt;    &lt;a href="http://www.ycc.idv.tw" target="blank"&gt;www.ycc.idv.tw&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img src="http://www.ycc.idv.tw/media/Me/my_picture.png"; width="150"&gt;&lt;/img&gt;&lt;/p&gt;
&lt;h2&gt;YC Chen 陳宜昌&lt;/h2&gt;
&lt;p&gt;你好，我是宜昌，我從小就對這個世界充滿著好奇心，喜歡接觸新的事物，喜歡思考並鑽研知識，物理本科生的我，身體裡永遠住著那追根究柢的科學家，因此我習慣於追溯原理來認識世界。&lt;/p&gt;
&lt;p&gt;現在我正在學習Machine Learning，希望成為這領域的專家，為此目標，開啟了我的自學旅程，從基礎電腦科學到演算法，從Machine Learning理論學習到實務操作，一步一腳印的積累，而寫作正是自我檢驗的最好方法，因此這個網站我會與大家分享一些我學到的東西，希望你們會喜歡，有任何疑問都可以在文章下面留言，或者我也很樂意你寫信與我討論。&lt;/p&gt;
&lt;h2&gt;Contact Me&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Email:&lt;/strong&gt;    &lt;a href="mailto:ycc.tw.email@gmail.com" target="blank"&gt;ycc.tw.email@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt;    &lt;a href="https://github.com/GitYCC" target="blank"&gt;https://github.com/GitYCC&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Website:&lt;/strong&gt;    &lt;a href="http://www.ycc.idv.tw" target="blank"&gt;www.ycc.idv.tw&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>機器學習基石 學習筆記 (3)：機器可以怎麼樣學習?</title><link href="YCNote/ml-course-foundations_3.html" rel="alternate"></link><published>2016-08-07T12:00:00+08:00</published><updated>2016-08-07T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2016-08-07:YCNote/ml-course-foundations_3.html</id><summary type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回中，我們已經了解了機器學習在理論上有怎樣的條件才可以達成，所以接下來我們就可以正式的來看有哪一些機器學習的方法。&lt;/p&gt;
&lt;p&gt;在這一篇中，我會帶大家初探：&lt;strong&gt;機器可以怎麼樣學習?&lt;/strong&gt; 內容包括：Gradient Descent、Linear Regression、Logistic Regression、使用迴歸法做二元分類問題等等。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Gradient Descent（梯度下降）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="https://dl.dropbox.com/s/9wwibe1ix3cs1od/MachineLearningFoundations.009.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;還記得上一回我們歸納出了一套ML的流程，複習一下&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;準備好足夠的數據&lt;/li&gt;
&lt;li&gt;把Model建立好，d&lt;sub&gt;VC&lt;/sub&gt;必須要是有限的，而且大小要適中&lt;/li&gt;
&lt;li&gt;定義好評估E&lt;sub&gt;in&lt;/sub&gt;的Error Measurement&lt;/li&gt;
&lt;li&gt;使用演算法找出最佳參數把E&lt;sub&gt;in&lt;/sub&gt;降低&lt;/li&gt;
&lt;li&gt;最後評估一下是否有Overfitting的狀況，確保E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;請容許我先不管Model這部份該怎麼建立，我們先來看如何找到最佳參數這部份，&lt;strong&gt;假設今天我知道E&lt;sub&gt;in&lt;/sub&gt;的評估方法，我該如何找到最佳的參數來使得E&lt;sub&gt;in&lt;/sub&gt;更小？有一套普遍的方法叫做Gradient Descent&lt;/strong&gt;，很強大，甚至連現今流行的 …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回中，我們已經了解了機器學習在理論上有怎樣的條件才可以達成，所以接下來我們就可以正式的來看有哪一些機器學習的方法。&lt;/p&gt;
&lt;p&gt;在這一篇中，我會帶大家初探：&lt;strong&gt;機器可以怎麼樣學習?&lt;/strong&gt; 內容包括：Gradient Descent、Linear Regression、Logistic Regression、使用迴歸法做二元分類問題等等。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Gradient Descent（梯度下降）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="https://dl.dropbox.com/s/9wwibe1ix3cs1od/MachineLearningFoundations.009.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;還記得上一回我們歸納出了一套ML的流程，複習一下&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;準備好足夠的數據&lt;/li&gt;
&lt;li&gt;把Model建立好，d&lt;sub&gt;VC&lt;/sub&gt;必須要是有限的，而且大小要適中&lt;/li&gt;
&lt;li&gt;定義好評估E&lt;sub&gt;in&lt;/sub&gt;的Error Measurement&lt;/li&gt;
&lt;li&gt;使用演算法找出最佳參數把E&lt;sub&gt;in&lt;/sub&gt;降低&lt;/li&gt;
&lt;li&gt;最後評估一下是否有Overfitting的狀況，確保E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;請容許我先不管Model這部份該怎麼建立，我們先來看如何找到最佳參數這部份，&lt;strong&gt;假設今天我知道E&lt;sub&gt;in&lt;/sub&gt;的評估方法，我該如何找到最佳的參數來使得E&lt;sub&gt;in&lt;/sub&gt;更小？有一套普遍的方法叫做Gradient Descent&lt;/strong&gt;，很強大，甚至連現今流行的「深度學習」找最佳解的機制也是從Gradient Descent衍生出來的。&lt;/p&gt;
&lt;p&gt;想像一下你是一位登山客，你在爬一座由E&lt;sub&gt;in&lt;/sub&gt;所決定的高山，你的目標是去這座山最低的山谷，也就是E&lt;sub&gt;in&lt;/sub&gt;最小的地方，因為村莊正在那裡，但是很不幸的你沒有地圖，這個時候有什麼方法可以知道低谷在哪裡呢？答案是就一直下坡吧！反正我知道村莊在山谷裡，那我就一路下山應該就可以找到村莊了，這就是Gradient Descent的精髓。&lt;/p&gt;
&lt;p&gt;在數學上有一個衡量函數變化的東西，這就是Gradient（梯度），Gradient是一個向量，它的「方向」指向函數值增加量最大的方向，而它的「大小」反應這個變化有多大，其實就是一次微分啦！只不過Gradient推廣到高維度而已。所以我們和這個登山客做一樣的事情，我們朝著下降最多的方向前進，這就是Gradient Descent（梯度下降法），我剛剛說了，梯度是指向函數值增加量最大的方向，那顯然我們往反方向走就可以達到最大下降，所以如果我們有一個Error函數E&lt;sub&gt;in&lt;/sub&gt;，它的Gradient就是∇E&lt;sub&gt;in&lt;/sub&gt;，那我們的下降方向就是-∇E&lt;sub&gt;in&lt;/sub&gt;。&lt;/p&gt;
&lt;p&gt;來看一下上圖中Gradient Descent的流程，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定義出Error函數&lt;/li&gt;
&lt;li&gt;Error函數讓我們可以去評估E&lt;sub&gt;in&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;算出它的梯度∇E&lt;sub&gt;in&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;朝著∇E&lt;sub&gt;in&lt;/sub&gt;的反方向更新參數W，而每次只跨出η大小的一步&lt;/li&gt;
&lt;li&gt;反覆的計算新參數W的梯度，並一再的更新參數W&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;這邊要特別注意，流程中的第四項中，有提到η，&lt;strong&gt;η稱為Learning Rate，它影響的是更新步伐的大小&lt;/strong&gt;，η的選擇要適當，如果η太小的時候，我們可能要花很多時間才可以走到低點，但如果η太大的話，又可能導致我們在兩個山腰間跳來跳去，甚至越更新越往高處跑，&lt;strong&gt;所以選擇適當的η相當的重要，所以下次如果你發現E&lt;sub&gt;in&lt;/sub&gt;一直降不下來甚至在增大，試著將η減小看看&lt;/strong&gt;。另外η也可以是變動的值，我們可以直接設η＝|∇E&lt;sub&gt;in&lt;/sub&gt;|，這麼一來遇到陡坡的時候它就會跨大一點的步伐，遇到緩坡的時候就會跨小步一點，隨狀況調整η的值。&lt;/p&gt;
&lt;p&gt;Gradient Descent (GD, 梯度下降) 有兩個變形，分別為Stochastic Gradient Descent (SGD, 隨機梯度下降) 和 Batch Gradient Descent (BGD, 批次梯度下降)，這差別只在於評估∇E&lt;sub&gt;in&lt;/sub&gt;的時候所考慮的Data數量，正常來說必須要考慮所有的Data，我們才會得到真正的E&lt;sub&gt;in&lt;/sub&gt;，才有辦法算出正確的∇E&lt;sub&gt;in&lt;/sub&gt;，但這樣所要付出的代價就是較大的計算量。&lt;/p&gt;
&lt;p&gt;所以&lt;strong&gt;Stochastic Gradient Descent的作法是一次只拿一筆Data來求E&lt;sub&gt;in&lt;/sub&gt;'，並且更新參數W&lt;/strong&gt;，這樣的更新方法顯然會比較不穩定，但我們假設，經過好幾輪的更新後，已經完整看過整個數據了，所以平均來說效果和一般的Gradient Descent一樣。&lt;/p&gt;
&lt;p&gt;另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，&lt;strong&gt;Batch Gradient Descent一次評估k組數據，並更新參數W&lt;/strong&gt;，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的計算方法你可以想像成在做矩陣計算，矩陣元素在計算的時候往往是可以拆開計算的，此時GPU利用它強大的平行化運算將這些元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，&lt;strong&gt;所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Batch Gradient Descent，是既有效率又穩定的作法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Gradient Descent求最佳解其實是會產生問題的，還記得我們的目標嗎？我們希望可以走到最低點的山谷裡，所以我們採取的策略是不斷的下降，這個時候如果遇到兩種情形就會動彈不得，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;小山谷，數學上稱為&lt;strong&gt;Local Minimum&lt;/strong&gt;，雖然在那點看起來，那邊的確是低點，但卻不是整個E&lt;sub&gt;in&lt;/sub&gt;的最低點&lt;/li&gt;
&lt;li&gt;平原，數學上稱為&lt;strong&gt;Saddle Point（鞍點）&lt;/strong&gt;，在一片很平的區域，∇E&lt;sub&gt;in&lt;/sub&gt;=0，所以就停止不動了&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;針對這些問題有一些改良後的演算法，在這裡不詳述，請參考&lt;a href="http://ruder.io/optimizing-gradient-descent/"&gt;S. Ruder的整理&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;好！我們已經了解了怎麼使用Gradient Descent去找到E&lt;sub&gt;in&lt;/sub&gt;最小的最佳參數，那我們可以回頭看Model有哪一些？Error Measure該怎麼定？&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Linear Regression&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="https://dl.dropbox.com/s/prx1u719y743s56/MachineLearningFoundations.010.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;先從最簡單的看起，那就是線性迴歸（Linear Regression），假設今天我要用三種變數(x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, x&lt;sub&gt;3&lt;/sub&gt;)來建立一個簡單的線性模型，那就是&lt;/p&gt;
&lt;p&gt;w&lt;sub&gt;0&lt;/sub&gt;+w&lt;sub&gt;1&lt;/sub&gt;x&lt;sub&gt;1&lt;/sub&gt;+w&lt;sub&gt;2&lt;/sub&gt;x&lt;sub&gt;2&lt;/sub&gt;+w&lt;sub&gt;3&lt;/sub&gt;x&lt;sub&gt;3&lt;/sub&gt;，&lt;/p&gt;
&lt;p&gt;這個又稱為Score，標為s，為了方便起見，我們會額外增加x&lt;sub&gt;0&lt;/sub&gt;=1的參數，這麼一來Score就可以寫成矩陣形式&lt;/p&gt;
&lt;p&gt;s = w&lt;sub&gt;0&lt;/sub&gt;x&lt;sub&gt;0&lt;/sub&gt;+w&lt;sub&gt;1&lt;/sub&gt;x&lt;sub&gt;1&lt;/sub&gt;+w&lt;sub&gt;2&lt;/sub&gt;x&lt;sub&gt;2&lt;/sub&gt;+w&lt;sub&gt;3&lt;/sub&gt;x&lt;sub&gt;3&lt;/sub&gt;=W&lt;sup&gt;T&lt;/sup&gt;x&lt;/p&gt;
&lt;p&gt;W = [w&lt;sub&gt;0&lt;/sub&gt;, w&lt;sub&gt;1&lt;/sub&gt;, w&lt;sub&gt;2&lt;/sub&gt;, w&lt;sub&gt;3&lt;/sub&gt;]&lt;/p&gt;
&lt;p&gt;x = [x&lt;sub&gt;0&lt;/sub&gt;=1, x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, x&lt;sub&gt;3&lt;/sub&gt;]&lt;/p&gt;
&lt;p&gt;在線性模型中，這個 s 就正好是我們Model預測的 y，通常我們會把預測得來的 y 記作 ŷ (y hat)，如果今天這個 y 和 ŷ 是實數的話，那這就是一個標準的Linear Regression問題，那如何去衡量預測的好或不好呢？&lt;strong&gt;我們可以使用Squared Error來衡量，err(ŷ,y)=(ŷ-y)&lt;sup&gt;2&lt;/sup&gt;&lt;/strong&gt;，所以 ŷ 和 y 越靠近Error就越小。&lt;/p&gt;
&lt;p&gt;Squared Error的E&lt;sub&gt;in&lt;/sub&gt;平面比較簡單，就是一個單純的開口向上的拋物線，所以它的最低點其實是有解析解的，我們可以靠著數學上的&lt;strong&gt;Pseudo-Inverse方法&lt;/strong&gt;在評估完全部的Data之後把最佳參數給算出來，這麼簡單的E&lt;sub&gt;in&lt;/sub&gt;平面是很難見到的，我們之前介紹的Gradient Descent則是靠著逐步更新的方式去尋找近似解，這個方法是不管E&lt;sub&gt;in&lt;/sub&gt;平面有多麼複雜都可以處理，但是需要特別注意別卡在Local Minimum和Saddle Point。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="https://dl.dropbox.com/s/ugchv7yzd1bcm1a/MachineLearningFoundations.011.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;在上一回討論二元分類問題時，我們考慮的狀況是「沒有雜訊」的情形，不過在實際情況下，「雜訊」是一定需要考慮的。在「沒有雜訊」的情形下，一筆Data只會有一個確定的答案，&lt;strong&gt;如果考慮「雜訊」，一筆Data有可能有多個答案，呈現機率分布&lt;/strong&gt;，對於正確答案的機率也許會高一點，但因為雜訊的干擾的原因並非能百分之一百的出現正確答案。&lt;/p&gt;
&lt;p&gt;在二元分類的答案因為雜訊出現了機率分布，可能會產生像下面一樣的情況，&lt;/p&gt;
&lt;p&gt;ℙ(◯|X&lt;sup&gt;1&lt;/sup&gt;) = 0.9 ;   ℙ(✕|X&lt;sup&gt;1&lt;/sup&gt;) = 0.1&lt;/p&gt;
&lt;p&gt;而之前PLA的分類方法是屬於非黑及白的，這種分類法我們稱為Hard Classification，並不能描述這種機率分布，所以我們來考慮另外一種分類法，稱之為Soft Classification。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Soft Classification看待每個答案不是非黑及白的，而是去評估每個答案出現的機會有多大，以此作為分類&lt;/strong&gt;，我們打算使用Regression的連續特性來產生Soft Classification，我們需要引入一個重要的函數—Logistic Function，這個函數可以將所有實數映射到0到1之間，如上圖下方中間的圖示所示，&lt;strong&gt;Logistic Function會將極大的值映射成1，而將極小值映射成0，這個0到1的值剛剛好可以拿來當作機率的大小&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以我們就可以來建立一個有機率概念的模型，這個Model的預測值是一個機率，一樣的先給予輸入變數x權重W求出Score s，再把 s 放到Logistic Function當中，我們就可以映射出在一個機率空間，我們藉由調整W來改變Model以描述我們的Data，有了這個新的Model，我們就可以用機率的方式來描述二元分類，&lt;/p&gt;
&lt;p&gt;ℙ(◯|X&lt;sup&gt;1&lt;/sup&gt;) = Θ(s) ;   ℙ(✕|X&lt;sup&gt;1&lt;/sup&gt;) = 1 - Θ(s) = Θ(-s)&lt;/p&gt;
&lt;p&gt;OK! 決定好Model，我們就可以來定義它的Error Measurement的方式了，這個時候如果使用Squared Error來作為Error Measurement你會發現這種評估方式有一點失焦了，我們並不是要將雜訊給放進去Model之中，而是要在考慮雜訊之下盡可能的去描述數據背後真正的機制。&lt;/p&gt;
&lt;p&gt;所以我們來探討一下「可能性」，在考慮採樣數據過程因為雜訊造成的機率分布的前提下，我們去看會採樣到這組Data的可能性，我們應該合理的認為採樣出來的這組Data應該具有最大的「可能性」，這個「可能性」可以表示成&lt;/p&gt;
&lt;p&gt;Assume ◯ ≡ (y=+1) and ✕ ≡ (y=-1)&lt;/p&gt;
&lt;p&gt;ℙ(likelihood of ◯) = ℙ(x&lt;sup&gt;1&lt;/sup&gt;)Θ(y&lt;sup&gt;1&lt;/sup&gt;×s&lt;sup&gt;1&lt;/sup&gt;) × ℙ(x&lt;sup&gt;2&lt;/sup&gt;)Θ(y&lt;sup&gt;2&lt;/sup&gt;×s&lt;sup&gt;2&lt;/sup&gt;) × … × ℙ(x&lt;sup&gt;N&lt;/sup&gt;)Θ(y&lt;sup&gt;N&lt;/sup&gt;×s&lt;sup&gt;N&lt;/sup&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;所以我們需要設計一組Error Measurement，使得Error降低的同時可以使得ℙ of likelihood可以增大，這個Error Measurement就是Cross-Entropy，Error&lt;sub&gt;ce&lt;/sub&gt;=ln[1+exp(-ys)]。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;來推導一下Cross-Entropy怎麼來的，&lt;/p&gt;
&lt;p&gt;Max. ℙ(likelihood of ◯) &lt;/p&gt;
&lt;p&gt;= Max. Θ(y&lt;sup&gt;1&lt;/sup&gt;×s&lt;sup&gt;1&lt;/sup&gt;) × Θ(y&lt;sup&gt;2&lt;/sup&gt;×s&lt;sup&gt;2&lt;/sup&gt;) × … × Θ(y&lt;sup&gt;N&lt;/sup&gt;×s&lt;sup&gt;N&lt;/sup&gt;)&lt;/p&gt;
&lt;p&gt;= Min. 𝚺 -ln[Θ(y&lt;sup&gt;n&lt;/sup&gt;×s&lt;sup&gt;n&lt;/sup&gt;)]&lt;/p&gt;
&lt;p&gt;= Min. 𝚺 ln[1+exp(-y&lt;sup&gt;n&lt;/sup&gt;×s&lt;sup&gt;n&lt;/sup&gt;)]&lt;/p&gt;
&lt;p&gt;= Min. 𝚺 Error&lt;sub&gt;ce, n&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我們可以使用Gradient Descent來降低Cross-Entropy，這又稱為Logistic Regression，在這個問題中就沒有簡單的解析解可以直接算，只能使用近似解來處理。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;使用迴歸法做二元分類問題&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="https://dl.dropbox.com/s/01zyuaqal2achqu/MachineLearningFoundations.012.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;剛剛介紹了Logistic Regression，我們可以使用Regression方式來做二元分類問題，我們來看一下實際上該怎麼做？&lt;/p&gt;
&lt;p&gt;線性模型的標準方法，我們會將變數x做線性組合得到Linear Scoring Function — s，線性組合的係數和Threshold稱為權重W，我們可以調整權重W來改變Model，那針對看待s的不同方式就衍生出不同的方法。那為了可以將Regression問題轉換成二元分類問題，所以通常我們會假設(y=+1)為◯，(y=-1)為✕。&lt;/p&gt;
&lt;p&gt;先回顧一下之前&lt;a href="http://www.ycc.idv.tw/YCNote/post/25"&gt;PLA的作法&lt;/a&gt;，我們把 &lt;code&gt;s&amp;gt;0&lt;/code&gt; 的狀況視為◯，也就是(y=+1)；然後把 &lt;code&gt;s&amp;lt;0&lt;/code&gt; 的狀況視為✕，也就是(y=-1)，把這個概念畫成上圖右側的圖，圖中藍色的階梯函數就是PLA的Error Measurement，正是因為它是一個階梯函數，所以我們不能使用Gradient Descent等Regression方法來處理，&lt;strong&gt;因為在階梯的每一點∇E&lt;sub&gt;in&lt;/sub&gt;都是0（除了原點外），也就是如此PLA在更新的過程才無法確保趨近於最佳解，而需要使用Pocket PLA來解決這個問題&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那如果我們用Linear Regression來做這件事呢？我們把Squared Error畫在上圖右側小圖的紅線，你會發現它的低點會落在ys=1的地方，這應該不是我們要的結果，雖然它一樣可以把錯誤的判斷修正回正確，但是面對過度確定的正確答案，它反而會去修正它往錯誤的方向，很顯然這不是我們想要的。&lt;/p&gt;
&lt;p&gt;最好的方式就是Logistic Regression了，我們將s做Logistic Function的轉換，轉換成機率，並在評估最大化Likelihood的條件下定義出Cross-Entropy來當作Error Measurement，在上圖右側的小圖，我們稍微調整Cross-Entropy，使得它的Error Function可以在ys=0的地方和Squared Error相切，&lt;strong&gt;這張圖告訴我們的是隨著Grandient Descent每次的更新，Logistic Regression會把分類做的越來越好，把◯和✕拉的更遠&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;後話&lt;/h3&gt;
&lt;p&gt;在這一篇當中，我們介紹了Grandient Descent這一個相當重要的演算法，並且運用在兩種Regression上：Linear Regression和Logistic Regression，Linear Regression是最簡單的Regression方法，甚至它還可以使用Pseudo-Inverse的方法直接算出最佳解，Logistic Regression考慮了有雜訊的Data產生的機率分布，我們可以用Logistic Regression做Soft Binary Classification，而且我們也說明了Logistic Regression為何適合拿來用在二元分類上。本篇我們對於ML的實際作法有了基本認識，在下一篇，我們繼續討論還有沒有什麼方式可以讓ML做的更好。&lt;/p&gt;</content><category term="機器學習基石"></category></entry><entry><title>機器學習基石 學習筆記 (2)：為什麼機器可以學習?</title><link href="YCNote/ml-course-foundations_2.html" rel="alternate"></link><published>2016-06-26T12:00:00+08:00</published><updated>2016-06-26T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2016-06-26:YCNote/ml-course-foundations_2.html</id><summary type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們初探了機器學習，了解了什麼時候適合使用機器學習，而不是一般的Hard Coding，那今天這篇文章要繼續問下去。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;為什麼機器可以學習(Why Can Machines Learn?)&lt;/strong&gt;，本篇會介紹學理上機器學習（ML）必須要有哪些條件才可行，這些理論有非常多的數學，但卻是了解機器學習非常重要的內功，我會盡量避開繁複的數學運算，而帶大家直接的了解式子所要告訴我們的觀念。&lt;/p&gt;
&lt;h3&gt;機器可以學習嗎?&lt;/h3&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.001" src="https://dl.dropbox.com/s/rjxzcwyfabb02ae/MachineLearningFoundations.001.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;還記得上面這張圖嗎? 上次帶大家初探了Machine Learning(ML)的基本架構，可以把整個概念總結成上面這張圖。&lt;/p&gt;
&lt;p&gt;我們來複習一下，先從最上面的盆子開始看起，我們用Target Function代表你想要學習的技能，在非常理想的情況下，也就是沒有noise的情況，每組輸入變數 Xn都會找到一組精確的輸出 yn，而這個Target Function能產生多個Data，圖中那些小球就是代表各個單筆的Data，今天我從中隨機抽取出N組Data來做機器學習，接下來Learning Algorithm會利用這些取出的Data去從Hypothesis Set中找出最像Target Function的Hypothesis，那這組Hypothesis就成了我們學習出來的結果，我們可以利用這個結果來預測新的問題。&lt;/p&gt;
&lt;p&gt;那麼上面這張圖真的合理嗎? 我們真的有辦法用上面的方法讓機器學習嗎? &lt;/p&gt;
&lt;p&gt;先介紹幾個名詞，我們會稱&lt;strong&gt;抽樣的Data為In-sample …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們初探了機器學習，了解了什麼時候適合使用機器學習，而不是一般的Hard Coding，那今天這篇文章要繼續問下去。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;為什麼機器可以學習(Why Can Machines Learn?)&lt;/strong&gt;，本篇會介紹學理上機器學習（ML）必須要有哪些條件才可行，這些理論有非常多的數學，但卻是了解機器學習非常重要的內功，我會盡量避開繁複的數學運算，而帶大家直接的了解式子所要告訴我們的觀念。&lt;/p&gt;
&lt;h3&gt;機器可以學習嗎?&lt;/h3&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.001" src="https://dl.dropbox.com/s/rjxzcwyfabb02ae/MachineLearningFoundations.001.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;還記得上面這張圖嗎? 上次帶大家初探了Machine Learning(ML)的基本架構，可以把整個概念總結成上面這張圖。&lt;/p&gt;
&lt;p&gt;我們來複習一下，先從最上面的盆子開始看起，我們用Target Function代表你想要學習的技能，在非常理想的情況下，也就是沒有noise的情況，每組輸入變數 Xn都會找到一組精確的輸出 yn，而這個Target Function能產生多個Data，圖中那些小球就是代表各個單筆的Data，今天我從中隨機抽取出N組Data來做機器學習，接下來Learning Algorithm會利用這些取出的Data去從Hypothesis Set中找出最像Target Function的Hypothesis，那這組Hypothesis就成了我們學習出來的結果，我們可以利用這個結果來預測新的問題。&lt;/p&gt;
&lt;p&gt;那麼上面這張圖真的合理嗎? 我們真的有辦法用上面的方法讓機器學習嗎? &lt;/p&gt;
&lt;p&gt;先介紹幾個名詞，我們會稱&lt;strong&gt;抽樣的Data為In-sample Data&lt;/strong&gt;，並且稱&lt;strong&gt;Hypothesis預測In-sample Data的誤差為In-sample Error，表示為E&lt;sub&gt;in&lt;/sub&gt;&lt;/strong&gt;，因此Learning Algorithm的目的就是找出那組Hypothesis使得E&lt;sub&gt;in&lt;/sub&gt;最小。&lt;/p&gt;
&lt;p&gt;回想一下二元分類問題，在上一篇當中我們使用PLA來挑選Hypothesis Set，還記得我們做了什麼事來確保我們可以得到最佳解嗎? 那就是Pocket的方法，Pocket的目的就是去留住一組能預測最好的Hypothesis，也就是能保留一組參數使得E&lt;sub&gt;in&lt;/sub&gt;最小。&lt;/p&gt;
&lt;p&gt;但如果E&lt;sub&gt;in&lt;/sub&gt;真的已經可以壓到0了，我們就可以說機器學習已經完成了嗎？&lt;/p&gt;
&lt;p&gt;並不是這樣的，回到目的，我們真正希望的是機器有辦法預測新的問題，所以真正的目標是將取樣前的母群體給預測好。&lt;/p&gt;
&lt;p&gt;我們會稱&lt;strong&gt;抽樣前的母群體為Out-sample Data&lt;/strong&gt;，並且稱&lt;strong&gt;Hypothesis預測Out-sample Data的誤差為Out-sample Error，表示為E&lt;sub&gt;out&lt;/sub&gt;，我們最終目的就是把E&lt;sub&gt;out&lt;/sub&gt;壓下來&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;但遺憾的是我們不會真正知道E&lt;sub&gt;out&lt;/sub&gt;的大小，所以我們只能評估E&lt;sub&gt;in&lt;/sub&gt;來選取Model參數，因此重要的是需要E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;這個條件成立，否則一切的學習都是無效的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;總結一下機器學習的條件，我們必須建立一個 Learning Model在N筆資料輸入的情況下可以確保E&lt;sub&gt;in &lt;/sub&gt;≈ E&lt;sub&gt;out&lt;/sub&gt;，所以在Learning Algorithm選出最小E&lt;sub&gt;in&lt;/sub&gt;的Hypothesis，同時這組Hypothesis也可以很好的預測Out-sample，我們就可以說機器已經會學習了。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;E&lt;sub&gt;in&lt;/sub&gt;和E&lt;sub&gt;out&lt;/sub&gt;的差異&lt;/h3&gt;
&lt;p&gt;&lt;img alt="image" src="https://dl.dropbox.com/s/z6s7d0wsq00c5nn/MachineLearningFoundations.005.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;剛剛我們已經提到了如果機器能學習，那就必須先確保E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;，下面我會引入Hoeffding不等式來說明這個條件怎麼成立。&lt;/p&gt;
&lt;p&gt;我們先想像一下我有一個桶子，這個桶子裝了兩種顏色的很多顆小球，分別為橘色和綠色，今天如果桶子內橘色球佔的比例為μ，而今天我們從中隨機抽樣出N顆小球，並且計算出這N顆小球中橘色佔的比例為ν，此時我們可以想像的到，μ=ν不一定會成立，但μ也不至於離ν太遠，所以Hoeffding不等式就告訴我們|μ-ν|會被限制在一個範圍內，大家可以看一下上圖中左側的圖例。&lt;/p&gt;
&lt;p&gt;接下來我們再把橘球和綠球的意義換成是，一組Hypothesis預測每筆Data的好或壞，預測正確的是綠球，預測失敗的是橘球，所以橘球的比例正是一組Hypothesis的預測誤差，所以在Out-sample就是E&lt;sub&gt;out&lt;/sub&gt;，在In-sample就是E&lt;sub&gt;in&lt;/sub&gt;，也就得到上圖右側的公式。&lt;/p&gt;
&lt;p&gt;如果我們定義E&lt;sub&gt;in&lt;/sub&gt;和E&lt;sub&gt;out&lt;/sub&gt;差異大於 ε 的情形為Bad Data(不好的數據)，則上述式子保證的是出現這樣Bad Data的機率將被一個定值給限制住，所以只要出現Bad Data的機率不是太大，基本上我們就可以說E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="https://dl.dropbox.com/s/yq5l9mz9y5ulh4h/MachineLearningFoundations.006.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;而事實上，我們的hypothesis不會只有一個，所以接下來來考慮如果有M個Hypotheses的情況下我們的E&lt;sub&gt;in&lt;/sub&gt;和E&lt;sub&gt;out&lt;/sub&gt;的差異會怎麼被參數影響。&lt;/p&gt;
&lt;p&gt;如果我們考慮M組Hypotheses，就會發現每種Hypothesis出現Bad Data的地方可能不一樣，因此大大的減少能使用的Data，如上圖左側所示。&lt;/p&gt;
&lt;p&gt;今天如果我有1000份從Target Function取N個Data的情形，然後只用一個Hypothesis來衡量，根據Hoeffding's Inequality，1000份裡面假設大概5份會出現Bad Data，但今天我再增加一組Hypothesis來衡量，對於這個Hypothesis也可能有自己的5份Bad Data，如果很不幸的，剛剛好這5份Bad Data和前5份沒有重疊，因此用這兩個hypotheses來評估的話，1000份裡頭將會出現10份的Bad Data，由此類推，如果有M組Hypotheses，最差的情況會發生在什麼時候呢? 那就是M個Hypotheses的每份Bad Data彼此都沒有交集，夠慘吧! 所以把這些出現Bad Data的機率取聯集得到上圖右側的公式。&lt;/p&gt;
&lt;p&gt;大家現在回想一下上一篇所提到的Perceptron Hypothesis Set就會發現，糟糕了! Perceptron Hypothesis Set 裡有無限多組的Hypotheses，也就是M→∞，那我們不就需要無限多的Data才能做到E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;，否則機器根本不會學習，所以前一篇都在亂講，PLA根本無法學習，等一下，先沉住氣，聽我解釋一下，你就會明白PLA還是可以做到機器學習的。&lt;/p&gt;
&lt;h3&gt;VC Generalization Bound&lt;/h3&gt;
&lt;p&gt;&lt;img alt="image" src="https://dl.dropbox.com/s/5p6sz6y53oh58xe/MachineLearningFoundations.007.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;問題出在這裡，我們在Multi-Bin Hoeffding’s Inequality中採用了一個假設，就是假設每組Hypotheses的Bad Data彼此間都沒有重疊，所以在M→∞的情況下，當然會有一個無限大的上限值，但如果考慮了Bad Data重疊的情形，縱使M→∞的情況下還是有機會把Bad Data的出現機率壓在一個有限的定值之下。&lt;/p&gt;
&lt;p&gt;我們回到二元分類問題，看一下上圖中左側的圖例，如果今天在二維平面上做二元分類，當n=1時，就算你的Hypotheses有無限多組，對於一組Data來說就只有兩種而已，再來看n=2的情況，一樣的無限多組的Hypotheses也只能分類成4種。&lt;/p&gt;
&lt;p&gt;因此Hypotheses彼此之間因為Data數量的關係，而出現重疊的狀況。但聰明的你一定想到，如果今天n的數量不斷的增加，則Hypotheses被分類的數量就會增加，Hypotheses彼此之間的重疊就會漸漸減少，我們還是無法限制住Bad Data出現的機率。&lt;/p&gt;
&lt;p&gt;我們繼續看下去，當n=3，沒有意外的Hypotheses會被分類為8種，那接下來n=4時，你就會發現一個有趣的現象，開始有一些情況是不會出現在這一組Hypothesis Set的，因此我們擔心因為Data數量增加而造成Hypotheses的種類暴增的情形被排除了，有一些狀況是不會出現的。&lt;/p&gt;
&lt;p&gt;剛剛所提到的分類方式的數量又稱為Dichotomy。在n=1、n=2到n=3的情形，所有列得出來的方式都可被完整分類開來，我們稱這情形為Shatter，但是到了n=4的時候，有些不可能被分類的情形出現了，稱為不可被Shatter，另外又稱此情形開始發生的那點為Break Point，這邊注意一下喔! 會不會有Break Point取決於你的Hypothesis Set長怎麼樣，現在是因為線性二元分類的Hypothesis Set，所以Break Point才會在n=4，其他的Hypothesis Set就不一定了。&lt;/p&gt;
&lt;p&gt;Break Point的出現非常重要，他所代表的是Bad Data的出現機率不會無所限制的大下去，因此把這概念帶入Multi-Bin Hoeffding’s Inequality，經過繁複的計算，就可以得到上圖右側的公式，原本的M消失了，取而代之的是Growth Function，Growth Function與Data數量N有關，這就是我們剛剛解說的，決定Hypothesis Set的種類的其實是 Data的數量N。&lt;/p&gt;
&lt;p&gt;那麼Growth Function要怎麼和Break Point連結起來呢？&lt;/p&gt;
&lt;p&gt;先定義一下VC Dimension：d&lt;sub&gt;VC&lt;/sub&gt;= Break Point-1，Break Point代表首次出現不Shatter的情況，那比它小一級代表的正是最大可以Shatter的點，上面的例子中d&lt;sub&gt;VC&lt;/sub&gt;=3。而這個VC Dimension就可以和我們在意的Growth Function連接起來，經過數學推倒可以得到上圖右側下方的關係式。&lt;/p&gt;
&lt;p&gt;所以我們就知道啦！&lt;strong&gt;只要有Break Point存在，VC Dimension就是一個有限的值，也因此Growth Function是一個有限的值，VC Bound就產生了，就可以確保Bad Data出現的機率被壓在一個定值之下，所以一樣的只要資料量N夠多就可以確保E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;，機器將可以學習。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外一件重要的事，VC Dimension在數學上是有意義的，&lt;strong&gt;d&lt;sub&gt;VC&lt;/sub&gt; ≈ 可調控變數的個數&lt;/strong&gt;，像是上述的二維二元分類問題，它的可調控變數有w0, w1 和 w2，總共3個，所以d&lt;sub&gt;VC&lt;/sub&gt;=3。&lt;strong&gt;也就是說Hypothesis Set的可調變參數如果是有限，大部分都可以做機器學習。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;機器要能學習的三要素&lt;/h3&gt;
&lt;p&gt;前面拉哩拉雜的講了一堆，終於要推出我們的結論了! 所以如果剛剛的數學讓你感到很挫敗，沒關係，讀懂這段那就足夠了。&lt;/p&gt;
&lt;p&gt;從VC Generalization Bound，我們可以知道機器學習是可能的，只要它具備三點要素：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Good Hypothesis Set: Hypothesis Set 必須有Break Point的存在，也意味著VC Dimension是有限的，而且越小越好，在意義上代表可以調控的變數不要太多。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Good Data: 數據量越大越好，可以壓低VC Generalization Bound&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Good Learning Algorithm: 以上兩點可以確定的是E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;，接下來好的Learning Algorithm要有能力找到E&lt;sub&gt;in&lt;/sub&gt; 最小的參數。很直觀的，當我們可以調控的變數越多，我們的選擇就越多，也就是我們可以找到更小E&lt;sub&gt;in&lt;/sub&gt; 的機會變多了，所以可以調控的變數不可以太少。&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;眼尖的你有沒有發現矛盾啊! 可以調控的變數很少，我們能確保E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;，但是如果我想要找到更小的E&lt;sub&gt;in&lt;/sub&gt; 又必須有更多的調控變數，這個矛盾是機器學習上一個重要的課題，&lt;strong&gt;解法是我們必須要能找到適當的調控變數數量，也就是適當大小的d&lt;sub&gt;VC&lt;/sub&gt; &lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://dl.dropbox.com/s/0dxzdyi0r8ourz6/MachineLearningFoundations.000.02.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://d396qusza40orc.cloudfront.net/ntumlone/lecture_slides/07_handout.pdf"&gt;https://d396qusza40orc.cloudfront.net/ntumlone/lecture_slides/07_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上圖中，我們把VC Generalization Bound公式帶入Growth Function和d&lt;sub&gt;VC&lt;/sub&gt;的關係式，並且設δ 為最大可以容忍的Bad Data出現機率，把它帶入取代掉ε，整理一下，就可以推出上圖的公式，後面帶根號的紅字稱為Model Complexity，這一項代表的是Hypothesis Set造成的模型複雜度，我們可以看到它隨著d&lt;sub&gt;VC&lt;/sub&gt;增加而增加。Model Complexity越大代表Bad Data更容易出現，所以E&lt;sub&gt;in&lt;/sub&gt;和E&lt;sub&gt;out&lt;/sub&gt;開始被帶開了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這個現象有一個很常見的名字叫做Overfitting，指的是使用非常複雜的Model來Fitting，雖然可以把手頭上的數據Fit的很漂亮，但是拿到其他的數據來看就會發現這Model的預測性非常的差，原因就是因為Model Complexity造成E&lt;sub&gt;in&lt;/sub&gt;和E&lt;sub&gt;out&lt;/sub&gt;脫鉤了，所以選擇一個複雜度適中的Model是很重要的。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;機器學習架構一般化&lt;/h3&gt;
&lt;p&gt;&lt;img alt="image" src="https://dl.dropbox.com/s/h8qabqhjjaew5gs/MachineLearningFoundations.008.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;最後我們來總結一下機器學習的流程，上圖中是之前提到的機器學習的架構，額外的我們還需要考慮到一些真實情形，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每筆Data出現的機會不一定，同樣的採樣結果也是會受機率的影響，所以上圖中標示為P(x)，這個修改並不會影響機器學習的流程和結果。&lt;/li&gt;
&lt;li&gt;Data可能會受到Noise的影響，所以給定X&lt;sub&gt;n&lt;/sub&gt;並不一定會百分之一百得到y&lt;sub&gt;n&lt;/sub&gt;，他存在著可能會出錯，上圖標示為P(y|x)，我們可以增大我們採樣的數量N來減少Noise的影響。&lt;/li&gt;
&lt;li&gt;我們是採用E&lt;sub&gt;in&lt;/sub&gt;來當作選擇Model參數的指標，因此我們需要訂出Error的評估方式，常見的有Squared Error = (y&lt;sub&gt;n&lt;/sub&gt; - y&lt;sub&gt;prediction&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;跟著架構我們就有一套機器學習的&lt;strong&gt;標準流程&lt;/strong&gt;，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;準備好足夠的數據&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;把Model建立好，d&lt;sub&gt;VC&lt;/sub&gt;必須要是有限的，而且大小要適中&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定義好評估E&lt;sub&gt;in&lt;/sub&gt;的Error Measurement&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;使用演算法找出最佳參數把E&lt;sub&gt;in&lt;/sub&gt;降低&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最後評估一下是否有Overfitting的狀況，確保E&lt;sub&gt;in&lt;/sub&gt; ≈ E&lt;sub&gt;out&lt;/sub&gt;&lt;/strong&gt;（未來會講怎麼做）&lt;/li&gt;
&lt;/ol&gt;</content><category term="機器學習基石"></category></entry><entry><title>機器學習基石 學習筆記 (1)：何時可以使用機器學習?</title><link href="YCNote/ml-course-foundations_1.html" rel="alternate"></link><published>2016-06-06T12:00:00+08:00</published><updated>2016-06-06T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:None,2016-06-06:YCNote/ml-course-foundations_1.html</id><summary type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;經過幾個月的努力，終於完成田神在Coursera上machine learning的兩門課中的第一門課—&lt;a href="https://www.coursera.org/course/ntumlone"&gt;機器學習基石&lt;/a&gt;，田神不愧為田神的名號，整門課上起來非常流暢，每個觀念講得非常得清晰，考究學理，但是又不會單單只有理論而已，課程中會舉很多實用的例子，讓你了解每個觀念如何實踐。因此，非常推薦大家去把Coursera上面的課程完整聽一次，應該會收益良多，接下來一系列的文章，我會摘要出《機器學習基石》之中主要的概念，適合對Machine Learning（ML）有興趣的初學者來一窺它的脈絡。&lt;/p&gt;
&lt;p&gt;《機器學習基石》一共有16堂課，主要分為四個方向，第一個方向，&lt;strong&gt;何時可以使用機器學習(When Can Machines Learn? )&lt;/strong&gt;，點出什麼是機器學習，適合在哪些情形下使用，並引入貫穿整個課程的二元分類問題，第二個方向，&lt;strong&gt;為什麼機器可以學習(Why Can Machines Learn?)&lt;/strong&gt;，介紹學理上機器學習必須要有哪些條件才可行，這些理論是了解機器學習非常重要的內功，第三個方向，&lt;strong&gt;機器可以怎麼樣學習(How Can Machines Learn?)&lt;/strong&gt;，學習完了學理 …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;經過幾個月的努力，終於完成田神在Coursera上machine learning的兩門課中的第一門課—&lt;a href="https://www.coursera.org/course/ntumlone"&gt;機器學習基石&lt;/a&gt;，田神不愧為田神的名號，整門課上起來非常流暢，每個觀念講得非常得清晰，考究學理，但是又不會單單只有理論而已，課程中會舉很多實用的例子，讓你了解每個觀念如何實踐。因此，非常推薦大家去把Coursera上面的課程完整聽一次，應該會收益良多，接下來一系列的文章，我會摘要出《機器學習基石》之中主要的概念，適合對Machine Learning（ML）有興趣的初學者來一窺它的脈絡。&lt;/p&gt;
&lt;p&gt;《機器學習基石》一共有16堂課，主要分為四個方向，第一個方向，&lt;strong&gt;何時可以使用機器學習(When Can Machines Learn? )&lt;/strong&gt;，點出什麼是機器學習，適合在哪些情形下使用，並引入貫穿整個課程的二元分類問題，第二個方向，&lt;strong&gt;為什麼機器可以學習(Why Can Machines Learn?)&lt;/strong&gt;，介紹學理上機器學習必須要有哪些條件才可行，這些理論是了解機器學習非常重要的內功，第三個方向，&lt;strong&gt;機器可以怎麼樣學習(How Can Machines Learn?)&lt;/strong&gt;，學習完了學理，我們來看機器學習有哪些的使用方法，最後一個方向，&lt;strong&gt;機器可以怎麼樣學得更好(How Can Machines Learn Better?)&lt;/strong&gt;，探討哪些問題會造成機器學不好，然後怎麼去改善。&lt;/p&gt;
&lt;h3&gt;什麼是Machine Learning (ML)&lt;/h3&gt;
&lt;p&gt;在了解機器學習之前，我們不妨來想想「你」從小是怎麼學習的，有人會說學習就是一個不斷記憶的過程，但這樣的說法顯然不夠全面，你總不會認為把考題的所有答案都背起來的學生就已經學會一門知識了吧！所以，考題只是表象，我們真正要學習的是它背後的觀念，可以拿來推敲未知的知識。&lt;/p&gt;
&lt;p&gt;同樣的，ML的學習方式也有點類似於人類的學習，機器從Data中開始學習起，這些Data就像是一道一道的考題，而ML做的事正是去學習Data後面的觀念，而不是單純把Data給儲存起來，有了Data背後的觀念才能舉一反三，才算是真正的學會了。&lt;/p&gt;
&lt;p&gt;所以，做ML有點像是手把手的造一顆大腦，並且訓練它學會Data背後的知識。那這個大腦要怎麼設計呢？這個大腦用我們學物理的人的說法就是建一個Model，而餵給它Data的過程就是Fit Model。&lt;/p&gt;
&lt;p&gt;那什麼是Model呢？讓我來解釋一下，&lt;strong&gt;所謂的Model就是一個用來描述未知現象的架構&lt;/strong&gt;，舉個例子，我們都知道力的公式是F=ma（力＝質量x加速度），但如果你今天拿一顆皮球來，你就會發現這個公式不那麼正確，因為皮球會形變，那怎麼辦呢？我們可以假設形變會把部份的力給抵消掉，所以式子改寫成(F-F1)=ma，在這邊F1就是那個抵消的力，這樣就是設計了一個Model來描述這個現象，而F1是一個未知的值，我們可以用實驗數據來推估F1，這就是所謂的Model Fitting。&lt;/p&gt;
&lt;p&gt;物理上的Model通常是這樣做的，我們先觀察未知現象，然後從中猜測可能造成這現象的原因，總結這些原因來設計一個Model，Model中可能有一些參數還沒被決定，此時我們就可以用數據來決定它，這就是Model Fitting。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.001" src="https://dl.dropbox.com/s/rjxzcwyfabb02ae/MachineLearningFoundations.001.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;了解了Model的概念就相當好了解ML的架構，上圖是ML的基本架構，&lt;strong&gt;假設我們今天要讓機器學一樣技術，這個技術我們用一個函數來表示，稱之為Target Function，這個Target Function就是隱藏在Data後面的真正道理&lt;/strong&gt;，每個變數X會有相應的正確答案Y。&lt;/p&gt;
&lt;p&gt;今天我從Target Function中取出N組當作Data來給我的機器學習，那目標是什麼?&lt;strong&gt;目標當然是讓機器學習出這個Target Function啦！&lt;/strong&gt;所以我們要先設計我們的Model，最終目的是決定Model裡的參數之後，這個被選擇的Model就是Target Function。&lt;/p&gt;
&lt;p&gt;Model就是上圖中的Hypothesis Set，在Model參數還沒被決定之前，你可以想像它就像一個集合包含很多可以選擇的函數，而使用數據Model Fitting以後，選出一組最佳化的參數，就好像從這個集合中挑選一組函數一樣。&lt;/p&gt;
&lt;p&gt;在這個找最佳化參數的過程，我們需要一個機制，這個機制可以評估Hypothesis Set中每組函數描述Data的好壞，並且找出描述Data最好的那組參數，這個機制就是上圖中的Learning Algorithm。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;建立Model，使用Data加上Learning Algorithm找出最佳參數，這就是ML的架構輪廓&lt;/strong&gt;。當然這邊要補充一下，物理上的Model通常是建基在已知的知識之上，而常見的ML強大之處是不需要太多的人為的智慧，機器可以自行學習，所以我這裡指的Model是比物理上的Model更為廣義的。&lt;/p&gt;
&lt;h3&gt;Machine Learning (ML)的使用時機&lt;/h3&gt;
&lt;p&gt;剛剛帶大家初探了ML的架構，接下來帶大家了解什麼時候我們適合使用ML。&lt;/p&gt;
&lt;p&gt;舉幾個例子，大家可能比較有感覺，譬如說Netflix曾辦過一場競賽，競賽的內容是利用客戶的影片評分紀錄，來預測未評分影片的得分，如果可以增進預測率10%，就可以獨得100萬美元獎金，這個問題就可以使用ML，Data是過去得評分紀錄，Target Function是用戶評分的規律，如此一來，機器學到了這個技術，未來就可以舉一反三的推出未評分影片的分數，和用戶喜歡的影片可能有哪些。&lt;/p&gt;
&lt;p&gt;再多看幾個例子，例如設計火星勘查機，人類目前對火星的了解仍相當有限，所以我們沒辦法完全猜測勘查機在火星會遇到什麼問題，所以必須讓勘查機有ML的能力去學習各種問題的解決方法。&lt;/p&gt;
&lt;p&gt;再來個例子，現在很夯的汽車自動駕駛也需要ML技術，機器去學習辨識交通號誌。&lt;/p&gt;
&lt;p&gt;看了這麼多例子，我們會發現這些例子都很難以寫出簡單的規則，但是卻又存在著一種規律，這種情形正是適合用ML來做。&lt;/p&gt;
&lt;p&gt;在以往電腦工程幾乎都是由工程師用嚴謹的邏輯去逐條的把規則一一的寫上，這樣的機器不具有學習能力，或稱得上人工智慧，因為它只是單純反應工程師的工人智慧而已，但如果遇到一些困難的問題，譬如告訴機器什麼是狗，這時候你就會發現很難用規則來描述，有尾巴，可是是怎樣的尾巴？有耳朵，那這耳朵怎麼和貓區分開來？此時Hard Coding就太困難了，我們不這麼做，反過來我們設計架構讓機器自己去從Data中學習。&lt;/p&gt;
&lt;p&gt;總結一下，ML的使用時機有以下三種&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;你想要學習的技術存在一種模式&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;要學習的技術不容易簡單的Hard Coding&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;有可以代表這個要學習模式的Data&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;二元分類問題&lt;/h3&gt;
&lt;p&gt;&lt;img alt="img" src="https://dl.dropbox.com/s/4chm6lt80pnb684/MachineLearningFoundations.000.01.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://class.coursera.org/ntumlone-003/lecture/17"&gt;https://class.coursera.org/ntumlone-003/lecture/17&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;好! 大家現在應該對於機器學習有一些認識了，那接下來我們來實作一些例子來了解機器學習架構怎麼運作。像個小學生一樣，我們先從簡單的是非題來學起，雖然是非題看起來非常簡單，但它其實非常的powerful，是非題饒口一點的講法就是「二元分類問題」，這樣的問題將會貫穿整個16堂課程，相當重要!&lt;/p&gt;
&lt;p&gt;舉個例子，今天有一家銀行想要開發一款ML的軟體，這個軟體可以根據過去信用卡核發用戶的資料，去判斷要不要核發信用卡給這個新的申請人，這些過去的資料可能包括：用戶年齡、用戶性別、用戶年薪等等，讓機器藉由這些資料去學習判斷要不要核發信用卡。把這樣的二元分類問題化作&lt;/p&gt;
&lt;p&gt;Target Function：f: X =&amp;gt; y&lt;/p&gt;
&lt;p&gt;X有年齡、性別和年薪這些變數，而y則是個二元類別，不是y=1(核發)就是y= -1(不核發)。&lt;/p&gt;
&lt;p&gt;那接下來，我們就要決定我們的Learning Model，也就是Hypothesis Set。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.002" src="https://dl.dropbox.com/s/r2vv0p2k097v6wb/MachineLearningFoundations.002.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;引入&lt;strong&gt;Perceptron(感知器) Hypothesis Set&lt;/strong&gt;來當作我們的Hypothesis Set，如上圖，我們給予我們的輸入變數個別的權重，然後相加起來，並且看這個值是正還是負，來決定輸出值是+1或-1，sign函數的作用是假設輸入的值為正則輸出+1，反之則輸出-1。&lt;/p&gt;
&lt;p&gt;對應核發信用卡這個例子，&lt;/p&gt;
&lt;p&gt;x1 = 用戶年齡; x2 = 用戶性別; x3 = 用戶年薪，&lt;/p&gt;
&lt;p&gt;在分別乘上weight w1, w2, w3，這個變數前面的weight代表這個變數對於答案Y有什麼影響，如果是正向影響，weight &amp;gt; 0，如果沒有影響，weight = 0，如果負向影響，weight &amp;lt; 0，舉個例子，高年薪也許可以提升核發信用卡的機會，那它前面的weight應該就是正的，也許性別並不影響核發信用卡的機會，則weight = 0，那麼考慮到這些input變數對結果影響的評估，我們會得到一個數值 w1*x1+w2*x2+....。&lt;/p&gt;
&lt;p&gt;此時我們要用這個數值去做「二元分類」，也就是一分為二，怎麼做呢? 很簡單，給他一分水嶺，高於一個值(-w0)我就給他 y=+1，低於(-w0)我就給他 y=-1，用數學表示就是 sign(w0+w1*x1+w2*x2+...) ，w0可以看成是一個閥值。&lt;/p&gt;
&lt;p&gt;上圖中的 s = w0+w1*x1+w2*x2+... 就像一個分數(score)一樣，高分 s&amp;gt;0 的我就核發(+1)，低分 s &amp;lt; 0 的我就不核發(-1)，其中權重 w0, w1, w2, ... 都可以由機器學習去調整，這些不同的weight就構成了Hypothesis Set，也就是Model，那接下來我們還需要Learning Algorithm來取出最佳參數，也就是決定一組最佳weight。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.003" src="https://dl.dropbox.com/s/51aipr85rfbylj3/MachineLearningFoundations.003.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;如上圖所示，&lt;strong&gt;Perceptron Learning Algorithm(PLA)&lt;/strong&gt;是用於處理Perceptron Hypothesis Set的一種演算法。&lt;/p&gt;
&lt;p&gt;它的作法簡單來講是，藉由一筆一筆的數據去逐步的更新它的weight使得Model可以描述這筆數據，直到不需要再更新為止，此時所有的Data都可以用這個Model表示，更新的方法是先判斷進來的這筆數據是否符合目前的Model，如果不符合，則朝變數向量Xn的方向，跨出或後退大小為Learning Rate的一步來更新weight，前進還是後退端看你的Data是y=-1或+1，y=+1就往前跨，y=-1就往後退。&lt;/p&gt;
&lt;p&gt;因此，這個跨步更新的動作必須可以使Model接近正確答案，這麼神奇，真的假的？不太直覺，先從score來想起，假設有一筆資料為(Xn,yn)，則Score：s = Wt・Xn，在Wt和Wn向量彼此有同向分量的情況下，s &amp;gt; 0，如果這個時候yn剛好為+1，則sign(s)=yn，這個時候Wt描述這個數據就很好啊，我們就不需要去更新它；如果相反yn=-1，這個Wt描述這個數據就不正確，也就是說Wt 和 Xn不應該同向，所以我們讓Wt加上-Xn(=yn*Xn)，把Wt從原本與Xn同向的狀態反向拉離開來。那如果在Wt和Xn向量彼此不同向的情況下，s &amp;lt; 0，這個時候如果yn剛好為-1，則sign(s)=yn，很好我們不去更新它；如果相反yn=+1，這個Wt描述這個數據不正確，也就是說Wt 和 Xn不應該反向，所以我們讓Wt加上Xn(=yn*Xn)，把Wt拉到和Xn同向一點。這就是PLA找到更好Wt的機制。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.004" src="https://dl.dropbox.com/s/aq6n1491d91z906/MachineLearningFoundations.004.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Seeing is believing，上面這張圖帶我們來看PLA如何運作，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initially: 在最一開始的時候，我們weight Wt先設成零向量&lt;/li&gt;
&lt;li&gt;Update 1: PLA更新把零向量的Wt拉成W(t+1)&lt;/li&gt;
&lt;li&gt;Update 2: 上一輪的W(t+1)已經是這一輪的Wt，也就是紅色的那個向量，Wt決定了一條壁壘分明的二元分類邊界，這條線的方程式其實就是 w0+w1x1+... = 0，如果你還記得高中數學的話，這條邊界必然會和Wt垂直，如圖所示，而Wt的方向是屬於y=+1的區域，這一輪剛剛好找到一個圈(y=+1)落在y=-1的區域，因此我們需要更新weight，做法是把Wt 和 yn*Xn(=Xn)相加成為新的weight Wt+1&lt;/li&gt;
&lt;li&gt;...........以此類推&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;如果資料線性可分的話，PLA在迭代多次後，是可以用一條線完全區分兩種數據&lt;/strong&gt;。但如果數據不是線性可分，不存在一條線來區分數據，此時最佳解就必須評估整體犯錯有多少，找出犯錯最少的那條直線就是最佳解，但可惜的是PLA方法並不會在迭代中趨向於犯錯最少的那條線，什麼時候該停止迭代是個世紀難解的NP-Hard問題（如果不了解這個名詞，&lt;a href="http://www.ycc.idv.tw/YCNote/post/19"&gt;詳見&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;因此要改變一下PLA，這個方法我們稱之為Pocket，當每次得到一組weight的時候，都拿它來評估它對所有Data的區分能力好或壞，而只留下一組最好的放進口袋裡，所以當迭代次數做多了，保留在口袋的這組解就可以看成是最佳解，就這麼簡單。&lt;/p&gt;
&lt;h3&gt;多元學習&lt;/h3&gt;
&lt;p&gt;機器學習和人類學習一樣，有各式各樣的學習型態。剛剛的&lt;strong&gt;「二元分類問題」&lt;/strong&gt;就像考「是非題」一樣，答案要嘛是Yes不然就是No，表示為 &lt;strong&gt;y={-1, 1}&lt;/strong&gt;，這就像是機器在小學時代的問題，較為簡單。&lt;/p&gt;
&lt;p&gt;現在機器脫離國小來到了國中，考試題目開始出現「選擇題」，這和機器學習中的&lt;strong&gt;「多元分類問題」&lt;/strong&gt;一樣，必須從兩個以上有限的答案中作選擇，表示為 &lt;strong&gt;y={1, 2, ... , k}&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;另外機器還可能遇到傷透腦筋的「計算題」，在機器學習裏頭稱為&lt;strong&gt;「Regression 問題」&lt;/strong&gt;，這個時候答案已經放寬到整個實數系了，表示為 &lt;strong&gt;y∈R&lt;/strong&gt;，舉個例子，譬如利用過去天氣的數據去預測明日氣溫，或者利用歷史股價資料預測未來股價，都是Regression的應用。&lt;/p&gt;
&lt;p&gt;此時，機器到了大學，開始碰到不那麼容易回答，甚至不存在單一答案的「申論題」，這在ML中像是&lt;strong&gt;「Structure Learning 問題」&lt;/strong&gt;，答案的選擇換成了各種結構，表示為 &lt;strong&gt;y={structures}&lt;/strong&gt;，舉個例子可能比較好理解，例如：自然語言，我們都希望有一天電腦可以理解我們的語言，我們可以不再需要以機器語言來和電腦溝通，而是用人類的語言直接和電腦溝通，聽起來很棒對吧! 這個部分的ML就需要Structure Learning來學習語言的文法結構。&lt;/p&gt;
&lt;p&gt;我們教機器學習也有各種不同的教育方法。&lt;/p&gt;
&lt;p&gt;有像是填鴨式教育的&lt;strong&gt;「Supervised Learning」(監督式學習）&lt;/strong&gt;，直接告訴機器考題和答案，讓機器從中學習，這種情況下每筆資料Xn對應的yn都有明確Label，答案是人類直接告訴機器的。&lt;/p&gt;
&lt;p&gt;有像是培養科學家教育方法一樣的&lt;strong&gt;「Unsupervised Learning」(非監督式學習）&lt;/strong&gt;，此時每筆資料Xn對應的yn都沒有Label，所以機器要自己歸納整理，然後從中學到規律，通常用於分群問題，對資料做分類找出規律性。&lt;/p&gt;
&lt;p&gt;那還有折衷於上述兩種方法的啟發式教育，&lt;strong&gt;「Semi-supervised Learning」(半監督式學習）&lt;/strong&gt;，在這個情形下有部分資料yn是有Label的，機器可以藉由有Label的正確答案和資料的規律性來做更好的學習，一個有名的例子是Facebook的人臉辨識標記功能，有部分已經被用戶標記的照片，這屬於有Label的yn，但有更多沒有標記的照片，這些照片也可以幫助ML學習。&lt;/p&gt;
&lt;p&gt;那還有像是訓練小狗的方法，當我跟小狗說坐下，如果牠真的坐下了，這個時候我就給牠獎勵，譬如說餵牠好吃的食物，久而久之牠就會學會聽從這個命令，&lt;strong&gt;「Reinforcement Learning」(強化式學習）&lt;/strong&gt;就是不直接表明yn的Label，但是機器能知道yn結果的好壞，再從這個好壞當作回饋去優化它的學習。&lt;/p&gt;
&lt;p&gt;Data給的方法也可以有很多種類。&lt;/p&gt;
&lt;p&gt;剛剛舉的ML例子都是屬於&lt;strong&gt;「Batch Learning」&lt;/strong&gt;，也就是一次給你所有的Data。另外一種給Data的方法叫做&lt;strong&gt;「Online Learning」&lt;/strong&gt;，這個情形下Data會一個一個以序列的方式餵給機器，這麼方式下的Model可以隨時更新。最後一種方式是&lt;strong&gt;「Active Learning」&lt;/strong&gt;，機器不僅是被動的接受 Data，而是會根據它自己的需求向使用者索取它想要的Data。&lt;/p&gt;
&lt;p&gt;另外，除了有輸出值yn有多種種類之外，輸入的變數 Xn的來源也有很多種，我們稱之為Features。&lt;/p&gt;
&lt;p&gt;如果具有物理意義的輸入變數，稱之為&lt;strong&gt;「Concrete Features」&lt;/strong&gt;，這些變數建立在人類知識的預先處理。還有輸入變數並不具有物理含意的情形，這稱之為&lt;strong&gt;「Abstract Features」&lt;/strong&gt;。那有些情形下直接採用不加以處理的原始數據，稱為&lt;strong&gt;「Raw Features」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;而使用工人智慧由人力從Raw Features中萃取出Concrete Features，這叫做Feature Engineering，而現在很夯的Deep Learning厲害的地方是他可以自行從Data中學習 Features。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;總結一下，機器學習有很多種型態，從Data的給予方式可分為Batch Learning、Online Learning和Active Learning。Data的表達形式由輸入變數 Xn和輸出值 yn所決定，從輸入變數 Xn的來源可分為Concrete Features、Raw Features和Abstract Features，從輸出值 yn的種類上可以分為二元分類、多元分類、Regression和Structured Learning 問題，從輸出值 yn的Label給予情況可分為Supervised Learning、Unsupervised Learning、Semi-supervised Learning 和 Reinforcement Learning。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;順道一提，這16堂課裡頭主要聚焦在探討Batch Supervised Learning with Concrete Features。&lt;/p&gt;
&lt;h3&gt;後話&lt;/h3&gt;
&lt;p&gt;這篇文章帶大家初探了一眼機器學習，介紹了機器學習的架構和種類，以及它的使用時機，還有介紹了整門課非常重要的二元分類問題。但是講這麼多，機器學習真的可能嗎? 那如果可以做到，會需要哪一些要素呢? 這就必須深入理論之中，才能找到答案，在下一篇文章裡，我將介紹這門課的第二個部分：Why Can Machines Learn? &lt;/p&gt;</content><category term="機器學習基石"></category></entry></feed>