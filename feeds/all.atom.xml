<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>YC Note</title><link href="https://ycc.idv.tw/" rel="alternate"></link><link href="https://ycc.idv.tw/feeds/all.atom.xml" rel="self"></link><id>https://ycc.idv.tw/</id><updated>2024-04-05T12:00:00+08:00</updated><subtitle>ML/DL Tech Blog</subtitle><entry><title>About Me: Yi-Chang Chen</title><link href="https://ycc.idv.tw/about-me.html" rel="alternate"></link><published>2024-04-05T12:00:00+08:00</published><updated>2024-04-05T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2024-04-05:/about-me.html</id><summary type="html"></summary><content type="html">&lt;p&gt;&lt;a href="https://www.facebook.com/ai.ycc" target="_blank"&gt;&lt;img src="/media/AboutMe/facebook_follow_banner.png" height="80"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img src="/media/AboutMe/PR_portrait.png" alt="portrait" width="200"/&gt;&lt;/p&gt;
&lt;p style="text-align:center;font-size:20px;font-weight: bold;"&gt;YC / Yi-Chang Chen / 陳宜昌&lt;/p&gt;

&lt;p&gt;YC currently serves as a Senior Technology Manager at MediaTek Research. He has over 6 years of research and development experience in machine learning, with a focus on language, visual, and speech AI. With 3 years of team leadership experience, he adeptly leads teams through the entire process from research to application of technologies. He has published 5 machine learning papers, including in renowned conferences such as INTERSPEECH, ASRU, and ICIP. Additionally, he serves as the webmaster of YC Note.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;| Linkedin:&lt;/strong&gt; &lt;a href="https://www.linkedin.com/in/yi-chang-chen-aba1b6114/"&gt;https://www.linkedin.com/in/yi-chang-chen-aba1b6114/&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;| Github:&lt;/strong&gt; &lt;a href="https://github.com/GitYCC"&gt;https://github.com/GitYCC&lt;/a&gt; &lt;br /&gt;
&lt;strong&gt;| Facebook:&lt;/strong&gt; &lt;a href="https://www.facebook.com/ai.ycc"&gt;https://www.facebook.com/ai.ycc&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;| Fanpage:&lt;/strong&gt; &lt;a href="https://www.facebook.com/yc.note"&gt;https://www.facebook.com/yc.note&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;| Website:&lt;/strong&gt; &lt;a href="https://ycc.idv.tw/"&gt;https://ycc.idv.tw/&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;| Email:&lt;/strong&gt; &lt;a href="mailto:ycc.tw.email@gmail.com"&gt;ycc.tw.email@gmail.com&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;KEY ACHIEVEMENTS&lt;/strong&gt;&lt;br /&gt;
📌 [Language] Developed &lt;a href="https://huggingface.co/MediaTek-Research/Breeze-7B-Instruct-v1_0"&gt;Breeze-7B&lt;/a&gt; and &lt;a href="https://huggingface.co/MediaTek-Research/Breexe-8x7B-Instruct-v0_1"&gt;BreeXe-8x7B&lt;/a&gt;, with Breeze outperforming other models at the same size level in Chinese, and BreeXe performing on par with GPT3.5 in both English and Traditional Chinese.&lt;br /&gt;
📌 [Speech] Proposed a novel prompt-sensitive speech recognition model, &lt;a href="https://github.com/mtkresearch/clairaudience"&gt;Clairaudience&lt;/a&gt;. Accepted for the &lt;a href="https://ieeexplore.ieee.org/document/10389617"&gt;paper&lt;/a&gt; at ASRU 2023.&lt;br /&gt;
📌 [Vision] Proposed innovative unsupervised domain adaptation methods for text recognition seq-to-seq models, achieving SoTA results on public datasets. Accepted for the &lt;a href="https://ieeexplore.ieee.org/document/9897599/"&gt;paper&lt;/a&gt; at ICIP 2022.&lt;br /&gt;
📌 [Language] Introduced a novel architecture for Mandarin grapheme-to-phoneme (g2p) conversion, named g2pW, achieving SoTA performance on a public dataset. Accepted for the &lt;a href="https://www.isca-archive.org/interspeech_2022/chen22d_interspeech.html"&gt;paper&lt;/a&gt; at INTERSPEECH 2022.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EXPERIENCE&lt;/strong&gt;&lt;br /&gt;
📌 MediaTek Research, Senior Technology Manager | Mar. 2024 - Present @Taipei, Taiwan &lt;br /&gt;
📌 MediaTek Research, Senior Machine Learning Researcher | Mar. 2023 - Mar. 2024 @Taipei, Taiwan&lt;br /&gt;
📌 E.Sun Commercial Bank, Lead Machine Learning Researcher | Feb. 2021 - Mar. 2023 @Taipei, Taiwan&lt;br /&gt;
📌 E.Sun Commercial Bank, Machine Learning Engineer | Mar. 2020 - Feb. 2021 @Taipei, Taiwan&lt;br /&gt;
📌 Bridgewell, Machine Learning Engineer | Mar. 2018 - Mar. 2020 @Taipei, Taiwan&lt;br /&gt;
📌 Ur Warm Pet Cafe, Resident Singer | Mar. 2017 - Mar. 2018 @Hsinchu, Taiwan&lt;br /&gt;
📌 TSMC, Photo Lithography Software R&amp;amp;D Engineer | Nov. 2015 - Mar. 2018 @Hsinchu, Taiwan  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PUBLICATIONS&lt;/strong&gt;&lt;br /&gt;
📌 Feng-Ting Liao, Yung-Chieh Chan, Yi-Chang Chen, Chan-Jan Hsu, Da-shan Shiu, “Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning,” in &lt;em&gt;ASRU&lt;/em&gt;, 2023. [&lt;a href="https://ieeexplore.ieee.org/document/10389617"&gt;Paper&lt;/a&gt;][&lt;a href="https://github.com/mtkresearch/clairaudience"&gt;Code&lt;/a&gt;]  &lt;br /&gt;
📌 Yi-Chang Chen, Yu-Chuan Chang, Yen-Cheng Chang, and Yi-Ren Yeh, “g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin,” in &lt;em&gt;INTERSPEECH&lt;/em&gt;, 2022. [&lt;a href="https://www.isca-archive.org/interspeech_2022/chen22d_interspeech.html"&gt;Paper&lt;/a&gt;][&lt;a href="https://github.com/GitYCC/g2pW"&gt;Code&lt;/a&gt;]&lt;br /&gt;
📌 Yen-Cheng Chang, Yi-Chang Chen, Yu-Chuan Chang, and Yi-Ren Yeh, “SMILE: Sequence-to-Sequence Domain Adaptation with Minimizing Latent Entropy for Text Image Recognition,” in &lt;em&gt;ICIP&lt;/em&gt;, 2022. [&lt;a href="https://ieeexplore.ieee.org/document/9897599/"&gt;Paper&lt;/a&gt;]&lt;br /&gt;
📌 Yi-Chang Chen, Yu-Chuan Chang, Yen-Cheng Chang, and Yi-Ren Yeh, “Traditional Chinese Synthetic Datasets Verified with Labeled Data for Scene Text Recognition,” in &lt;em&gt;DLVDR Workshop on ICPR&lt;/em&gt;, 2022. [&lt;a href="https://arxiv.org/abs/2111.13327"&gt;Paper&lt;/a&gt;][&lt;a href="https://github.com/esun-ai/traditional-chinese-text-recogn-dataset"&gt;Code&lt;/a&gt;]&lt;br /&gt;
📌 Yi-Chang Chen, Chun-Yen Cheng, Chien-An Chen, Ming-Chieh Sung, and Yi-Ren Yeh, “Integrated Semantic and Phonetic Post-correction for Chinese Speech Recognition,” in &lt;em&gt;ROCLING&lt;/em&gt;, 2021. [&lt;a href="https://aclanthology.org/2021.rocling-1.13/"&gt;Paper&lt;/a&gt;][&lt;a href="https://github.com/esun-ai/phonetic_mlm"&gt;Code&lt;/a&gt;]  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HONORS&lt;/strong&gt;&lt;br /&gt;
📌 The top 5% (rank: 34/12,784) of STAGE 1 of the Global Alphathon 2022, hosted by WorldQuant (2022)&lt;br /&gt;
📌 T-Brain: AICUP2021 Traditional Chinese scene text recognition 2nd Place (2021)&lt;br /&gt;
📌 Best Paper Award in ROCLING 2021 (2021)&lt;br /&gt;
📌 T-Brain: AICUP2021 Traditional Chinese scene text detection 7th Place (2021)&lt;br /&gt;
📌 Kaggle: Shopee - Price Match Guarantee Top 3% (2021)&lt;br /&gt;
📌 Kaggle: Shopee Code League - Address Elements Extraction Top 3% (2021)&lt;br /&gt;
📌 T-Brain E.Sun House Price Prediction Competition 1st Place (2019)&lt;br /&gt;
📌 Deans' Award for Excellent Master Thesis (2014)&lt;br /&gt;
📌 Honorary member, Phi Tau Phi Scholastic Honor Society of The Republic of China (2012)&lt;br /&gt;
📌 Excellent Graduate of Bachelor in Physics (2012)  &lt;/p&gt;</content><category term="Life"></category></entry><entry><title>極簡說明Multi-thread/Multi-process、CPU-bound/IO-bound和GIL</title><link href="https://ycc.idv.tw/multithread-multiprocess-gil.html" rel="alternate"></link><published>2022-11-26T12:00:00+08:00</published><updated>2022-11-26T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2022-11-26:/multithread-multiprocess-gil.html</id><summary type="html">&lt;p&gt;Python GIL 究竟怎麼影響我們應該採用Multi-thread或Multi-process？ Multi-thread和Multi-process又是什麼？什麼是CPU-bound和IO-bound呢？&lt;/p&gt;</summary><content type="html">&lt;div class="toc"&gt;&lt;span class="toctitle"&gt;Table of Contents&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#tldr"&gt;TL;DR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#multi-thread-and-multi-process"&gt;Multi-Thread and Multi-Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#cpu-bound-and-io-bound"&gt;CPU-bound and IO-bound&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#gil-global-interpreter-lock"&gt;GIL (Global Interpreter Lock)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3 id="tldr"&gt;TL;DR&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multi-Thread中每個Thread會共享資源，因此其優點是：資源利用較有效率、共享資源存取較快速&lt;/li&gt;
&lt;li&gt;Multi-process將這些資源複製且隔離，其優點是：Process間不共享記憶體、程式碼比較好寫&lt;/li&gt;
&lt;li&gt;特別注意：不管在 Multi-Thread 和 Multi-process 都可以吃到完整的多顆CPU，因為所有工作都會丟到OS Thread Scheduler，Scheduler會管控CPU的使用，但是Python可能有GIL需另外討論。&lt;/li&gt;
&lt;li&gt;等待CPU計算較多的工作稱為CPU-bound Task，等待DMA拉資訊進記憶體較多的工作稱為IO-bound Task。&lt;/li&gt;
&lt;li&gt;Python 在原始 default 的編譯器是採用CPython，而 CPython 採用了GIL，這導致了一個Python Job（一個Process）同一時間只能跑一個Thread。最終導致，CPython 的 Multi-thread 不能同時吃到多顆CPU，因此無法解決CPU-bound Task的問題。&lt;/li&gt;
&lt;li&gt;粗略的原則：在CPython的情況下，遇到CPU-bound Task使用Multi-process，遇到 IO-bound Task使用Multi-thread。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="multi-thread-and-multi-process"&gt;Multi-Thread and Multi-Process&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/CS/IMG_thread_and_process.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single Process Single Thread&lt;ul&gt;
&lt;li&gt;Single Process中記憶體存放Code、Data&lt;/li&gt;
&lt;li&gt;Single Thread 進行中需使用 Stack 來追蹤紀錄&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Single Process Multi-Thread&lt;ul&gt;
&lt;li&gt;單一Process中的多個Thread共享資源（記憶體存放的Code、Data）&lt;/li&gt;
&lt;li&gt;也因為共享資源的緣故，要小心撰寫程式碼避免dead lock和race condition等問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multi-process&lt;ul&gt;
&lt;li&gt;每個Process都有自己一份的Code、Data，process彼此間不共享這些資源&lt;/li&gt;
&lt;li&gt;所以開許多Process比較吃記憶體&lt;/li&gt;
&lt;li&gt;而且如果想要讓Process之間共享資訊也會比較不容易，需要要做 serialize、memory copy、de-serialize 等等 OS 的開銷&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;譬喻：&lt;ul&gt;
&lt;li&gt;Process 像是一間工廠，工廠裡面有許多器具（Code、Data）&lt;/li&gt;
&lt;li&gt;Thread 像是裡頭的工人&lt;/li&gt;
&lt;li&gt;Multi-Thread就像是一間工廠有多個工人，這樣的話那就需要避免他們互搶設備或互相等待&lt;/li&gt;
&lt;li&gt;Multi-Process就像是蓋多間工廠，蓋更多工廠意味著更多的資本支出，而且工廠和工廠間要資源交換比較不容易&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multi-Thread v.s. Multi-process&lt;ul&gt;
&lt;li&gt;Multi-Thread 優點：資源利用較有效率；共享資源存取較快速&lt;/li&gt;
&lt;li&gt;Multi-Thread 缺點：因為共用記憶體，所以使用上要注意dead lock和race condition等問題；程式碼比較難寫&lt;/li&gt;
&lt;li&gt;Multi-processing 優點：process之間不共享記憶體；程式碼比較好寫&lt;/li&gt;
&lt;li&gt;Multi-processing 缺點：資源利用比較沒效率，較佔記憶體；共享資源需要額外開銷，有一些東西沒辦法serialize處理上就很麻煩；如果有跨Process的分享資源的話，仍需注意dead lock和race condition等問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;特別注意：不管在 Multi-Thread 和 Multi-process 都可以吃到完整的多顆CPU，因為所有工作都會丟到OS Thread Scheduler，Scheduler會管控CPU的使用，但是Python情況特殊我們待會討論。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="cpu-bound-and-io-bound"&gt;CPU-bound and IO-bound&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CPU-bound&lt;ul&gt;
&lt;li&gt;當某個工作高度仰賴CPU計算，我們會稱之為CPU-bound Task&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IO-bound&lt;ul&gt;
&lt;li&gt;當某個工作大量等待外部存取（例如：從硬碟讀寫、網路溝通），我們會稱之為IO-bound Task&lt;/li&gt;
&lt;li&gt;外部存取：意味著從外部將資訊拉近記憶體，此工作主要由DMA來完成，CPU在這過程並不需要參與，所以會有一段等待的時間&lt;/li&gt;
&lt;li&gt;DMA（Direct Memory Access）允許某些電腦內部的硬體子系統，可以獨立地直接讀寫系統記憶體，而不需CPU介入處理 。很多硬體的系統會使用DMA，包含硬碟控制器、繪圖顯示卡、網路卡和音效卡。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="gil-global-interpreter-lock"&gt;GIL (Global Interpreter Lock)&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/CS/IMG_cpython_thread_and_process.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 在原始 default 的編譯器是採用CPython，而 CPython 採用了GIL，這導致了一個Python Job（一個Process）同一時間只能跑一個Thread，這意味著就算有多顆CPU的存在，同時間還是沒辦法吃一顆以上的CPU，也因此會有以下情況。&lt;/li&gt;
&lt;li&gt;當我們需要處理CPU-bound Task時，如果採用Multi-thread，因為GIL的緣故導致無法同時使用多顆CPU，也因此計算效能無法提升。相反如果採用Multi-process，因為開了好幾個Process，每個Process都有自己獨立的GIL，因此才能吃到多顆CPU，進而解決CPU-bound的問題。&lt;/li&gt;
&lt;li&gt;當我們需要處理IO-bound Task時，如果採用Multi-thread，因為CPU的使用量並不高，大部分thread的時間都是在等待IO，所以多個thread其實就足夠解決IO-bound問題。當然如果使用Multi-process也是可以做到，但是這會多造成資源的浪費。&lt;/li&gt;
&lt;/ul&gt;</content><category term="CS"></category><category term="GIL"></category><category term="Python"></category></entry><entry><title>Tesla AI Day 2022 筆記</title><link href="https://ycc.idv.tw/tesla-aiday2022.html" rel="alternate"></link><published>2022-10-15T12:00:00+08:00</published><updated>2022-10-15T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2022-10-15:/tesla-aiday2022.html</id><summary type="html">&lt;p&gt;在2022年舉辦的Tesla AI Day上，他們推出了人形機器人Optimus，以及介紹他們在自動駕駛上的技術推進。這篇是YC觀賞完AI Day後的筆記，摘要了一些重點，並且在最後提出我的觀點。&lt;/p&gt;</summary><content type="html">&lt;div class="toc"&gt;&lt;span class="toctitle"&gt;Table of Contents&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#tesla-bot-optimus"&gt;Tesla Bot: Optimus&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#demo"&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hardware-architecture"&gt;Hardware Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#hardware-simulation"&gt;Hardware Simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#actuators-hand"&gt;Actuators（致動器）＆ Hand&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#optimus-software"&gt;Optimus Software&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#full-self-driving"&gt;Full Self Driving&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#intro"&gt;Intro.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#planning"&gt;Planning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#occupancy-network"&gt;Occupancy Network&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#training-infra"&gt;Training Infra.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#lanes"&gt;Lanes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#auto-labeling"&gt;Auto Labeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#simulation"&gt;Simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#data-engine"&gt;Data Engine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#_1"&gt;我的觀點&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;原始影片：&lt;a href="https://www.youtube.com/watch?v=ODSJsviD_SU&amp;amp;t=9274s"&gt;youtube&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="tesla-bot-optimus"&gt;Tesla Bot: Optimus&lt;/h2&gt;
&lt;h3 id="demo"&gt;Demo&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/optimus.gif" /&gt;&lt;/p&gt;
&lt;p&gt;上次 Tesla AI Day 預告的人形機器人 Optimus 終於亮相了，Musk 說開發這個機器主要是要幫助人類完成一些枯燥或危險的工作，為了達到這個目的，Optimus 的手指有特別設計，可以靈巧的完成人類的工作，Musk 強調說目前市面上大部分的機器人都缺乏大腦，而且售價昂貴，Optimus 未來售價將壓在2萬美元以下，並且預計在明年開始量產。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/optimus_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Optimus 目前已經可以完成一些基本動作。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/optimus_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;Optimus 採用在自動駕駛上使用的視覺系統，含有語義分割的功能，所以在 Optimus 的眼中它可以認得出它自身的部分、地板、花圃、澆水器、等。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/optimus_4.png" /&gt;&lt;/p&gt;
&lt;h3 id="hardware-architecture"&gt;Hardware Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/human_form.png" /&gt;&lt;/p&gt;
&lt;p&gt;Optimus 以人的身體來打造，全身上下有多於200個自由度，自由度可以想像可以彎曲的地方，類比於人類的關節，而手有多達27個自由度，從影片仔細觀察 Optimus 的大拇指甚至可以向內折去碰觸小指頭，相當的細緻。而 Optimus 的重量也跟一個成人差不多約73公斤。並且 Tesla 團隊特別設計讓 Optimus 在待機的狀態下可以比較省電。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/latest_generation.png" /&gt;&lt;/p&gt;
&lt;p&gt;上圖橘色的部分為 Actuators（致動器，是一種將能源轉換成機械動能的裝置），而藍色的部分為電力系統，Optimus 同樣裝備著如電動車上的晶片，這晶片可以用來做影像處理、路線規劃，甚至他們希望未來我們可以跟 Optimus 溝通，所以裝備著無線連線器和聲音接收器。&lt;/p&gt;
&lt;h3 id="hardware-simulation"&gt;Hardware Simulation&lt;/h3&gt;
&lt;p&gt;延續 Tesla 造車的經驗，Tesla 透過物理引擎模擬來設計機械結構，舉例：利用物理引擎模擬電動車撞擊的狀況來安全的設計車體結構。同樣的，可以透過這個物理引擎來模擬 Optimus 跌倒的狀況，並且設計 Optimus 在各種跌倒狀態下能避免電池和電路板等重要地方嚴重損壞。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/structure_for_mass_production.png" /&gt;&lt;/p&gt;
&lt;p&gt;同樣的，可以利用物理引擎來模擬機器人走路的受力狀況，並且設計出好的結構來應付各類動作。物理引擎可以有效的節省實體試錯的時間。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/knee_joint.png" /&gt;&lt;/p&gt;
&lt;p&gt;例如在設計膝關節也得益於物理引擎的幫忙，為了仿造膝蓋關節的結構 Tesla 經過模擬後採用「四軸系統」。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/knee_requirement_from_tasks.png" /&gt;&lt;/p&gt;
&lt;p&gt;上圖綠色的曲線代表四軸系統，而藍色的曲線代表兩軸系統，其呈現的是在關節不同角度下的模擬受力狀況，可以發現兩軸系統在膝蓋彎曲的時候受力相當大，而四軸系統則能穩定維持低受力狀況，因此四軸系統比較有效率。&lt;/p&gt;
&lt;h3 id="actuators-hand"&gt;Actuators（致動器）＆ Hand&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/actuator_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;由電驅動的致動器已經用於電動車上，但電動車只需要前後兩個致動器，而為了支援人形機器人的各類動作，例如：走路、跑步、搬重物、爬樓梯等，Optimus 需要使用到28個致動器。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/actuator_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;為了要選擇和設計這28個致動器，透過讓機器人走路和轉彎模擬並畫出各致動器的力矩-速度的軌跡，有了這個軌跡圖我們就可以計算某個致動器的耗能狀況，進而推算出其系統成本。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/actuator_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;針對某一個位置，Tesla 團隊模擬了好幾十萬顆致動器並得到所有致動器在系統成本-致動器重量的分布，我們想要選擇系統成本越低、重量越輕的致動器，所以會選擇最靠近左下角原點的致動器，透過 Pareto Front 我們可以找出那最佳的致動器。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/actuator_4.png" /&gt;&lt;/p&gt;
&lt;p&gt;在28個位置都可以做同樣的計算，因此我們得到28種最佳的致動器，但是考慮到未來量產，28種款式顯然太多了，縱使因為左右對稱，可以省一半，但仍然還是太多款式，因此 Tesla 團隊透過 Commonality Study（共性研究）在權衡之下，使用6種致動器裝配在這28個地方。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/actuator_5.png" /&gt;&lt;/p&gt;
&lt;p&gt;這6種致動器如上圖所示，上排為旋轉式、下排為線性式。而講者特別展示他們的致動器有能力舉起一架三腳鋼琴，這並不是花俏的功能，這是必須要有的，因為人類的某些肌肉也同樣有此張力。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/hands.png" /&gt;&lt;/p&gt;
&lt;p&gt;為了要打造像人類一樣靈巧的雙手，Optimus 手部使用了6個致動器，並提供了11個自由度。擁有可調整的抓握能力，可以依照情境去調整抓握的方法與力氣，可以提取約9公斤的重物，並且使用工具，甚至是相當小的物件。而特別有趣的一點是，Optimus 的手有嵌入 Sensors，所以在抓握的時候 Optimus 可以了解它與物件接觸的狀況來調整抓握的方法。&lt;/p&gt;
&lt;h3 id="optimus-software"&gt;Optimus Software&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/robot_occupancy.png" /&gt;&lt;/p&gt;
&lt;p&gt;Optimus 的視覺系統採用和自駕車相同的 Occupancy Network（詳見下方介紹），而因為通常在室內，所以沒有GPS可以參考，因此 Optimus 更仰賴於視覺系統。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/visual_navigation.png" /&gt;&lt;/p&gt;
&lt;p&gt;為了能在室內行走而不去撞到東西，Optimus 建基在 Occupancy Network 上預測圖上藍色的高頻關注點，可以控制避免 Optimus 去撞上這些點的同時，也可以可以利用這些點的實時變化來了解機器人自身的動作。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;讓機器人走路並不是件容易的事情，要考慮以下幾件事：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對物理自身的了解：譬如腳的長度、重量、腳掌的長度&lt;/li&gt;
&lt;li&gt;怎樣的走路姿態是節能的&lt;/li&gt;
&lt;li&gt;平衡&lt;/li&gt;
&lt;li&gt;協同動作&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/locomaotion_plan_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;為了考量這些，Tesla 的作法是利用模擬去訂定移動計畫，並且規劃出軌跡，包括：左腳掌和右腳掌要踏的位置、腳掌的移動軌跡、骨盆的移動軌跡、等等。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/locomaotion_plan_real_world.png" /&gt;&lt;/p&gt;
&lt;p&gt;但縱使使用模擬周全考慮後，在真實世界還是存在著誤差，例如：機器人可能會振動偏移、Sensors 也存在著觀測誤差，所以有許多狀況是超出模擬之外的，這導致一開始機器人是不穩定的，很容易跌倒。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/motion_control.png" /&gt;&lt;/p&gt;
&lt;p&gt;所以 Tesla 提出了修正，建基在模擬預測之上，Optimus 使用 Sensors 去估計當下狀態，計算當下狀態與理想值的差異，並進行實時調整，才終於讓 Optimus 可以在正常世界行走。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/natrual_motion_reference.png" /&gt;&lt;/p&gt;
&lt;p&gt;為了要讓 Optimus 可以做許多人類的操作，Tesla 團隊蒐集了許多由人類示範的操作動作紀錄，把它們存成一個 Library。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/online_motion_adaption.png" /&gt;&lt;/p&gt;
&lt;p&gt;而就算是拿箱子的這一個動作有存在 Library 中，但是實際狀況箱子可能有大有小、有輕有重，也可能如上圖所示放在不同的地方、高度，所以我們不能直接使用 Library 中的示範動作，因此需要透過一個軌跡優化程式來調整雙手的軌跡，才能正確的完成任務。&lt;/p&gt;
&lt;h2 id="full-self-driving"&gt;Full Self Driving&lt;/h2&gt;
&lt;h3 id="intro"&gt;Intro.&lt;/h3&gt;
&lt;p&gt;全自動駕駛從去年的2千個使用者，到今年2022已經提升到16萬個使用者，總共經歷了35個版本、訓練了近7.5萬個模型，大約每8分鐘就有一個新的模型。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/fsd_2022.png" /&gt;&lt;/p&gt;
&lt;p&gt;這是今天的大綱，Neural Networks 的部分包含了 Occupancy Networks 和 Lanes &amp;amp; Objects Networks，Occupancy Networks 作為底層描述幾何關係的預測，目標是從影像資料轉成實體世界，可以預測樹、牆、建築物、車，甚至預測它未來的移動方向；除此之外，Lanes &amp;amp; Objects Networks 提供了更詳細的語義預測。Training Data 的部分有去年提過的 Auto Labeling 和 Simulation，Data Engine 的部分則是描述他們針對錯誤的預測做改正的系統性作法。Planning 則是自動駕駛計畫它軌跡的程式。&lt;/p&gt;
&lt;h3 id="planning"&gt;Planning&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/assert_to_pedestrain.png" /&gt;&lt;/p&gt;
&lt;p&gt;計畫行車軌跡是相當複雜的，其需要考慮交互關係，如上圖，紅色的車輛想要左轉，卻遇到左前方的行人和右前方疾駛的車輛，它應該怎麼行駛呢？當下急轉顯然不是一個好行為，可能會撞到行人，所以人類駕駛會選擇讓行人先通過，再左轉從行人後方通過，但如果選擇這樣的路徑，右方疾駛的車輛可能會撞上，所以更好的作法是等待行人通過以及等待右側車輛通過再左轉。所以好的自動駕駛應該要能考慮接下來路徑的所有交互作用，但是這一切計算只能在50ms內完成，如果有大約20個物件要考慮，大概需要考慮100種的交互作用，要如何在這麼短的時間內將一切都考慮進去呢？Tesla 採用平行計算的樹搜尋法。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/interaction_search_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/interaction_search_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/interaction_search_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;基於預測的 3D Vector Space，可以計算出多條可能的路徑，針對每一種路徑 Tesla 會逐一的檢查所有可能的交互作用，最後挑選出最佳的路徑，這個過程可以看作一個 Tree Search，並且可以採用平行運算來加速這整個過程。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/interaction_search_4.png" /&gt;&lt;/p&gt;
&lt;p&gt;在樹中的每一個節點都會經歷三個歷程：路徑生成、路徑評分、基於交互作用或中間目標的分岔。&lt;/p&gt;
&lt;p&gt;路徑生成的部分，他們曾經嘗試使用物理數值優化，但這過程太過緩慢，平均一個動作需要花費1-5ms，如果有100個動作則最少需花費100ms，這超出了我們的限制，所以他們接著考慮使用神經網路的方法，讓神經網路學習人類的操作示範或數值優化後的路徑，如此一來便可以將一個動作壓在約100us，成功的解決問題。&lt;/p&gt;
&lt;p&gt;路徑生成過後，他們會針對路徑作評分，會考慮剛剛提到的「可能撞擊檢查」，好的路徑要禮讓行人、避免與其他車輛擦撞，同時也考慮了「行車舒適度」，並且使用神經網路的方法去評估「車主取消自動駕駛接管的可能性」和「這路徑像不像人類的行駛方法」。&lt;/p&gt;
&lt;h2 id="occupancy-network"&gt;Occupancy Network&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/occupancy_network.png" /&gt;&lt;/p&gt;
&lt;p&gt;Occupancy Network 作為底層描述幾何關係的預測，目標是從8顆鏡頭影像資料轉成實體世界，並提供所需的語義。它有以下特色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;能描述 Volumetric Occupancy （體積佔用）&lt;/li&gt;
&lt;li&gt;考慮了多鏡頭、影片的上下文&lt;/li&gt;
&lt;li&gt;能穩定的預測物件，不容易出現物件跳躍的情況&lt;/li&gt;
&lt;li&gt;同時能預測物件的語義&lt;/li&gt;
&lt;li&gt;同時能預測物件的動向&lt;/li&gt;
&lt;li&gt;有效率的記憶體使用和計算能力&lt;/li&gt;
&lt;li&gt;~10ms 的更新率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而上圖為其結構，其實跟去年相去不遠，可以說是將去年提及的 HydraNets 和類似 NeRF 描述 Voxels (可以想成在3D中的Pixels) 的技術相結合，去年提及的部分就不贅述了（請詳見 &lt;a href="/tesla-aiday2021.html"&gt;Tesla AI Day 2021 筆記&lt;/a&gt;），讓我們專注的討論今年值得一提的更新或更詳細的內容，如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 Deconvolutions 後，會進行 Volume Outputs 和 Surface Outputs 的預測，除了預測幾何關係外，還會預測語義和動向&lt;/li&gt;
&lt;li&gt;為了增加預測的解析度，他們設計了 Queryable Outputs 將原本 Volume Outputs 的資訊壓入 MLP 中，接著就可以查詢這個 MLP 來得到解析度更高的資訊，你可以想像在這個過程模型考量了許多資訊後給出了良好的內插結果，所以可以得到更高解析度的資訊&lt;/li&gt;
&lt;li&gt;同時，他們也使用類似 NeRF 的方法來描述 Voxels，今年他們給出了相關的論文— &lt;a href="https://arxiv.org/abs/2112.05131"&gt;Plenoxels: Radiance Fields without Neural Networks, Alex Yu et al., 2022&lt;/a&gt;，這個技術不需要神經網路也能訓練 NeRF，並且在品質些微提升的情況下降低了時間複雜度兩個數量級&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="training-infra"&gt;Training Infra.&lt;/h3&gt;
&lt;p&gt;為了要應付龐大的計算量，Tesla 裝備著14K顆GPU，其中4K顆作 Auto Labeling 之用、10K顆作模型訓練之用；並且裝備著30PB的分散式影片快取記憶體。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/optimizing_video_model_training.png" /&gt;&lt;/p&gt;
&lt;p&gt;但是並不是將大量的GPU和大容量的記憶體裝上去就完事了，Tesla 團隊還為底層做了很多的優化，如上圖所示，例如，擴充原本的 PyTorch 加速影片訓練速度、設計新的儲存格式 .smol 來降低容量和減少輸出寫入的操作次數。在優化過後，訓練 Occupancy Network 的速度提升了2.3倍。&lt;/p&gt;
&lt;h3 id="lanes"&gt;Lanes&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/fsd_lanes.png" /&gt;&lt;/p&gt;
&lt;p&gt;縱使我們已經使用 Occupancy Network 重建了整個實體世界，包括標線、車輛、紅綠燈，但行車的路線仍然不是顯而易見的，如上圖所示你可以了解行車路線有多麼複雜，它包含著行車方向、變換車道、道路切換，所以我們希望使用神經網路來預測所有可能的路線。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/fsd_lanes_nn.png" /&gt;&lt;/p&gt;
&lt;p&gt;FSD Lanes Neural Network 分為三個部分，Vision Component 設計的如同 Occupancy Network 的 Backbone 一樣；Map Component 則是希望融入 Navigation Map 的資訊，包括：路的幾何和拓撲、行車方向、車道數量、車道的拓撲、是否為公車道、是否為高乘載車道、.. 等；Language Component 則是希望以稀疏的形式輸出所有可能的路線，&lt;strong&gt;這邊的作法是借鏡了語言生成（如：GPT3）的方法，將路線化作一種語言，希望模型在參考視覺之後以 Autoregression 的方式生成這個關於路線的語言&lt;/strong&gt;，以下詳細說明。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/lang_of_lanes_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;右側是鳥瞰圖，我們要在其上預測所有路徑，左側是 Autoregressive Decoder，這邊採用多層次的預測，要注意每一層的預測皆有使用 Cross Attention 參考影像及地圖的資訊。首先要預測節點位置，但因為計算量的考量，所以採用先在大的網格上預測位置，這次輸出是 index 18，我們可找到它並反白它的位置。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/lang_of_lanes_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;接下來接續著預測它細緻的位置，此時輸出index 31，我們可以找到它並反白，此時已經確認了點位。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/lang_of_lanes_3.png" /&gt;&lt;/p&gt;
&lt;p&gt;接著預測這個點的「拓撲類型」，這邊預測的是 "Start"，也就是道路的起始點。我們可以將這些所有輸出組合成為一個 Character，然後接續的輸出下一個 Character 直到結束，是不是很像一種語言！&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/lang_of_lanes_4.png" /&gt;&lt;/p&gt;
&lt;p&gt;當我們接續的輸出下一個 Character，這次是輸出黃色的點位，其拓撲類型為 "Continue"，這意味著與上一個點相連，而相連的軌跡則由 Spline Coefficient Predictor 預測，它會透過預測係數來描述兩點連接的平滑曲線的形狀，如此一來我們就使用語言的方式連出一條「路線」。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/lang_of_lanes_5.png" /&gt;&lt;/p&gt;
&lt;p&gt;當然一個點可能有包含多條岔路，上面的例子預測藍綠色的點，其拓撲類型為 "Fork"，而 Fork Point Predictor 預測 index 0，代表是從第0個位置的點分岔出來，而 Spline Coefficient Predictor 同樣的預測兩點連線的曲線。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/lang_of_lanes_6.png" /&gt;&lt;/p&gt;
&lt;p&gt;這個操作一路的進行下去，直到出現拓撲類型為 "End of sentence" 為止，我們就成功的建構了所有的路現，是不是很聰明的想法！順道一提這個技術同樣的用在機器人 Optimus 上。&lt;/p&gt;
&lt;h3 id="auto-labeling"&gt;Auto Labeling&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/auto_labeling_for_lanes_net.png" /&gt;&lt;/p&gt;
&lt;p&gt;在2018年 Tesla 還在 Image Space 上作標注，那時每一個 Clip 需要標注533小時。而目前使用的 multi-trip 版本每個 Clip 只需標注不到6分鐘，Auto Labeling 的技術帶來巨大的改變。其作法和去年的差不多（請詳見 &lt;a href="/tesla-aiday2021.html"&gt;Tesla AI Day 2021 筆記&lt;/a&gt;），他這邊特別強調三個步驟來達到這樣的效果：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;高精度的軌跡：使用模型在每個軌跡上預測高精度的資訊，在目前任務中這個資訊就是指 Lanes&lt;/li&gt;
&lt;li&gt;多旅程重建：因為一條路徑可能有多台 Tesla 電動車在不同時間不同天氣開過無數遍，所以可以將這些資訊疊加在一起去雜訊，建構出更穩固的 Label&lt;/li&gt;
&lt;li&gt;運用於新路徑：當然 Auto Labeling 也可以運用在新的路徑的 Label 生成&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/auto_labeling_failed_case.png" /&gt;&lt;/p&gt;
&lt;p&gt;當然還有一些時機 Auto Labeling 並不能表現的很好，例如：缺乏光線、濃霧、遮擋和雨天，還有待加強，不過不用擔心的是，因為在同一個路段會有不同時機的 Clip，所以縱使當下表現不好，也可以拿過去的資訊來彌補。&lt;/p&gt;
&lt;h3 id="simulation"&gt;Simulation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;可以亂數生成行車路徑，再基於此路徑生成路面、行道樹、建築、天空，並在合理的位置生成紅綠燈，並且創造車輛行駛在可能的路徑上，這一切只要幾分鐘就可以用模擬器自動生成&lt;/li&gt;
&lt;li&gt;同一個場景，模擬器可以產生不同天氣、日光&lt;/li&gt;
&lt;li&gt;甚至可以基於 Google 地圖去生成模擬城市&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="data-engine"&gt;Data Engine&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/how_data_helps.png" /&gt;&lt;/p&gt;
&lt;p&gt;Tesla 建構一個標注的流程，讓標注人員針對模型預測錯誤的部分進行修正，並且將這些資料放到資料集當中。舉例，預測停車的問題，上圖中模型預測車子不處於停車的狀態，但標注人員從 Clip 看出車內沒有人，所以他手動標注為停車狀態，而這筆資料就相當有價值，會放進去讓模型重新訓練。和學術界不同，Tesla 不固定他的資料集，而是持續擴增他的資料集，為的就是確保在各種情況都能安全的駕駛。&lt;/p&gt;
&lt;h2 id="_1"&gt;我的觀點&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tesla 花不到一年的時間打造了人形機器人 Optimus，雖然相比於 Boston Dynamics 的 Atlas 機器人來說，Optimus 像個小嬰兒一樣，但不到一年就有如此成果，個人覺得已經是相當快速了，要知道雙足行走是相當困難的。&lt;/li&gt;
&lt;li&gt;我們從設計來看，Tesla 確實想要打造一台可為人類工作、低價、可量產的機器人。Optimus 具備純影像視覺系統，可以感知物件並了解其語義，並且仿造人類來設計機身，手部的靈活度也是少見於機器人的，因此這樣的機器人才有機會可以做到像人的動作，才有機會讓它來幫助人類完成一些枯燥或危險的工作。並且在設計初期，他們已經在考慮未來如何降低售價和量產了，我們看到他們如何利用模擬來找到低成本和低重量的致動器，我們看到他們使用共性研究將28種致動器降至6種。因此我是相當期待 Optimus 接下來的發展的！&lt;/li&gt;
&lt;li&gt;個人認為如果想要打造可為人類工作的機器人還有一件很重要的事情，很可惜它沒在這場演講被提及和說明，那就是「互動介面」，例如我想要機器人幫我洗衣服，我可能必須指示它一些步驟，例如：把衣服從籃子拿出來、放到洗衣機、加入洗衣劑、等，但我怎麼告訴機器人。是想要透過語音輸入嗎？可是語言理解是個大坑，在語音和語言理解這塊，Tesla 在資料上和 Know-how 上並沒有優勢，所以也許要透過如此自然的方法互動還需要好幾年的時間開發。抑或是想要透過程式操作？為了要能靈活操作各類任務，這勢必要設計一種程式框架來描述這模糊的世界，並且這樣的框架還可以跟影像視覺系統緊密結合，我會非常期待看到這樣的程式框架。期待明年我的疑問可以得到解答。&lt;/li&gt;
&lt;li&gt;自動駕駛的部分與去年相去不遠，但是今年有一些新東西和更仔細的說明。路徑規劃的部分有說明應該如何考慮與其他物件的交互作用，Occupancy Network 的部分有提到他們生成 Voxels 技術是採用不需要神經網路也能訓練的 NeRF。最令人驚艷的是 Lane Networks 採用語言生成的方法來預測所有可能的行車路線。&lt;/li&gt;
&lt;li&gt;去年我留下的疑問（怎麼利用駕駛人操作的資料來優化？）今年得到了解答，今年有提到使用物理引擎來規劃路徑速度太慢，所以他們使用神經網路來預測路徑，而這神經網路就是訓練在駕駛人操作的資料之上的。&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI.ML"></category><category term="Tesla"></category></entry><entry><title>Tesla AI Day 2021 筆記</title><link href="https://ycc.idv.tw/tesla-aiday2021.html" rel="alternate"></link><published>2022-10-09T12:00:00+08:00</published><updated>2022-10-09T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2022-10-09:/tesla-aiday2021.html</id><summary type="html">&lt;p&gt;在2021年舉辦的Tesla AI Day上，Tesla 揭露了他們開發自動駕駛的技術，並且預告他們將開發人形機器人。這篇是YC觀賞完AI Day後的筆記，我摘要了一些重點，並且在最後提出我的觀點。&lt;/p&gt;</summary><content type="html">&lt;div class="toc"&gt;&lt;span class="toctitle"&gt;Table of Contents&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#how-do-we-make-a-car-autonomous"&gt;How do we make a car autonomous?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#vision"&gt;Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#planning-control"&gt;Planning &amp;amp; Control&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#how-do-we-generate-training-data"&gt;How do we generate training data?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#manual-labeling"&gt;Manual labeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#auto-labeling"&gt;Auto Labeling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#simulation"&gt;Simulation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#dojo"&gt;Dojo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#tesla-bot-optimus"&gt;Tesla Bot — Optimus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#_1"&gt;我的觀點&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;原始影片：&lt;a href="https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=7750s"&gt;youtube&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="how-do-we-make-a-car-autonomous"&gt;How do we make a car autonomous?&lt;/h2&gt;
&lt;h3 id="vision"&gt;Vision&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/convert_to_vector_space.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;在 Full Self-Driving（FSD）的任務當中，需要利用多顆鏡頭的影像來重建出 3-D 的 Vector Space，而在這 Vector Space 中我們就可以靠著 Planing Algorithm 來駕駛汽車。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/detection_head.png" /&gt;&lt;/p&gt;
&lt;p&gt;在簡單的 FSD 原型中，可以輸入一個 Frame 的圖片進入 Feature Extractor，這裡採用RegNet [&lt;a href="https://arxiv.org/abs/2003.13678"&gt;Designing Network Design Spaces, Radosavovic et al., 2020&lt;/a&gt;]，並且採用 Bi-directional Feature Pyramid Network (BiFPN) [&lt;a href="https://arxiv.org/abs/1911.09070"&gt;EfficientDet: Scalable and Efficient Object Detection, Mingxing Tan et al., 2019&lt;/a&gt;] 來融合不同規模的資訊，Andrej 舉例當一輛車子在低解析度時無法被確認，此時如果使用高解析度的資訊就可以判別。最後就可以在其上設計 Detection Head，可以做分類任務或迴歸任務。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/hydranet.png" /&gt;&lt;/p&gt;
&lt;p&gt;而在 FSD 中需要更多的資訊，需要物件偵測、交通號誌偵測並判別、車道預測等等，所以需要多任務的訓練，所以 Tesla 採用多個 Detection Head，整個結構稱為 HydraNets。這樣的結構帶來三點好處，第一，因為共享 Feature，這會在 Inference 時更為有效率；第二，因為各項任務的解耦合，我們可以在不影響其他任務的情況下微調單一任務；第三，因為抽出了 Multi-scale Features 的緣故，我們可以將它存起來，這帶來了使用上的彈性，有時使用暫存的 Multi-scale Features 來微調任務，而有時可以進行 End-to-end 的訓練。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/hydranet_first_demo.gif" /&gt;&lt;/p&gt;
&lt;p&gt;上面的demo片段是最初版本的 HydraNets 結果，特別強調這邊使用單一 Frame 的圖片進行預測，從畫面中你可以看到，模型標示著停車指示、停止線、車道邊緣、其他車輛（並指示是否為停車的狀態）、交通號誌等，看起來一切好像不錯，但是當我們想將所有鏡頭的資訊綜合轉成 Vector Space 就出現了問題。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/problem_per_camera_detection_then_fusion.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;上圖揭露了問題，在每一個鏡頭下的每一個 Frame 都能清楚的預測標線，但是當我們想要利用這些資訊建立 3-D 的 Vector Space 時候，就會發現其匹配的狀況並不良好，從鳥瞰圖看來所有的線並不能正確的貼齊，效果相當差，原因在於每個圖片在預測深度時存在著誤差，誤差疊加之後就造成 Vector Space 的訊號不穩定。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/learning_where_to_look_end_to_end.png" /&gt;&lt;/p&gt;
&lt;p&gt;於是乎 Tesla 的研發團隊打算另闢蹊徑，&lt;strong&gt;假設我們直接 End-to-End 的讓模型直接輸出 Vector Space，是不是就可以解決這個問題&lt;/strong&gt;，舉個例子：當一輛卡車出現在八個鏡頭中的五個，如果使用每個鏡頭都獨立判斷的方式，模型難以感知這是同一輛卡車，但是如果我們可以綜合八顆鏡頭的資訊並且直接輸出 Vector Space，就有機會讓模型學習到這五個鏡頭內的卡車是同一輛，並且落在 Vector Space 的某個地方。要往這方向前進會先遇到兩個問題：1. 怎麼從 Image Space 轉成 Vector Space？ 2. 要做到 Vector Space 的預測，我們要有在 Vector Space 標注的資料集。問題2我們會在後續討論，而問題1的解法是引入 Self-attention [&lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention Is All You Need, Vaswani et al., 2017&lt;/a&gt;] 來結合八顆鏡頭的 Multi-scale Features 產生 Vector Space 的 Features。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/rectify_to_a_common_virtual_camera.png" /&gt;&lt;/p&gt;
&lt;p&gt;由於每輛出場的車輛其八顆鏡頭的參數可能存在著差異，如果將這些有差異的影像輸入到單一模型就可能達不到原有的效果。而 Tesla 的作法是將每顆鏡頭先做 Camera Calibration (Rectify Layer)，具體的作法是將八顆鏡頭轉換到 Synthetic Virtual Camera 作校正。上圖右側顯示校正前後的差異，其中將後側鏡頭（Repeater）的影像疊加，可以發現校正後圖片變得清晰了，這意味著所有車輛在調整完鏡頭後影像更為一致。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/vector_space_edges_and_lines.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;上圖右側的鳥瞰圖是 End-to-end 的結果，預測在 Vector Space 成效就明顯的提升了。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/detection_singlecam_multicam.gif" /&gt;&lt;/p&gt;
&lt;p&gt;在多顆鏡頭一同學習的幫助下，可以有效的解決遮擋的問題，能更穩定的預測周遭車輛。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/video_nn.png" /&gt;&lt;/p&gt;
&lt;p&gt;Tesla 的研究小組發現有一些預測需要上下文的資訊，於是他們設計一個 Feature Queue 並將每一個時間段 Frame 的 Feature 推入，並且利用 Video Module 去綜合萃取 Feature，Video Module 能更加穩定的預測周遭車輛。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/spatial_rnn.png" /&gt;&lt;/p&gt;
&lt;p&gt;除了時間尺度之外，他們還考慮了空間尺度，譬如左轉或右轉的標誌可能在先前的路上顯示，我們不能因為車輛等個紅燈就忘記了先前這些重要的資訊，所以除了時間尺度上的紀錄，我們還需要空間尺度上的紀錄，於是他們提出了 Spatial RNN 的方法在每一個空間點持續演化其 Feature，更棒的是不同 Tesla 車輛是可以共用這個空間 Feature Queue 的，所以經過不同車輛、不同時間、不同地點的大量蒐集資料，Tesla 精準的掌握豐富的資訊地圖。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/tesla_vision_summary.png" /&gt;&lt;/p&gt;
&lt;p&gt;以上就是 Tesla Vision 模型的大架構。&lt;/p&gt;
&lt;h3 id="planning-control"&gt;Planning &amp;amp; Control&lt;/h3&gt;
&lt;p&gt;我們已經有了由 Tesla Vision 建立的 Vector Space，接下來我們就可以在這個空間裡開車，但這不是一件簡單的事，會遇到 Non-Convex    和 High-Dimensional 兩個問題。Non-Convex 意味著路線規劃中存在著多條足夠好的路徑，但是想要找到全局最優的那個路徑會變得困難，很有可能會卡在局部最優解；High-Dimensional 的原因是因為車輛需要為接下來10-15秒做規劃，要估計這一個時間區間的位置、速度、加速度，因此要描述一整條路徑是高維的。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/planning_and_control.gif" /&gt;&lt;/p&gt;
&lt;p&gt;優化方法有兩種策略 — Discrete Search 和 Continuous Function Optimization，但這兩種策略都有侷限性，Discrete Search 用在 High-Dimensional 的情況下會造成計算複雜度相當高，而 Continuous Function Optimization 用在 Non-Convex 容易陷在局部最佳解，因此 Tesla 的解法是採用混合式，&lt;strong&gt;先使用 Discrete Search 來建立一條可走的通道，來侷限路徑，稱為 Convex Corridor，再使用 Continuous Function Optimization 的方法找出一條平穩的路徑&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/need_consider_other.png" /&gt;&lt;/p&gt;
&lt;p&gt;另外，我們在做行車規劃時還需要考慮其他車輛的移動，例如遇到會車的狀況，自動駕駛要能分辨來車是否要讓道，如果對方讓道，我們的車子前進；如果對方不讓道前進，我們的車子要讓道。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/example_parking_problem.png" /&gt;&lt;/p&gt;
&lt;p&gt;在一般道路，尤其是高速公路，路徑搜索還算簡單，但是有一些狀況卻是相當棘手的，例如：停車問題。如上圖，停車場充斥著許多障礙物，在路徑搜索中最常見的方法是使用 A* 演算法，但是這個簡單的方法展開次數要多達近40萬次，這樣的運算速度是不能接受的，所以 Tesla 再進一步加入行進方向導航，可以讓 A* 演算法進步到展開次數大概2萬次，但這仍然不夠快速。原因是因為目前作法並不能綜觀全局（使用鳥瞰圖）來找路徑，於是我們借助神經網路的幫忙，&lt;strong&gt;利用類似AlphaGO的方式來做路徑搜索，結果展開次數可以降到近300次而已，差異相當的巨大&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id="how-do-we-generate-training-data"&gt;How do we generate training data?&lt;/h2&gt;
&lt;h3 id="manual-labeling"&gt;Manual labeling&lt;/h3&gt;
&lt;p&gt;演算法僅僅提供了效能的上界，我們還是需要充足的資料來調整演算法的參數，所以建構一個品質良好且量大的資料集是重要的，因此 &lt;strong&gt;Tesla 聘僱了大約1000名專業的標注人員進行標注，並且開發了專屬的標注軟體&lt;/strong&gt;。在一開始他們是在2維的圖片上面標注，後來如上述所提及的，模型要直接預測 Vector Space 才能表現的夠好，所以他們開始在 Vector Space 上標注，直接標注在3維的空間加上時間尺度。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/labeding_tools.png" /&gt;&lt;/p&gt;
&lt;h3 id="auto-labeling"&gt;Auto Labeling&lt;/h3&gt;
&lt;p&gt;人與電腦擅長的東西不同，人擅長語意相關的東西，而電腦擅長幾何、重建、追蹤等等，所以 Tesla 的標注軟體充分利用兩方的所長，進行人機協作，來更快速的建立良好的資料集，這就是Auto Labeling的技術。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/restructuring_the_road.png" /&gt;&lt;/p&gt;
&lt;p&gt;Tesla 會從車輛上蒐集影片的資料，並上傳至伺服器 Dojo，利用這些資料我們就可以重建整個道路，如上圖左側的圖例，輸入 (x,y) 座標查詢地面高度、道路指示線、路緣等等。你可能會想利用三維網格來描述路面，但因為拓撲的限制，這樣的表示是不可微分的，並不適合重建或生成，所以他們實際的作法是用類似於Neural Radiance Fields (NeRF) [&lt;a href="https://arxiv.org/abs/2003.08934"&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, Ben Mildenhall et al., 2020&lt;/a&gt;] ，&lt;strong&gt;NeRF 能做到利用多個角度鏡頭的拍攝來重建三維資訊（當然，必須要有這些鏡頭與拍攝物的空間相對關係）&lt;/strong&gt;，而 Tesla 的八顆鏡頭正提供了這個設置，NeRF 的精髓在於將物體和場景的資訊以隱性的方法編碼進 MLP 中，所以當我們輸入 (x,y) 座標到 MLP，我們可以獲得我們想要的空間資訊。經過大量的查詢 (x,y) 得到 z，我們就可以將路面給建構起來。&lt;/p&gt;
&lt;p&gt;更棒的是，每一輛行走的 Tesla 車輛都會持續蒐集並重建這些路段，有些路段還很有可能由多輛 Tesla 電動車一起建構來消除雜訊，所以愈多 Tesla 電動車在路面上開，獲得的資料就越全面、越精確，自動駕駛的穩定度就更高。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/walls_barriers_and_everythingelse.png" /&gt;&lt;/p&gt;
&lt;p&gt;同樣的技術要重建靜態物也是沒問題的。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/and_that_solve_the_problem.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;利用這樣的 Auto Labeling 技術 Tesla 成功的在三個月內將光雷達給移除&lt;/strong&gt;，初期因為資料不夠充足，如果遇到視線不佳的情況，預測將會失準，如上圖左側所示。為了解決這個問題，Tesla 團隊從大量車輛中找尋類似視線不佳的影片一萬枚，並且利用 Auto Labeling 技術自動標注，大概只花了一週就完成，而這在過去採用純人工標記可能需要好幾個月，結果模型在加上這樣的資料集訓練後，就能在視線不佳的情況下做出良好的預測，如上圖右測所示，縱使視線不佳也能穩定的預測前方車輛，這為 Tesla 帶來移除光雷達的信心。&lt;/p&gt;
&lt;h3 id="simulation"&gt;Simulation&lt;/h3&gt;
&lt;p&gt;Tesla 團隊也嘗試的使用模擬的方式人工合成影像資料來訓練模型。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/simulation.png" /&gt;&lt;/p&gt;
&lt;p&gt;使用模擬的一大好處是 Label 會同時產生，他可以幫助我們創造一些不容易出現的場景，例如：有人在高速公路跑步，或者創造出不容易標注的場景，例如：有一群人在走動同時存在著大量的遮擋問題。人工合成資料可以補足真實世界資料的不足，那要怎麼創造以假亂真的合成資料呢？有五個特點：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;精確傳感器的模擬：模擬的目的不是為了好看，而是要能反應真實，例如在不同相機曝光度的調整下，模擬的要盡可能接近實際的狀況，為了做到這一點，他們對傳感器進行建模，包括：傳感器噪音、運動模糊、光學失真、前燈傳輸、擋風玻璃折射特性。&lt;/li&gt;
&lt;li&gt;保持模擬的擬真：使用抗鋸齒算法，甚至是光線追蹤技術，來讓模擬更真實&lt;/li&gt;
&lt;li&gt;多樣的場景配置&lt;/li&gt;
&lt;li&gt;基於算法的場景生成：盡量減少在場景生成時的人員介入，讓這項技術可以大量生成多樣的場景，雖然可能大部分的合成資料模型都預測的不錯，但是他們會針對那些模型沒辦法預測好的場景多產生一些資料讓模型學習&lt;/li&gt;
&lt;li&gt;場景重建：更神的是，他們可以將真實世界的片段轉化成為虛擬的場景，如此一來就可以針對困難的場景多生成多筆資料&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="dojo"&gt;Dojo&lt;/h2&gt;
&lt;p&gt;為了應付自動駕駛的龐大計算量，Tesla 還自建了他們自己的超級電腦—Dojo。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/dojo.png" /&gt;&lt;/p&gt;
&lt;p&gt;Tesla 幾乎重頭打造整個系統，從晶片設計，到集成電路，到計算叢集，還有軟體設計。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/dojo_training_node.png" /&gt;&lt;/p&gt;
&lt;p&gt;上圖展示了訓練節點的內部結構，這是一個64位Superscalar CPU，圍繞著矩陣運算單元和向量SIMD進行優化。這個節點可以提供每秒1萬億次的浮點計算（FLOP）。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/dojo_chip.png" /&gt;&lt;/p&gt;
&lt;p&gt;集成354個 Training Node，就得到中間黃色的計算陣列。在計算陣列周圍使用了576個高速低功耗的SerDes圍繞，使得該晶片擁有極高的I/O帶寬。結合這些就得到了 D1 Chip ，其採用 7 nm 製程，在645平方毫米下可容納500億個電晶體。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/dojo_chip_scaling.png" /&gt;&lt;/p&gt;
&lt;p&gt;Dojo D1幾乎是業界第一。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/dojo_system.png" /&gt;&lt;/p&gt;
&lt;p&gt;含有多個Dojo D1的集成電路，可提供每秒9萬億次的浮點計算（FLOP）。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/dojo_software.png" /&gt;&lt;/p&gt;
&lt;p&gt;得益於 Tesla 構建的強大的編譯器，只需要少許的改動原本的 Pytorch 就可以使用 Dojo。&lt;/p&gt;
&lt;h2 id="tesla-bot-optimus"&gt;Tesla Bot — Optimus&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="/media/Tesla/tesla_bot.png" /&gt;&lt;/p&gt;
&lt;p&gt;Tesla 將打造機器人，他們想延伸自動駕駛上面技術來打造一個通用機器人。&lt;/p&gt;
&lt;h2 id="_1"&gt;我的觀點&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;End-to-end 是個重要的趨勢，當預測目標與真實目標貼近時往往表現得更好。Tesla 一開始嘗試想從影像的預測去推算出 Vector Space 的預測，但因為誤差累積的緣故，無法得到好的預測結果，當他們採用 End-to-end 直接預測在 Vector Space 後才得以解決問題。&lt;/li&gt;
&lt;li&gt;讓模型合理的掌握所有資訊是重要的，例如在這裡：綜合八顆鏡頭的影像、在時間尺度和空間尺度上讓模型使用這些上下文的資訊。&lt;/li&gt;
&lt;li&gt;Tesla 將自己打造成一家資料公司，當資料愈多，訓練出來的模型就越準；自動駕駛越優質，更能吸引市場大眾購買他們的電動車，又反過來增強他們的資料，這形成一個良性循環。&lt;/li&gt;
&lt;li&gt;為了有效的利用龐大的資料，Tesla 發展了 Auto Labeling 的技術以及打造超級電腦 Dojo，這同樣形成一個良性循環，Auto Labeling 讓標注速度大幅度提升，可以快速迭代模型；而模型更準確，Auto Labeling 的重建技術就越準確。在這個循環下，Tesla 可以漸漸的不需要那麼多標注人員。&lt;/li&gt;
&lt;li&gt;Auto Labeling 技術應該是推動這整個專案最重要的技術。&lt;/li&gt;
&lt;li&gt;你是否有發現 Tesla 幾乎從頭打造開發自動駕駛的一切，晶片自己設計、集成電路自己打造、編譯器自己設計、標注軟體自行打造、創造自己的模擬引擎。這是 Musk 一貫的方法，重新打造並精練整個開發過程，最終會帶來精準打擊目標同時降低成本的效果，並且創造公司技術的護城河。&lt;/li&gt;
&lt;li&gt;Tesla 確實有創造可量產人形機器人的底氣在，自動駕駛技術就是電機械技術與AI的交會，而且 Tesla 還懂得如何降低成本及量產，善用這些Know-how來打造人形機器人是個聰明的決策，也同時創造公司的另外一個上升曲線。&lt;/li&gt;
&lt;li&gt;原本我也是一個對於移除光雷達的懷疑者，但聽完這個演講後我漸漸能理解 Tesla 為何採用純視覺的方法，一般駕駛員也是憑藉著視覺開車，更何況 Tesla 電動車有八顆鏡頭，其視野能力已經比人還好了，並且模型的預測是經過大量資料的驗證，反觀人類開車上路前的驗證是相對少的。而配置光雷達是需要成本的，如果大量資料都在在的顯示模型的有效性，拿掉光雷達的決策也是可以理解的。&lt;/li&gt;
&lt;li&gt;我覺得每個駕駛自行開車的操作也是一個重要的資訊，這場演講沒有講到他們怎麼使用這些資訊，有點想知道！&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI.ML"></category><category term="Tesla"></category></entry><entry><title>擴散模型（Diffusion Model）：生成模型的新成員</title><link href="https://ycc.idv.tw/diffusion-model.html" rel="alternate"></link><published>2022-05-20T12:00:00+08:00</published><updated>2022-05-20T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2022-05-20:/diffusion-model.html</id><summary type="html">&lt;p&gt;本篇從概念到深入數學的介紹擴散模型（Diffusion Model）。&lt;/p&gt;</summary><content type="html">&lt;div class="toc"&gt;&lt;span class="toctitle"&gt;Table of Contents&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#generative-model"&gt;生成模型 (Generative Model) 家族&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#variational-inference-evidence-lower-bound-elbo"&gt;從 Variational Inference 到 Evidence Lower Bound (ELBO)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#loss-function"&gt;擴散模型的Loss Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#appendix-a-kl-divergence-between-two-gaussians"&gt;Appendix A: KL Divergence between two Gaussians&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reference"&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id="generative-model"&gt;生成模型 (Generative Model) 家族&lt;/h2&gt;
&lt;p&gt;在過去，作為生成模型的 GAN (Generative Adversarial Network) 最廣為人使用，GAN是在2014年由 Goodfellow 所提出來的方法，其結構由兩個網路所組成：生成器網路和鑑別器網路，生成器網路負責生成以假亂真的合成樣本，而鑑別器網路負責仔細區分出真實樣本和合成樣本，經由兩者交替對抗學習，最終我們可以得到一個好的生成器。這個生成器網路通常輸入為一組取樣自高斯分布的亂數，而輸出就是合成樣本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;為何我們需要取樣於一個「分布」？&lt;/strong&gt;以圖片為例，假設是長32寬32的一張圖片可以由一組長度為32x32x3的向量來表示，這並不意味著一組長度為32x32x3的隨機向量就能產生一張「有意義的」圖片，所以在32x32x3的空間中存在著能產生有意義圖片的不同機率分布，我們只要能把這個機率分布估準了，就可以從這個機率分布抽樣出有意義的圖片，這就是為何生成模型往往需要抽樣至某個分布，可想而知要用簡單的數學式來表示這樣的分布是多們困難的一件事，因此科學家們設計了一個簡單分布—高斯分布，並希望透過若干複雜的轉換（通常使用深度網路）後可以得到這個複雜的分布，通常我們會稱這個高斯分布為Hidden Space，因此「從樣本空間抽樣」等效於「從Hidden Space抽樣再轉換」。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Generative/hidden-to-sample-space.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;繼GAN以後有一個後起之秀 — Flow-based Generative Model，不同於 GAN 需要一個鑑別器網路來輔助訓練生成器網路，Flow-based Generative Model 透過一個可逆推的網路結構來訓練，這個可逆推網路的兩端就是Hidden Space跟我們想要得到的Sample Space，既然網路是可以逆推的，我們就可以輸入一群真實樣本訓練網路將其分布轉換為高斯分布的Hidden Space，待網路訓練完成我們就可以逆推來作為生成器使用。&lt;/p&gt;
&lt;p&gt;&lt;img alt="ddpm" src="/media/Generative/ddpm.png" /&gt;&lt;/p&gt;
&lt;p&gt;近期，生成模型的家族又多了一個新的模型，就是本篇要介紹的擴散模型（Diffusion Model），Diffusion Model 的中心思想是使用若干個微幅轉換來轉換Hidden Space成為Sample Space，如上圖所示， &lt;span class="math"&gt;\(\pmb{x}_T\)&lt;/span&gt; 代表抽樣自高斯分布Hidden Space的圖片，&lt;span class="math"&gt;\(\pmb{x}_0\)&lt;/span&gt; 代表抽樣自Sample Space的圖片，從 &lt;span class="math"&gt;\(\pmb{x}_0\)&lt;/span&gt; 到 &lt;span class="math"&gt;\(\pmb{x}_T\)&lt;/span&gt; 是模糊化的過程，中間經過若干事先定義好的操作 &lt;span class="math"&gt;\(q(\pmb{x}_{t}\mid \pmb{x}_{t-1})\)&lt;/span&gt;，而生成模型旨於學習模糊化的逆向轉換 &lt;span class="math"&gt;\(p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_t)\)&lt;/span&gt; ，當我們有了&lt;span class="math"&gt;\(p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_t)\)&lt;/span&gt; 就可以隨機抽樣自Hidden Space並轉成一張合成的圖片。實際操作上，&lt;span class="math"&gt;\(p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_t)\)&lt;/span&gt; 是一個神經網路，其輸入為圖片 &lt;span class="math"&gt;\(\pmb{x}_t\)&lt;/span&gt; 和其所在的step &lt;span class="math"&gt;\(t\)&lt;/span&gt;，其輸出為預測模糊化中被添加的雜訊 &lt;span class="math"&gt;\(\pmb{z}_\theta(\pmb{x}_t ,t)\)&lt;/span&gt; ，經理論的推導證明：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{x}_{t-1}=\frac{1}{\sqrt{\bar{\alpha}_t}}(\pmb{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\pmb{z}_\theta(\pmb{x}_t ,t))+\sigma_t\pmb{z}\ \ \text{ ; }\pmb{z}\sim\mathcal{N}(0;\pmb{I})
$$&lt;/div&gt;
&lt;p&gt;
(詳見【2.22】)，因此只要能成功預測隨機數 &lt;span class="math"&gt;\(\pmb{z}_\theta(\pmb{x}_t ,t)\)&lt;/span&gt; 就可以得到逆推模糊化的反轉換。&lt;/p&gt;
&lt;p&gt;閱讀到這裡的你已經把它的概念弄懂八成了，接下來要進入到可怕的數學時間，Are you ready?&lt;/p&gt;
&lt;h2 id="variational-inference-evidence-lower-bound-elbo"&gt;從 Variational Inference 到 Evidence Lower Bound (ELBO)&lt;/h2&gt;
&lt;p&gt;近年來面對極其複雜（e.g. NN）的機率模型，傳統的優化方式變得不可行，如： Expectation-Maximization Algorithm ，所以接下來要跟大家介紹的 Variational Inference (VI) 就變得開始廣為人使用。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Generative/variational_inference.png" /&gt;&lt;/p&gt;
&lt;p&gt;常見的生成模型可以表示成如上圖所示，&lt;span class="math"&gt;\(\pmb{x}\)&lt;/span&gt;  為生成之樣本，而 &lt;span class="math"&gt;\(\pmb{z}\)&lt;/span&gt; 座落於 latent space &lt;span class="math"&gt;\(Z\)&lt;/span&gt; 上的一個點，這個 latent space 可以具有各類可能的分布，為求方便通常會定義為一個 &lt;a href="/deep-dl_1.html"&gt;高斯分佈&lt;/a&gt;，即 &lt;span class="math"&gt;\(p (\pmb{z})\sim\mathcal{N}(\pmb{z}:\pmb{0};\pmb{I})\)&lt;/span&gt; 。假設 &lt;span class="math"&gt;\(p (\pmb{x}\mid \pmb{z})\)&lt;/span&gt; (給定&lt;span class="math"&gt;\(\pmb{z}\)&lt;/span&gt; 之後求 &lt;span class="math"&gt;\(\pmb{x}\)&lt;/span&gt;  的分布) 是已知的，則聯合機率為
&lt;/p&gt;
&lt;div class="math"&gt;$$
p (\pmb{x},\pmb{z})=p (\pmb{x}\mid \pmb{z})p (\pmb{z})   \ \ 【1.1】
$$&lt;/div&gt;
&lt;p&gt;
也可推得
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\pmb{z}\mid \pmb{x})=\frac{p (\pmb{x},\pmb{z})}{p (\pmb{x})}  \ \ 【1.2】
$$&lt;/div&gt;
&lt;p&gt;
【1.2】是無法輕易求得的，這是雞生蛋蛋生雞的問題，分母的 &lt;span class="math"&gt;\(p(\pmb{x})\)&lt;/span&gt; 不正是我們想要學習的目標，所以在缺乏這一項的情況下求取 &lt;span class="math"&gt;\(p(\pmb{z}\mid \pmb{x})\)&lt;/span&gt; 是做不到的。&lt;/p&gt;
&lt;p&gt;Variational Inference 的技巧就是引入 &lt;span class="math"&gt;\(q(\pmb{z}\mid \pmb{x})\)&lt;/span&gt; 來近似 &lt;span class="math"&gt;\(p(\pmb{z}\mid \pmb{x})\)&lt;/span&gt;，這麼做可以得到一個較易計算的 Evidence Lower Bound (ELBO)。接下來我們來推導一下，由於我們希望 &lt;span class="math"&gt;\(q(\pmb{z}\mid \pmb{x})\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(p(\pmb{z}\mid \pmb{x})\)&lt;/span&gt;分布盡可能的靠近，所以需要最小化他們之間的 KL Divergence：
&lt;/p&gt;
&lt;div class="math"&gt;$$
min\ D_{KL}[q(\pmb{z}\mid \pmb{x})\mid\mid p(\pmb{z}\mid \pmb{x})]  \ \ 【1.3】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}[q(\pmb{z}\mid \pmb{x})\mid\mid p(\pmb{z}\mid \pmb{x})]\\
= \int q(\pmb{z}\mid \pmb{x})\ log\ \frac{q(\pmb{z}\mid \pmb{x})}{p(\pmb{z}\mid \pmb{x})}d\pmb{z}\\
= \int q(\pmb{z}\mid \pmb{x})\ log\ \frac{p(\pmb{x})q(\pmb{z}\mid \pmb{x})}{p(\pmb{x},\pmb{z})}d\pmb{z}\ \ \ \text{;因為 }p(\pmb{x},\pmb{z})=p (\pmb{z}\mid \pmb{x})p(\pmb{x})\\
=log\ p(\pmb{x})-\int q(\pmb{z}\mid \pmb{x})\ log\ \frac{p(\pmb{x},\pmb{z})}{q(\pmb{z}\mid \pmb{x})}d\pmb{z}
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
ELBO_{\pmb{x},\pmb{z}}=\int q (\pmb{z}\mid \pmb{x})\ log\ \frac{p(\pmb{x},\pmb{z})}{q(\pmb{z}\mid \pmb{x})}d\pmb{z} \ \ 【1.4】
$$&lt;/div&gt;
&lt;p&gt;
這一項被稱為 Evidence Lower Bound (ELBO)，因為 &lt;span class="math"&gt;\(p(\pmb{x})\)&lt;/span&gt; 是樣本空間機率，應該是一個上帝決定好的定值，所以如果想要讓 &lt;span class="math"&gt;\(q(\pmb{z}\mid \pmb{x})\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(p(\pmb{z}\mid \pmb{x})\)&lt;/span&gt;分布盡可能的靠近，就需要最大化ELBO。而這項是可以計算的，將其寫成抽樣估計的形式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
ELBO_{\pmb{x},\pmb{z}}=E_{q (\pmb{z}\mid \pmb{x})}[log\ \frac{p(\pmb{x}\mid\pmb{z})p(\pmb{z})}{q(\pmb{z}\mid \pmb{x})}]\ \ 【1.5】
$$&lt;/div&gt;
&lt;p&gt;
上式中的 &lt;span class="math"&gt;\(p(\pmb{z})\)&lt;/span&gt; 是定義好的分布，通常為高斯分布，&lt;span class="math"&gt;\(p(\pmb{x}\mid\pmb{z})\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(q(\pmb{z}\mid \pmb{x})\)&lt;/span&gt; 也是兩個已知的函式，所以【1.5】是可求得的。&lt;/p&gt;
&lt;p&gt;回過頭來看Diffusion Model，每一個 Step 中模糊化 &lt;span class="math"&gt;\(q(\pmb{x}_{t}\mid \pmb{x}_{t-1})\)&lt;/span&gt; 與逆模糊化 &lt;span class="math"&gt;\(p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_t)\)&lt;/span&gt; 應該存在ELBO的限制，在待會的推倒中我們會看到這一點。&lt;/p&gt;
&lt;h2 id="loss-function"&gt;擴散模型的Loss Function&lt;/h2&gt;
&lt;p&gt;接著我們來完整推導Diffusion Model 吧！每一次的模糊化我們可以定義為
&lt;/p&gt;
&lt;div class="math"&gt;$$
q(\pmb{x}_t\mid\pmb{x}_{t-1})=\mathcal{N}(\pmb{x}_t:\sqrt{1-\beta_t}\pmb{x}_{t-1};\beta_t\pmb{I}) \ \ 【2.1】
$$&lt;/div&gt;
&lt;p&gt;
其中 &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;是介於0到1之間，這項可以是學來的，也可以是事前定義的定值，在 DDPM 論文中，&lt;span class="math"&gt;\(\beta\)&lt;/span&gt; 是一個定好的值。而經過 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 次（事先定義）的模糊化後，我們希望最終的 &lt;span class="math"&gt;\(\pmb{x}_T\)&lt;/span&gt; 可以接近高斯分布，即：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{x}_T\sim\mathcal{N}(\pmb{x}_T:0;\pmb{I}) \ \ 【2.2】
$$&lt;/div&gt;
&lt;p&gt;
這個過程我們稱之為正向擴散過程（forward diffusion process）。而逆模糊化我們定義成：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_\theta (\pmb{x}_{t-1}\mid\pmb{x}_{t})\sim\mathcal{N}(\pmb{x}_{t-1}:\pmb{\mu}_\theta (\pmb{x}_t ,t);\pmb{\Sigma}_\theta (\pmb{x}_t ,t)) \ \ 【2.3】
$$&lt;/div&gt;
&lt;p&gt;
其中 &lt;span class="math"&gt;\(p_\theta (\pmb{x}_{t-1}\mid\pmb{x}_{t})\)&lt;/span&gt; 用來近似 &lt;span class="math"&gt;\(q(\pmb{x}_{t-1}\mid\pmb{x}_{t})\)&lt;/span&gt;，&lt;span class="math"&gt;\(\pmb{\mu}_\theta\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\pmb{\Sigma}_\theta\)&lt;/span&gt; 代表模型 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 預測的平均值和標準差。&lt;/p&gt;
&lt;p&gt;然而模糊化 &lt;span class="math"&gt;\(q(\pmb{x}_{t}\mid \pmb{x}_{t-1})\)&lt;/span&gt; 與逆模糊化 &lt;span class="math"&gt;\(p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_t)\)&lt;/span&gt; 應該存在ELBO的限制，從 【1.4】出發：
&lt;/p&gt;
&lt;div class="math"&gt;$$
ELBO_{\pmb{x}_{0:T}}=\int q (\pmb{x}_{1:T}\mid \pmb{x}_0)\ log\ \frac{p_{\theta}(\pmb{x}_{0:T})}{q (\pmb{x}_{1:T}\mid \pmb{x}_0)}d\pmb{x}_{1:T}\\
=-\int q (\pmb{x}_{1:T}\mid \pmb{x}_0)\ [log\frac{q (\pmb{x}_{T}\mid \pmb{x}_{0})}{p_{\theta}(\pmb{x}_{T})}+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}-log\ p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})]d\pmb{x}_{1:T}\\
=-\{D_{KL}[q (\pmb{x}_{T}\mid \pmb{x}_{0})\mid\mid p_{\theta}(\pmb{x}_{T})]+\sum_{t=2}^{T}D_{KL}[q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})\mid\mid p_{\theta}(\pmb{x}_{t-1}\mid\pmb{x}_{t})]-log\ p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})\} \ \ 【2.4】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
log\ \frac{p_{\theta}(\pmb{x}_{0:T})}{q (\pmb{x}_{1:T}\mid \pmb{x}_0)}\\
=-log\ \frac{q (\pmb{x}_{1:T}\mid \pmb{x}_0)}{p_{\theta}(\pmb{x}_{0:T})}\\
=-log\ \frac{\prod_{t=1}^{T} q (\pmb{x}_{t}\mid \pmb{x}_{t-1})}{p_{\theta}(\pmb{x}_{T})\prod_{t=1}^{T}p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}\\
=-[-log\ p_{\theta}(\pmb{x}_{T})+\sum_{t=1}^{T}log\frac{q (\pmb{x}_{t}\mid \pmb{x}_{t-1})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}]\\
=-[-log\ p_{\theta}(\pmb{x}_{T})+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t}\mid \pmb{x}_{t-1})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}+log\frac{q (\pmb{x}_{1}\mid \pmb{x}_{0})}{p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})}] \\
=-[-log\ p_{\theta}(\pmb{x}_{T})+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}\frac{q (\pmb{x}_{t}\mid \pmb{x}_{0})}{q(\pmb{x}_{t-1}\mid \pmb{x}_{0})}+log\frac{q (\pmb{x}_{1}\mid \pmb{x}_{0})}{p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})}]\\
\text{;因為 }q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})=\frac{q (\pmb{x}_{t-1}, \pmb{x}_{t},\pmb{x}_{0})}{q (\pmb{x}_{t},\pmb{x}_{0})}=\frac{q (\pmb{x}_{t}\mid\pmb{x}_{t-1})q (\pmb{x}_{t-1}\mid\pmb{x}_{0})q (\pmb{x}_{0})}{q (\pmb{x}_{t}\mid\pmb{x}_{0})q (\pmb{x}_{0})}\\
=-[-log\ p_{\theta}(\pmb{x}_{T})+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t}\mid \pmb{x}_{0})}{q(\pmb{x}_{t-1}\mid \pmb{x}_{0})}+log\frac{q (\pmb{x}_{1}\mid \pmb{x}_{0})}{p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})}]\\
=-[-log\ p_{\theta}(\pmb{x}_{T})+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}+log\frac{q (\pmb{x}_{T}\mid \pmb{x}_{0})}{q(\pmb{x}_{1}\mid \pmb{x}_{0})}+log\frac{q (\pmb{x}_{1}\mid \pmb{x}_{0})}{p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})}]\\
=-[log\frac{q (\pmb{x}_{T}\mid \pmb{x}_{0})}{p_{\theta}(\pmb{x}_{T})}+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}-log\ p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})]
$$&lt;/div&gt;
&lt;p&gt;
我們需要最大化ELBO，從【2.4】可得優化任務為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta^*=\text{argmin}_{\theta}\ -ELBO_{\pmb{x},\pmb{z}}=\text{argmin}_{\theta}\{L_T+L_{T-1}+...+L_{1}+L_0\} \ \ 【2.5】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_T=D_{KL}[q (\pmb{x}_{T}\mid \pmb{x}_{0})\mid\mid p_{\theta}(\pmb{x}_{T})] \ \ 【2.6】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
L_{1\leq t\leq T-1}=D_{KL}[q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})\mid\mid p_{\theta}(\pmb{x}_{t-1}\mid\pmb{x}_{t})] \ \ 【2.7】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
L_0=-log\ p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1}) \ \ 【2.8】
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;計算&lt;span class="math"&gt;\(L_T\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(L_T\)&lt;/span&gt; 與 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 無關，不需要優化，可以忽略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;計算&lt;span class="math"&gt;\(L_0\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因為【2.3】，所以 &lt;span class="math"&gt;\(p_\theta(\pmb{x}_0\mid\pmb{x}_1)\)&lt;/span&gt; 是可以由模型預測而得的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;計算&lt;span class="math"&gt;\(L_{1\leq t\leq T-1}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(L_t\)&lt;/span&gt; 這一項先從 &lt;span class="math"&gt;\(q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})\)&lt;/span&gt; 開始做起，先給公式後面再補上證明：
&lt;/p&gt;
&lt;div class="math"&gt;$$
q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})=\mathcal{N}(\pmb{x}_{t-1}:\tilde{\pmb{\mu}}_t (\pmb{x}_t ,\pmb{x}_0);\tilde{\beta}_t\pmb{I}) \ \ 【2.9】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{\pmb{\mu}}_t (\pmb{x}_t ,\pmb{x}_0)=\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_{t}}\pmb{x}_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{(1-\bar{\alpha}_{t})}\pmb{x}_t \ \ 【2.10】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\tilde{\beta}_t=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_t \ \ 【2.11】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\alpha_t=1-\beta_t \ \ 【2.12】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\bar{\alpha}_t=\prod_{s=1}^t\alpha_s \ \ 【2.13】
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;接下來回過頭來，我們要來證明【2.9】，在那之前我們要來證明一個好用的式子，由【2.1】搭配【2.12】、【2.13】置換變數可得
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{x}_t=\sqrt{\alpha_t}\pmb{x}_{t-1}+\sqrt{1-\alpha_t}\pmb{\epsilon}_{t-1}\\
=\sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}\pmb{x}_{t-2}+\sqrt{1-\alpha_{t-1}}\pmb{\epsilon}_{t-2})+\sqrt{1-\alpha_t}\pmb{\epsilon}_{t-1} \\
=\sqrt{\alpha_t\alpha_{t-1}}\pmb{x}_{t-2}+[\sqrt{\alpha_t-\alpha_t\alpha_{t-1}}\pmb{\epsilon}_{t-2}+\sqrt{1-\alpha_t}\pmb{\epsilon}_{t-1}]\\
=\sqrt{\alpha_t\alpha_{t-1}}\pmb{x}_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\bar{\pmb{\epsilon}}_{t-2}\\
...\\
=\sqrt{\bar{\alpha}_t}\pmb{x}_{0}+\sqrt{1-\bar{\alpha}_t}\pmb{\epsilon}\text{ ; }\pmb{\epsilon}\sim\mathcal{N}(0;\pmb{I}) \ \ 【2.14】\\
\Rightarrow q (\pmb{x}_{t}\mid \pmb{x}_{0})=\mathcal{N}(\sqrt{\bar{\alpha}_t}\pmb{x}_{0};(1-\bar{\alpha}_t)\pmb{I}) \ \ 【2.15】
$$&lt;/div&gt;
&lt;p&gt;
上式中 &lt;span class="math"&gt;\(\bar{\epsilon}\)&lt;/span&gt; 代表兩個 Guaissan 的相加，其分布遵循 &lt;span class="math"&gt;\(\sum_{i=1}^{n}a_i\cdot \mathcal{N}(z:\mu_i;\sigma^2_i)=\mathcal{N}(z:\sum_{i=1}^{n}a_i\mu_i;\sum_{i=1}^{n}a_i^2\sigma_i^2)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;如此一來就可以來計算目標了，引入【2.1】、【2.15】可得
&lt;/p&gt;
&lt;div class="math"&gt;$$
q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})=q (\pmb{x}_{t}\mid \pmb{x}_{t-1},\pmb{x}_{0})\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{0})}{q (\pmb{x}_{t}\mid \pmb{x}_{0})}\\
\propto exp\{-\frac{1}{2}[\frac{(\pmb{x}_{t}-\sqrt{\alpha_t}\pmb{x}_{t-1})^2}{\beta_t}+\frac{(\pmb{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}}\pmb{x}_{0})^2}{1-\bar{\alpha}_{t-1}}-\frac{(\pmb{x}_{t}-\sqrt{\bar{\alpha}_{t}}\pmb{x}_{0})^2}{1-\bar{\alpha}_{t}}]\}\\
=exp\{-\frac{1}{2}[(\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar{\alpha}_{t-1}})\pmb{x}_{t-1}^2-(\frac{2\sqrt{\alpha_t}}{\beta_t}\pmb{x}_{t}+\frac{2\sqrt{\bar{\alpha}_t}}{1-\bar{\alpha}_t}\pmb{x}_{0})\pmb{x}_{t-1}+C(\pmb{x}_{t},\pmb{x}_{0})]\}
$$&lt;/div&gt;
&lt;p&gt;
整理可得平均值和方差為【2.10】和【2.11】。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;接下來為求方便計算，我們假設 &lt;span class="math"&gt;\(\pmb{\Sigma}_{\theta}\)&lt;/span&gt; 為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{\Sigma}_\theta (\pmb{x}_t ,t)=\sigma_{t,\theta} \pmb{I} \ \ 【2.16】
$$&lt;/div&gt;
&lt;p&gt;【2.16】、【2.9】、【2.3】和【A.1】代入【2.7】可得
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_{1\leq t\leq T-1}=D_{KL}[\mathcal{N}(\pmb{x}_{t-1}:\tilde{\pmb{\mu}}_t (\pmb{x}_t ,\pmb{x}_0);\tilde{\beta}_t\pmb{I})\mid\mid \mathcal{N}(\pmb{x}_{t-1}:\pmb{\mu}_\theta (\pmb{x}_t ,t);\pmb{\Sigma}_\theta (\pmb{x}_t ,t))] \\
=\sum_{j=1}^{J} D_{KL}[\mathcal{N}(x_{t-1,j}:\tilde{\mu}_{t,j} (\pmb{x}_t ,\pmb{x}_0);\tilde{\beta}_{t,j})\mid\mid \mathcal{N}(x_{t-1,j}:\mu_{\theta,j} (\pmb{x}_t ,t);\sigma_{t,\theta} (\pmb{x}_t ,t)\pmb{I})]\\
=\sum_{j=1}^{J} log\frac{\sigma_{t,\theta}}{\tilde{\beta}_{t,j}}+\frac{\tilde{\beta}_{t,j}^2+[\tilde{\mu}_{t,j} (\pmb{x}_t ,\pmb{x}_0)-\mu_{\theta,j} (\pmb{x}_t ,t)]^2}{2\sigma_{t,\theta}^2}-\frac{1}{2} \ \ 【2.17】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(\sigma_{t,\theta}\)&lt;/span&gt; 這一項在DDPM當中設為定值，作者實驗了兩種假設 &lt;span class="math"&gt;\(\sigma_{t,\theta}=\beta_t\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\sigma_{t,\theta}=\tilde{\beta}_t\)&lt;/span&gt; 發現對成效來說沒太大的差別。而Improved DDPM這一項則是用學的，作者假設 &lt;span class="math"&gt;\(\sigma_{t,\theta}=exp(v\ log\beta_t+(1-v)\ log\tilde{\beta}_t)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;觀察【2.17】可發現平均值的優化：
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_{t,mean}=\frac{1}{2\sigma_{t,\theta}^2}[\tilde{\pmb{\mu}}_{t} (\pmb{x}_t ,\pmb{x}_0)-\pmb{\mu}_{\theta} (\pmb{x}_t ,t)]^2\ \ 【2.18】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{\pmb{\mu}}_{t} (\pmb{x}_t ,\pmb{x}_0) =\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_{t}}\pmb{x}_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{(1-\bar{\alpha}_{t})}\pmb{x}_t
$$&lt;/div&gt;
&lt;p&gt;
我們可以藉由變換上式來得到優化目標，我們可以優化還原狀況（也就是原圖 &lt;span class="math"&gt;\(\pmb{x}_0\)&lt;/span&gt;），也可以預測添加的雜訊，而 DDPM作者實驗發現預測添加的雜訊得到的效果比較好，因此我們使用【2.14】替換掉 &lt;span class="math"&gt;\(\pmb{x}_0\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_{t}}\times\frac{1}{\sqrt{\bar{\alpha}_t}}[\pmb{x}_t-\sqrt{1-\bar{\alpha}_t}\pmb{\epsilon}]+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{(1-\bar{\alpha}_{t})}\pmb{x}_t\\
=\frac{1}{\sqrt{\bar{\alpha}_t}}(\pmb{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\pmb{\epsilon})\ \ 【2.19】
$$&lt;/div&gt;
&lt;p&gt;
【2.18】式中 &lt;span class="math"&gt;\(\pmb{\mu}_{\theta} (\pmb{x}_t ,t)\)&lt;/span&gt; 是我們可以假設的，假設我讓它預測添加的雜訊，我們可以假設為 
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{\mu}_{\theta} (\pmb{x}_t ,t)=\frac{1}{\sqrt{\bar{\alpha}_t}}(\pmb{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\pmb{z}_\theta(\pmb{x}_t ,t)) \ \ 【2.20】
$$&lt;/div&gt;
&lt;p&gt;
因此，我們可以推得逆模糊的關鍵公式，【2.20】代入【2.3】得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_\theta (\pmb{x}_{t-1}\mid\pmb{x}_{t})\sim\mathcal{N}(\pmb{x}_{t-1}:\frac{1}{\sqrt{\bar{\alpha}_t}}(\pmb{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\pmb{z}_\theta(\pmb{x}_t ,t));\pmb{\Sigma}_\theta (\pmb{x}_t ,t)) \ \ 【2.21】
$$&lt;/div&gt;
&lt;p&gt;
上式也可以寫作：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{x}_{t-1}=\frac{1}{\sqrt{\bar{\alpha}_t}}(\pmb{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\pmb{z}_\theta(\pmb{x}_t ,t))+\sigma_t\pmb{z}\ \ \text{ ; }\pmb{z}\sim\mathcal{N}(0;\pmb{I}) \ \ 【5.22】
$$&lt;/div&gt;
&lt;p&gt;【2.19】和【2.20】代入【2.18】得
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_{t,mean}=\frac{\beta^2_t}{2\bar{\alpha}_t(1-\bar{\alpha}_t)\sigma_{t,\theta}^2}\mid\mid\pmb{\epsilon}-\pmb{z}_\theta(\sqrt{\bar{\alpha}_t}\pmb{x}_{0}+\sqrt{1-\bar{\alpha}_t}\pmb{\epsilon},t)\mid\mid^2\ \ 【2.23】
$$&lt;/div&gt;
&lt;p&gt;
在DDPM中，作者發現使用去除上式Weighting的優化式效果更好，寫作：
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_{t,mean,simple}=\mid\mid\pmb{\epsilon}-\pmb{z}_\theta(\sqrt{\bar{\alpha}_t}\pmb{x}_{0}+\sqrt{1-\bar{\alpha}_t}\pmb{\epsilon},t)\mid\mid^2\ \ 【2.24】
$$&lt;/div&gt;
&lt;p&gt;
因此DDPM的訓練和取樣示例代碼如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/Generative/DDPM-algo.png" /&gt;&lt;/p&gt;
&lt;p&gt;其中應用到【2.24】和【2.22】。&lt;/p&gt;
&lt;h2 id="appendix-a-kl-divergence-between-two-gaussians"&gt;Appendix A: KL Divergence between two Gaussians&lt;/h2&gt;
&lt;div class="math"&gt;$$
D_{KL}[\mathcal{N}(z:\mu_1;\sigma^2_1)\mid\mid\mathcal{N}(z:\mu_2;\sigma^2_2)]\\
=\int \mathcal{N}(z:\mu_1;\sigma^2_1)[log\ \mathcal{N}(z:\mu_1;\sigma^2_1)-log\ \mathcal{N}(z:\mu_2;\sigma^2_2)]dz\\
=\int \mathcal{N}(z:\mu_1;\sigma^2_1)\{-\frac{1}{2}[log\ 2\pi\cdot \sigma^2_1+\frac{(z-\mu_1)^2}{\sigma_1^2}]+\frac{1}{2}[log\ 2\pi\cdot \sigma^2_2+\frac{(z-\mu_2)^2}{\sigma_2^2}]\}dz \\
\text{;因為 }\mathcal{N}(\mu;\sigma^2)=\frac{1}{\sqrt{2\pi\cdot \sigma^2}}exp(-\frac{(z-\mu)^2}{2\sigma^2})\\
=-\frac{1}{2}[log\ 2\pi\cdot \sigma^2_1+\frac{(\sigma_1^2+\mu_1^2)-2\mu_1\mu_1+\mu_1^2}{\sigma_1^2}]+\frac{1}{2}[log\ 2\pi\cdot \sigma^2_2+\frac{(\sigma_1^2+\mu_1^2)-2\mu_2\mu_1+\mu_2^2}{\sigma_2^2}] \\
\text{;因為 }\int z\cdot\mathcal{N}(\mu;\sigma^2)dz=\mu\text{ 且 }\int z^2\cdot\mathcal{N}(\mu;\sigma^2)dz=\sigma^2+\mu^2\\
=log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2} \ \ 【A.1】
$$&lt;/div&gt;
&lt;h2 id="reference"&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Auto-Encoding Variational Bayes: https://arxiv.org/pdf/1312.6114.pdf&lt;/li&gt;
&lt;li&gt;An Introduction to Variational Inference: https://arxiv.org/pdf/2108.13083.pdf&lt;/li&gt;
&lt;li&gt;From Autoencoder to Beta-VAE: https://lilianweng.github.io/posts/2018-08-12-vae/&lt;/li&gt;
&lt;li&gt;Denoising Diffusion Probabilistic Models: https://arxiv.org/pdf/2006.11239.pdf&lt;/li&gt;
&lt;li&gt;What are Diffusion Models: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="生成模型"></category></entry><entry><title>OCR：CRNN+CTC開源加詳細解析</title><link href="https://ycc.idv.tw/crnn-ctc.html" rel="alternate"></link><published>2020-10-12T12:00:00+08:00</published><updated>2020-10-12T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-10-12:/crnn-ctc.html</id><summary type="html">&lt;p&gt;Pytorch CRNN+CTC 開源囉！並且在這篇中會仔細介紹 CRNN 的架構，以及 CTC 的架構、訓練的參數優化和其三種 Inference 方法（greedy decode, beam search decode, prefix beam search decode）&lt;/p&gt;</summary><content type="html">&lt;div class="toc"&gt;&lt;span class="toctitle"&gt;Table of Contents&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#pytorchcrnnctc"&gt;開源：pytorch版本的CRNN+CTC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#_1"&gt;網路架構&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ctc-connectionist-temporal-classification"&gt;CTC (Connectionist Temporal Classification)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#ctc-loss-forward-backward-algorithm"&gt;CTC Loss: Forward-Backward Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#inference-of-ctc"&gt;Inference of CTC&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#greedy-decode"&gt;Greedy Decode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#beam-search-decode"&gt;Beam Search Decode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#prefix-beam-search-decode"&gt;Prefix Beam Search Decode&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#_2"&gt;結語&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reference"&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;場景文字辨識 OCR (Optical Character Recognition) 應用場景非常多，例如：Evernote提供的名片辨識、諸多銀行內部使用的存摺影像辨識、新型停車場提供的車牌辨識系統、證件識別工具、旅遊業會用到護照識別，不勝枚舉！凡是想要將生活場景中的文字藉由機器去讀取的都是OCR的範疇，它會為企業省下許多人工判讀的人力成本。&lt;/p&gt;
&lt;p&gt;在場景文字辨識中相當經典的模型就是 &lt;a href="http://arxiv.org/abs/1507.05717"&gt;CRNN+CTC&lt;/a&gt;，Github 上也有許多相關的 Repo.，但 YC 發現沒有一個將這個模型寫的夠好、夠容易修改的 pytorch 開源程式碼，所以 YC 決定自己幹一個並且將它開源，目前搭配資料集 Synth90k 已經可以在英文辨識上做到 93.9 % 的 Sequence Accuracy，歡迎大家來嘗試使用！&lt;/p&gt;
&lt;h2 id="pytorchcrnnctc"&gt;開源：pytorch版本的CRNN+CTC&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/GitYCC/crnn-pytorch"&gt;GitYCC/crnn-pytorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;👆🏼👆🏼👆🏼👆🏼👆🏼&lt;/p&gt;
&lt;p&gt;麻煩大家不吝給予星星 ⭐️，並且分享給更多人知道！&lt;/p&gt;
&lt;p&gt;&lt;img alt="reading" src="/media/CV/170_READING_62745.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="showtime" src="/media/CV/178_Showtime_70541.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="novel" src="/media/CV/78_Novel_52433.jpg" /&gt;&lt;/p&gt;
&lt;h2 id="_1"&gt;網路架構&lt;/h2&gt;
&lt;p&gt;CRNN+CTC 結構源於論文&lt;a href="http://arxiv.org/abs/1507.05717"&gt;An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition (2015), Baoguang Shi et al.&lt;/a&gt;，其網路架構其實並不複雜，講白了就是 CNN 的 Backbone 再搭配 Bi-directional RNN，最後對每個時間點作Softmax分類問題，但是在衡量輸出時則是需要綜觀每個時間點的 CTC Loss。&lt;/p&gt;
&lt;p&gt;&lt;img alt="crnn_structure" src="/media/CV/crnn_structure.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="crnn_structure_detail" src="/media/CV/crnn_structure_detail.png" /&gt;&lt;/p&gt;
&lt;p&gt;首先，將圖片轉成灰階並且伸縮至高度32，通過多層的Conv2D、MaxPool和BatchNormalization抽取圖片相關的特徵。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;img_height&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;img_width&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="n"&gt;channels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;kernel_sizes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;strides&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;paddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;cnn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# shape of input: (batch, input_channel, height, width)&lt;/span&gt;
    &lt;span class="n"&gt;input_channel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;output_channel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_sizes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;paddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;batchnorm&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BatchNorm2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_channel&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;relu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LeakyReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;leaky_relu&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# size of image: (channel, height, width) = (img_channel, img_height, img_width)&lt;/span&gt;
&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pooling0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# (64, img_height // 2, img_width // 2)&lt;/span&gt;

&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pooling1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# (128, img_height // 4, img_width // 4)&lt;/span&gt;

&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;pooling2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# (256, img_height // 8, img_width // 4)&lt;/span&gt;

&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;pooling3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# (512, img_height // 16, img_width // 4)&lt;/span&gt;

&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# (512, img_height // 16 - 1, img_width // 4 - 1)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;接下來藉由一個 DenseLayer 去將圖片轉成維度為 &lt;code&gt;map_to_seq_hidden&lt;/code&gt; 的序列，這樣就可以接續的使用 RNN 來萃取序列的特徵。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map_to_seq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_channel&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;output_height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;map_to_seq_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;這邊特別注意一下，當你經過 CNN Backbone 之後的維度是：&lt;code&gt;(batch, channel, new_height, new_width)&lt;/code&gt; ，但是你希望進RNN前的維度是：&lt;code&gt;(seq_len, batch, map_to_seq_hidden)&lt;/code&gt; ，所以需要將 &lt;code&gt;channel, new_height&lt;/code&gt; 攤平再過 DenseLayer，如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;channel&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;permute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# (width, batch, feature)&lt;/span&gt;
&lt;span class="n"&gt;seq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map_to_seq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="map_to_seq" src="/media/CV/map_to_seq.png" /&gt;&lt;/p&gt;
&lt;p&gt;最後再過兩層 Bi-directional LSTM 萃取序列相關的特徵，並且過 DenseLayer 來將維度轉成與分類的數量相同，再過 Softmax 就大功告成！注意：分類的類別是所有要預測的字元再加上 blank ε ，一般我們會讓 blank ε 在Onehot Encoding 時放在 index=0 的位置。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/CV/biLSTM.png" /&gt;&lt;/p&gt;
&lt;p&gt;詳細模型架構請詳見：&lt;a href="https://github.com/GitYCC/crnn-pytorch/blob/master/src/model.py"&gt;src/model.py&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="ctc-connectionist-temporal-classification"&gt;CTC (Connectionist Temporal Classification)&lt;/h2&gt;
&lt;p&gt;RNNs 與其他傳統方法相比，如：HMMs (Hidden Markov Model) 和 CRFs (Conditional Random Field) 有以下優勢：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNNs 無須多餘的先驗假設&lt;/li&gt;
&lt;li&gt;RNNs 提供了相當強大且一般化的機制去描述時間序列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是 RNNs 只能訓練在已經分配好的序列上，所以當你僅有 Sequence Label 是不夠的，還需要知道這些 Label 要怎麼被&lt;strong&gt;分配&lt;/strong&gt;。舉個例子：如果今天你要辨識的圖片包含 &lt;code&gt;"CAT"&lt;/code&gt; 這個詞，你想要使用 RNN 進行辨識，而這個 RNN 長度假設為 7，為了要能訓練 RNN 我們需要給它 Ground True，此時你就需要將 &lt;code&gt;"CAT"&lt;/code&gt; 這 3 個字&lt;strong&gt;分配&lt;/strong&gt;到 7 格中，但是這樣的標注是費工的、不切實際的，&lt;strong&gt;我們希望可以直接訓練在沒有分配前的 Sequence Label 上&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;CTC 設計了一個機制讓我們可以做到這件事：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在原本要預測的字元中多加入了 blank ε &lt;/li&gt;
&lt;li&gt;透過映射機制 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 將 RNN 的輸出轉化成 Sequence Prediction&lt;/li&gt;
&lt;li&gt;映射機制 &lt;span class="math"&gt;\(B\)&lt;/span&gt;：合併相鄰的字元並且除去 blank ε &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="ctc mapping" src="/media/CV/ctc_mapping.png" /&gt;&lt;/p&gt;
&lt;p&gt;有了這個映射機制 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 我們就可以不需要事先分配 Sequence Label 也能訓練網路，但是要注意：能映射到一組 Sequence Label 的可能性是有很多組合的，例如：&lt;code&gt;"hee-l-lloo"&lt;/code&gt; 和 &lt;code&gt;"hheel-lloo"&lt;/code&gt; 都會映射到 &lt;code&gt;"hello"&lt;/code&gt;，所以要特別去設計它的 Loss 讓其可以考慮各種可能性，這會在下一節中仔細闡述。&lt;/p&gt;
&lt;h2 id="ctc-loss-forward-backward-algorithm"&gt;CTC Loss: Forward-Backward Algorithm&lt;/h2&gt;
&lt;p&gt;接下來這一個部分將會是本篇最難理解的部分，而且也最為數學，所以在陷進去數學漩渦之前，我們先來概念性的了解 CTC Loss 究竟是做了什麼。簡言之，&lt;strong&gt;我們需要讓 CTC Loss 降低的同時，等同於做到提升產生 Ground True Sequence Label 的機率，而在映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 的作用下產生這個 Ground True Sequence Label 會有多種來源組合，這些組合都必須被考慮進去，然後我們有了 CTC Loss 與參數的關係式就可以使用梯度下降進行優化。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以我們需要窮舉所有可能的組合才能正確的將產生 Ground True Sequence Label 的機率算出來，我們就來試著羅列。假設我們的 Sequence Label 是 &lt;code&gt;"CAT"&lt;/code&gt; ，見圖一，所以我們的 &lt;span class="math"&gt;\(\ell\)&lt;/span&gt; 為 &lt;code&gt;"CAT"&lt;/code&gt;，為了把 blank ε 列入考慮，我們設計了 &lt;span class="math"&gt;\(\ell'\)&lt;/span&gt;，其長度關係為 &lt;span class="math"&gt;\(|\ell'|=2|\ell|+1\)&lt;/span&gt; 。參照映射函數的規則，只有圖中兩個紅點是可能的出發點、兩個藍點是可能的結束點，因此我們需要羅列從出發點到結束點可能的路徑 &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;，這些路徑都可以在經過映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 後產生 Sequence Label &lt;code&gt;"CAT"&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/CV/IMG_ctc_example_01.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖一
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;加總所有可能透過 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 映射至 Ground True Sequence Label &lt;span class="math"&gt;\(\ell\)&lt;/span&gt; 的條件機率：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\ell|y)=\sum_{π:B(π)=\ell}p(π|y)\ \text{ ↪︎【1】}
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(π|y)=\prod_{t=1}^{T}y^{t}_{π_t}\ \text{ ↪︎【2】}
$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(y^{t}_{π_t}\)&lt;/span&gt; 表示在時間 &lt;span class="math"&gt;\(t\)&lt;/span&gt; Label 為 &lt;span class="math"&gt;\(π_t\)&lt;/span&gt; 的機率。&lt;/p&gt;
&lt;p&gt;而我們的目標就是想要最大化 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt; ，取 &lt;span class="math"&gt;\(ln\)&lt;/span&gt; 在加上負號，就會變換成為最小化問題：
&lt;/p&gt;
&lt;div class="math"&gt;$$
min_{y}\ -ln[p(\ell|y)]\ \text{ ↪︎【3】}
$$&lt;/div&gt;
&lt;p&gt;
只要算其微分，就可以使用梯度下降的方法進行優化：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial}{\partial y}\{ -ln[p(\ell|y)]\}=\frac{-1}{p(\ell|y)}\frac{\partial p(\ell|y)}{\partial y}\ \text{ ↪︎【4】}
$$&lt;/div&gt;
&lt;p&gt;
所以只要解決兩件事：如何計算 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\frac{\partial p(\ell|y)}{\partial y}\)&lt;/span&gt; ，就可以優化參數了！&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;但是窮舉各種可能路徑有這麼簡單嗎？幸好我們有 &lt;a href="https://zh.wikipedia.org/zh-tw/动态规划"&gt;Dynamic Programming&lt;/a&gt; ，這裡借鏡解 HMM 優化問題會使用的 Forward-Backward Algorithm 來解決這個問題。 &lt;a href="https://zh.wikipedia.org/zh-tw/动态规划"&gt;Dynamic Programming&lt;/a&gt; 是一種遞迴的演算法，只要有初始值和遞迴式就可以一路算下去。運用在這個問題上，我們只需要找到在 &lt;span class="math"&gt;\(y_t\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(y_{t-1}\)&lt;/span&gt; 的機率累加遞迴關係，以及 &lt;span class="math"&gt;\(y_1\)&lt;/span&gt; 時刻的機率分布，就可以一路算到最後一個時刻點 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 的機率累積。&lt;/p&gt;
&lt;p&gt;首先，我們先來定義「Forward Algorithm」的 &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; ：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\alpha_t(s)\equiv\sum_{π:B(π_{1:t})=\ell_{1:s}}\prod_{t'=1}^{t}y^{t'}_{π_{t'}}\ \text{ ↪︎【5】}
$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(\alpha_t(s)\)&lt;/span&gt; 表示從可能的起始點累積到 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 時刻且 Label &lt;span class="math"&gt;\(\ell'=s\)&lt;/span&gt;  那點的總機率，可以從式子的右式看出，加總所有能將 &lt;span class="math"&gt;\(π_{1:t}\)&lt;/span&gt; (圖一中的衡欄) 映射到 &lt;span class="math"&gt;\(\ell_{1:s}\)&lt;/span&gt; (圖一中的縱欄) 的路徑 &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; ，並且將這路徑 &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; 上的各點機率相乘 (因為互為獨立事件)。&lt;/p&gt;
&lt;p&gt;再參照圖一和【1】式，我們知道 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt; 可以用 &lt;span class="math"&gt;\(\alpha_t(s)\)&lt;/span&gt; 表示：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\ell|y)=\alpha_T(|\ell'|)+\alpha_T(|\ell'|-1)\ \text{ ↪︎【6】}
$$&lt;/div&gt;
&lt;p&gt;
右式的兩項代表結束的兩個藍點。&lt;/p&gt;
&lt;p&gt;從【5】式我們可以得到 &lt;span class="math"&gt;\(t=1\)&lt;/span&gt; 的初始值，因為只有兩個紅點是可能的路徑，所以得到：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \alpha_1(1)=y_\epsilon^1 \\
    \alpha_1(2)=y_{\ell_1}^1 \\
    \alpha_1(s)=0,\ \forall s&amp;gt; 2
\end{cases}\ \text{ ↪︎【7】}
$$&lt;/div&gt;
&lt;p&gt;
從【5】式我們也可以得到遞迴式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\alpha_t(s)=[\sum_{π:B(π_{1:t-1})=\ell_{1:s}}\prod_{t'=1}^{t}y^{t'}_{π_{t'}}]y^{t}_{l'_{s}}=[\sum_{s'\in connected\ to\ (t,s)}\alpha_{t-1}(s')]y^{t}_{l'_{s}}\ \text{ ↪︎【8】}
$$&lt;/div&gt;
&lt;p&gt;
連接到 &lt;span class="math"&gt;\((t,s)\)&lt;/span&gt; 可分為三種路徑情況，如下圖：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/CV/IMG_ctc_example_02.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖二：Forward Algorithm可能路徑
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;所以我們可以把【8】遞迴式寫得更加清楚：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\alpha_t(s)= 
\begin{cases}
    [\alpha_{t-1}(s)+\alpha_{t-1}(s-1)]y_{l'_s}^t&amp;amp; \text{if } l'_s=\epsilon\text{ or }l'_{s-2}=l'_s\\
    [\alpha_{t-1}(s)+\alpha_{t-1}(s-1)+\alpha_{t-1}(s-2)]y_{l'_s}^t   &amp;amp; \text{otherwise}
\end{cases}\ \text{ ↪︎【9】}
$$&lt;/div&gt;
&lt;p&gt;
善用【9】式遞迴式和【7】式初始值，你就可以求得圖一中所有格子的 &lt;span class="math"&gt;\(\alpha_t(s)\)&lt;/span&gt; 值，再藉由【6】式就可以得到 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;我們順利的解決了【4】式中的 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt;，那接下來只剩下 &lt;span class="math"&gt;\(\frac{\partial p(\ell|y)}{\partial y}\)&lt;/span&gt;，為了算這一項我們還需要「Backward Algorithm」，「Backward Algorithm」的 &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; 定義如下： 
&lt;/p&gt;
&lt;div class="math"&gt;$$
\beta_t(s)\equiv\sum_{π:B(π_{t:T})=\ell_{s:|\ell|}}\prod_{t'=t}^{T}y^{t'}_{π_{t'}}\ \text{ ↪︎【10】}
$$&lt;/div&gt;
&lt;p&gt;
從【10】式我們可以得到 &lt;span class="math"&gt;\(t=T\)&lt;/span&gt; 的初始值，因為只有兩個藍點是可能的路徑，所以得到：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \beta_T(|\ell'|)=y_\epsilon^T \\
    \beta_T(|\ell'|-1)=y_{\ell_{|\ell|}}^T \\
    \beta_T(s)=0,\ \forall s&amp;lt; |\ell'|-1
\end{cases}\ \text{ ↪︎【11】}
$$&lt;/div&gt;
&lt;p&gt;
從【10】式我們也可以得到遞迴式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\beta_t(s)=y^{t}_{l'_{s}}[\sum_{π:B(π_{t:T})=\ell_{s:|\ell|}}\prod_{t'=t+1}^{T}y^{t'}_{π_{t'}}]=y^{t}_{l'_{s}}[\sum_{s'\in connected\ from\ (t,s)}\beta_{t+1}(s')]\ \text{ ↪︎【12】}
$$&lt;/div&gt;
&lt;p&gt;
從 &lt;span class="math"&gt;\((t,s)\)&lt;/span&gt; 連接的情況一樣也可分為三種情況，如下圖：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/CV/IMG_ctc_example_03.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖三：Backward Algorithm可能路徑
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;所以我們可以把【12】遞迴式寫得更加清楚：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\beta_t(s)= 
\begin{cases}
    [\beta_{t+1}(s)+\beta_{t+1}(s+1)]y_{l'_s}^t&amp;amp; \text{if } l'_s=\epsilon\text{ or }l'_{s+2}=l'_s\\
    [\beta_{t+1}(s)+\beta_{t+1}(s+1)+\beta_{t+1}(s+2)]y_{l'_s}^t   &amp;amp; \text{otherwise}
\end{cases}\ \text{ ↪︎【13】}
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;當我們把 Forward Algorithm 和 Backward Algorithm 合在一起就會得到一個好用的公式，綜【5】和【10】式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\alpha_t(s)\beta_t(s)=\sum_{π:B(π)=\ell\ \&amp;amp;\ \pi_t=\ell'_s}\ y_{\ell'_s}^t\prod_{t'=1}^{T}y^{t'}_{π_{t'}}\ \text{ ↪︎【14】}
$$&lt;/div&gt;
&lt;p&gt;
再考慮【2】式
&lt;/p&gt;
&lt;div class="math"&gt;$$
\Rightarrow \frac{\alpha_t(s)\beta_t(s)}{y_{\ell'_s}^t}=\sum_{π:B(π)=\ell\ \&amp;amp;\ \pi_t=\ell'_s}\ \prod_{t'=1}^{T}y^{t'}_{π_{t'}}＝\sum_{π:B(π)=\ell\ \&amp;amp;\ \pi_t=\ell'_s}p(π|y)\ \text{ ↪︎【15】}
$$&lt;/div&gt;
&lt;p&gt;
再考慮【1】式
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\ell|y)=\sum_{s=1}^{|\ell'|}\sum_{π:B(π)=\ell\ \&amp;amp;\ \pi_t=\ell'_s}p(π|y)=\sum_{s=1}^{|\ell'|}\frac{\alpha_t(s)\beta_t(s)}{y_{\ell'_s}^t}\ \text{ ↪︎【16】}
$$&lt;/div&gt;
&lt;p&gt;
【16】式提供了另外一個求 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt; 的方法，而且這個算法可以在任意時間點 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 作計算，更棒的是它可以讓我們很方便的求得 &lt;span class="math"&gt;\(\frac{\partial p(\ell|y)}{\partial y}\)&lt;/span&gt; ：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial p(\ell|x)}{\partial y_k^t}=\frac{\partial}{\partial y_k^t}[\sum_{s=1}^{|\ell'|}\frac{\alpha_t(s)\beta_t(s)}{y_{\ell'_s}^t}]\ \text{ ↪︎【17】}
$$&lt;/div&gt;
&lt;p&gt;
因為微分的關係，上式加總中與 &lt;span class="math"&gt;\(y_k^t\)&lt;/span&gt; 無關的項都會化為零，可以進一步改寫：
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\frac{\partial}{\partial y_k^t}[\sum_{s\in \{\ell'_s=k\}}\frac{\alpha_t(s)\beta_t(s)}{y_{\ell'_s}^t}]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\frac{\partial}{\partial y_k^t}[\sum_{s\in \{\ell'_s=k\}}\frac{(\cdots)y_{l'_s}^t\times y_{l'_s}^t(\cdots)}{y_{l'_s}^t}]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\sum_{s\in \{\ell'_s=k\}}\frac{(\cdots)y_{l'_s}^t\times y_{l'_s}^t(\cdots)}{[y_{l'_s}^{t}]^2}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\Rightarrow \frac{\partial p(\ell|x)}{\partial y_k^t}=\frac{1}{[y_{l'_s}^{t}]^2}\sum_{s\in \{\ell'_s=k\}}\alpha_t(s)\beta_t(s)\ \text{ ↪︎【18】}
$$&lt;/div&gt;
&lt;p&gt;因此綜【6】和【18】我們就可以解【4】式，然後就可以對網路做反向傳播優化參數了！&lt;/p&gt;
&lt;h2 id="inference-of-ctc"&gt;Inference of CTC&lt;/h2&gt;
&lt;p&gt;在 &lt;a href="https://github.com/GitYCC/crnn-pytorch"&gt;GitYCC/crnn-pytorch&lt;/a&gt; 中我有實作了&lt;a href="https://github.com/GitYCC/crnn-pytorch/blob/master/src/ctc_decoder.py"&gt;三種 CTC 的 Inference 方法&lt;/a&gt;，分別為 &lt;code&gt;greedy_decode&lt;/code&gt;、&lt;code&gt;beam_search_decode&lt;/code&gt; 和 &lt;code&gt;prefix_beam_decode&lt;/code&gt;，精確度依序越來越準確，但是隨著精確度提高所花的 Inference 時間也就越長。&lt;/p&gt;
&lt;p&gt;事實上，這三種方式都不是最佳解，真正的最佳解得枚舉出所有可能的 Sequence Label 組合再取最大機率的那一個，它必須要衡量 &lt;span class="math"&gt;\((C+1)^T\)&lt;/span&gt; 條路徑且每條路徑都需要要做 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 次的乘法，所以時間複雜度為 &lt;span class="math"&gt;\(O(T\times (C+1)^T)\)&lt;/span&gt;，這個計算量大到不切實際，假設英文字母 &lt;span class="math"&gt;\(C=26\)&lt;/span&gt; ，然後 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 假設為 &lt;span class="math"&gt;\(20\)&lt;/span&gt;，時間複雜度會達到 &lt;span class="math"&gt;\(8.4e\text{+}29\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;所以接下來我會逐一介紹三種常見的近似方法。&lt;/p&gt;
&lt;h3 id="greedy-decode"&gt;Greedy Decode&lt;/h3&gt;
&lt;div class="math"&gt;$$
\ell^*\approx B(\pi^*)\ ;\ \pi^*= \{argmax_{s}\ y^{t}_{s}\ \text{ for t=1..T}\}
$$&lt;/div&gt;
&lt;p&gt;這是最 heuristic 的方法，直接找在每個時間點 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 最大的 &lt;span class="math"&gt;\(s\)&lt;/span&gt; 當作路徑 &lt;span class="math"&gt;\(\pi^*\)&lt;/span&gt; 再過映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt;。這個方法雖然簡單，但在大部分情況下會是正確的。在最佳解時我們希望的是在過完映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 之後得到最大可能的路徑 &lt;span class="math"&gt;\(\ell^*\)&lt;/span&gt; ，所以需要考慮所有可能的路徑並將其機率加總才能得到最佳解，但是  Greedy Decode 則是只考慮了最高機率的一條路徑。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/CV/IMG_ctc_greed_decode.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖四：Greedy Decode
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_reconstruct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="beam-search-decode"&gt;Beam Search Decode&lt;/h3&gt;
&lt;p&gt;既然只選一條機率最大的路徑不那麼精確，那麼我就選最大的前 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 條路徑再將其機率相加，這個就是 Beam Search 的近似方法。為了達到這個目的，我們在每一個時間點 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 都會保留前 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 條累積相乘機率最大的可能路徑，如下圖所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/CV/IMG_ctc_beam_search.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖五：Beam Search Decode (beam size = 2)
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;beam_search_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;beam_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;beam_size&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;emission_threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;emission_threshold&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DEFAULT_EMISSION_THRESHOLD&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

    &lt;span class="n"&gt;beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[([],&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;  &lt;span class="c1"&gt;# (prefix, accumulated_log_prob)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;new_beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accumulated_log_prob&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;beams&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_count&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;log_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;emission_threshold&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;continue&lt;/span&gt;
                &lt;span class="n"&gt;new_prefix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="c1"&gt;# log(p1 * p2) = log_p1 + log_p2&lt;/span&gt;
                &lt;span class="n"&gt;new_accu_log_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;accumulated_log_prob&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;
                &lt;span class="n"&gt;new_beams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;new_prefix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_accu_log_prob&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="c1"&gt;# sorted by accumulated_log_prob&lt;/span&gt;
        &lt;span class="n"&gt;new_beams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_beams&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;beam_size&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# sum up beams to produce labels&lt;/span&gt;
    &lt;span class="n"&gt;total_accu_log_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accu_log_prob&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;beams&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_reconstruct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="c1"&gt;# log(p1 + p2) = logsumexp([log_p1, log_p2])&lt;/span&gt;
        &lt;span class="n"&gt;total_accu_log_prob&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
            &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;accu_log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_accu_log_prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NINF&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

    &lt;span class="n"&gt;labels_beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;accu_log_prob&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accu_log_prob&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;total_accu_log_prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
    &lt;span class="n"&gt;labels_beams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;labels_beams&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;其中用到 &lt;code&gt;logsumexp&lt;/code&gt; 的用意是因為我們操作在 Log Probability 上面，雖然機率相乘即是 Log Probability 相加，很方便操作。但是如果碰到需要機率相加時，就需要先取 &lt;span class="math"&gt;\(exp\)&lt;/span&gt; 還原後再相加再取 &lt;span class="math"&gt;\(log\)&lt;/span&gt; ，即：&lt;span class="math"&gt;\(log(p1 + p2) = logsumexp([log(p1), log(p2)])\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id="prefix-beam-search-decode"&gt;Prefix Beam Search Decode&lt;/h3&gt;
&lt;p&gt;我們可以再進一步讓它更精確一點，剛剛的 Beam Search 是在映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 之前找 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 條路徑，Prefix Beam Search 更進一步拿前 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 條經過映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 後的 Prefix 當作評估的方式 ，如此會更接近我們想要找到映射後的最高機率的 Sequence。&lt;/p&gt;
&lt;p&gt;在 Prefix 的世界裡不存在 blank ε，但是 blank ε 卻是會影響 Prefix，例如：&lt;code&gt;"A-A"&lt;/code&gt; 映射完會是 &lt;code&gt;"AA"&lt;/code&gt;，但是 &lt;code&gt;"AA"&lt;/code&gt; 映射完則會是 &lt;code&gt;"A"&lt;/code&gt; ，所以在 Prefix Beam Search 存在 &lt;code&gt;ProbabilityWithBlank&lt;/code&gt; 和 &lt;code&gt;ProbabilityNoBlank&lt;/code&gt; 兩種累積機率，這兩種情況分別是結尾有 blank 的&lt;strong&gt;累積&lt;/strong&gt;機率和結尾沒有 blank 的&lt;strong&gt;累積&lt;/strong&gt;機率，特別注意：&lt;strong&gt;累積&lt;/strong&gt;機率代表從開始到目前的總機率。&lt;/p&gt;
&lt;p&gt;有了這個概念，我們來看會遇到什麼樣的狀況，並且在每個狀況下我們要怎麼去計算 &lt;code&gt;ProbabilityWithBlank&lt;/code&gt; 和 &lt;code&gt;ProbabilityNoBlank&lt;/code&gt; ，以下符號 &lt;code&gt;*&lt;/code&gt; 代表上一時刻的 prefix，&lt;code&gt;E&lt;/code&gt; 代表上一時刻 prefix 的最後一個字元，&lt;code&gt;ε&lt;/code&gt; 代表 blank，以下的情況皆是在考慮新字元進來要怎麼去累加 &lt;code&gt;ProbabilityWithBlank&lt;/code&gt; 和 &lt;code&gt;ProbabilityNoBlank&lt;/code&gt; ：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/CV/prefix_beam_search_formula.png" /&gt;&lt;/p&gt;
&lt;p&gt;特別注意，以上初始化是遇到新的字元就假設其後會不帶 blank。&lt;/p&gt;
&lt;p&gt;當然每次考慮一個新的時間點，我們都需要去累加可能會產生這個 Prefix 的各種情況，因此在以下的程式碼中，我們的 &lt;code&gt;ProbabilityWithBlank&lt;/code&gt; 和 &lt;code&gt;ProbabilityNoBlank&lt;/code&gt; 是會迭代累加的。當累積完這個時間點的所有 Prefix 機率後，我們會取前 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 大機率的 Prefix 留下來繼續往下一個時間點累加，此時要用 &lt;code&gt;ProbabilityWithBlank&lt;/code&gt; 和 &lt;code&gt;ProbabilityNoBlank&lt;/code&gt; 的合來當作排序的依據。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/CV/IMG_ctc_prefix_beam_search.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖六：Prefix Beam Search Decode (beam size = 2)
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;
&lt;span class="normal"&gt;53&lt;/span&gt;
&lt;span class="normal"&gt;54&lt;/span&gt;
&lt;span class="normal"&gt;55&lt;/span&gt;
&lt;span class="normal"&gt;56&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;prefix_beam_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;beam_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;beam_size&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;emission_threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;emission_threshold&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DEFAULT_EMISSION_THRESHOLD&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

    &lt;span class="n"&gt;beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NINF&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;  &lt;span class="c1"&gt;# (prefix, (blank_log_prob, non_blank_log_prob))&lt;/span&gt;
    &lt;span class="c1"&gt;# initial of beams: (empty_str, (log(1.0), log(0.0)))&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;new_beams_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NINF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NINF&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# log(0.0) = NINF&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_nb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;beams&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_count&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;log_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;emission_threshold&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;continue&lt;/span&gt;

                &lt;span class="n"&gt;end_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

                &lt;span class="c1"&gt;# if new_prefix == prefix&lt;/span&gt;
                &lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_lp_nb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_nb&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
                        &lt;span class="n"&gt;new_lp_nb&lt;/span&gt;
                    &lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="k"&gt;continue&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;end_t&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;new_lp_nb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_nb&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                    &lt;span class="p"&gt;)&lt;/span&gt;

                &lt;span class="c1"&gt;# if new_prefix == prefix + (c,)&lt;/span&gt;
                &lt;span class="n"&gt;new_prefix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
                &lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_lp_nb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;new_prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;end_t&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;new_prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;new_lp_nb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_nb&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                    &lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;new_prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;new_lp_nb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                    &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# sorted by log(blank_prob + non_blank_prob)&lt;/span&gt;
        &lt;span class="n"&gt;beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beams&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;beam_size&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beams&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h2 id="_2"&gt;結語&lt;/h2&gt;
&lt;p&gt;我們在這篇文章當中清楚的了解到 CRNN 的架構，以及 CTC 的架構、訓練的參數優化和其三種 Inference 方法。看完了這些原理，也該動手試玩看看，在 &lt;a href="https://github.com/GitYCC/crnn-pytorch"&gt;GitYCC/crnn-pytorch&lt;/a&gt; 中已經有已經 pretrained 的模型可以使用，不妨跟著以下步驟實際動手玩玩看 OCR 場景辨識吧！&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;git clone https://github.com/GitYCC/crnn-pytorch.git
&lt;span class="nb"&gt;cd&lt;/span&gt; crnn-pytorch/
pip install -r requirements.txt
python src/predict.py demo/*.jpg
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h2 id="reference"&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1507.05717.pdf"&gt;An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition (2015), Baoguang Shi et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf"&gt;Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks (2006), Alex Graves et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1408.2873.pdf"&gt;First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs (2014), Awni Y. Hannun et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://distill.pub/2017/ctc/"&gt;Sequence Modeling With CTC, Awni Hannun.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/39266552"&gt;对《CTC 原理及实现》中的一些算法的解释&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="CV"></category></entry><entry><title>資源整理：跟上AI前沿知識</title><link href="https://ycc.idv.tw/latest_ai_info.html" rel="alternate"></link><published>2020-07-04T12:00:00+08:00</published><updated>2020-07-04T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-07-04:/latest_ai_info.html</id><summary type="html">&lt;p&gt;AI領域日新月異，在這領域的玩家應該要持續的跟上最前沿的知識和技術，本篇文章整理了相關學術研討會、部落格，讓讀者可以輕易的接觸到可靠的新資源。（也歡迎讀者補充更多資訊）(持續更新)&lt;/p&gt;</summary><content type="html">&lt;h3 id="arxiv"&gt;arXiv&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/arxiv_logo.png" /&gt;&lt;/p&gt;
&lt;p&gt;通常在&lt;a href="https://arxiv.org/"&gt;arXiv&lt;/a&gt;上幾乎可以搜到所有AI領域重要的論文，而且還可以拿到第一手的論文，但是arXiv並沒有嚴格的審核機制，所以在尚未經過其他研討會和期刊審核過之前務必要對內容執懷疑的態度。&lt;/p&gt;
&lt;h5 id="arxiv-computer-science"&gt;&lt;a href="https://arxiv.org/corr/subjectclasses"&gt;arXiv: Computer Science分類項目&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="ai"&gt;&lt;a href="https://arxiv.org/list/cs.AI/recent"&gt;AI項目最新論文&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="ml"&gt;&lt;a href="https://arxiv.org/list/cs.LG/recent"&gt;ML項目最新論文&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="cv"&gt;&lt;a href="https://arxiv.org/list/cs.CV/recent"&gt;CV項目最新論文&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="nlp"&gt;&lt;a href="https://arxiv.org/list/cs.CL/recent"&gt;NLP項目最新論文&lt;/a&gt;&lt;/h5&gt;
&lt;h3 id="conference"&gt;Conference 學術研討會&lt;/h3&gt;
&lt;p&gt;AI領域大多數的成果都會以學術研討會的方式呈現，以下列出較具權威的學術研討會。&lt;/p&gt;
&lt;p&gt;以下研討會的順序排名以Google Scholar Metrics為主，h-index代表所有發表論文中至少有h篇分別被引用了至少h次；h-median代表被引用最多的h篇（由h-index決定）論文當中引用次數的中位數。舉例：一個研討會有五篇文章，其被引用次數如下：17, 9, 6, 3, 2，其h-index為3，所以其具影響力的h篇文章被引用數如下：17, 9, 6，因此中位數9就是h-median。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/neurips_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="neurips-neural-information-processing-systems"&gt;&lt;a href="https://nips.cc"&gt;NeurIPS (Neural Information Processing Systems)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年的12月舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 169&lt;/code&gt;   &lt;code&gt;h5-median: 334&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;NeurIPS 是目前 AI &amp;amp; ML 領域中最大的研討會，前稱為NIPS&lt;/li&gt;
&lt;li&gt;專注於神經網路與深度學習&lt;/li&gt;
&lt;li&gt;同時在學術界和產業界具有名望&lt;/li&gt;
&lt;li&gt;相較於其他 AI &amp;amp; ML 研討會，NeurIPS 較著重在理論和方法論上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/iclr_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="iclr-international-conference-on-learning-representations"&gt;&lt;a href="https://iclr.cc"&gt;ICLR (International Conference on Learning Representations)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年的4-5月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 150&lt;/code&gt;   &lt;code&gt;h5-median: 276&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;顧名思義，ICLR 是最專注於深度學習的研討會&lt;/li&gt;
&lt;li&gt;ICLR 是非常新的研討會，於2013年舉辦第一屆，但是近年來深度學習火熱，再加上兩位神級創辦人 Yoshua Bengio 和 Yann LeCun 的加持之下，ICLR 的聲望自然是水漲船高&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/icml_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="icml-international-conference-on-machine-learning"&gt;&lt;a href="https://icml.cc"&gt;ICML (International Conference on Machine Learning)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年的7月舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 135&lt;/code&gt;   &lt;code&gt;h5-median: 254&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;與 NeurIPS 和 ICLR 相比，ICML 專注於更廣義的機器學習&lt;/li&gt;
&lt;li&gt;相較於其他 AI &amp;amp; ML 研討會，ICML 較著重在理論和方法論上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/aaai_logo.jpg" /&gt;&lt;/p&gt;
&lt;h5 id="aaai-aaai-conference-on-artificial-intelligence"&gt;&lt;a href="https://aaai.org"&gt;AAAI (AAAI Conference on Artificial Intelligence)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年的2月舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 95&lt;/code&gt;   &lt;code&gt;h5-median: 153&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;屬於綜合性的 AI 研討會&lt;/li&gt;
&lt;li&gt;相較於其他 AI &amp;amp; ML 研討會，AAAI 較著重在實務運用&lt;/li&gt;
&lt;li&gt;AAAI 在產業界相當有名氣&lt;/li&gt;
&lt;li&gt;相當古老的研討會，最早自1980年&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/kdd_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="sigkdd-special-interest-group-on-knowledge-discovery-and-data-mining"&gt;&lt;a href="https://www.kdd.org"&gt;SIGKDD (Special Interest Group on Knowledge Discovery and Data Mining)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年的8月舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 86&lt;/code&gt;   &lt;code&gt;h5-median: 138&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;也就是俗稱的 KDD，由 ACM (電腦協會) 主辦，ACM通過它的35個特別興趣組 (Special Interest Group，SIG) 提供特殊的技術資訊和服務，其中的SIGKDD主要是專注於資料探勘，它是資料探勘領域最好的研討會&lt;/li&gt;
&lt;li&gt;相較於其他 AI &amp;amp; ML 研討會，KDD 較著重在實務運用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/ijcai_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="ijcai-international-joint-conference-on-artificial-intelligence"&gt;&lt;a href="https://www.ijcai.org"&gt;IJCAI (International Joint Conference on Artificial Intelligence)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：2015年之前是單數年舉辦，目前已改成每年7-8月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 67&lt;/code&gt;   &lt;code&gt;h5-median: 100&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;屬於綜合性的 AI 研討會&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/colt.png" /&gt;&lt;/p&gt;
&lt;h5 id="colt-conference-on-learning-theory"&gt;&lt;a href="http://www.learningtheory.org"&gt;COLT (Conference on Learning Theory)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年6-7月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 48&lt;/code&gt;   &lt;code&gt;h5-median: 65&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;這是計算理論最好的研討會，由 ACM (電腦協會) 主辦&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="computer-vision"&gt;Computer Vision 電腦視覺&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/cvpr_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="cvpr-international-conference-on-computer-vision-and-pattern-recognition"&gt;&lt;a href="http://cvpr2020.thecvf.com"&gt;CVPR (International Conference on Computer Vision and Pattern Recognition)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年6-7月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 240&lt;/code&gt;   &lt;code&gt;h5-median: 383&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最具權威的電腦視覺研討會，歷史悠久，最早自1985年&lt;/li&gt;
&lt;li&gt;電腦視覺三大頂尖研討會之一&lt;/li&gt;
&lt;li&gt;由 IEEE 舉辦&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/eccv.png" /&gt;&lt;/p&gt;
&lt;h5 id="eccv-european-conference-on-computer-vision"&gt;&lt;a href="https://eccv2020.eu"&gt;ECCV (European Conference on Computer Vision)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：與ICCV交替舉辦，於偶數年舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 137&lt;/code&gt;   &lt;code&gt;h5-median: 263&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;電腦視覺三大頂尖研討會之一&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/iccv.png" /&gt;&lt;/p&gt;
&lt;h5 id="iccv-international-conference-on-computer-vision"&gt;&lt;a href="http://iccv2019.thecvf.com"&gt;ICCV (International Conference on Computer Vision)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：與ECCV交替舉辦，於奇數年舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 129&lt;/code&gt;   &lt;code&gt;h5-median: 220&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;電腦視覺三大頂尖研討會之一&lt;/li&gt;
&lt;li&gt;由 IEEE 舉辦&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="natural-language-processing"&gt;Natural Language Processing 自然語言處理&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/acl_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="acl-meeting-of-the-association-for-computational-linguistics"&gt;&lt;a href="https://www.aclweb.org"&gt;ACL (Meeting of the Association for Computational Linguistics)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年7-8月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 106&lt;/code&gt;   &lt;code&gt;h5-median: 168&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="emnlp-conference-on-empirical-methods-in-natural-language-processing"&gt;&lt;a href="https://www.aclweb.org"&gt;EMNLP (Conference on Empirical Methods in Natural Language Processing)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年6-7月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 88&lt;/code&gt;   &lt;code&gt;h5-median: 157&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="blogs"&gt;Blogs 部落格&lt;/h3&gt;
&lt;p&gt;許多歐美科技大廠都有寫它們的研究部落格，通常較為通俗易懂。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/googleai_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="google-ai-blog"&gt;&lt;a href="https://ai.googleblog.com/"&gt;Google AI blog&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/deepmind_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="deepmind-blog"&gt;&lt;a href="https://deepmind.com/blog/"&gt;Deepmind blog&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/openai_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="openai-research"&gt;&lt;a href="https://openai.com/research/"&gt;OpenAI Research&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/fb_logo.jpg" /&gt;&lt;/p&gt;
&lt;h5 id="facebook-ai-research"&gt;&lt;a href="https://research.fb.com/category/facebook-ai-research/"&gt;Facebook AI Research&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/ms_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="microsoft-ai-blog"&gt;&lt;a href="https://blogs.microsoft.com/ai/"&gt;Microsoft AI Blog&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/amazon_ai_logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="amazon-ai"&gt;&lt;a href="https://blog.aboutamazon.com/amazon-ai"&gt;Amazon AI&lt;/a&gt;&lt;/h5&gt;
&lt;h5 id="aws-machine-learning-blog"&gt;&lt;a href="https://aws.amazon.com/tw/blogs/machine-learning/"&gt;AWS Machine Learning Blog&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/nvidia_logo.jpg" /&gt;&lt;/p&gt;
&lt;h5 id="nvidia-news-ai"&gt;&lt;a href="https://news.developer.nvidia.com/category/artificial-intelligence/"&gt;NVIDIA News AI&lt;/a&gt;&lt;/h5&gt;
&lt;hr /&gt;
&lt;p&gt;還有學術界較知名的部落格。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/mit.png" /&gt;&lt;/p&gt;
&lt;h5 id="mits-artificial-intelligence-news"&gt;&lt;a href="http://news.mit.edu/topic/artificial-intelligence2"&gt;MIT’s artificial intelligence news&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/BAIR_Logo.png" /&gt;&lt;/p&gt;
&lt;h5 id="berkeley-artificial-intelligence-research-bair-blog"&gt;&lt;a href="https://bair.berkeley.edu/blog/"&gt;Berkeley Artificial Intelligence Research (BAIR) Blog&lt;/a&gt;&lt;/h5&gt;
&lt;hr /&gt;
&lt;p&gt;最後列一些其他知名的部落格。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/distill.png" /&gt;&lt;/p&gt;
&lt;h5 id="distill"&gt;&lt;a href="https://distill.pub/"&gt;Distill&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;嚴格來說Distill並不算是部落格，它比較像是線上的期刊。Distill旨在以一種更加易懂的方式展示AI研究，並結合互動式圖表和圖形，幫助讀者更輕鬆地理解研究。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/ml_theory.png" /&gt;&lt;/p&gt;
&lt;h5 id="machine-learning-theory"&gt;&lt;a href="http://hunch.net/"&gt;Machine Learning (Theory)&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;此部落格的作者為微軟首席研究員 &lt;a href="https://www.microsoft.com/en-us/research/people/jcl/"&gt;John Langford&lt;/a&gt; ，Langford 常常提供有關機器學習理論的深刻見解，並提供了有關國際機器學習大會的最新信息。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/ml_mastery.png" /&gt;&lt;/p&gt;
&lt;h5 id="machine-learning-mastery"&gt;&lt;a href="https://machinelearningmastery.com/blog/"&gt;Machine Learning Mastery&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;身為開發者的 Jason Brownlee 早在幾年前就開始撰寫這個部落格，內容較為貼近如何實用機器學習。到目前它仍然是尋求增加ML概念知識的業界人士的首選部落格。 &lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/LatestAIInfo/analytics_vidhya.png" /&gt;&lt;/p&gt;
&lt;h5 id="analytics-vidhya"&gt;&lt;a href="https://www.analyticsvidhya.com/blog/"&gt;Analytics Vidhya&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;Analytics Vidhya的網站為數據科學家們提供了許多的乾貨，包括大量有關AI、機器學習和深度學習的材料。&lt;/p&gt;
&lt;h3 id="reference"&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=jcL5iOjV7as&amp;amp;feature=youtu.be"&gt;youtube: Top Conferences in Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=nPdwLY15Y-o&amp;amp;feature=youtu.be"&gt;youtube: Top Conferences for Natural Language Processing (NLP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=VtjTgSnKb-I"&gt;youtube: Top Conferences for Computer Vision and Image Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scholar.google.com/citations?view_op=top_venues&amp;amp;hl=en&amp;amp;vq=eng"&gt;google scholar rank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mropengate.blogspot.com/2019/01/ai.html"&gt;AI 深度學習：工作後的學術資源總整理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/What-are-the-best-conferences-and-journals-about-machine-learning"&gt;quora: What are the best conferences and journals about machine learning?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.springboard.com/blog/machine-learning-blog/"&gt;40 Must-Read AI / Machine Learning Blogs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI.ML"></category></entry><entry><title>剖析深度學習 (4)：Sigmoid, Softmax怎麼來？為什麼要用MSE和Cross Entropy？談廣義線性模型</title><link href="https://ycc.idv.tw/deep-dl_4.html" rel="alternate"></link><published>2020-03-14T12:00:00+08:00</published><updated>2020-03-14T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-03-14:/deep-dl_4.html</id><summary type="html">&lt;p&gt;學習一段時間深度學習的你是不是有一個疑惑：Activation Function為什麼要用Sigmoid和Softmax？Loss Function為什麼要用MSE和Cross Entropy？其他狀況要用什麼？當然你可以把它們看作是個合理定義，但是學習深度就端看你是不是可以用最少的定義表示最多的東西，如果你仔細google一下就會發現有一個相關的名詞—廣義線性定理，但是大部分的文章和教材都沒辦法將它講的很清楚，原因是因為沒有先介紹「充分統計量」的概念。在本講你會學到如何用「充分統計量」來說明在廣義線性定理中的Canonical Link Function，進而推導出Activation Function，你會學到如何藉由MLE和MAP來推導出Loss Function，學完以後你會對Activation Function和Loss Function有更深的認識。&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;深度學習發展至今已經有相當多好用的套件，使得進入的門檻大大的降低，因此如果想要快速的實作一些深度學習或機器學習，通常是幾行程式碼可以解決的事。但是，如果想要將深度學習或機器學習當作一份工作，深入了解它背後的原理和數學是必要的，才有可能因地制宜的靈活運用，YC準備在這一系列當中帶大家深入剖析深度學習。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;前面的&lt;a href="/deep-dl_2.html"&gt;第二講&lt;/a&gt;和&lt;a href="/deep-dl_3.html"&gt;第三講&lt;/a&gt;其實都是為了這一講而存在。&lt;/p&gt;
&lt;p&gt;學習一段時間深度學習的你是不是有一個疑惑：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Activation Function為什麼要用Sigmoid和Softmax？&lt;/li&gt;
&lt;li&gt;Loss Function為什麼要用MSE和Cross Entropy？&lt;/li&gt;
&lt;li&gt;其他狀況要用什麼？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;當然你可以把它們看作是個合理定義，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;但是學習深度就端看你是不是可以用最少的定義表示最多的東西&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果你仔細google一下就會發現有一個相關名詞—廣義線性定理，但是大部分的文章和教材都沒辦法將它講的很清楚，原因是因為沒有先介紹「充分統計量」的概念。&lt;/p&gt;
&lt;p&gt;在本講你會學到如何用「充分統計量」來說明在廣義線性定理中的Canonical Link Function，進而推導出Activation Function，你會學到如何藉由MLE和MAP來推導出Loss Function，學完以後你會對Activation Function和Loss Function有更深的認識。&lt;/p&gt;
&lt;p&gt;這一篇我可以非常自豪的說，網路上的資料在這個議題上找不到寫的比我更詳細的，這是我看過很多書和教材融會貫通而成的，請大家一定要看到最後，必定收穫滿滿。&lt;/p&gt;
&lt;h3 id="_1"&gt;前情提要&lt;/h3&gt;
&lt;p&gt;&lt;a href="/deep-dl_3.html"&gt;上一講&lt;/a&gt;中我們清楚的了解頻率學派和貝氏學派各自的觀點，並且從兩者觀點出發去探討機器學習問題。&lt;/p&gt;
&lt;p&gt;頻率學派使用Maximum Likelihood Estimation (MLE) 來優化，優化關係式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)  \ \ ↪︎【1】
$$&lt;/div&gt;
&lt;p&gt;
此式等價於最小化Data與Model之間的Cross Entropy，或等價於最小化Data與Model之間的KL Divergence，與&lt;a href="/deep-dl_2.html"&gt;第二講的資訊理論&lt;/a&gt;完美契合。&lt;/p&gt;
&lt;p&gt;貝氏學派則使用Maximum A Posterior (MAP) 來優化，優化關係式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)+\operatorname{ln}p(\theta\mid m)  \ \ ↪︎【2】
$$&lt;/div&gt;
&lt;p&gt;
除了第一項與MLE一樣之外，我們還需要考慮第二項，此項考慮了參數的出現分布，當參數分布是均等時，MAP和MLE是等價的。但是我們希望 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 可以接近0，所以一般會去假設 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 為一個Variance有限且平均值為0的分布，如果選擇使用Normal Distribution，則會得到L2 Regularization Term；如果選擇用Laplace Distribution，則會得到L1 Regularization Term。&lt;/p&gt;
&lt;p&gt;所以接下來要讓機器可以學習只剩下最後一哩路，如何將【1】或【2】變換成擬合問題呢？&lt;/p&gt;
&lt;p&gt;其實只需要找到合適的分布代入 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 就可以了。&lt;/p&gt;
&lt;h3 id="sufficient-statistic"&gt;充分統計量 (sufficient statistic)&lt;/h3&gt;
&lt;p&gt;在這之前要引入一個重要的統計工具，那就是「充分統計量」。&lt;/p&gt;
&lt;p&gt;什麼「充分統計量」呢？Wiki的定義是&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在統計學中，關於一個統計模型和其相關的未知參數的充分統計量是指「沒有任何其他可以從同一樣本中計算得出的統計量可以提供任何有關未知參數的額外信息」。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其數學表示式為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_\theta (y_1,y_2,..,y_n)=h(y_1,y_2,..,y_n)g_\theta(T(y_1,y_2,..,y_n))  \ \ ↪︎【3】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(f_\theta(.)\)&lt;/span&gt;為你的Model假定的分布，當中包含決定Model的參數&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;；&lt;span class="math"&gt;\(y_1,y_2,..,y_n\)&lt;/span&gt; 為多筆資料；&lt;span class="math"&gt;\(T(.)\)&lt;/span&gt; 稱為充分統計量，可以是一個單值或矩陣，它可以讓唯一包含 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 資訊的 &lt;span class="math"&gt;\(g_\theta\)&lt;/span&gt; 不在直接depend on &lt;span class="math"&gt;\(y_i\)&lt;/span&gt;，而是depend on &lt;span class="math"&gt;\(T(.)\)&lt;/span&gt; ，也因此做到了「沒有任何其他可以從同一樣本中計算得出的統計量可以提供任何有關未知參數的額外信息」，因為唯一包含未知參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 的 &lt;span class="math"&gt;\(g_\theta(.)\)&lt;/span&gt; 只需要 &lt;span class="math"&gt;\(T(.)\)&lt;/span&gt; 當作Input，其餘的統計量皆不需要，此時我們會稱 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 為充分統計量。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;還是以我們相當熟悉的Normal Distribution當作例子 （如果不熟悉，&lt;a href="/deep-dl_1.html"&gt;請詳見第一講&lt;/a&gt;） ：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{\theta}(y_1,y_2,..,y_n)=(\frac{1}{\sqrt{2\pi}\sigma_\theta})^n exp\{{\sum_i-\frac{1}{2\sigma_\theta^2}(y_i-\mu_\theta)^2}\}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=(\frac{1}{\sqrt{2\pi}\sigma_\theta})^nexp\{{\sum_i-\frac{1}{2\sigma_\theta^2}(y_i^2-2\mu_\theta y_i+\mu_\theta^2)}\}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=(\frac{1}{\sqrt{2\pi}\sigma_\theta})^nexp\{{-\frac{1}{2\sigma_\theta^2}(\sum_iy_i^2-2\mu_\theta \sum_iy_i+n\mu_\theta^2)}\}  \ \ ↪︎【4】
$$&lt;/div&gt;
&lt;p&gt;當我定義兩個充分統計量 &lt;span class="math"&gt;\(S_1=\sum_iy_i\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(S_2=\sum_iy_i^2\)&lt;/span&gt; ，所以充分統計量為
&lt;/p&gt;
&lt;div class="math"&gt;$$
T=\begin{bmatrix} S_1 \\ S_2 \end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;
代入得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{\theta}(y_1,y_2,..,y_n)=(\frac{1}{\sqrt{2\pi}\sigma_\theta})^n exp\{{-\frac{1}{2\sigma_\theta^2}(\begin{bmatrix} -2\mu_\theta \ 1 \end{bmatrix}\begin{bmatrix} S_1 \\ S_2 \end{bmatrix}+n\mu_\theta^2)}\}  \ \ ↪︎【5】
$$&lt;/div&gt;
&lt;p&gt;
此時整個分布都不需要depend on &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; ，只depend on 充分統計量 &lt;span class="math"&gt;\(T(.)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;將【5】式取 &lt;span class="math"&gt;\(\operatorname{ln}\)&lt;/span&gt; ，就得到它的log probability：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\operatorname{ln}p_{\theta}(y_1,y_2,..,y_n)=n\operatorname{ln}\frac{1}{\sqrt{2\pi}\sigma_\theta}-\frac{1}{2\sigma_\theta^2}(S_2-2\mu_\theta S_1+n\mu_\theta^2)  \ \ ↪︎【6】
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;這樣分離有什麼好處？好處是當我們需要Maximum &lt;span class="math"&gt;\(\operatorname{ln}p_{\theta}(y_1,y_2,..,y_n)\)&lt;/span&gt; 時事情會變得容易。&lt;/p&gt;
&lt;p&gt;假設我想利用數據 &lt;span class="math"&gt;\(y_1,y_2,..,y_n\)&lt;/span&gt; 找一組參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 使 &lt;span class="math"&gt;\(\operatorname{ln}p_{\theta}(y_1,y_2,..,y_n)\)&lt;/span&gt; 最大：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta^*=argmax_\theta\ \operatorname{ln}f_{\theta}(y_1,y_2,..,y_n)=argmax_\theta\ \operatorname{ln}h(y_1,y_2,..,y_n)+\operatorname{ln}g_\theta(\{T(y_1,y_2,..,y_n)\})
$$&lt;/div&gt;
&lt;p&gt;
其優化式為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial}{\partial \theta}\operatorname{ln}f_{\theta}(y_1,y_2,..,y_n)|_{\theta^*}=\frac{\partial}{\partial \theta}\operatorname{ln}g_\theta(\{T(y_1,y_2,..,y_n)\})|_{\theta^*}
$$&lt;/div&gt;
&lt;p&gt;
所以：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial}{\partial \theta}\operatorname{ln}g_\theta(\{T(y_1,y_2,..,y_n)\})|_{\theta^*}=0  \ \ ↪︎【7】
$$&lt;/div&gt;
&lt;p&gt;
這個關係式就足以讓我們找到最佳的 &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; ，而其只與充分統計量 &lt;span class="math"&gt;\(\{T(...)\}\)&lt;/span&gt; 有關，也就是說：&lt;strong&gt;當我從數據當中統計出充分統計量 &lt;span class="math"&gt;\(\{T(...)\}\)&lt;/span&gt; 就足以讓我找到最佳的Model參數&lt;/strong&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;繼續剛剛的例子，將【6】代入【7】就可以得到Model的最佳參數：
&lt;/p&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial}{\partial \mu_{\theta}}[n\operatorname{ln}\frac{1}{\sqrt{2\pi}\sigma_\theta}-\frac{1}{2\sigma_\theta^2}(S_2-2\mu_\theta S_1+n\mu_\theta^2)]=\frac{1}{2\sigma_\theta^2}(-2S_1+2n\mu_{\theta})
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\Rightarrow \mu_{\theta}=\frac{1}{n}S_1=\frac{1}{n}\sum_iy_i  \ \ ↪︎【8】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial}{\partial \sigma_{\theta}}[n\operatorname{ln}\frac{1}{\sqrt{2\pi}\sigma_\theta}-\frac{1}{2\sigma_\theta^2}(S_2-2\mu_\theta S_1+n\mu_\theta^2)]=-\frac{n}{\sigma_\theta}+\frac{1}{\sigma_\theta^3}(S_2-\frac{1}{n}S_1^2)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\Rightarrow \sigma_\theta^2=\frac{S_2}{n}-(\frac{S_1}{n})^2=(\frac{1}{n}\sum_iy_i^2)-(\frac{1}{n}\sum_iy_i)^2  \ \ ↪︎【9】
$$&lt;/div&gt;
&lt;p&gt;是不是跟我們之前學的東西是自恰的啊！&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;現在你知道我不得不提「充分統計量」這個概念的原因了吧！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有了「充分統計量」的概念，拿到一個分布你可以清楚的知道：我需要哪些必要的統計量才可以擬合這個分布，並且可以透過Maximun Log Probability輕易的找到這些充分統計量對應的Model變數。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="generalized-linear-models-glm"&gt;廣義線性模型（Generalized Linear Models, GLM）&lt;/h3&gt;
&lt;p&gt;如果你看其他的介紹文章，通常會先講古典線性模型，再講廣義線性模型，我這邊會反過來講，因為古典線性模型只是廣義線性模型的特例 — 當假設Normal Distribution時，所以只要真正搞懂廣義線性模型，古典線性模型也就懂了。&lt;/p&gt;
&lt;p&gt;我們手上現在會有兩個東西：分布模型和線性擬合模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分布模型：就是 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt;，搭配優化準則MLE和MAP就可以找最佳參數&lt;/li&gt;
&lt;li&gt;擬合模型：寫作為 &lt;span class="math"&gt;\(h(x_i,m,\theta)=\theta^0+\sum_k \theta_i^kx_i^k  \ \ ↪︎【10】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如何將這兩者連繫起來呢？我們透過Mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; 和 Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; 來做到：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu=E_{y\sim p(y_i\mid x_i,m,\theta)}[y]  \ \ ↪︎【11】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
g(\mu)=h(x_i,m,\theta)  \ \ ↪︎【12】
$$&lt;/div&gt;
&lt;p&gt;其中：擬合模型 &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 負責擬合Mean經Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; 轉換後的值。這個Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; 其實限制很少，只需要符合兩點即可：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Link Function必須是單調遞增（monotonic）&lt;/li&gt;
&lt;li&gt;Link Function的值域必須能夠覆蓋理論分布的空間&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;所以接下來作法就容易了，只要依以下步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;決定好分布模型 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 且決定好Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;算出模型的Mean並透過Link Function來連接Mean和擬合模型： &lt;span class="math"&gt;\(\mu=E_{y\sim p(y_i\mid x_i,m,\theta)}[y]=g^{-1}(h(x_i,m,\theta))\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;用上面的關係式將 &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 代換到 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 裡&lt;/li&gt;
&lt;li&gt;利用MLE和MAP來找尋最佳參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="_2"&gt;古典線性模型&lt;/h3&gt;
&lt;p&gt;接下來我們就來演示一下廣義線性模型的特例—古典線性模型，&lt;strong&gt;古典線性模型使用Normal Distribution當分布模型 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt;，並且使用Identity Function當作 Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; &lt;/strong&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用Normal Distribution和Identity Link Function
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(y_i-\mu)^2}\}  \ \ ↪︎【13】
   $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
   g(\mu)=\mu  \ \ ↪︎【14】
   $$&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;算出模型的Mean並透過Link Function來連接Mean和線性擬合模型
   &lt;div class="math"&gt;$$
   \mu=g^{-1}(h(x_i,m,\theta))=h(x_i,m,\theta)  \ \ ↪︎【15】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用上面的關係式將 &lt;span class="math"&gt;\(h(x_i,\theta)\)&lt;/span&gt; 代換到 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 裡
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(y_i-h(x_i,m,\theta))^2}\}  \ \ ↪︎【16】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用MLE和MAP來找尋最佳參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;
   將【16】式代入【1】式，經化簡可得：
   &lt;div class="math"&gt;$$
   \theta_{MLE}=argmax_\theta\ \sum_{i}\ -\frac{1}{2\sigma^2}(y_i-h(x_i,m,\theta))^2  \ \ ↪︎【17】
   $$&lt;/div&gt;
   因為擬合模型只對 &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; 感興趣，所以這裡令 &lt;span class="math"&gt;\(\sigma^2=1\)&lt;/span&gt; ，得：
   &lt;div class="math"&gt;$$
   \theta_{MLE}=argmin_\theta\ \sum_{i}\ \frac{1}{2}(y_i-h(x_i,m,\theta))^2  \ \ ↪︎【18】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上面的式子就是Loss Function為Mean Squared Error (MSE) 的Regression，它背後假設的分布就是Normal Distribution。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這裡的 &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 可以是線性的，當然如果將feature space先作非線性轉換得 &lt;span class="math"&gt;\(z_i\)&lt;/span&gt; 再代入得 &lt;span class="math"&gt;\(h(z_i,m,\theta)\)&lt;/span&gt; 也可以是非線性的，當然 &lt;span class="math"&gt;\(h(z_i,m,\theta)\)&lt;/span&gt; 可以是Neural Network，先用前面幾層Hidden Layers做非線性轉換，在拿轉換後的結果與數據做線性擬合。其中：&lt;span class="math"&gt;\(m\)&lt;/span&gt; 代表的是Network的Hyperparameters，包括：Network的結構、Learning Rate的大小、Batch Size、...等等；而 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 則是Network中需要學習的權重。&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;接下來我要問一個問題：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;其他分布能使用古典線性模型嗎？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假設今天是Binary Classification的問題，則其分布不再是Normal Distribution，而是Bernoulli Distribution：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(y_i\mid x_i,m,\theta)=\pi^{y_i}(1-\pi)^{1-y_i}  \ \ ↪︎【19】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(y_i \in \{0,1\}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\pi\)&lt;/span&gt; 代表 &lt;span class="math"&gt;\(y_i=1\)&lt;/span&gt; 的機率。&lt;/p&gt;
&lt;p&gt;一樣假設Identity Function當作 Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; 
&lt;/p&gt;
&lt;div class="math"&gt;$$
h(x_i,m,\theta)=E_{y\sim p_{model}}[y_i]=\sum_{y_i=0}^{y_i=1} y_i \pi^{y_i}(1-\pi)^{1-y_i}=\pi
$$&lt;/div&gt;
&lt;p&gt;
這樣對嗎？其實是錯誤的。Link Function沒有對應到正確的值域，所以 &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 可以是整個實數空間，而 &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; 卻只能是落在0到1之間，等式不能成立。&lt;/p&gt;
&lt;p&gt;所以古典線性模型並不能適用於非Normal Distribution，我們必須找其他合適的Link Function。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;事實上，認真找可以找到若干個正確符合的 Link Function，但是只有唯一一種Link Function符合讓Mean符合「充分統計」，這就是 Canonical Link Function。而其他符合的 Link Function 則稱為 Non-Canonical Link Function。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="exponential-family-canonical-link-function"&gt;指數族分布（Exponential Family）與 Canonical Link Function&lt;/h3&gt;
&lt;p&gt;要談Canonical Link Function就必須要談「指數族分布」，指數族分布很好的囊括了常見的分布，包括：Normal Distribution、Bernoulli Distribution、Poisson Distribution、...等等，而且還具備了許多良好的性質。&lt;/p&gt;
&lt;p&gt;指數族分布定義如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(y\mid \eta)=h(y)exp\{\eta^T T(y)-A(\eta)\}  \ \ ↪︎【20】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta\)&lt;/span&gt; : Natural Parameters or Linear Predictor&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A(\eta)\)&lt;/span&gt;: Log Partition Function or Log Normalizer&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y)\)&lt;/span&gt;: sufficient statistics (充分統計量)，通常的分布是 &lt;span class="math"&gt;\(T(y)=y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h(y)\)&lt;/span&gt;: base measure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因為機率加總為1，所以 &lt;span class="math"&gt;\(A(\eta)\)&lt;/span&gt; 的型式是受其他變數的影響：
&lt;/p&gt;
&lt;div class="math"&gt;$$
A(\eta)=ln[\int h(y)exp\{\eta^T T(y)\}dy]  \ \ ↪︎【21】
$$&lt;/div&gt;
&lt;p&gt;
然後指數族分布有一些重要的數學關係式（證明詳見&lt;a href="https://www.cs.princeton.edu/~bee/courses/scribe/lec_09_02_2013.pdf"&gt;此篇&lt;/a&gt;）
&lt;/p&gt;
&lt;div class="math"&gt;$$
E[T(y)|\eta]=\frac{\partial A(\eta)}{\partial \eta}  \ \ ↪︎【22】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
Var[T(y)|\eta]=\frac{\partial^2 A(\eta)}{\partial \eta\partial \eta}  \ \ ↪︎【23】
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;列下考慮多筆Data的情況：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(y_1,y_2,..,y_n|\eta)=[\prod_i h(y_i)]exp\{\eta^T\sum_i T(y_i)-nA(\eta)\}  \ \ ↪︎【24】
$$&lt;/div&gt;
&lt;p&gt;
仔細觀察【24】式和【3】式：指數族分布的 &lt;span class="math"&gt;\(T(y)\)&lt;/span&gt; 符合「充分統計」，你會發現 &lt;span class="math"&gt;\(exp\{.\}\)&lt;/span&gt; 這裡對映到的是 &lt;span class="math"&gt;\(g_\theta\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;所以我們可以依循著剛剛的套路找到最佳參數與統計量的對映，使用【7】式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial}{\partial \eta}[\eta^T \sum_iT(y_i)-nA(\eta)]=\sum_i T(y_i)-n\frac{\partial A(\eta)}{\partial \eta}=\sum_i T(y_i)-n\cdot E[T(y)|\eta]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\Rightarrow E[T(y)|\eta^*]=\frac{1}{n} \sum_{i=1}^{n} T(y_i)  \ \ ↪︎【24】
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;因此指數族分布隱含著一個相當好的特性：在最好的參數 &lt;span class="math"&gt;\(\eta^*\)&lt;/span&gt; 之下，分布對 &lt;span class="math"&gt;\(T(y)\)&lt;/span&gt; 的期望值就等同於你量測 &lt;span class="math"&gt;\(T(y_i)\)&lt;/span&gt; 的平均值，而且 &lt;span class="math"&gt;\(T(y_i)\)&lt;/span&gt; 還是一個充分統計量，也就是說你已經不需要其他統計量了。&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;當 &lt;span class="math"&gt;\(T(y)=y\)&lt;/span&gt; (大部分情形都是)，可得與Mean的關係式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu=\frac{1}{n} \sum_{i=1}^{n} y_i=E[y|\eta^*]=\frac{\partial A(\eta)}{\partial \eta}|_{\eta^*}  \ \ ↪︎【25】
$$&lt;/div&gt;
&lt;p&gt;
因此我們建立了 Mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\eta\)&lt;/span&gt; 的關係，假設：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial A(\eta)}{\partial \eta}=g^{-1}(\eta)=\mu  \ \ ↪︎【26】
$$&lt;/div&gt;
&lt;p&gt;
&lt;strong&gt;其中： &lt;span class="math"&gt;\(g^{-1}(.)\)&lt;/span&gt; 就是大名鼎鼎的 Activation Function 。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;此時，讓 &lt;span class="math"&gt;\(\eta=h(x_i,m,\theta)\)&lt;/span&gt;，則Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; 稱為Canonical Link Function：
&lt;/p&gt;
&lt;div class="math"&gt;$$
g(.)=(\frac{\partial A(\eta)}{\partial \eta})^{-1}  \ \ ↪︎【27】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
g(\mu)=h(x_i,m,\theta)  \ \ ↪︎【28】
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;在符合指數族分布的情況下，採用 &lt;span class="math"&gt;\(\eta\)&lt;/span&gt; 和 Canonical Link Function，會讓Mean變成為充分統計量，這意味著我們很有效率的使用著數據。&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;現在我們可以回頭加深觀念：&lt;strong&gt;為什麼在古典模型當中會選擇 Identity Link Function 呢？因為對於Normal Distribution而言，Identity Link Function 是 Canonical Link Function。&lt;/strong&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{normal}(y)=\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(y-\mu)^2}\}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\frac{1}{\sqrt{2\pi}}exp\{\frac{\mu}{\sigma^2}y-\frac{1}{2\sigma^2}y^2-(ln\sigma+\frac{1}{2\sigma^2}\mu^2)\}  \ \ ↪︎【29】
$$&lt;/div&gt;
&lt;p&gt;對應【20】式得：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;div class="math"&gt;$$
  h(y)=\frac{1}{\sqrt{2\pi}}  \ \ ↪︎【30】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class="math"&gt;$$
  \eta=\begin{bmatrix} \mu/\sigma^2 \\ -1/2\sigma^2 \end{bmatrix}  \ \ ↪︎【31】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class="math"&gt;$$
  T(y)=\begin{bmatrix} y \\ y^2 \end{bmatrix}  \ \ ↪︎【32】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class="math"&gt;$$
  A(\eta)=\operatorname{ln}\sigma+\frac{1}{2\sigma^2}\mu^2=-\frac{1}{2}\operatorname{ln}(-2\eta_2) -\frac{\eta_1^2}{4\eta_2}  \ \ ↪︎【33】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再代入【20】式找 Link Function：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial A(\eta)}{\partial \eta}=\begin{bmatrix} -\eta_1/2\eta_2 \\ -1/2\eta_2+\eta_1^2/4\eta_2^2 \end{bmatrix}=\begin{bmatrix} \mu \\ \mu^2+\sigma^2 \end{bmatrix}  \ \ ↪︎【34】
$$&lt;/div&gt;
&lt;p&gt;
有兩項充分統計量，所以要擬合一個Normal Distribution需要兩個統計量（其實我們剛才我們已經知道）。但是對於線性模型我們只需要 Mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; ，所以我們只考慮第一項：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial A(\eta)}{\partial \eta}|_0=g^{-1}(\eta)=\mu  \ \ ↪︎【35】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\mu=g^{-1}(\eta) \Rightarrow g(\mu)=\mu  \ \ ↪︎【36】
$$&lt;/div&gt;
&lt;p&gt;得證，確實古典模型在使用Normal Distribution時選擇的 Identity Link Function 是Canonical Link Function。&lt;/p&gt;
&lt;h3 id="binary-classificationglmsigmoidcross-entropy"&gt;Binary Classification：從GLM推出Sigmoid和(狹義的)Cross Entropy&lt;/h3&gt;
&lt;p&gt;再重新來看Bernoulli Distribution。&lt;/p&gt;
&lt;p&gt;再寫一次【19】式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(y_i\mid x_i,m,\theta)=\pi^{y_i}(1-\pi)^{1-y_i}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=exp\{\operatorname{ln}(\frac{\pi}{1-\pi})y+\operatorname{ln}(1-\pi)\}
$$&lt;/div&gt;
&lt;p&gt;對應【20】式得：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h(y)=1  \ \ ↪︎【37】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta=\operatorname{ln}(\frac{\pi}{1-\pi}) \ \ ↪︎【38】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y)=y  \ \ ↪︎【39】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A(\eta)=-\operatorname{ln}(1-\pi)=ln(1+e^\eta)  \ \ ↪︎【40】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再代入【20】式找 Canonical Link Function：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial A(\eta)}{\partial \eta}=\frac{1}{1+e^{-\eta}}=g^{-1}(\eta)=\mu \Rightarrow g(\mu)=\operatorname{ln}(\frac{\mu}{1-\mu})  \ \ ↪︎【41】
$$&lt;/div&gt;
&lt;p&gt;
其中： &lt;span class="math"&gt;\(\frac{1}{1+e^{-\eta}}\)&lt;/span&gt; 就是Sigmoid Function，計作 &lt;span class="math"&gt;\(\sigma(.)\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
\Rightarrow \mu=\sigma(h(x_i,m,\theta))  \ \ ↪︎【42】
$$&lt;/div&gt;
&lt;p&gt;
上式就是我們擬合的關係式， &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 可以是線性方程式，也可以是Neural Network，然後有注意到嗎？&lt;strong&gt;Sigmoid 剛剛好是 &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 輸出後的最後一層，我們使用廣義線性定理就自然而然的得到Activation Function，這就是為什麼在Binary Classification問題中NN最後一層是Sigmoid的原因 &lt;/strong&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;接下來就按步驟求出最佳參數：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用Bernoulli Distribution和相應的 Canonical Link Function
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=\pi^{y_i}(1-\pi)^{1-y_i}  \ \ ↪︎【43】
   $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
   g(\mu)=\operatorname{ln}(\frac{\mu}{1-\mu})  \ \ ↪︎【44】
   $$&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;算出模型的Mean並透過Link Function來連接Mean和線性擬合模型
   &lt;div class="math"&gt;$$
   \pi=g^{-1}(h(x_i,m,\theta))=\sigma (h(x_i,m,\theta) ) \ \ ↪︎【45】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用上面的關係式將 &lt;span class="math"&gt;\(h(x_i,\theta)\)&lt;/span&gt; 代換到 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 裡
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=(\sigma (h(x_i,m,\theta) ))^{y_i}(1-\sigma (h(x_i,m,\theta) ))^{1-y_i}  \ \ ↪︎【46】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用MLE和MAP來找尋最佳參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;
   將【46】式代入【1】式，經化簡可得：
   &lt;div class="math"&gt;$$
   \theta_{MLE}=argmin_\theta\ \sum_{i}\ -y_i \operatorname{ln}(p_i)-(1-y_i)\operatorname{ln}(1-p_i) \ \ ↪︎【47】
   $$&lt;/div&gt;
   其中： &lt;span class="math"&gt;\(p_i=\sigma (h(x_i,m,\theta))\)&lt;/span&gt;。沒錯！我們推出了(狹義的)Cross Entropy。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="multi-class-classificationglmsoftmaxcross-entropy"&gt;Multi-class Classification：從GLM推出Softmax和(狹義的)Cross Entropy&lt;/h3&gt;
&lt;p&gt;Categorical Distribution的分布：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(y_i\mid x_i,m,\theta)=\prod_{j=1}^{k-1}\phi_j^{\delta(y_i=j)}\cdot \phi_k^{1-\sum_{j=1}^{k-1}\delta(y_i=j)}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=exp\{\sum_{j=1}^{k-1}\delta(y_i=j)\operatorname{ln}\phi_j+(1-\sum_{j=1}^{k-1}\delta(y_i=j))\operatorname{ln}\phi_k \}
$$&lt;/div&gt;
&lt;p&gt;將第二個 &lt;span class="math"&gt;\(\sum\)&lt;/span&gt; 猜開放到前一個&lt;span class="math"&gt;\(\sum\)&lt;/span&gt; 裡：
&lt;/p&gt;
&lt;div class="math"&gt;$$
=exp\{\sum_{j=1}^{k-1}\delta(y_i=j)\operatorname{ln}\frac{\phi_j}{\phi_k}+\operatorname{ln}\phi_k \} \ \ ↪︎【48】
$$&lt;/div&gt;
&lt;p&gt;
對應【20】式得：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h(y)=1  \ \ ↪︎【49】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta_j=\operatorname{ln}\frac{\phi_j}{\phi_k};\ \ \ (j=1,...,k-1) \ \ ↪︎【50】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T_j(y)=\delta(y=j)  \ \ ↪︎【51】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A(\eta_1,...,\eta_{k-1})=-\operatorname{ln}\phi_k=\operatorname{ln}[\sum_{j=1}^{k}e^{\eta_j}]  \ \ ↪︎【52】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;[堆導] &lt;span class="math"&gt;\(A(\eta_j)\)&lt;/span&gt; 的計算過程，從【50】式出發：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\eta_j=\operatorname{ln}(\frac{\phi_j}{\phi_k}) \Rightarrow \phi_ke^{\eta_j}=\phi_j  \ \ ↪︎【53】
$$&lt;/div&gt;
&lt;p&gt;胡亂假設 &lt;span class="math"&gt;\(\phi_k\)&lt;/span&gt; 存在，接下來加總所有的 &lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt; 應該為 1：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\Rightarrow \sum_{j=1}^{k}\phi_ke^{\eta_j}=\sum_{j=1}^{k}\phi_j=1  \ \ ↪︎【54】
$$&lt;/div&gt;
&lt;p&gt;
所以：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi_k=\frac{1}{\sum_{j=1}^{k}e^{\eta_j}}  \ \ ↪︎【55】
$$&lt;/div&gt;
&lt;p&gt;回代【53】式，得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi_j=\frac{e^{\eta_j}}{\sum_{j=1}^{k}e^{\eta_j}}  \ \ ↪︎【56】
$$&lt;/div&gt;
&lt;p&gt;剛剛我雖然胡亂假設有&lt;span class="math"&gt;\(\phi_k\)&lt;/span&gt;的存在，不過做完的結果並不違和，&lt;span class="math"&gt;\(e^{\eta_j}\)&lt;/span&gt; 作為各項的機率，並且除上所有機率的相加 &lt;span class="math"&gt;\(\sum_{j=1}^{k}e^{\eta_j}\)&lt;/span&gt;，可以確保所有機率總和為 &lt;span class="math"&gt;\(\sum_{j=1}^{k}\phi_j=1\)&lt;/span&gt; 。所以：
&lt;/p&gt;
&lt;div class="math"&gt;$$
A(\eta_1,...,\eta_{k-1})=-\operatorname{ln}\phi_k=\operatorname{ln}[\sum_{j=1}^{k}e^{\eta_j}]  \ \ ↪︎【57】
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;將【52】式代入【20】式找 Canonical Link Function：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial A(\eta_1,...,\eta_{k-1})}{\partial \eta_j}=\frac{e^{\eta_j}}{\sum_{j=1}^{k-1}e^{\eta_j}}=g^{-1}(\eta_j)  \ \ ↪︎【58】
$$&lt;/div&gt;
&lt;p&gt;
其中： &lt;span class="math"&gt;\(\frac{e^{\eta_j}}{\sum_{j=1}^{k-1}e^{\eta_j}}\)&lt;/span&gt; 就是Softmax Function，計作 &lt;span class="math"&gt;\(softmax\{.\}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu_j=softmax(h_j(x_i,m,\theta))  \ \ ↪︎【59】
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;接下來就按步驟求出最佳參數：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用Categorical Distribution和相應的 Canonical Link Function
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=\prod_{j=1}^{k}\phi_j^{\delta(y_i=j)}  \ \ ↪︎【60】
   $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
   g^{-1}(\eta_j)=softmax\{\eta_j\}  \ \ ↪︎【61】
   $$&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;算出模型的Mean並透過Link Function來連接Mean和線性擬合模型
   &lt;div class="math"&gt;$$
   \phi_j=g^{-1}(h(x_i,m,\theta))=softmax\{h_j(x_i,m,\theta)\} \ \ ↪︎【62】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用上面的關係式將 &lt;span class="math"&gt;\(h(x_i,\theta)\)&lt;/span&gt; 代換到 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 裡
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=\prod_{j=1}^{k}softmax\{h_j(x_i,m,\theta)\}^{\delta(y_i=j)}  \ \ ↪︎【63】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用MLE和MAP來找尋最佳參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;
   將【63】式代入【1】式，經化簡可得：
   &lt;div class="math"&gt;$$
   \theta_{MLE}=argmin_\theta\ \sum_{i}\sum_{j} -\delta(y_i=j)\operatorname{ln}p_{i,j} \ \ ↪︎【64】
   $$&lt;/div&gt;
   其中： &lt;span class="math"&gt;\(p_{i,j}=softmax\{h_j(x_i,m,\theta)\}\)&lt;/span&gt;。我們也推出了Multi-class Cross Entropy。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="_3"&gt;結論&lt;/h3&gt;
&lt;p&gt;恭喜大家堅持到這裡，應該會收穫不少。以後別人問你為什麼使用Mean Square Error？為什麼這裡要加Sigmoid？為什麼這裡要用 (狹義的) Cross Entropy？為什麼這裡卻要用Softmax？你都可以輕易的回答，甚至給你另外一個Distribution，例如：Possion Distribution，你也可以推出它的Canonical Link Function，也可以知道應該要用什麼樣的Loss去優化，你已經融會貫通了！&lt;/p&gt;
&lt;p&gt;再複習一下！&lt;/p&gt;
&lt;p&gt;為了將上章節提到的MLE和MAP化作擬合問題實際用數據去訓練Model，我們需要廣義線性定理，廣義線性定理必須藉由 Link Function 來連接擬合模型和分布模型，這樣就可以藉由MLE和MAP來優化擬合模型內的參數， Link Function 的限制只有兩條：單調遞增和值域覆蓋。&lt;/p&gt;
&lt;p&gt;但是任意取的話，其平均值不一定是模型的「充分統計量」，而當 Link Function 為 Canonical Link Function時，平均值正是「充分統計量」，因此意味著我們可以很有效率的使用著數據，而Canonical Link Function的反函數正是大名鼎鼎的Activation Function，所以在廣義線性模型的推導中自然會得到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression問題時，Normal Distribution使用Linear當Activation Function&lt;/li&gt;
&lt;li&gt;Binary Classification問題時，Bernoulli Distribution使用Sigmoid當Activation Function&lt;/li&gt;
&lt;li&gt;Multi-class Classification問題時，Categorical Distribution使用Softmax當Activation Function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;當定義完成含有擬合參數的分布模型後，我們就可以用MLE或MAP來找到擬合的優化方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression問題時，使用Mean Square Error&lt;/li&gt;
&lt;li&gt;Binary Classification問題時，使用（狹義的）Cross Entropy&lt;/li&gt;
&lt;li&gt;Multi-class Classification問題時，使用 Multi-class Cross Entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有這些以前不加解釋的東西，都可以由廣義線性定理推導出來。&lt;/p&gt;
&lt;h3 id="reference"&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.deeplearningbook.org"&gt;Ian Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Christopher Bishop. Pattern Recognition and Machine Learning. 2006.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/ml-notes-why-the-least-square-error-bf27fdd9a721"&gt;ML notes: why the Least Square Error?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.princeton.edu/~bee/courses/scribe/lec_09_02_2013.pdf"&gt;Introduction: exponential family, conjugacy, and sufficiency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.princeton.edu/~bee/courses/scribe/lec_09_02_2013.pdf"&gt;Generalized Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/generalized-linear-models-9cbf848bb8ab"&gt;towardsdatascience: Generalized linear models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Sufficient_statistic"&gt;Wiki: sufficient statistic&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_model"&gt;Wiki: Generalized linear model&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Exponential_family#Properties"&gt;Wiki: Exponential family&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;http://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf&lt;/li&gt;
&lt;li&gt;http://zhouyichu.com/machine-learning/Generalized-Linear-Models/&lt;/li&gt;
&lt;li&gt;http://www.airc.org.tw/newsfiles/r.pdf&lt;/li&gt;
&lt;li&gt;https://www.flutterbys.com.au/stats/tut/tut10.4.html&lt;/li&gt;
&lt;li&gt;https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function&lt;/li&gt;
&lt;li&gt;https://stats.stackexchange.com/questions/288451/why-is-mean-squared-error-the-cross-entropy-between-the-empirical-distribution-a&lt;/li&gt;
&lt;li&gt;https://ithelp.ithome.com.tw/articles/10200862&lt;/li&gt;
&lt;li&gt;&lt;a href="http://benz.nchu.edu.tw/~kucst/數統CH7-CH9.pdf"&gt;Chapter 7 &amp;amp; 8 Sufficient Statistics &amp;amp; More about Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;https://beginningwithml.wordpress.com/2018/06/22/3-4-softmax-regression/&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;[此文章為原創文章，轉載前請註明文章來源]&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;20200603: 修正從【4】到【9】式的公式錯誤（感謝 俊嘉 細心的揪出公式的錯誤）&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="剖析深度學習"></category></entry><entry><title>剖析深度學習 (3)：MLE、MAP差在哪？談機器學習裡的兩大統計觀點</title><link href="https://ycc.idv.tw/deep-dl_3.html" rel="alternate"></link><published>2020-03-07T12:00:00+08:00</published><updated>2020-03-07T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-03-07:/deep-dl_3.html</id><summary type="html">&lt;p&gt;本講主要探討統計的兩大學派（頻率學派和貝氏學派）對於機器如何學習的觀點。頻率學派主張Maximum Likelihood Estimation (MLE)，會提到這等同於最小化data與model之間的Cross Entropy或KL Divergence。而貝氏學派則主張Maximum A Posterior (MAP) ，會提到這會等同於極大化Likelihood並同時考慮Regularization Term，我們也可以在本講看到L1和L2 Regularation Term是怎麼被導出的。&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;深度學習發展至今已經有相當多好用的套件，使得進入的門檻大大的降低，因此如果想要快速的實作一些深度學習或機器學習，通常是幾行程式碼可以解決的事。但是，如果想要將深度學習或機器學習當作一份工作，深入了解它背後的原理和數學是必要的，才有可能因地制宜的靈活運用，YC準備在這一系列當中帶大家深入剖析深度學習。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本講主要探討統計的兩大學派（頻率學派和貝氏學派）對於機器如何學習的觀點。頻率學派主張Maximum Likelihood Estimation (MLE)，會提到這等同於最小化data與model之間的Cross Entropy或KL Divergence。而貝氏學派則主張Maximum A Posterior (MAP) ，會提到這會等同於極大化Likelihood並同時考慮Regularization Term，我們也可以在本講看到L1和L2 Regularation Term是怎麼被導出的。&lt;/p&gt;
&lt;h3 id="_1"&gt;條件機率&lt;/h3&gt;
&lt;p&gt;因為本講會牽涉到許多條件機率的計算，所以把條件機率常用的公式先列下來，讓大家溫習一下。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;邊際機率（marginal probability）
    &lt;div class="math"&gt;$$
    p(X=x_i)=\sum_j p(X=x_i, Y=y_j)  \ \ ↪︎【1】
    $$&lt;/div&gt;
(如下圖所示)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;條件機率（conditional probability）
  &lt;div class="math"&gt;$$
  p(X=x_i, Y=y_j)=p(Y=y_j\mid X=x_i)p(X=x_i)  \ \ ↪︎【2】
  $$&lt;/div&gt;
(如下圖所示)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;條件機率的鏈鎖法則
  &lt;div class="math"&gt;$$
  p(a,b,c)=p(a\mid b,c)\cdot p(b,c)=p(a\mid b,c)\cdot p(b\mid c)\cdot p(c)  \ \ ↪︎【3】
  $$&lt;/div&gt;
(因為 &lt;span class="math"&gt;\(a\cap b\cap c=a\cap (b\cap c)\)&lt;/span&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;獨立性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if X and Y are independent, &lt;span class="math"&gt;\(p(X=x_i,Y=y_j)=p(X=x_i)\cdot p(Y=y_j)\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;if X and Y are independent, 
    &lt;span class="math"&gt;\(p(X=x_i,Y=y_j\mid Z=z_k)=p(X=x_i\mid Z=z_k)\cdot p(Y=y_j\mid Z=z_k)  \ \ ↪︎【4】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="/media/DeepDL/IMG_conditional_probability.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
碰到條件機率，心中應該要有這張圖，如果公式忘記了也可以輕易的推出來
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="vs"&gt;頻率學派 v.s. 貝氏學派&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;頻率學派和貝氏學派為統計上面重要的兩大觀點，透徹的了解這兩個學派的觀點，可以幫助你在做機器學習或深度學習時選擇模型有所幫助。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;舉個例子，如果你在做詞性標記（POS tagging）的任務，你採用隱馬爾科夫模型 (HMM)，那麼你採用的是類似貝氏學派的觀點；但如果你採用 Conditional Random Field (CRF)，那麼你就是採用類似頻率學派的觀點。&lt;/p&gt;
&lt;p&gt;再舉個平易近人一點的例子，Loss Function中的Regularization Term其實可以看作是從先驗機率 (Prior Probability) 來的，這也有貝氏學派的味道，待會會再仔細的介紹。&lt;/p&gt;
&lt;p&gt;簡單講，&lt;strong&gt;頻率學派相信世界的本質是穩定的&lt;/strong&gt;，所有現象背後都有一個穩定的母群體，所以我們只要透過來自母群體大量隨機且可重複實驗的事件，就可以透過&lt;strong&gt;期望值&lt;/strong&gt;估算各種統計量，這就是頻率學派的認知：不需要太多其他的人為假設，只需要單純從母群體抽樣即可。&lt;/p&gt;
&lt;p&gt;但是貝氏學派不這麼認為，他們認為至少有一些現象是不穩定的，例如：北極的冰是否會在本世紀溶解殆盡，這種問題不存在穩定的母群體讓你可以在短時間內「大量」的量測，因為系統不斷的在演進，因此我們需要有一個具有演進特性的統計模型，那就是貝氏機率，&lt;strong&gt;貝氏機率的精髓是演進，手中先握著一個先驗機率 (Prior Probability) ，再透過不斷觀察新的證據來更新手上的機率&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在機器學習上，&lt;strong&gt;頻率學派的優點是無額外的假設&lt;/strong&gt;，相反的貝氏學派需要假設先驗機率 (Prior Probability) ，錯誤的先驗機率可能會去誤導模型，讓它反而忽視甚至曲解數據帶來的信息。另外，&lt;strong&gt;貝氏學派的優點也正是因為它有先驗機率，所以在資料筆數不多的情況下較不容易出錯&lt;/strong&gt;，如果你使用頻率學派的觀點在小樣本上，由於統計量不足可能導致你的估計也不準確。&lt;/p&gt;
&lt;p&gt;所以說，先驗機率的使用是把雙面刃，水可載舟亦可覆舟。&lt;/p&gt;
&lt;h3 id="maximum-likelihood-estimation-mle"&gt;Maximum Likelihood Estimation (MLE)&lt;/h3&gt;
&lt;p&gt;首先我們來看頻率學派，也就是傳統學派，究竟用什麼什麼觀點讓機器學習的，我們從頭到腳來理一遍。&lt;/p&gt;
&lt;p&gt;給定一個Dataset &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; ，並且決定好Model的Hyperparameters &lt;span class="math"&gt;\(m\)&lt;/span&gt;，此時我只需要尋找Model權重參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; ，就可以決定好一個Model。&lt;/p&gt;
&lt;p&gt;那怎麼決定 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 呢？頻率學派認為只要先決定好Model (決定&lt;span class="math"&gt;\(m\)&lt;/span&gt;和&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;)，再算算看這個Model產生Dataset &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 的機率，這就是Likelihood，我們希望這個Likelihood可以越高越好，也就是Maximum Likelihood。化成數學式子： ⚠️
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ p(\mathcal{D}\mid m,\theta)  \ \ ↪︎【5】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(p(\mathcal{D}\mid m,\theta)\)&lt;/span&gt; 稱為Likelihood。&lt;/p&gt;
&lt;p&gt;而Dataset &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 理當有多筆資料：
&lt;/p&gt;
&lt;div class="math"&gt;$$
=argmax_\theta\ p_{\mathcal{D}_1,...,\mathcal{D}_N}(d_1,...,d_N\mid m,\theta)  \ \ ↪︎【6】
$$&lt;/div&gt;
&lt;p&gt;
假設從Dataset &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 抽樣資料的過程每一筆是Independent的，則
&lt;/p&gt;
&lt;div class="math"&gt;$$
=argmax_\theta\ \prod_{i}\ p_{\mathcal{D}_i}(d_i\mid m,\theta)  \ \ ↪︎【7】
$$&lt;/div&gt;
&lt;p&gt;
再假設從Dataset &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 抽樣資料的過程是從同一個分布來的 (identically distributed) ，則
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \prod_{i}\ p(d_i\mid m,\theta)  \ \ ↪︎【8】
$$&lt;/div&gt;
&lt;p&gt;
接下來，為了方便計算而取 &lt;span class="math"&gt;\(\operatorname{ln}\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(d_i\mid m,\theta)  \ \ ↪︎【9】
$$&lt;/div&gt;
&lt;p&gt;
再來我們&lt;strong&gt;考慮監督式學習的情況&lt;/strong&gt;，每一筆數據 &lt;span class="math"&gt;\(d_i\)&lt;/span&gt; 是由一對輸入與輸出所組成 &lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt; ：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(x_i,y_i\mid m,\theta)  \ \ ↪︎【10】
$$&lt;/div&gt;
&lt;p&gt;
因為 &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; 是depend on &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 的，所以使用【3】式將 &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 往後搬，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
=argmax_\theta\ \sum_{i}\ \operatorname{ln}[p(y_i\mid x_i,m,\theta)p(x_i\mid m,\theta)]  \ \ ↪︎【11】
$$&lt;/div&gt;
&lt;p&gt;
 &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 與 model參數 &lt;span class="math"&gt;\(m\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 應該是互相獨立的，
&lt;/p&gt;
&lt;div class="math"&gt;$$
=argmax_\theta\ \sum_{i}\ \operatorname{ln}[p(y_i\mid x_i,m,\theta)p(x_i)]  \ \ ↪︎【12】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)+\operatorname{ln}p(x_i)  \ \ ↪︎【13】
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(x_i)\)&lt;/span&gt; 與&lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 無關，在優化過程可忽略，得： ⚠️
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)  \ \ ↪︎【14】
$$&lt;/div&gt;
&lt;p&gt;
上面這個式子是可以透過實驗來找到 &lt;span class="math"&gt;\(\theta_{MLE}\)&lt;/span&gt; 的，其中 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 表示在Model固定的情況下，估計輸入&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;會得到&lt;span class="math"&gt;\(y_i\)&lt;/span&gt;的機率，我們可以透過Grandient Descent的方法調整參數來最大化這一項，詳細怎麼做我會在&lt;a href="/deep-dl_4.html"&gt;下一講&lt;/a&gt;交代清楚。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;還沒有結束喔！大家有沒有覺得【14】式怪眼熟的，這很像是我們在&lt;a href="/deep-dl_2.html"&gt;上一講&lt;/a&gt;討論的東西。&lt;/p&gt;
&lt;p&gt;我們把【14】式乘上負號，再除上資料筆數 &lt;span class="math"&gt;\(N\)&lt;/span&gt;，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmin_\theta\ \frac{1}{N}\sum_{i}-\operatorname{ln}p(y_i\mid x_i,m,\theta)  \ \ ↪︎【15】
$$&lt;/div&gt;
&lt;p&gt;
有沒有看出來！沒有的話，我們繼續導下去，&lt;span class="math"&gt;\(\frac{1}{N}\sum_i\)&lt;/span&gt; 其實就是代表對data的採樣並平均，將它表示成為期望值
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmin_\theta\ E_{x\sim p_{data}}[-\operatorname{ln}p_{model}(y_i\mid x_i,m,\theta)]＝argmin_\theta\ H(p_{data},p_{model})  \ \ ↪︎【16】
$$&lt;/div&gt;
&lt;p&gt;
&lt;strong&gt;所以說Maximum Likelihood就是在最小化Data和Model的Cross Entropy&lt;/strong&gt;，當&lt;span class="math"&gt;\(p_{data}=p_{model}\)&lt;/span&gt;時有最小的Cross Entropy，這也間接證明了Maximum Likelihood的最優解就是&lt;span class="math"&gt;\(p_{data}=p_{model}\)&lt;/span&gt;，也就是model分布等同於母體分布。&lt;/p&gt;
&lt;p&gt;當然，&lt;strong&gt;你也可以說Maximum Likelihood就是在最小化KL Divergence&lt;/strong&gt;，KL Divergence：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(p_{data}\| p_{model})=E_{x\sim p_{data}}[\operatorname{ln}p_{data}(x)-\operatorname{ln}p_{model}(x)]  \ \ ↪︎【17】
$$&lt;/div&gt;
&lt;p&gt;
但是因為 &lt;span class="math"&gt;\(\operatorname{ln}p_{data}(x)\)&lt;/span&gt; 與 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 無關，所以計算還是只剩下第二項的Cross Entropy。&lt;/p&gt;
&lt;p&gt;特別注意，&lt;strong&gt;剛剛的所有推論並沒有提到我們的問題是Regression問題還是Classification問題，再次強調Cross Entropy並不專屬於Sigmoid或Softmax&lt;/strong&gt;，甚至&lt;a href="/deep-dl_4.html"&gt;下一講&lt;/a&gt;我還會提到Mean Squared Error是源於Data分布與Normal Distribution的Cross Entropy。&lt;/p&gt;
&lt;h3 id="maximum-a-posterior-map"&gt;Maximum A Posterior (MAP)&lt;/h3&gt;
&lt;p&gt;再來我們聊聊貝氏學派，我們好奇頻率學派究竟有什麼不足？MLE有什麼不足？為什麼我們需要一個新的觀點。&lt;/p&gt;
&lt;p&gt;最重要的一點是&lt;strong&gt;MLE認為所有 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 出現的機率是均等的&lt;/strong&gt;，我們優化的目標是 &lt;span class="math"&gt;\(p(\mathcal{D}\mid m,\theta)\)&lt;/span&gt; [from 【5】]，式子中是直接「給定」一組 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; ，所以這個 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 怎麼來的、 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 出現的機率，這些都不在MLE的考量當中。&lt;/p&gt;
&lt;p&gt;我們知道這樣並不好，假設以下有兩組 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 都可以得到一樣的Likelihood，依照你學機器學習和深度學習的經驗，你會喜歡哪組？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一組： &lt;span class="math"&gt;\(\theta_1=0.5,\ \theta_2=0.1,\ \theta_3=-0.1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;第二組： &lt;span class="math"&gt;\(\theta_1=1000.0,\ \theta_2=12.5,\ \theta_3=-500.0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以前的所學告訴我們第一組比較好，&lt;strong&gt;因為 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 接近 0，這樣的參數會讓我的輸入和輸出不會差異太大，Model會比較穩定，比較不會Overfitting，普遍作法是將Loss加入Regularization Term來壓低 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 的大小&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那麼這個Regularization Term究竟怎麼來的？&lt;/p&gt;
&lt;p&gt;一開始學機器學習或深度學習時，你應該會覺得貿然的加上Regularization Term怪怪的，而今天你可以從這裡得到更好的解釋。神奇的是！當你在優化MAP時假設了參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 分布的同時，Regularization Term會被自然的推導出來，讓我們繼續看下去。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;透過【3】式 &lt;span class="math"&gt;\(p(\mathcal{D},m,\theta)\)&lt;/span&gt; 可以列出以下關係式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\mathcal{D}, m, \theta)=p(\mathcal{D},m\mid \theta)p(\theta)=p(\theta\mid \mathcal{D},m)p(\mathcal{D},m)  \ \ ↪︎【18】
$$&lt;/div&gt;
&lt;p&gt;
稍作移項可得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\theta\mid \mathcal{D},m)=\frac{p(\mathcal{D},m\mid \theta)p(\theta)}{p(\mathcal{D},m)}  \ \ ↪︎【19】
$$&lt;/div&gt;
&lt;p&gt;
因為 &lt;span class="math"&gt;\(m\)&lt;/span&gt; 是給定的，我們想要將它向後移，使用【1】、【2】、【3】去轉換右式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{p(\mathcal{D},m\mid \theta)p(\theta)}{p(\mathcal{D},m)}=\frac{p(\mathcal{D}\mid m,\theta)[p(m\mid \theta)p(\theta)]}{p(\mathcal{D\mid m})p(m)}＝\frac{p(\mathcal{D}\mid m,\theta)}{p(\mathcal{D}\mid m)}\frac{p(m,\theta)}{p(m)}=\frac{p(\mathcal{D}\mid m,\theta)p(\theta\mid m)}{p(\mathcal{D}\mid m)}  \ \ ↪︎【20】
$$&lt;/div&gt;
&lt;p&gt;
於是我們得到了大名鼎鼎的貝氏定理： ⚠️
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\theta\mid \mathcal{D},m)=\frac{p(\mathcal{D}\mid m,\theta)p(\theta\mid m)}{p(\mathcal{D}\mid m)}  \ \ ↪︎【21】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(p(\mathcal{D}\mid m,\theta)\)&lt;/span&gt; 就是Likelihood&lt;/strong&gt;，Maximum這一項就是MLE，不用再特別討論。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 稱為先驗機率 (Prior Probability)&lt;/strong&gt;，它所代表的意義正是描述 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 這個參數在給定 &lt;span class="math"&gt;\(m\)&lt;/span&gt; 之後出現的機率有多大，這一項是先於經驗的，這裡的經驗指的是「這一次的實驗」，而這個 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 是與本次實驗 &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 無關的。因此這個 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 是需要人為給定的，你可以自己假設分布，例如：假設為有最少假設的Normal Distribution，或者是從過去的歷史紀錄去統計出 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 也行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(p(\mathcal{D}\mid m)\)&lt;/span&gt; 稱為資料機率（probability of data）&lt;/strong&gt;，通常數據與模型的Hyperparameters應該是相互獨立的，所以其實可以簡寫成  &lt;span class="math"&gt;\(p(\mathcal{D})\)&lt;/span&gt; ，這一項只需要統計一下Dataset的分布即可得到。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(p(\theta\mid \mathcal{D},m)\)&lt;/span&gt; 稱為後驗機率 (Posterior Probability)&lt;/strong&gt;，它所代表的意義是給定數據 &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 和 Hyperparameters &lt;span class="math"&gt;\(m\)&lt;/span&gt; 之後，會出現 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 的機率，有注意到嗎？我們在【21】中同時連結了先驗機率和後驗機率，這代表的是手上原先有一個 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 分布（先驗機率），經過觀察數據後，我重新的去更新這個 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 分布（後驗機率），這充分的傳達了貝氏學派的演進概念。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;後驗機率也是我們準備要最大化的目標，所以此方法才稱為Maximum A Posterior (MAP) 。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最大化後驗機率MAP是直接問貝氏定理什麼樣的 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 在給定條件下出現機率最大？而最大化Likelihood則是間接的希望找出一組 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 讓Model能產生Data的機率變高，兩者的優化邏輯是不一樣的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;從 【21】式出發：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ p(\theta\mid \mathcal{D},m)=argmax_\theta\ \frac{p(\mathcal{D}\mid m,\theta)p(\theta\mid m)}{p(\mathcal{D}\mid m)}  \ \ ↪︎【22】
$$&lt;/div&gt;
&lt;p&gt;
然後如【6】、【7】、【8】式一樣假設取樣是i.i.d. (independent and identically distributed)，得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ [\prod_{i}\ \frac{p(d_i\mid m,\theta)}{p(d_i\mid m)}]p(\theta\mid m)  \ \ ↪︎【23】
$$&lt;/div&gt;
&lt;p&gt;
接下來，為了方便計算如【9】式取 &lt;span class="math"&gt;\(\operatorname{ln}\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ \{\sum_{i}\ [\operatorname{ln}p(d_i\mid m,\theta)-\operatorname{ln}p(d_i\mid m)]\}+\operatorname{ln}p(\theta\mid m)  \ \ ↪︎【24】
$$&lt;/div&gt;
&lt;p&gt;
其中 &lt;span class="math"&gt;\(-\operatorname{ln}p(d_i\mid m)\)&lt;/span&gt; 與 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 無關，可忽略：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ \{\sum_{i}\ \operatorname{ln}p(d_i\mid m,\theta)\}+\operatorname{ln}p(\theta\mid m)  \ \ ↪︎【25】
$$&lt;/div&gt;
&lt;p&gt;
第一項其實就是跟MLE一模模一樣樣，所以直接套用【9】到【14】式的推論，得：⚠️
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ \{\sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)\}+\operatorname{ln}p(\theta\mid m)  \ \ ↪︎【26】
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;觀察 【26】式，非常清楚的我們在優化第一項就如同優化MLE，&lt;strong&gt;但是MAP比MLE多了 &lt;span class="math"&gt;\(\operatorname{ln}p(\theta\mid m)\)&lt;/span&gt; &lt;/strong&gt;，我們就針對這一項來討論，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假設 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 為一個Uniform Distribution，也就是說所有的 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 出現機率均等，則 &lt;span class="math"&gt;\(p(\theta\mid m)=const.\)&lt;/span&gt;，這麼一來這一項在【26】式可以直接槓掉，因為它與 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 無關，此時 &lt;span class="math"&gt;\(\theta_{MAP}=\theta_{MLE}\)&lt;/span&gt; 。所以說在貝氏學派的觀點下，MLE只是MAP的一個特例，MLE只是假設 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 出現機率均等的MAP，&lt;a href="/deep-dl_2.html"&gt;上一講有提過&lt;/a&gt;Uniform Distribution為所有分布當中Entropy最大的，也就是不確定程度最大的，也就是人為假設幾乎為零的分布，確實符合頻率學派的觀點：不需要太多人為假設。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是基於剛剛的推論 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 的均等並不是我們想要的，我們希望 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 可以多出現在接近 &lt;span class="math"&gt;\(\theta=0\)&lt;/span&gt; 的附近，&lt;strong&gt;因此我們希望  &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 的分布具有 有限Variance且平均值接近0 (&lt;span class="math"&gt;\(E_{p(\theta\mid m)}[x]=0\)&lt;/span&gt;)&lt;/strong&gt;。此時的第一首選就是Normal Distribution，因為我們在&lt;a href="/deep-dl_1.html"&gt;第一講&lt;/a&gt;中說過：Normal Distribution是具有限Variance分布中具有最少人為假設的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假設 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 為一個Normal Distribution且平均值為0，則
  &lt;div class="math"&gt;$$
  p(\theta\mid m)=\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{\theta^2}{2\sigma^2}}\}  \ \ ↪︎【27】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
  \operatorname{ln}p(\theta\mid m)=\operatorname{ln}(\frac{1}{\sqrt{2\pi}\sigma})-\frac{\theta^2}{2\sigma^2}  \ \ ↪︎【28】
  $$&lt;/div&gt;
&lt;p&gt;代入【26】，並且取負號，得
  &lt;/p&gt;
&lt;div class="math"&gt;$$
  \theta_{MAP}=argmin_\theta\ \sum_{i}-\operatorname{ln}p(y_i\mid x_i,m,\theta)+\frac{1}{2\sigma^2}\theta^2  \ \ ↪︎【29】
  $$&lt;/div&gt;
&lt;p&gt;
  哇！ L2 Regularization Term &lt;span class="math"&gt;\(\frac{1}{2\sigma^2}\theta^2\)&lt;/span&gt; 就在這不經意間被導出了，所以&lt;strong&gt;未來看到L2 Regularization Term就要知道它隱藏了參數分布呈現Normal Distribution的假設&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;可以預期的，&lt;strong&gt;當假設不同的參數分布 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 就會得到不同型式的Regularization Term&lt;/strong&gt;，如果我假設Laplace Distribution則會得到L1 Regularization Term，來看一下。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假設 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 為一個Laplace Distribution且平均值為0，則
  &lt;div class="math"&gt;$$
  p(\theta\mid m)=\frac{1}{2b}exp\{{-\frac{|\theta|}{b}}\}  \ \ ↪︎【30】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
  \operatorname{ln}p(\theta\mid m)=\operatorname{ln}(\frac{1}{2b})-\frac{|\theta|}{b}  \ \ ↪︎【31】
  $$&lt;/div&gt;
&lt;p&gt;代入【26】，並且取負號，得
  &lt;/p&gt;
&lt;div class="math"&gt;$$
  \theta_{MAP}=argmin_\theta\ \sum_{i}-\operatorname{ln}p(y_i\mid x_i,m,\theta)+\frac{1}{b}|\theta|   \ \ ↪︎【32】
  $$&lt;/div&gt;
&lt;p&gt;
  第二項就是L1 Regularization Term。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/DeepDL/Gaussian-distribution-and-Laplace-distribution.ppm.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  Courtesy &lt;a href="https://www.researchgate.net/figure/Gaussian-distribution-and-Laplace-distribution_fig7_321825093"&gt;Youngjoo Kim&lt;/a&gt;
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="_2"&gt;結語&lt;/h3&gt;
&lt;p&gt;這一講我們清楚的了解了頻率學派和貝氏學派各自的觀點，並且從兩者觀點出發去探討機器學習問題。&lt;/p&gt;
&lt;p&gt;頻率學派使用Maximum Likelihood Estimation (MLE) 來優化，優化關係式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)
$$&lt;/div&gt;
&lt;p&gt;
此項經過轉換會等同於最小化Data與Model之間的Cross Entropy，或等同於最小化Data與Model之間的KL Divergence，與&lt;a href="/deep-dl_2.html"&gt;上一講的資訊理論&lt;/a&gt;完美契合。&lt;/p&gt;
&lt;p&gt;貝氏學派則使用Maximum A Posterior (MAP) 來優化，優化關係式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)+\operatorname{ln}p(\theta\mid m)
$$&lt;/div&gt;
&lt;p&gt;
除了第一項與MLE一樣之外，此時我們還需考慮了參數可能的分布，當參數分布是均等時，MAP和MLE是等價的。但是我們希望 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 可以接近0，所以一般會去假設 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 為一個Variance有限且平均值為0的分布，如果選擇使用Normal Distribution，則會得到L2 Regularization Term；如果選擇用Laplace Distribution，則會得到L1 Regularization Term。&lt;/p&gt;
&lt;p&gt;本講已經給出了兩個觀點的機率優化式，但是要怎麼變換成擬合問題呢？這需要一大篇幅來介紹，我們會在&lt;a href="/deep-dl_4.html"&gt;下一講&lt;/a&gt;來仔細討論這個問題，敬請期待囉！&lt;/p&gt;
&lt;h3 id="reference"&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.deeplearningbook.org"&gt;Ian Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Christopher Bishop. Pattern Recognition and Machine Learning. 2006.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/32480810"&gt;聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/"&gt;MLE vs MAP: the connection between Maximum Likelihood and Maximum A Posteriori Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.metaflow.fr/ml-notes-why-the-log-likelihood-24f7b6c40f83"&gt;Morgan. ML notes: Why the log-likelihood?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;[此文章為原創文章，轉載前請註明文章來源]&lt;/em&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="剖析深度學習"></category></entry><entry><title>剖析深度學習 (2)：你知道Cross Entropy和KL Divergence代表什麼意義嗎？談機器學習裡的資訊理論</title><link href="https://ycc.idv.tw/deep-dl_2.html" rel="alternate"></link><published>2020-02-25T12:00:00+08:00</published><updated>2020-02-25T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-25:/deep-dl_2.html</id><summary type="html">&lt;p&gt;在深度學習裡面，尤其是分類問題，常常會用到Cross Entropy，教學上通常會從Maximum Likelihood推導而來，但是Cross Entropy其實具有更廣義的涵義，甚至不限於分類問題使用。還有學習過程也經常會出現KL Divergence這樣既熟悉又陌生的東西，甚至到了GAN會用到更多種類的Divergence，例如：JS Divergence。這全部都與資訊理論息息相關，這一講讓我們來搞清楚Entropy、Cross Entropy、KL Divergence和f-Divergence到底具有什麼涵義。&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;深度學習發展至今已經有相當多好用的套件，使得進入的門檻大大的降低，因此如果想要快速的實作一些深度學習或機器學習，通常是幾行程式碼可以解決的事。但是，如果想要將深度學習或機器學習當作一份工作，深入了解它背後的原理和數學是必要的，才有可能因地制宜的靈活運用，YC準備在這一系列當中帶大家深入剖析深度學習。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="/deep-dl_1.html"&gt;在上一講當中&lt;/a&gt;，我鉅細靡遺的介紹了Normal Distribution。其中我有稍微的提到Entropy的概念，並且說在未來會有一講專門來談機器學習裡面會用到的資訊理論，而那個未來就是現在！&lt;/p&gt;
&lt;p&gt;在深度學習裡面，尤其是分類問題，常常會用到Cross Entropy，教學上通常會從Maximum Likelihood推導而來，但是Cross Entropy其實具有更廣義的涵義，甚至不限於分類問題使用。&lt;/p&gt;
&lt;p&gt;還有學習過程也經常會出現KL Divergence這樣既熟悉又陌生的東西，甚至到了GAN會用到更多種類的Divergence，例如：JS Divergence。&lt;/p&gt;
&lt;p&gt;這全部都與資訊理論息息相關，這一講讓我們來搞清楚Entropy、Cross Entropy、KL Divergence和f-Divergence到底具有什麼涵義。&lt;/p&gt;
&lt;p&gt;這一切都要先從Entropy開始講起。&lt;/p&gt;
&lt;h3 id="information-entropy"&gt;資訊熵（Information Entropy）&lt;/h3&gt;
&lt;p&gt;資訊理論是應用數學的一個分支，主要是對訊號中存在的資訊多寡做量化。最初研究目的是為了數據傳輸的編碼，探討要怎麼編碼資料傳輸才有效率。&lt;/p&gt;
&lt;p&gt;資訊理論背後的直覺是，越是不容易發生的事件帶給我們的資訊量越大，資訊量的大小可以看作是事件給我們的驚訝程度。舉個例子，「今天早上太陽升起」這樣幾乎永遠都是對的事件，能帶給我們的資訊量可以說是零（你不用告訴我，我也知道）；相反的「今天早上有日蝕」的事件則含有相對多的資訊量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(事件的資訊量 \propto 事件的不確定程度\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有了這樣的洞見，我們怎麼把它化成數學呢？我們可以定義事件的Self-Information為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
I(x)=-log_2p(x)  \ \ ↪︎【1】
$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(log_2\)&lt;/span&gt;以2為底，則代表所採用的單位為 bits (比特)；&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;代表的是事件的出現機率，此式子符合以下四個特性&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(0&amp;lt;p(x)\leq 1\)&lt;/span&gt;時，則&lt;span class="math"&gt;\(I(x)&amp;gt;0\)&lt;/span&gt;，這意味著資訊量必為正，如果遇到雜訊干擾才有可能扣掉一些資訊量&lt;/li&gt;
&lt;li&gt;當事件永遠是對的時，則&lt;span class="math"&gt;\(p(x)=1\)&lt;/span&gt;，相應的 &lt;span class="math"&gt;\(I(x)=0\)&lt;/span&gt;，代表資訊量為零&lt;/li&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;越小，則&lt;span class="math"&gt;\(I(x)\)&lt;/span&gt;越大，意味著事件出現的機率越小，它所攜帶的資訊量越大&lt;/li&gt;
&lt;li&gt;若&lt;span class="math"&gt;\(p(x)=p_1(x)\times p_2(x)\)&lt;/span&gt;則代表兩個獨立事件發生的機率是相乘的關係，此時資訊量應該是相加的關係，關係式&lt;span class="math"&gt;\(I(x)=I_1(x)+I_2(x)\)&lt;/span&gt;正表示這樣的關係&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Self-Information只處理單一結果，我們更想要關注整個系統，Shannon Entropy可以量化整個機率分布中不確定性的程度：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(x)=E_{x\sim p}[I(x)]=-E_{x\sim p}[log_2p(x)]  \ \ ↪︎【2】
$$&lt;/div&gt;
&lt;p&gt;
上式所傳達的意思是：Shannon Entropy即是評估Self-Information的期望值。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/DeepDL/Entropy_flip_2_coins.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;為了進一步的了解Shannon Entropy的內涵，我們來舉個例子。假設你的系統是離散的，可以進一步表示成為
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(x)=-E_{x\sim p}[log_2p(x)]=-\sum_i p_ilog_2 p_i  \ \ ↪︎【3】
$$&lt;/div&gt;
&lt;p&gt;
假設你要描述的事件有三個獨立事件A、B和C，A事件出現機率為&lt;span class="math"&gt;\(1/2\)&lt;/span&gt;，B事件出現機率為&lt;span class="math"&gt;\(1/4\)&lt;/span&gt;，C事件出現機率為&lt;span class="math"&gt;\(1/4\)&lt;/span&gt;，代入式【3】求系統的Shannon Entropy：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H=-\frac{1}{2}log_2(\frac{1}{2})-\frac{1}{4}log_2(\frac{1}{4})-\frac{1}{4}log_2(\frac{1}{4})=0.5+0.5+0.5=1.5  \ \ ↪︎【4】
$$&lt;/div&gt;
&lt;p&gt;
這代表什麼意義呢？這代表今天如果你設計一個系統得當的話，你只需要用到平均1.5個bits來傳輸。怎麼說呢？我們逐步拆解式【4】帶給我們的觀念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A事件的&lt;span class="math"&gt;\(I=1\)&lt;/span&gt;，代表用1個bit去傳輸A事件，即&lt;span class="math"&gt;\(bits_A=1\)&lt;/span&gt;，可能編碼為&lt;code&gt;0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;B事件的&lt;span class="math"&gt;\(I=2\)&lt;/span&gt;，代表用2個bits去傳輸B事件，即&lt;span class="math"&gt;\(bits_B=2\)&lt;/span&gt;，可能編碼為&lt;code&gt;10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;C事件的&lt;span class="math"&gt;\(I=2\)&lt;/span&gt;，代表用2個bits去傳輸C事件，即&lt;span class="math"&gt;\(bits_C=2\)&lt;/span&gt;，可能編碼為&lt;code&gt;11&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你會發現經常出現的事件A就用較少的bits來傳輸，而較不常發生的事件B就用比較多的bits來傳輸，如此一來傳輸會更有效率。假設有200個事件要傳輸，依照機率分布，其中應該有100件屬於A事件，有50件屬於B事件，有50件屬於C事件，依照上述的編碼方式，我們傳輸這200個事件所需要的預期平均bits數就是1.5。
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{n_A\times bits_A+n_B\times bits_B+n_C\times bits_C}{n_A+n_B+n_C}=\frac{100\times 1+50\times 2+50\times 2}{200}=1.5  \ \ ↪︎【5】
$$&lt;/div&gt;
&lt;p&gt;
而理論告訴我們這 &lt;span class="math"&gt;\(1.5\)&lt;/span&gt; 其實就是最小的預期位元，你無法找到比這個更小的，也就是說：你無法找到比這個更有效率的編碼系統。理論如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Noiseless Coding Theorm (Shannon, 1948):&lt;/p&gt;
&lt;p&gt;The entropy is a lower bound on the number of bits needed to transmit the state of a random variable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;來做個小結論，仔細回想剛剛的過程你會更能了解公式隱藏的意義。如果你仔細的去理解剛剛我舉的例子，你會發現： &lt;strong&gt;&lt;span class="math"&gt;\(I(x)\)&lt;/span&gt;，或者Entropy中的&lt;span class="math"&gt;\(-log\)&lt;/span&gt; ，所扮演的角色是「編碼」，決定需要用幾個bits來傳輸事件，而Entropy的意義就是這套「編碼」運用到系統的bits期望值，並且Shannon的理論告訴我們&lt;span class="math"&gt;\(-log\ p(x)\)&lt;/span&gt;已經是最有效率的「編碼」，它可以得到最小的bits期望值，所以Entropy是bits期望值的下界&lt;/strong&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;在物理上或是機器學習上，我們常使用自然對數&lt;span class="math"&gt;\(e\)&lt;/span&gt;當作底，所以Entropy為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p)=E_{x\sim p}[-\operatorname{ln}p(x)]  \ \ ↪︎【6】
$$&lt;/div&gt;
&lt;p&gt;
其實就只是把度量的單位從比特(bits)換成奈特(nats)，只是「編碼」的單位改變而已，其意義都跟上述的一樣。只是如果使用nats當單位在計算上會方便許多，因為許多的分布都可以表示成以&lt;span class="math"&gt;\(e\)&lt;/span&gt;為底的指數，例如：Normal Distribution。&lt;/p&gt;
&lt;p&gt;還記得&lt;a href="/deep-dl_1.html"&gt;上一講中&lt;/a&gt;我請大家記住的【2】到【4】式嗎？現在套用到這裡的【6】，可得：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;連續情境下， &lt;span class="math"&gt;\(H=\int -p(x)\operatorname{ln}p(x)dx  \ \ ↪︎【7】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;離散情境下， &lt;span class="math"&gt;\(H=\sum_i -p_i\operatorname{ln}p_i  \ \ ↪︎【8】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;實驗情境下，&lt;span class="math"&gt;\(H=\sum_k -(\frac{n_k}{N})\operatorname{ln}(\frac{n_k}{N})  \ \ ↪︎【9】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;最後，來看看什麼分布的Entropy最大，在給定 &lt;span class="math"&gt;\(\int p(x)dx=1\)&lt;/span&gt; 的情況下試圖找到一個 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 可以使 Entropy &lt;span class="math"&gt;\(H\)&lt;/span&gt; 最大，引入&lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;Lagrange Multiplier&lt;/a&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$
L=\int^{\infty}_{-\infty}-\operatorname{ln}p(x)\cdot p(x)dx+\lambda (\int^{\infty}_{-\infty}p(x)dx-1)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\int^{\infty}_{-\infty}[\lambda p(x)-p(x)\operatorname{ln}(p(x))]dx-\lambda  \ \ ↪︎【11】
$$&lt;/div&gt;
&lt;p&gt;接下來對【11】微分求極值
&lt;/p&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial L}{\partial p(x)}|_{p^*(x)}=\int^{\infty}_{-\infty}[\lambda-\operatorname{ln}(p^*(x))-1]dx  \ \ ↪︎【12】
$$&lt;/div&gt;
&lt;p&gt;
所以
&lt;/p&gt;
&lt;div class="math"&gt;$$
p^*(x)=exp\{\lambda-1\}  \ \ ↪︎【13】
$$&lt;/div&gt;
&lt;p&gt;此時的分布是一個與 &lt;span class="math"&gt;\(x\)&lt;/span&gt; 無關的常數，所以 &lt;span class="math"&gt;\(p^*(x)\)&lt;/span&gt; 是一個Uniform Distribution。所以&lt;strong&gt;平均分配會使得系統得到最大的Entropy，也就是Uniform Distribution是隨機性最大的分布，也是資訊量最大的分布&lt;/strong&gt; ，這與我們剛剛的討論是自恰的。（注意：&lt;a href="/deep-dl_1.html"&gt;在有限Variance的情況下是Normal Distribution有最大Entropy&lt;/a&gt;）&lt;/p&gt;
&lt;h3 id="cross-entropy"&gt;Cross Entropy&lt;/h3&gt;
&lt;p&gt;接下來聊聊學過機器學習和深度學習都知道的Cross Entropy，在分類問題當中Cross Entropy被定義成：
&lt;/p&gt;
&lt;div class="math"&gt;$$
Cross\ Entropy=-y\ \operatorname{ln}(q)-(1-y)\operatorname{ln}(1-q)  \ \ ↪︎【14】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(y_i=0,1\)&lt;/span&gt;為data的labels，&lt;span class="math"&gt;\(q_i\)&lt;/span&gt;為模型預測的輸出值。&lt;/p&gt;
&lt;p&gt;但它其實有更一般的定義：
&lt;/p&gt;
&lt;div class="math"&gt;$$
Cross\ Entropy:\ H(p, q)=E_{x\sim p}[-\operatorname{ln}q(x)]  \ \ ↪︎【15】
$$&lt;/div&gt;
&lt;p&gt;一般&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;代表的是目標分布，也就是想要學習的未知分布；而&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;則代表是模型的輸出分布。&lt;/p&gt;
&lt;p&gt;我們剛剛說過Entropy是bits或nats期望值的下界，所以：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p,q)=E_{x\sim p}[-\operatorname{ln}q(x)]\geq E_{x\sim p}[-\operatorname{ln}p(x)]=H(p,p)=H(p)  \ \ ↪︎【16】
$$&lt;/div&gt;
&lt;p&gt;
所以當我們試圖減少Cross Entropy時，其實就是試圖調整&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;使其接近&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;，因為當&lt;span class="math"&gt;\(q(x)=p(x)\)&lt;/span&gt;時，&lt;span class="math"&gt;\(H(p,q)=H(p)\)&lt;/span&gt;有最小的Cross Entropy。&lt;/p&gt;
&lt;p&gt;延續剛剛「編碼」的概念套用在Cross Entropy，今天我雖然知道 &lt;span class="math"&gt;\(-\operatorname{ln}p(x)\)&lt;/span&gt;是最好的「編碼」，但是我不知道 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 長什麼樣子，所以退一步我們使用模型的 &lt;span class="math"&gt;\(q(x)\)&lt;/span&gt; 來做「編碼」，&lt;strong&gt;Cross Entropy的意義就是利用 &lt;span class="math"&gt;\(-\operatorname{ln}q(x)\)&lt;/span&gt; 這套「編碼」去算系統的nats期望值，並且想辦法改善編碼方法來降低Cross Entropy&lt;/strong&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;接下來我想要帶大家從式【15】到【14】導一遍式子。&lt;/p&gt;
&lt;p&gt;因為我們知道目標分布是Binary的離散系統，所以可以把【15】寫成：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p,q)=-p_{positive}\cdot \operatorname{ln}(q_{positive})-p_{negative}\cdot \operatorname{ln}(q_{negative})  \ \ ↪︎【17】
$$&lt;/div&gt;
&lt;p&gt;
因為是Binary Classification的問題，只有兩種states其機率相合為1，因此：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p,q)=-p_{positive}\cdot \operatorname{ln}(q_{positive})-(1-p_{positive})\cdot \operatorname{ln}(1-q_{positive})  \ \ ↪︎【18】
$$&lt;/div&gt;
&lt;p&gt;
接下來，&lt;span class="math"&gt;\(p\)&lt;/span&gt; 是目標分布，當label為positive (&lt;span class="math"&gt;\(y=1\)&lt;/span&gt;) 則&lt;span class="math"&gt;\(p_{positive}=1\)&lt;/span&gt;，當label為 (&lt;span class="math"&gt;\(y=0\)&lt;/span&gt;) negative則&lt;span class="math"&gt;\(p_{positive}=0\)&lt;/span&gt;；&lt;span class="math"&gt;\(q\)&lt;/span&gt; 是模型預測，其目標是預測positive的可能機率，所以最後寫成：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p,q)=-y\cdot \operatorname{ln}(q)-(1-y)\cdot \operatorname{ln}(1-q)
$$&lt;/div&gt;
&lt;p&gt;
就跟【14】式一模一樣了，&lt;strong&gt;這裡注意一點，在推導的過程當中我都不需要去假設模型的長相，我不需要假設模型為Sigmoid，推出Cross Entropy的過程是和模型的選擇無關的，所以千萬不要認為選擇使用Cross Entropy是因為Sigmoid的緣故。進一步說Cross Entropy其實可以用在各種問題（或分布），包括：Regression問題，我們會在&lt;a href="/deep-dl_3.html"&gt;接下來的文章&lt;/a&gt;裡讓大家真正了解這一點。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="kl-divergence-kullback-leibler-divergence"&gt;KL Divergence (Kullback-Leibler Divergence)&lt;/h3&gt;
&lt;p&gt;KL Divergence對碰過深度學習一段時間的大家應該是一個既熟悉又陌生的東西吧！我們就在這邊把它搞懂吧！&lt;/p&gt;
&lt;p&gt;如果有兩個獨立的機率分布&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;同時對應到同一個隨機變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;，也就是它們所在的空間是一樣的，則可以使用KL Divergence來測量這兩個分布的差異程度：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(p||q)=-E_{x\sim p}[\operatorname{ln}q(x)-\operatorname{ln}p(x)]=-E_{x\sim p}[\operatorname{ln}\frac{q(x)}{p(x)}]  \ \ ↪︎【19】
$$&lt;/div&gt;
&lt;p&gt;這個式子不好懂，沒關係！我們稍微代換一下式子：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(p||q)=E_{x\sim p}[-\operatorname{ln}q(x)]-E_{x\sim p}[-\operatorname{ln}p(x)]=H(p,q)-H(p)  \ \ ↪︎【20】
$$&lt;/div&gt;
&lt;p&gt;
有看出來了嗎？&lt;strong&gt;KL Divergence其實就是Cross Entropy扣掉目標分布的Entropy，更深層的說，KL Divergence表示的是目前的編碼方法最多還可以下降多少nats期望值&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;雖然你可以將KL Divergence視作距離，但是嚴格來說它不是，因為KL Divergence不具有對稱性：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(p||q)\neq D_{KL}(q||p)  \ \ ↪︎【21】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(q||p)＝-E_{x\sim q}[\operatorname{ln}p(x)-\operatorname{ln}q(x)]＝H(q,p)-H(q)  \ \ ↪︎【22】
$$&lt;/div&gt;
&lt;p&gt;
【21】式意味著從&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;到&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;所降低的nats期望值與從&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;到&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;所降低的nats期望值不相等。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/DeepDL/KL-Gauss-Example.png" /&gt;&lt;/p&gt;
&lt;h3 id="f-divergence"&gt;f-Divergence&lt;/h3&gt;
&lt;p&gt;Divergence其實有多個形式，定義如下：
當有兩個獨立的機率分布&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;同時對應到同一個隨機變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;，
&lt;/p&gt;
&lt;div class="math"&gt;$$
f-divergence:\ D_f(p\|q)=E_{x\sim q}[f(\frac{p(x)}{q(x)})]=\int q(x)f(\frac{p(x)}{q(x)})dx
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(f(.)\)&lt;/span&gt; 只需要遵守兩條規則就可以：&lt;span class="math"&gt;\(f(.)\)&lt;/span&gt; 是Convex的且&lt;span class="math"&gt;\(f(1)=0\)&lt;/span&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(f(u)=u\cdot \operatorname{ln}u\)&lt;/span&gt;，則 
  &lt;div class="math"&gt;$$
  D_f(p\|q)=\int q(x)(\frac{p(x)}{q(x)})\operatorname{ln}(\frac{p(x)}{q(x)})dx=\int p(x)\operatorname{ln}(\frac{p(x)}{q(x)})dx=D_{KL}(p\|q)
  $$&lt;/div&gt;
  ，為KL Divergence&lt;/li&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(f(u)=-\operatorname{ln}u\)&lt;/span&gt;，則 
  &lt;div class="math"&gt;$$
  D_f(p\|q)=-\int q(x)\operatorname{ln}(\frac{p(x)}{q(x)})dx=\int q(x)\operatorname{ln}(\frac{q(x)}{p(x)})dx=D_{KL}(q\|p)
  $$&lt;/div&gt;
  ，為Reverse KL Divergence&lt;/li&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(f(u)=(u-1)^2\)&lt;/span&gt;，則 
  &lt;div class="math"&gt;$$
  D_f(p\|q)=-\int q(x)(\frac{p(x)}{q(x)}-1)^2dx=\int \frac{(p(x)-q(x))^2}{q(x)}dx
  $$&lt;/div&gt;
  ，為Chi Square Divergence&lt;/li&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(f(u)=-(u+1)\operatorname{ln}\frac{u+1}{2}+u\operatorname{ln}u\)&lt;/span&gt;，則 
  &lt;div class="math"&gt;$$
  D_f(p\|q)=\frac{1}{2}[D_{KL}(p\|\frac{p+q}{2})+D_{KL}(q\|\frac{p+q}{2})]
  $$&lt;/div&gt;
  ，為 JS Divergence (用在GAN)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f-divergence在意義上代表：我在model的分布 &lt;span class="math"&gt;\(q(x)\)&lt;/span&gt; 上估計 &lt;span class="math"&gt;\(f(\frac{p(x)}{q(x)})\)&lt;/span&gt; ，&lt;span class="math"&gt;\(f(.)\)&lt;/span&gt;是用來評估分布間距離的函數，當&lt;span class="math"&gt;\(p(x)=q(x)\)&lt;/span&gt;時距離為0 — &lt;span class="math"&gt;\(f(\frac{p(x)}{q(x)})=0\)&lt;/span&gt;，當&lt;span class="math"&gt;\(p(x)\neq q(x)\)&lt;/span&gt;時距離為正 — &lt;span class="math"&gt;\(f(\frac{p(x)}{q(x)})&amp;gt; 0\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3 id="_1"&gt;結論&lt;/h3&gt;
&lt;p&gt;機器學習和深度學習充分的借用了資訊理論裡對資訊量的衡量技術，包括：Entropy、Cross Entropy和KL Divergence。因為機器學習中充斥著統計分布，而這些衡量方法可以幫助我們度量種種分布的資訊量，有了這把尺我們才可以進行各類優化來讓機器學習。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entropy:  &lt;span class="math"&gt;\(H(p)=E_{x\sim p}[-\operatorname{ln}p(x)]\)&lt;/span&gt; ，為最小可得的nats期望值&lt;/li&gt;
&lt;li&gt;Cross Entropy: &lt;span class="math"&gt;\(H(p, q)=E_{x\sim p}[-\operatorname{ln}q(x)]\)&lt;/span&gt; ，利用&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;來編碼可得的nats期望值&lt;/li&gt;
&lt;li&gt;KL Divergence: &lt;span class="math"&gt;\(D_{KL}(p\|q)=H(p,q)-H(p)\)&lt;/span&gt; ，表示的是目前的編碼方法最多還可以下降多少nats期望值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其實，這一講的內容可以說是為了接下來幾講而生，&lt;a href="/deep-dl_3.html"&gt;下一講&lt;/a&gt;我們會開始套用資訊理論的這些概念，帶大家重新了解優化和擬合是怎麼一回事，敬請期待！&lt;/p&gt;
&lt;h3 id="reference"&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.deeplearningbook.org"&gt;Ian Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Christopher Bishop. Pattern Recognition and Machine Learning. 2006.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)"&gt;Wiki: Entropy_(information_theory)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Cross_entropy"&gt;Wiki: Cross Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence"&gt;Wiki: Kullback–Leibler_divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://hackmd.io/@sXG2cRDpRbONCsrtz8jfqg/ry-0k0PwH"&gt;從計算機編碼的角度看Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=av1bqilLsyQ&amp;amp;list=PLJV_el3uVTsMq6JEFPW35BCiOQTsoqwNw&amp;amp;index=6&amp;amp;t=0s"&gt;李宏毅, GAN Lecture 5 (2018): General Framework&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;[此文章為原創文章，轉載前請註明文章來源]&lt;/em&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="剖析深度學習"></category></entry><entry><title>剖析深度學習 (1)：為什麼Normal Distribution這麼好用？</title><link href="https://ycc.idv.tw/deep-dl_1.html" rel="alternate"></link><published>2020-02-18T12:00:00+08:00</published><updated>2020-02-18T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-18:/deep-dl_1.html</id><summary type="html">&lt;p&gt;如果你已經學了好一陣子的機器學習或深度學習，應該對於Normal Distribution不陌生，但是你真的懂Normal Distribution嗎？本講會詳細的探討Normal Distribution，並且引入中央極限定理（Central Limit Theorm）來解釋為何自然界的隨機誤差大都呈現Normal Distribution，再來介紹Entropy，並且利用Entropy揭示Normal Distribution具有最少先驗知識（Prior Knowledge）的特性。&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;深度學習發展至今已經有相當多好用的套件，使得進入的門檻大大的降低，因此如果想要快速的實作一些深度學習或機器學習，通常是幾行程式碼可以解決的事。但是，如果想要將深度學習或機器學習當作一份工作，深入了解它背後的原理和數學是必要的，才有可能因地制宜的靈活運用，YC準備在這一系列當中帶大家深入剖析深度學習。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先第一講，我們來聊一個最常見的分布—正態分布（Normal Distribution），也稱為高斯分布（Gaussian Distribution）。&lt;/p&gt;
&lt;p&gt;如果你已經學了好一陣子的機器學習或深度學習，應該對於Normal Distribution不陌生，但是你真的懂Normal Distribution嗎？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;為什麼Normal Distribution通常作為雜訊的分布？&lt;/li&gt;
&lt;li&gt;為何在DL（deep learning），參數的初始化要用Normal Distribution？&lt;/li&gt;
&lt;li&gt;為何在Bayesian公式裡常常會使用Normal Distribution當作Prior Probability？&lt;/li&gt;
&lt;li&gt;在使用GAN（generative adversarial network）時，為什麼給予的輸入要假設Normal Distribution？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你不知道為什麼使用Normal Distribution，你用起來不會怕嗎？這一講我想要回答的是：為什麼Normal Distribution這麼好用？甚至已經到了無腦用的程度，我會從統計學和資訊理論來回答這個問題。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/DeepDL/best_gaussian.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  Courtesy &lt;a href="https://www.facebook.com/nas.mooty"&gt;Nas Mouti&lt;/a&gt;
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="normal-distribution"&gt;認識Normal Distribution&lt;/h3&gt;
&lt;p&gt;首先來看看Normal Distribution的數學表示式
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{normal}(x)=\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\}  \ \ ↪︎【1】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(\mu\)&lt;/span&gt; 剛好是Normal Distribution的 Mean（平均值），&lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; 剛好是Normal Distribution的 Variance（方差），來證明一下吧！&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;證明之前，先來了解什麼是「期望值」，期望值指的是在相同場景下隨機試驗多次，所有那些可能狀態的平均結果。期望值 &lt;span class="math"&gt;\(E[g(x)]\)&lt;/span&gt; 跟兩件事有關：試驗的物理量 &lt;span class="math"&gt;\(g(x)\)&lt;/span&gt; 和試驗的出現機率 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;連續形式寫成：&lt;/p&gt;
&lt;div class="math"&gt;$$
E[g(x)]\equiv\int^{\infty}_{-\infty}g(x)\cdot p(x)dx  \ \ ↪︎【2】
$$&lt;/div&gt;
&lt;p&gt;
另外期望值也有離散的形式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
E[g]\equiv \sum_{i}g_i\cdot p_i  \ \ ↪︎【3】
$$&lt;/div&gt;
&lt;p&gt;上面兩個式子有個前提是已知機率分布 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; 的情況下才能使用，如果今天我們不知道機率分布，只能使用實驗的方法求近似的期望值，此時&lt;span class="math"&gt;\(p_i\)&lt;/span&gt;可以用採樣來取代，則變換式【3】為以下公式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
E[g]= \frac{1}{N}\sum_{i=1}^{N}g_i  \ \ ↪︎【4】
$$&lt;/div&gt;
&lt;p&gt;
請大家牢記上面三個式子，在機器學習中會反覆使用到。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;有了期望值的概念，我們開始來看Mean和Variance的定義&lt;/p&gt;
&lt;p&gt;Mean的定義為
&lt;/p&gt;
&lt;div class="math"&gt;$$
E[x]\equiv\int^{\infty}_{-\infty}x\cdot p(x)dx  \ \ ↪︎【5】
$$&lt;/div&gt;
&lt;p&gt;
也就是求物理量 &lt;span class="math"&gt;\(x\)&lt;/span&gt; 的期望值。&lt;/p&gt;
&lt;p&gt;Variance的定義為
&lt;/p&gt;
&lt;div class="math"&gt;$$
Var[x]\equiv E[(x-E[x])^2]  \ \ ↪︎【6】
$$&lt;/div&gt;
&lt;p&gt;
將上式化約可得
&lt;/p&gt;
&lt;div class="math"&gt;$$
Var[x]=E[x^2]-E[x]^2  \ \ ↪︎【7】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(E[x^2]\equiv\int^{\infty}_{-\infty}x^2p(x)dx\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;接下來只要把式【1】的Normal Distribution代入就可以得到它的Mean和Variance。&lt;/p&gt;
&lt;p&gt;在這之前，我們先來看一個重要的積分式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int^{\infty}_{-\infty}exp\{-a(x+b)^2\}dx=\sqrt{\frac{\pi}{a}}  \ \ ↪︎【8】
$$&lt;/div&gt;
&lt;p&gt;上述式子證明稍嫌複雜，有興趣的詳見&lt;a href="https://zh.wikipedia.org/wiki/高斯积分"&gt;維基百科的證明&lt;/a&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;先看看Normal Distribution是不是機率總和為1&lt;/p&gt;
&lt;p&gt;將式【1】做積分
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int^{\infty}_{-\infty}p_{normal}(x)dx=\frac{1}{\sqrt{2\pi}\sigma}\int^{\infty}_{-\infty}exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\}dx
$$&lt;/div&gt;
&lt;p&gt;
接下來令&lt;span class="math"&gt;\(a=\frac{1}{2\sigma^2}\)&lt;/span&gt;、&lt;span class="math"&gt;\(b=-\mu\)&lt;/span&gt;，此時可以套用式【8】，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int^{\infty}_{-\infty}p_{normal}(x)dx=\frac{1}{\sqrt{2\pi}\sigma}\sqrt{\frac{\pi}{\frac{1}{2\sigma^2}}}=1  \ \ ↪︎【9】
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;再來求其Mean
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{x\sim normal}[x]=\int^{\infty}_{-\infty}x\cdot p_{normal}(x)dx=\frac{1}{\sqrt{2\pi}\sigma}\int^{\infty}_{-\infty}x\cdot exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\}dx
$$&lt;/div&gt;
&lt;p&gt;
令&lt;span class="math"&gt;\(s=x-\mu\)&lt;/span&gt; 代入上式，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\frac{1}{\sqrt{2\pi}\sigma}\int^{\infty}_{-\infty}(s+\mu)\cdot exp\{{-\frac{1}{2\sigma^2}s^2}\}ds
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\frac{1}{\sqrt{2\pi}\sigma}[\int^{\infty}_{-\infty}s\cdot exp\{{-\frac{1}{2\sigma^2}s^2}\}ds+\int^{\infty}_{-\infty}\mu\cdot exp\{{-\frac{1}{2\sigma^2}s^2}\}ds]
$$&lt;/div&gt;
&lt;p&gt;上式的第一項必為0，因為&lt;span class="math"&gt;\(s\)&lt;/span&gt;對原點為奇對稱，而&lt;span class="math"&gt;\(exp\{{-\frac{1}{2\sigma^2}s^2}\}\)&lt;/span&gt;對原點為偶對稱，所以&lt;span class="math"&gt;\(s\cdot exp\{{-\frac{1}{2\sigma^2}s^2}\}\)&lt;/span&gt;為奇對稱，積分後會相互抵銷為0。接下來把第二項的&lt;span class="math"&gt;\(s\)&lt;/span&gt;還原回去，得&lt;/p&gt;
&lt;div class="math"&gt;$$
=\mu\int^{\infty}_{-\infty} \frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\}dx=\mu \int^{\infty}_{-\infty}p_{normal}(x)dx
$$&lt;/div&gt;
&lt;p&gt;將式【9】代入，最後得到
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{x\sim normal}[x]=\mu  \ \ ↪︎【10】
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;最後來算一下Variance
&lt;/p&gt;
&lt;div class="math"&gt;$$
Var_{x\sim normal}[x]=E_{x\sim normal}[x^2]-E_{x\sim normal}[x]^2  \ \ ↪︎【11】
$$&lt;/div&gt;
&lt;p&gt;
第一項
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{x\sim normal}[x^2]=\int^{\infty}_{-\infty}x^2p_{normal}(x)dx=\frac{1}{\sqrt{2\pi}\sigma}\int^{\infty}_{-\infty}x^2\cdot exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\}dx
$$&lt;/div&gt;
&lt;p&gt;
令&lt;span class="math"&gt;\(a=\frac{1}{2\sigma^2}\)&lt;/span&gt;、&lt;span class="math"&gt;\(s=x -\mu\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\sqrt{\frac{a}{\pi}}\int^{\infty}_{-\infty}(s+\mu)^2\cdot exp\{{-a \cdot s^2}\}ds
$$&lt;/div&gt;
&lt;p&gt;
展開
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\sqrt{\frac{a}{\pi}}[\int^{\infty}_{-\infty}s^2\cdot exp\{{-a \cdot s^2}\}ds+\int^{\infty}_{-\infty}2s\mu\cdot exp\{{-a \cdot s^2}\}ds+\int^{\infty}_{-\infty}\mu^2\cdot exp\{{-a \cdot s^2}\}ds]
$$&lt;/div&gt;
&lt;p&gt;
第二項的積分裡面是奇函數，所以第二項積分完的結果是0。第三項把&lt;span class="math"&gt;\(\mu^2\)&lt;/span&gt;提出去，積分的部分其實就是式【9】。得
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{x\sim normal}[x^2]=\sqrt{\frac{a}{\pi}}\int^{\infty}_{-\infty}s^2\cdot exp\{{-a \cdot s^2}\}ds+\mu^2
$$&lt;/div&gt;
&lt;p&gt;
接下來有點tricky，上式的第一項可看成一個微分形式
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\sqrt{\frac{a}{\pi}}\frac{-\partial}{\partial a}(\int^{\infty}_{-\infty} exp\{{-a \cdot s^2}\}ds)+\mu^2=\sqrt{\frac{a}{\pi}}\frac{-\partial}{\partial a}(\sqrt{\frac{\pi}{a}})+\mu^2=\frac{1}{2a}+\mu^2
$$&lt;/div&gt;
&lt;p&gt;
所以
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{x\sim normal}[x^2]=\sigma^2+\mu^2  \ \ ↪︎【12】
$$&lt;/div&gt;
&lt;p&gt;
將【12】和【10】代入【11】，可得
&lt;/p&gt;
&lt;div class="math"&gt;$$
Var_{x\sim normal}[x]=\sigma^2+\mu^2-(\mu)^2=\sigma^2  \ \ ↪︎【13】
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;所以未來當你看到Normal Distribution的公式時，應該能夠馬上看出他的Mean和Variance。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/DeepDL/IMG_gaussian_distribution.png" /&gt;&lt;/p&gt;
&lt;h3 id="normal-distribution_1"&gt;隨機誤差大都呈現Normal Distribution&lt;/h3&gt;
&lt;p&gt;雖然說並非所有的隨機分布都是Normal Distribution。例如有：適用於二元系統的Bernoulli Distribution；適用於計數系統的Poisson Distribution；適用於時間間隔的Gamma Distribution；...等等。&lt;/p&gt;
&lt;p&gt;但是大多數情況下，沒有特別的理由，隨機誤差會遵循Normal Distribution。&lt;/p&gt;
&lt;p&gt;接下來我要試著用中央極限定理來解釋這個現象。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;先從中央極限定理（Central Limit Theorm）開始講起&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Central Limit Theorm:&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(\{x_1,x_2,...,x_n\}\)&lt;/span&gt; be a random sample of size &lt;span class="math"&gt;\(n\)&lt;/span&gt; — that is, a sequence of independent and identically distributed (i.i.d.) random variables drawn from a distribution of expected value &lt;span class="math"&gt;\(E[x_i]=\mu\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(Var[x_i]=\sigma^2&amp;lt;\infty\)&lt;/span&gt;. Suppose we are interested in the sample average: &lt;span class="math"&gt;\(S_n=(x_1+x_2+...+x_n)/n\)&lt;/span&gt; , Then as &lt;span class="math"&gt;\(n\rightarrow \infty\)&lt;/span&gt; , &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; follows normal distribution &lt;span class="math"&gt;\(p_{normal}(\mu,(\frac{\sigma}{\sqrt{n}})^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也就是說，今天我們從一個任意分布 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 當中採樣 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 筆，這&lt;span class="math"&gt;\(n\)&lt;/span&gt;筆採樣的過程符合不互相影響彼此（independent）且都從同一分布而來（identically distributed），即 i.i.d.。&lt;/p&gt;
&lt;p&gt;而如果我們已知這個任意分布 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 的Mean &lt;span class="math"&gt;\(E_{x\sim p(x)}[x]=\mu\)&lt;/span&gt; 和 Variance &lt;span class="math"&gt;\(Var_{x\sim p(x)}[x]=\sigma^2&amp;lt;\infty\)&lt;/span&gt;，注意：&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 不一定需要是Normal Distribution才能算Mean和Variance。&lt;/p&gt;
&lt;p&gt;我們關注這&lt;span class="math"&gt;\(n\)&lt;/span&gt;筆採樣的平均值，計作&lt;span class="math"&gt;\(S_n\)&lt;/span&gt;，統計學告訴我們：
&lt;/p&gt;
&lt;div class="math"&gt;$$
E[S_n]=\mu  \ \ ↪︎【14】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
Var[S_n]=\frac{\sigma^2}{n}  \ \ ↪︎【15】
$$&lt;/div&gt;
&lt;p&gt;當&lt;span class="math"&gt;\(n=1\)&lt;/span&gt;時，&lt;span class="math"&gt;\(S_n\)&lt;/span&gt;的分布其實就是 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 的分布，當然Mean和Variance會和原分布 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 一模一樣。&lt;/p&gt;
&lt;p&gt;中央極限定理告訴我們如果今天採樣數量 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 增加到一定的量，&lt;span class="math"&gt;\(S_n\)&lt;/span&gt;的分布會趨近於Normal Distribution，也就是說隨著 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 的增加，&lt;span class="math"&gt;\(S_n\)&lt;/span&gt; 的分布會從 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 變成接近 &lt;span class="math"&gt;\(p_{normal}(\mu,(\frac{\sigma}{\sqrt{n}})^2)\)&lt;/span&gt; 分布。&lt;/p&gt;
&lt;p&gt;眼見為憑，接下來我要透過&lt;a href="https://seeing-theory.brown.edu/probability-distributions/index.html"&gt;Seeing-Theory&lt;/a&gt;這個網站來Demo一下中央極限定理，&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/DeepDL/clt_demo.gif" /&gt;&lt;/p&gt;
&lt;p&gt;給定一個採樣分布（黃色），每次採樣 &lt;span class="math"&gt;\(n=15\)&lt;/span&gt; 作平均並打點記下來，經過多次的操作就可以得到累積分布圖（紅色），而因為 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 夠大，所以這個累積分布圖會逼近於Normal Distribution。&lt;/p&gt;
&lt;p&gt;再來看看採樣平均的累積分布怎麼隨著 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 增加而改變&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/DeepDL/clt_demo_2.gif" /&gt;&lt;/p&gt;
&lt;p&gt;觀察上面的動圖，會發現 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 越大，Variance越來越小，而且分布狀況也越接近Normal Distribution。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;好！講了這麼多，那這跟隨機誤差有什麼關係呢？&lt;/p&gt;
&lt;p&gt;中央極限定理告訴我們只要從一個固定的採樣分布當中作夠多的樣本平均，其分布會接近Normal Distribution。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;而自然界的巨觀現象往往是源自於微觀現象的累積，我們量測的物理量常常來自於多個微小貢獻疊加而成，而不管這些微小貢獻本身的分布狀況如何，其巨觀的物理量因為中央極限定理而成為Normal Distribution，這也是為什麼「隨機誤差大都呈現Normal Distribution」的原因。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;舉例，電壓就是反應電荷疊加的物理量，用普通方法我們是很難量到單一電荷的，所以我們能量到的已經是疊加過後的結果，也因此電壓的隨機分布才呈現Normal Distribution。&lt;/p&gt;
&lt;p&gt;所以，如果今天你沒有特別的理由，假設Normal Distribution往往是最接近真實的，這是第一個理由能讓你無腦使用Normal Distribution，還有第二個理由我們接下去討論。&lt;/p&gt;
&lt;h3 id="normal-distribution_2"&gt;Normal Distribution是所有機率分布當中假設最少的&lt;/h3&gt;
&lt;p&gt;首先來看一段從&lt;a href="https://www.deeplearningbook.org"&gt;Goodfellow的書&lt;/a&gt;中的一段話，這段話清楚的告訴我們選擇用Normal Distribution的理由&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;First, many distributions we wish to model are truly close to being normal distributions. The central limit theorem shows that the sum of many independent random variables is approximately normally distributed. This means that in practice, many complicated systems can be modeled successfully as normally distributed noise, even if the system can be decomposed into parts with more structured behavior.&lt;/p&gt;
&lt;p&gt;Second, out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers. We can thus think of the normal distribution as being the one that inserts the least amount of prior knowledge into a model. &lt;/p&gt;
&lt;p&gt;-- from: Deep Learning 3.9.3&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上述的第一段就是剛剛我們討論的那些，而我們接下去要討論的就是第二段的內容。&lt;/p&gt;
&lt;p&gt;總結一下Goodfellow在第二段說的內容：&lt;/p&gt;
&lt;p&gt;在所有有相同Variance的分布當中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normal Distribution是隨機性最大的分布&lt;/li&gt;
&lt;li&gt;Normal Distribution是最少先驗知識（Prior Knowledge）假設的&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;要討論這個問題，我們必須先了解一些資訊理論。&lt;/p&gt;
&lt;p&gt;在資訊理論當中，我們常常使用Entropy（熵）來衡量隨機性，Entropy的定義為
&lt;/p&gt;
&lt;div class="math"&gt;$$
H\equiv E[-ln\ p(x)]  \ \ ↪︎【16】
$$&lt;/div&gt;
&lt;p&gt;
因為篇幅的緣故，Entropy的完整介紹會在&lt;a href="/deep-dl_2.html"&gt;接下來的文章中介紹&lt;/a&gt;，請大家先把這個定義背起來。&lt;/p&gt;
&lt;p&gt;透過式【2】可以將Entropy寫成連續形式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H_{x\sim p(x)}=E[-ln\ p(x)]=- \int^{\infty}_{-\infty}ln\ p(x)\cdot p(x)dx  \ \ ↪︎【17】
$$&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;接下來將Normal Distribution 【1】式代入【17】
&lt;/p&gt;
&lt;div class="math"&gt;$$
H_{x\sim normal}=- \int^{\infty}_{-\infty}p_{normal}(x)\cdot ln\ p_{normal}(x) dx
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=- \int^{\infty}_{-\infty}\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\} \cdot [ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^2}(x-\mu)^2]dx
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=ln(\sqrt{2\pi}\sigma)+ \frac{1}{2\sigma^2} \int^{\infty}_{-\infty}\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\} \cdot (x-\mu)^2dx
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=ln(\sqrt{2\pi}\sigma)+ \frac{1}{2\sigma^2} E_{x\sim normal}[(x-\mu)^2]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=ln(\sqrt{2\pi}\sigma)+ \frac{1}{2\sigma^2} Var_{x\sim normal}[x]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
H_{x\sim normal}=ln(\sqrt{2\pi}\sigma)+\frac{1}{2}=\frac{1}{2}ln(2\pi e\sigma^2)  \ \ ↪︎【18】
$$&lt;/div&gt;
&lt;p&gt;我們因此得到了Normal Distribution 的Entropy，而這個Entropy是所有有相同Variance的分布當中最大的。緊接著來證明這件事。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;回到式【17】，我們可以列出一個有限制條件的優化問題：&lt;/p&gt;
&lt;p&gt;在給定：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\int p(x)dx=1  \ \ ↪︎【19】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(E[x]=\mu  \ \ ↪︎【20】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(Var[x]=\sigma^2  \ \ ↪︎【21】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;的情況下試圖找到一個 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 可以使 Entropy &lt;span class="math"&gt;\(H\)&lt;/span&gt; 最大：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p^*(x) = argmax_{p(x)}\ H_{x\sim p(x)}=argmin_{p(x)}\int^{\infty}_{-\infty}ln\ p(x)\cdot p(x)dx  \ \ ↪︎【22】
$$&lt;/div&gt;
&lt;p&gt;
引入&lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;Lagrange Multiplier&lt;/a&gt;結合【19】,【20】,【21】,【22】：
&lt;/p&gt;
&lt;div class="math"&gt;$$
L=\int^{\infty}_{-\infty}ln\ p(x)\cdot p(x)dx-\lambda_1 (\int^{\infty}_{-\infty}p(x)dx-1)-\lambda_2(\int^{\infty}_{-\infty}x\cdot p(x)dx-\mu)-\lambda_3(\int^{\infty}_{-\infty}(x-\mu)^2\cdot p(x)dx-\sigma^2)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\int^{\infty}_{-\infty}[-\lambda_1 p(x)-\lambda_2 p(x)x-\lambda_3 p(x)(x-\mu)^2+p(x)ln(p(x))]dx+\lambda_1 +\mu\lambda_2+\sigma^2\lambda_3  \ \ ↪︎【23】
$$&lt;/div&gt;
&lt;p&gt;接下來對【23】微分求極值
&lt;/p&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial L}{\partial p(x)}|_{p^*(x)}=\int^{\infty}_{-\infty}[-\lambda_1-\lambda_2 x-\lambda_3 (x-\mu)^2+ln(p^*(x))+1]dx  \ \ ↪︎【24】
$$&lt;/div&gt;
&lt;p&gt;
所以
&lt;/p&gt;
&lt;div class="math"&gt;$$
p^*(x)=exp\{\lambda_1+\lambda_2 x+\lambda_3 (x-\mu)^2-1\}  \ \ ↪︎【25】
$$&lt;/div&gt;
&lt;p&gt;上面還有三個未知變數 &lt;span class="math"&gt;\(\lambda_1\)&lt;/span&gt;, &lt;span class="math"&gt;\(\lambda_2\)&lt;/span&gt;, &lt;span class="math"&gt;\(\lambda_3\)&lt;/span&gt; ，這些變數必須滿足Constraints，所以將【25】代入 【19】,【20】,【21】得三個方程求解三個變數，可得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda_1=1-ln(\sqrt{2\pi}\sigma);\ \lambda_2=0;\ \lambda_3=-\frac{1}{2\sigma^2}  \ \ ↪︎【26】
$$&lt;/div&gt;
&lt;p&gt;
最後將【26】回代【25】就會得到剛剛好是Normal Distribution，🥳&lt;/p&gt;
&lt;p&gt;因此這邊我們證明了：&lt;strong&gt;在給定Mean和Variance下，Normal Distribution為所有分布當中Entropy最大的。這也同時意味著，Normal Distribution是隨機性最大的，Normal Distribution是額外假設最少的。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="back-to-the-question"&gt;Back to the Question&lt;/h3&gt;
&lt;p&gt;這一講也走到尾聲了，接下來我們已經有能力回答一開始問的問題，在回答問題之前我們複習一下剛剛學到了什麼。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中央極限定理告訴我們：如果今天採樣數量 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 增加到一定的量，&lt;span class="math"&gt;\(S_n\)&lt;/span&gt;的分布會趨近於Normal Distribution&lt;/li&gt;
&lt;li&gt;自然界的巨觀現象往往是源自於微觀現象的累積，我們量測的物理量常常來自於多個微小貢獻疊加而成，而不管這些微小貢獻本身的分布狀況如何，其巨觀的物理量因為中央極限定理而成為Normal Distribution，這也是為什麼「隨機誤差大都呈現Normal Distribution」的原因&lt;/li&gt;
&lt;li&gt;Entropy是衡量隨機性的指標，定義為：&lt;span class="math"&gt;\(H\equiv E[-ln\ p(x)]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;在給定Mean和Variance下，Normal Distribution為所有分布當中Entropy最大的。這也同時意味著，Normal Distribution是隨機性最大的，Normal Distribution是額外假設最少的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此我們有兩個原因去使用Normal Distribution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果沒有特別理由，請假設隨機誤差為Normal Distribution，因為自然界的隨機誤差大都呈現Normal Distribution&lt;/li&gt;
&lt;li&gt;如果想要人為假設一個分布，請優先選擇Normal Distribution，因為它是包含最少先驗知識（Prior Knowledge）的分布&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最後來逐一回答剛開始的問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q：為什麼Normal Distribution通常作為雜訊的分布？&lt;ul&gt;
&lt;li&gt;A：因為自然界的隨機誤差大都呈現Normal Distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：為何在DL（deep learning），參數的初始化要用Normal Distribution？&lt;ul&gt;
&lt;li&gt;A：因為它是包含最少先驗知識（Prior Knowledge）的分布&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：為何在Bayesian公式裡常常會使用Normal Distribution當作Prior Probability？&lt;ul&gt;
&lt;li&gt;A：因為它是包含最少先驗知識（Prior Knowledge）的分布&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：在使用GAN（generative adversarial network）時，為什麼給予的輸入要假設Normal Distribution？&lt;ul&gt;
&lt;li&gt;A：因為它是包含最少先驗知識（Prior Knowledge）的分布&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="reference"&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://seeing-theory.brown.edu/probability-distributions/index.html"&gt;Seeing Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.deeplearningbook.org"&gt;Ian Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Christopher Bishop. Pattern Recognition and Machine Learning. 2006.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;[此文章為原創文章，轉載前請註明文章來源]&lt;/em&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="剖析深度學習"></category></entry><entry><title>[入埃及記] Day9-10: 開羅【埃及博物館、哈利利市集】</title><link href="https://ycc.idv.tw/egypt-travel_8.html" rel="alternate"></link><published>2020-02-13T12:00:00+08:00</published><updated>2020-02-13T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-13:/egypt-travel_8.html</id><summary type="html"></summary><content type="html">&lt;p&gt;&lt;img alt="map" src="/media/EgyptTravel/egypt_map_d9.png" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Day9-10 行程：開羅【埃及博物館、哈利利市集】&amp;gt; ✈️(3.5 hrs) &amp;gt; 杜拜 &amp;gt; ✈️(8 hrs) &amp;gt; 桃園 &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="egyptian-museum"&gt;埃及博物館 Egyptian Museum&lt;/h3&gt;
&lt;p&gt;館內收集的埃及古文物，及手工藝品居世界首位，多達十萬件以上，以考古為主軸的博物館，沒有它者可以與之匹敵，縱使以每分鐘參觀一件展品的速度，也得花費9個月才足以參觀完畢。&lt;/p&gt;
&lt;p&gt;很幸運的，三個月前已經開放遊客用手機拍照，可以不用額外收錢。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3401" src="/media/EgyptTravel/IMG_3401.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
埃及博物館本身就有120年的歷史，裡面展示的文物更是古老
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3402" src="/media/EgyptTravel/IMG_3402.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
門碑上有三個神祉，中間的是Isis，兩側則是希臘、羅馬的女神
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3405" src="/media/EgyptTravel/IMG_3405.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3407" src="/media/EgyptTravel/IMG_3407.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
埃及博物館門票
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;首先Hassan直接帶我們到二樓，先參觀新王國第18王朝的一位部長的文物。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3414" src="/media/EgyptTravel/IMG_3414.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
部長和他老婆的面罩
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3518" src="/media/EgyptTravel/IMG_3518.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
部長的馬車
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3420" src="/media/EgyptTravel/IMG_3420.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3421" src="/media/EgyptTravel/IMG_3421.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
部長的金棺
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3424" src="/media/EgyptTravel/IMG_3424.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
部長的木乃伊
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3427" src="/media/EgyptTravel/IMG_3427.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
部長妻子的金棺
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3426" src="/media/EgyptTravel/IMG_3426.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
部長妻子的木乃伊
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3429" src="/media/EgyptTravel/IMG_3429.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3430" src="/media/EgyptTravel/IMG_3430.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3433" src="/media/EgyptTravel/IMG_3433.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
4個分別存放死者的胃、腸、肝和肺的卡諾皮克罐（Canopic Jars）
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;接下來參觀同樣是在二樓，大名鼎鼎的圖坦卡門，圖坦卡門是一位在位時間很短的法老王，但是他的墓穴出土的文物仍然相當驚人，可以想知如果其他法老王的陪葬品沒有被盜竊，例如：拉美西斯二世的陪葬品，一定是更驚人的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3437" src="/media/EgyptTravel/IMG_3437.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門墓室照片。陪葬品不是用展示的方式擺，是用堆放的方式
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3514" src="/media/EgyptTravel/IMG_3514.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門的ka雕像。ka可以解釋成靈魂，圖坦卡蒙當然不是黑人，塗黑色意味著死亡
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3439" src="/media/EgyptTravel/IMG_3439.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門的黃金王座。畫中的是圖坦卡門和他的妻子安克蘇拉姆
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3441" src="/media/EgyptTravel/IMG_3441.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門的黃金王座側面
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3442" src="/media/EgyptTravel/IMG_3442.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門的黃金王座背面
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3446" src="/media/EgyptTravel/IMG_3446.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3447" src="/media/EgyptTravel/IMG_3447.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3448" src="/media/EgyptTravel/IMG_3448.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3449" src="/media/EgyptTravel/IMG_3449.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3450" src="/media/EgyptTravel/IMG_3450.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
阿努比斯，墓室的守護神
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3452" src="/media/EgyptTravel/IMG_3452.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3456" src="/media/EgyptTravel/IMG_3456.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門的彩繪箱，刻畫著圖坦卡門帶軍作戰的英姿（不代表真的有打過戰）
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3457" src="/media/EgyptTravel/IMG_3457.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3461" src="/media/EgyptTravel/IMG_3461.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3463" src="/media/EgyptTravel/IMG_3463.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3467" src="/media/EgyptTravel/IMG_3467.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3469" src="/media/EgyptTravel/IMG_3469.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門的折疊床
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;接下來進去一處是不能拍照的地方，裡面有擺圖坦卡門的黃金面具和外棺。&lt;/p&gt;
&lt;p&gt;之前有說過法老王的棺木共七層，純金的內棺包進鍍金的棺木，鍍金的棺木放入花崗岩的棺木，這三層棺木再放進去四層的盒子內，如下圖。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3513" src="/media/EgyptTravel/IMG_3513.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門的第四層盒子
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3512" src="/media/EgyptTravel/IMG_3512.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門的第五層盒子
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3511" src="/media/EgyptTravel/IMG_3511.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門的第六層盒子
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3510" src="/media/EgyptTravel/IMG_3510.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門的第七層（最外層）盒子
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;講到圖坦卡門，就一定要提一下他的爸爸—阿肯納頓（Akenaten）。&lt;/p&gt;
&lt;p&gt;圖坦卡門的阿公 阿曼侯提普三世（Amenhotep III）修建了路克索神殿，是阿蒙神的忠實信徒，但是他的繼承人 阿肯納頓（Akenaten）並未繼承父業，他掀起宗教改革，排除阿蒙神獨尊太陽神Aten，屏棄傳統的多神信仰，倡導一神信仰，並且遷離古都底比斯。&lt;/p&gt;
&lt;p&gt;有一說 阿肯納頓 之所以這樣做，跟其成長過程受父母冷落有關，因此他上任時屏棄父母親的信仰，並且遷都離開傷心地。另外一說，是因為意識到了阿蒙神的祭司地位高漲，威脅到法老的地位，阿肯納頓想藉由宗教改革進而瓦解祭司階級制度，自立成為唯一的祭司，把王權和神權都集中於一手。&lt;/p&gt;
&lt;p&gt;但是後來 阿肯納頓 對宗教的狂熱，使他無視於嚴重的經濟衰退，更疏於聯繫同盟，在位17年就讓原本鼎盛的埃及開始走下坡，再加上百姓無法接受拋棄傳統信仰，民怨沸騰。因此，阿肯納頓 的兒子 圖坦卡門 繼任後重拾傳統，遷都回底比斯、重修祭司階級制度&lt;/p&gt;
&lt;p&gt;有此看來，阿蒙神的地位絲毫未受阿肯納頓宗教改革的影響，反而日益強大，而阿蒙神的祭司也如願的達到權傾天下，最後釀成祭司干政、篡奪王位，導致埃及走入混亂的黑暗時期，直到亞歷山大大帝揮軍而下，埃及才逐漸恢復平定。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3505" src="/media/EgyptTravel/IMG_3505.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
阿肯納頓（Akenaten）雕像
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3506" src="/media/EgyptTravel/IMG_3506.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
皇室家族石碑。阿肯納頓（Akenaten）崇拜太陽神Aten，那個有很多手的太陽就是Aten的形象
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;接下來我們回頭參觀一樓。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3477" src="/media/EgyptTravel/IMG_3477.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3478" src="/media/EgyptTravel/IMG_3478.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
女法老 哈賽普蘇的獅身人面像
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3482" src="/media/EgyptTravel/IMG_3482.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
女法老 哈賽普蘇的頭像
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3481" src="/media/EgyptTravel/IMG_3481.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3483" src="/media/EgyptTravel/IMG_3483.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
中王國的一位法老的雕像。辨認中王國法老的雕像的方法是臉看起來不開心，因為中王國非常的亂
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3484" src="/media/EgyptTravel/IMG_3484.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉和闐 ＆ 諾福蕾 雕像。拉和闐不是法老王，所以其雕像會比較接近真人，兩個雕像的眼睛是用水晶做的
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3486" src="/media/EgyptTravel/IMG_3486.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
古埃及最古老的彩色壁畫，有5000年歷史
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3487" src="/media/EgyptTravel/IMG_3487.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
書記雕像，埃鎊200元上的圖正是用這個
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3489" src="/media/EgyptTravel/IMG_3489.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
卡夫拉雕像
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3490" src="/media/EgyptTravel/IMG_3490.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
卡夫拉雕像背面有一隻老鷹守護著法老
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3491" src="/media/EgyptTravel/IMG_3491.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
平民雕像
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3493" src="/media/EgyptTravel/IMG_3493.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
孟卡拉在中間，兩位女神保護環抱著法老，意味著守護
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3494" src="/media/EgyptTravel/IMG_3494.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
納麥爾色盤正面。此浮雕記載著第一位法老王統一上下埃及
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3495" src="/media/EgyptTravel/IMG_3495.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
納麥爾色盤背面。此浮雕記載著第一位法老王統一上下埃及
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3497" src="/media/EgyptTravel/IMG_3497.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Zoser雕像。階梯金字塔是為他蓋的，盜墓人挖走他的水晶眼睛，破壞嚴重
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3504" src="/media/EgyptTravel/IMG_3504.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3507" src="/media/EgyptTravel/IMG_3507.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3508" src="/media/EgyptTravel/IMG_3508.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
娜芙蒂蒂未完成的雕像
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3524" src="/media/EgyptTravel/IMG_3524.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
我最喜歡的雕像
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3526" src="/media/EgyptTravel/IMG_3526.JPG" /&gt;&lt;/p&gt;
&lt;h3 id="khan-el-khalili"&gt;哈利利市集 Khan el-Khalili&lt;/h3&gt;
&lt;p&gt;&lt;img alt="IMG_3537" src="/media/EgyptTravel/IMG_3537.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3538" src="/media/EgyptTravel/IMG_3538.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3540" src="/media/EgyptTravel/IMG_3540.jpg" /&gt;&lt;/p&gt;</content><category term="Life"></category><category term="遊記"></category><category term="埃及"></category></entry><entry><title>[入埃及記] Day8: 開羅Cairo【吉薩金字塔群、人面獅身像、香精專賣店、紙莎草專賣店】</title><link href="https://ycc.idv.tw/egypt-travel_7.html" rel="alternate"></link><published>2020-02-12T12:00:00+08:00</published><updated>2020-02-12T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-12:/egypt-travel_7.html</id><summary type="html"></summary><content type="html">&lt;p&gt;&lt;img alt="map" src="/media/EgyptTravel/egypt_map_d8.png" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Day8 行程：開羅Cairo【吉薩金字塔群、人面獅身像、香精專賣店、紙莎草專賣店】 &amp;gt; Conrad Cairo Hotel @開羅😴&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="IMG_3391" src="/media/EgyptTravel/IMG_3391.JPG" /&gt;
&lt;center&gt;&lt;small&gt;
Conrad Cairo Hotel 
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;在前往吉薩金字塔群的路途中，Hassan叫我們看沿途的房屋，他問：為什麼很多房子都蓋到一半，一推房子還有鋼筋裸露？這些有住人嗎？&lt;/p&gt;
&lt;p&gt;Hassan感慨的說這塊地幾乎沒有人管，埃及的高官和有錢人都住在新開羅，他們只管新開羅好還是不好，開羅的其他地方就放給他爛，但是稅金當然要照收，在這塊土地的窮人，多半沒錢買房子但是有地，所以他們就在自己的土地上蓋房子，蓋著蓋著有錢就繼續蓋，沒錢就擱著，反正是自己要住的沒有要賣，外部的美觀也不是重點，如果家人結婚了需要房間，就再往上蓋一層。另外還有一個關鍵的原因，因為房屋的稅金重，所以窮人寧可不要蓋完，因為蓋完就一定會被課稅，所以就出現了這樣的奇景。&lt;/p&gt;
&lt;p&gt;但Hassan說雖然外面看起來這樣醜醜的，但是埃及人的內部會打點的乾乾淨淨的，因為埃及人會有一個想法，外面是別人的，不是自己的，他覺得這種觀念很不好。&lt;/p&gt;
&lt;p&gt;也有經過一座蓋一半的博物館，這個博物館比現有的博物館大，結果工程延宕了十年之久，其中的貪污腐敗相當嚴重。&lt;/p&gt;
&lt;h3 id="_1"&gt;吉薩金字塔群&lt;/h3&gt;
&lt;p&gt;金字塔為法老王的陵寢，從法老一登基就開始建造自己的墳墓，此區共發現八十多個金字塔，經過時間的流逝，許多都已坍塌。而最著名的三大金字塔是建於埃及古王國第四王朝，由子孫三代古夫、卡夫拉、曼卡拉三位法老王所建造的。&lt;/p&gt;
&lt;p&gt;分別由大到小為  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;古夫大金字塔（Great Pyramid of Khufu）&lt;/li&gt;
&lt;li&gt;卡夫拉金字塔（Pyramid of Khafra）&lt;/li&gt;
&lt;li&gt;孟卡拉金字塔（Pyramid of Menkaure）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這三座金字塔的前面都有用花崗岩造的廟，金字塔前的第一座稱為祭廟（或稱靈殿），祭廟很接近金字塔，而離祭廟稍遠的前方會有谷廟（或稱河谷神殿）。法老王死後如我們之前說的就要做成木乃伊，做木乃伊和曝曬太陽的地方通通在谷廟進行，好了之後會將法老的木乃伊送到祭廟，由大祭師祭奉兩個小時，再送入金字塔裡並封關，不管是祭廟和谷廟都僅僅使用一次而已。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/pyramids_of_giza_map.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
cite from: 埃及 / 墨刻編輯部
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;其實吉薩金字塔群並非一蹴可及，它是經過長久的實驗與技術累積才有的豐富結果。&lt;/p&gt;
&lt;p&gt;故事從古王國第三王朝的法老王Zoser開始說起，在他的時期，任命建築師Imhotep（印何闐）開始建立金字塔。古埃及人相信死後法老能到達太陽神Ra那裡，所以金字塔依照太陽由雲間穿透的形狀建成，完成了世界上第一個金字塔—階梯金字塔。&lt;/p&gt;
&lt;p&gt;到了古王國第四王朝的法老王Sneferu開始出現現在金字塔的雛形，Sneferu在位時間長，在多次失敗之後終於出現了第一座滑面金字塔—紅色金字塔。&lt;/p&gt;
&lt;p&gt;蓋第一座金字塔時，使用了55度的仰角，再蓋第一座的同時也在蓋第二座，同樣採用55度的仰角，結果第一座蓋到一半就崩掉了，因為採用這樣的角度上面的石頭太重，於是趕緊把第二座金字塔的角度中途改成仰角43度，採用內傾的結構，勉強用木材支撐，最後蓋出來就是一個彆扭的形狀，因此有彎曲金字塔之稱。&lt;/p&gt;
&lt;p&gt;有了之前的經驗，而且法老王還健在，所以又開始建立第三座金字塔，這次學乖了直接採用仰角43度，最後完美的完成了紅色金字塔，從階梯金字塔到紅色金字塔足足花了60年的實驗才建出今天所看到的金字塔。&lt;/p&gt;
&lt;p&gt;以古夫大金字塔為範例，我們來看看金字塔內部結構長什麼樣子&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/pyramid_khufu.png" /&gt;
&lt;center&gt;&lt;small&gt;
cite from: 埃及 / 墨刻編輯部
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;很重要的一點是減壓室和承材支撐的窄室的設計，都是為了減小金字塔天花板的重量，同時兼具支撐整個結構，這些都是實驗後的豐富成果。&lt;/p&gt;
&lt;p&gt;然後三座金字塔的內部都採用來自亞斯文的花崗岩，而外面則使用石灰岩，而原本的斜面是有壁畫，但是長期的風化已破壞殆盡。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3158" src="/media/EgyptTravel/IMG_3158.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
吉薩金字塔群門票
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;首先我們參觀的是古夫大金字塔，古夫大金字塔為139米高，下面的四方形邊長為220米，迄今已經有4500年歷史，當時用了20萬人，總共還蓋了21年才完工，蓋的方法如我們之前所說是用沙子做的斜坡來搬運石材。&lt;/p&gt;
&lt;p&gt;必須特別強調的，金字塔不是使用奴隸來建造的。考古學家在金字塔旁發現工人營地的遺址，遺址中有大量的魚、牛、羊骨，看起來吃得並不差，而且從工人墓地裡挖出的骨骼也檢測出頗高的蛋白質含量，少數工人甚至在遺骨上發現醫療手術的痕跡，並且也有工人們的請假紀錄被挖掘出來，總總證據證明興建金字塔的工人絕不是奴隸，專家推測除了一般雇用的民工外，也有不少趁尼羅河氾濫的空檔投入工事的農民，建金字塔以展示對法老王的崇拜，也算是一種宗教活動吧！&lt;/p&gt;
&lt;p&gt;古夫大金字塔目前一共找到三個房間，不過應該還有其他房間沒被發現，因為法老的木乃伊還沒找到，也沒有在黑市流通。在前年，有找到一個很走不進去的小通道，科學家用機器人往下探了25米深，結果發現有一個有青銅把手的門，所有一切目前還在研究當中。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3164" src="/media/EgyptTravel/IMG_3164.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3160" src="/media/EgyptTravel/IMG_3160.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3159" src="/media/EgyptTravel/IMG_3159.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2705" src="/media/EgyptTravel/IMG_2705.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2726" src="/media/EgyptTravel/IMG_2726.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3197" src="/media/EgyptTravel/IMG_3197.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2738" src="/media/EgyptTravel/IMG_2738.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2744" src="/media/EgyptTravel/IMG_2744.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2775" src="/media/EgyptTravel/IMG_2775.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;接下來參觀第二大的金字塔—卡夫拉金字塔，卡夫拉金字塔為136米高，但是其所處的位置比較高，所以乍看之下是三個裡頭最高的。&lt;/p&gt;
&lt;p&gt;這個金字塔我們有進去它的墓室看看，往墓室的通道相當狹小，必須要彎腰才能前進，然後是先下坡再上坡才到墓室，其實裡面空空如也，連壁畫都沒有，而且空氣有點稀薄，走起來不舒服。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3198" src="/media/EgyptTravel/IMG_3198.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
卡夫拉金字塔門票
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3196" src="/media/EgyptTravel/IMG_3196.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3211" src="/media/EgyptTravel/IMG_3211.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
往墓室的通道
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3215" src="/media/EgyptTravel/IMG_3215.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3220" src="/media/EgyptTravel/IMG_3220.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2779" src="/media/EgyptTravel/IMG_2779-2.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;接下來我們去一個可以同時拍到古夫金字塔、卡夫拉金字塔和孟卡拉金字塔的地方。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3253" src="/media/EgyptTravel/IMG_3253.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3259" src="/media/EgyptTravel/IMG_3259.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3264" src="/media/EgyptTravel/IMG_3264.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3268" src="/media/EgyptTravel/IMG_3268.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3283" src="/media/EgyptTravel/IMG_3283.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_4262" src="/media/EgyptTravel/IMG_4262.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_2"&gt;人面獅身像&lt;/h3&gt;
&lt;p&gt;又稱斯芬克斯(Sphinx)，是目前已知最古老的紀念雕像，位於卡夫拉金字塔前，外型為一個人頭配上獅子的身軀，長約73.5米、寬約6米、高約20.22米。&lt;/p&gt;
&lt;p&gt;人面獅身像原本是按照卡夫拉（Khafra）的相貌所造，是法老王形象的具體化，蹲踞在金字塔前守護著。&lt;/p&gt;
&lt;p&gt;其實人面獅身像在古埃及是很少見的，為什麼當初這裡會有這個雕像呢？&lt;/p&gt;
&lt;p&gt;當時卡夫拉每年會來檢查他的金字塔一次，要知道古埃及人相信墳墓比宮殿重要，因為他們相信復活，所以宮殿用石灰岩蓋一下就好，墳墓則是使用比較耐久的花崗岩（所以宮殿沒保留多少）。那時，卡夫拉的谷廟前面有一個尼羅河的支流，有一次卡夫拉巡視完畢非常滿意，準備從卡夫拉的谷廟前的尼羅河會去，結果他一看，不對勁！有一顆石灰岩擋到了他的金字塔，卡夫拉就不滿意，並且命令工人要處理，所以工人們就把這整塊石灰岩刻成卡夫拉形象的人面獅身像。&lt;/p&gt;
&lt;p&gt;因為石灰岩易風化，他的頭上原本戴有皇冠、額頭部分也有聖蛇浮雕、下顎處有鬍鬚，脖子也有項圈，但經過數千年的風吹雨打及人為破壞，皇冠與項圈已消失殆盡，而聖蛇浮雕脫落並保存於埃及博物館內，而鬍鬚脫落則被英國人帶去大英博物館裡頭保存，鼻子部分也遭到毀損了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3314" src="/media/EgyptTravel/IMG_3314.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
三個金字塔和人面獅身像
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3315" src="/media/EgyptTravel/IMG_3315.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
原本這邊有尼羅河的支流，在1971後因為蓋水壩，水量減少，支流從六支變成剩兩支
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3321" src="/media/EgyptTravel/IMG_3321.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3322" src="/media/EgyptTravel/IMG_3322.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2814" src="/media/EgyptTravel/IMG_2814.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_4348" src="/media/EgyptTravel/IMG_4348.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3340" src="/media/EgyptTravel/IMG_3340.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
中午餐廳
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="_3"&gt;香精專賣店&lt;/h3&gt;
&lt;p&gt;古埃及人就會製作香精，並且將香精獻給諸神，而香精也成了埃及目前的重要產業。&lt;/p&gt;
&lt;p&gt;我們來到了Golden Eagle Papyrus，由店裡的人跟我們介紹各類香精。&lt;/p&gt;
&lt;p&gt;香精不是香水，香水是香精加酒精，因此香水的揮發比較快，味道撐比較不久，而香精很濃可以撐很久。也可以用香精加橄欖油來做精油，用於按摩。&lt;/p&gt;
&lt;p&gt;有用一種花做成的香精，主要是用來治療用的（不過我存疑），有玫瑰花香精用來美白、防曬、保溼；有紙莎草花香精用來治偏頭痛；有蓮花香精（不同於台灣的蓮花，這裡的蓮花很香）用來治皮膚病；薄荷香精用來治鼻塞；有綠蘋果香精用來豐胸 等等。&lt;/p&gt;
&lt;p&gt;有用多種花做成的香精，主要是用在作為香水的。舉例，香精對應的香水品牌味道（有點懷疑他們怎麼可能知道國際大廠的配方）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生命的鑰匙：Boss古龍水&lt;/li&gt;
&lt;li&gt;圖坦卡門：CKI&lt;/li&gt;
&lt;li&gt;阿拉伯香味：YSL&lt;/li&gt;
&lt;li&gt;埃及艷后：DIOR&lt;/li&gt;
&lt;li&gt;五個秘密：香奈兒五號&lt;/li&gt;
&lt;li&gt;埃及王后：萬保龍&lt;/li&gt;
&lt;li&gt;拉美西斯二世：香奈兒Bleu&lt;/li&gt;
&lt;li&gt;尼菲爾提提：亞斯蘭代&lt;/li&gt;
&lt;li&gt;伊西斯神：Gucci&lt;/li&gt;
&lt;li&gt;阿蒙拉神：阿曼尼&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="IMG_3361" src="/media/EgyptTravel/IMG_3361.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3363" src="/media/EgyptTravel/IMG_3363.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_4"&gt;紙莎草專賣店&lt;/h3&gt;
&lt;p&gt;埃及人是目前所知世界最早發明用紙的民族，他們利用叢生在尼羅河溼地中的蘆葦，經過繁複的製作程序，加上自然的礦石與植物顏料，創造出歷久不衰的藝術作品。古埃及人利用紙莎草記載歷史，透過保存完整的古埃及紙莎草畫，讓現代人有機會了解古埃及人的思想、宗教觀及智慧。如今在埃及博物館中還可看到眾多3千到5千年前，顏色依舊鮮明的紙莎草畫。&lt;/p&gt;
&lt;p&gt;Hassan為我們演示了紙莎草的製作過程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;原料是蘆葦草桿，尼羅河水底的蘆葦草桿是實心的&lt;/li&gt;
&lt;li&gt;將蘆葦草桿泡在尼羅河的水一個禮拜，使其膨脹&lt;/li&gt;
&lt;li&gt;接下來去皮留下內部的纖維，外部的皮也可以拿來做床和拖鞋&lt;/li&gt;
&lt;li&gt;將內部纖維切一片一片&lt;/li&gt;
&lt;li&gt;用槌子敲一片一片的內部纖維，再用桿麵棍桿過，徹底將裡面的汁擠出&lt;/li&gt;
&lt;li&gt;再放到水裡泡一個禮拜，整個會變黑&lt;/li&gt;
&lt;li&gt;將一片一片的交錯放置，用兩塊布壓住一個禮拜將水吸乾&lt;/li&gt;
&lt;li&gt;拿出來就是紙莎草了&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;辨識真偽的方法是看它是不是有紋路，如果是機器做的會沒有紋路或者紋路異常整齊，再來正統的紙莎草放在水裡是不會爛掉的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3365" src="/media/EgyptTravel/IMG_3365.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
蘆葦草桿
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3369" src="/media/EgyptTravel/IMG_3369.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
槌子敲一片一片的內部纖維
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3370" src="/media/EgyptTravel/IMG_3370.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
用桿麵棍桿過一片一片的內部纖維
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3371" src="/media/EgyptTravel/IMG_3371.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
將一片一片的內部纖維交錯放置
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3374" src="/media/EgyptTravel/IMG_3374.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
用兩塊布壓住一個禮拜將水吸乾
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3376" src="/media/EgyptTravel/IMG_3376.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3377" src="/media/EgyptTravel/IMG_3377.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
正統的紙莎草放在水裡是不會爛掉的
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3382" src="/media/EgyptTravel/IMG_3382.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
紙莎草畫
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;</content><category term="Life"></category><category term="遊記"></category><category term="埃及"></category></entry><entry><title>[入埃及記] Day7: 虎加達Hurghada【紅海渡假勝地】</title><link href="https://ycc.idv.tw/egypt-travel_6.html" rel="alternate"></link><published>2020-02-11T12:00:00+08:00</published><updated>2020-02-11T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-11:/egypt-travel_6.html</id><summary type="html"></summary><content type="html">&lt;p&gt;&lt;img alt="map" src="/media/EgyptTravel/egypt_map_d7.png" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Day7 行程： 虎加達Hurghada【紅海渡假勝地】 &amp;gt; 🚐(485KM) &amp;gt; 開羅 &amp;gt; Conrad Cairo Hotel @開羅😴&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="hurghada-marriott-beach-resort"&gt;Hurghada Marriott Beach Resort @虎加達&lt;/h3&gt;
&lt;p&gt;虎加達位於紅海旁，屬於亞熱帶區的沙漠氣候，夏天非常炎熱，冬天較和暖。所以許多歐洲人會在冬天時候來虎加達度假，可以享受沙灘、海水。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3065" src="/media/EgyptTravel/IMG_3065.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3066" src="/media/EgyptTravel/IMG_3066.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3067" src="/media/EgyptTravel/IMG_3067.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
驚人的甜點
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3068" src="/media/EgyptTravel/IMG_3068.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
驚人的甜點
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3069" src="/media/EgyptTravel/IMG_3069.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
夕陽西下
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;今天對我來說就是一個放鬆的行程，早上睡到自然醒，起來吃個早餐、寫個明信片，然後就去游紅海。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3075" src="/media/EgyptTravel/IMG_3075.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
早起的鳥兒
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3079" src="/media/EgyptTravel/IMG_3079.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3080" src="/media/EgyptTravel/IMG_3080.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3082" src="/media/EgyptTravel/IMG_3082.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3083" src="/media/EgyptTravel/IMG_3083.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3085" src="/media/EgyptTravel/IMG_3085.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;游紅海成就達成！紅海的水流不湍，在飯店外面就有一片沙灘連接紅海，換好泳裝就來海灘這裡準備游泳。&lt;/p&gt;
&lt;p&gt;剛下水會有點冷，浸到剩半身時真的冷到不行，此時的大招就是不顧一切的往前撲，然後就開始不斷的游，大概兩分鐘其實就不冷了，但水真的蠻鹹的，我游到一個比較深的地方發現了一些魚，想要潛下去近一點看魚，不過沒有負重海水還蠻難下潛的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3089" src="/media/EgyptTravel/IMG_3089.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3094" src="/media/EgyptTravel/IMG_3094.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3099" src="/media/EgyptTravel/IMG_3099.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3101" src="/media/EgyptTravel/IMG_3101.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_7775" src="/media/EgyptTravel/IMG_7775.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3105" src="/media/EgyptTravel/IMG_3105.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
仿金字塔
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3109" src="/media/EgyptTravel/IMG_3109.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3110" src="/media/EgyptTravel/IMG_3110.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3111" src="/media/EgyptTravel/IMG_3111.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_1"&gt;拉車回開羅&lt;/h3&gt;
&lt;p&gt;接下來下午就是長達六小時的拉車，旅行社的安排有一個缺點就是想要把所有客群想去的點全部囊括進去，所以會導致點跟點的距離很長，如果是自助的話，因為我比較喜歡古文明，我會去掉亞斯文水壩和虎加達，然後看古蹟的時間就可以拉長一點，此次看古蹟的時間感覺是少了一點，感覺每次都是匆匆忙忙的，不過有導遊講解能更快的了解古蹟的內容，而不是走馬看花，所以跟團就真的有好有壞，好的是有導遊、有安排，壞的是無法自己分配參觀的時間。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3129" src="/media/EgyptTravel/IMG_3129.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3130" src="/media/EgyptTravel/IMG_3130.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3132" src="/media/EgyptTravel/IMG_3132.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3135" src="/media/EgyptTravel/IMG_3135.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Hassan感慨的講述埃及為何衰敗，主要就是源於官僚制度的腐敗，埃及有運河、有石油、有觀光、有花崗岩，但是這些好處都被上面寡斷了。一直以來，埃及雖然有總統但卻不是代表民眾的，他們都是軍方選出來的，到了茉莉花革命，埃及人推翻證明，民選出一名教授當總統，不過體制仍然是腐敗的，軍方和司法不斷找他麻煩，最後只好草草下台，結果又回到由軍方指派的老樣子。而且Hassan說，這些話他只敢用中文說，他們國家和中國一樣沒有言論自由，亂講話可能會被消失。
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3136" src="/media/EgyptTravel/IMG_3136.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
椰棗乾
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3139" src="/media/EgyptTravel/IMG_3139.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
埃及在地料理
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3140" src="/media/EgyptTravel/IMG_3140.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
埃及在地料理
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3141" src="/media/EgyptTravel/IMG_3141.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
埃及在地料理
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3144" src="/media/EgyptTravel/IMG_3144.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Conrad Cairo Hotel
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3145" src="/media/EgyptTravel/IMG_3145.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Conrad Cairo Hotel
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;</content><category term="Life"></category><category term="遊記"></category><category term="埃及"></category></entry><entry><title>[入埃及記] Day6: 路克索【帝王谷、曼儂巨像】、虎加達</title><link href="https://ycc.idv.tw/egypt-travel_5.html" rel="alternate"></link><published>2020-02-10T12:00:00+08:00</published><updated>2020-02-10T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-10:/egypt-travel_5.html</id><summary type="html"></summary><content type="html">&lt;p&gt;&lt;img alt="map" src="/media/EgyptTravel/egypt_map_d6.png" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Day6 行程： Luxor路克索【帝王谷、曼儂巨像】&amp;gt;  🚐(335KM) &amp;gt; 虎加達Hurghada &amp;gt; Hurghada Marriott Beach Resort @虎加達😴&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="_1"&gt;帝王谷&lt;/h3&gt;
&lt;p&gt;大約西元前1539年到西元前1075年間，古埃及新王國時期18到20王朝各代法老的陵墓大都放在帝王谷，包含62個陵墓。&lt;/p&gt;
&lt;p&gt;帝王谷目前找到共62個法老王的陵寢，但是其中的61個都被盜墓者清空了，唯一只有圖坦卡門的墓保留完整的一切，為什麼盜墓人唯獨闕漏了這個呢？圖坦卡門登基時只有8歲，卻在18歲時過世，時間倉促來不及挖新的墓，所以將他放在一個別人的墓，這個墓的大小很小，而且還在其他人的墓的下方，非常不容易發現。歷經三千多年的時間雖然墓室被盜空了，但是牆上的壁畫顏色依舊清晰可見。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/kv9.png" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯六世（編號KV9）（cite from: https://describingegypt.com/tours/ramessesvi/kv9_entrance_corridor_b/0/-5/80）
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;如上圖，陵墓內部的設計基本結構都是從入口處一道密佈浮雕的長廊，通道底部的主墓室。這些壁畫的主題不外乎是節錄自『死亡之書』的內容，另外通常在到主墓室前會有一個前廳，停棺的房間外用花崗岩封死，裡面放滿法老王生前使用的東西，棺木上有著胡狼造型的阿努比斯神，祂是死者的守護神，內棺裡有4個分別存放死者的胃、腸、肝和肺的卡諾皮克罐（Canopic Jars）。&lt;/p&gt;
&lt;p&gt;古埃及很聰明都把陵墓放在沙漠中，包括120座金字塔和62座帝王谷陵墓，沙漠乾燥有助於保存壁畫，所以帝王谷裡的壁畫顏色都還很鮮豔，而且帝王谷也離尼羅河很遠，才能免於氾濫破壞。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2881" src="/media/EgyptTravel/IMG_2881.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
帝王谷模型
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2883" src="/media/EgyptTravel/IMG_2883.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
所有的墓室，KV是Kings Valley的縮寫。其中KV7就是大名鼎鼎的拉美西斯二世的墓室，但是因為他名氣實在太大，所以樹大招風，墓室幾乎破壞殆盡，壁畫也被盜墓者割下，所以KV7也就沒有開放了
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;因為古埃及人真心的相信死後復活，所以陵墓的設計都是圍繞著這個主題。&lt;/p&gt;
&lt;p&gt;要能復活需要有三個要素：靈魂、木乃伊和心臟，埃及人相信靈魂長得是「人頭鳥身」，當他飛回自己的陵墓找到身體後，就要開始唸『死亡之書』的內容，因為很重要，要唸三次，但是埃及人怕靈魂過了太久忘記『死亡之書』的內容，所以墓室會刻下『死亡之書』的內容，這樣不保險，如果壁畫被破壞怎麼辦？所以一個墓室一定不會只刻一次，一定是重複的刻相同的內容。&lt;/p&gt;
&lt;p&gt;在唸『死亡之書』的同時，靈魂會請太陽神Ra或阿努比思幫忙用生命之匙將木乃伊開口，好讓靈魂可以從嘴巴飛進去，於是死者就甦醒了，所以陵墓中一定會放一些吃的東西，以及他生前用過的東西，讓復活的死者使用。&lt;/p&gt;
&lt;p&gt;接下來還沒結束，復活之後就要前去天堂，在前往天堂的路要搭船，並且經歷12個障礙，一個小時一個，太陽神Ra和阿努比思會幫忙死者排除一路上的困難，在沿途會遇到很多的怪物。&lt;/p&gt;
&lt;p&gt;抵達天堂後，就要開始進行知名的「秤心儀式」，天秤兩端分別是審判者的心臟與正義女神 瑪特Maat 頭上的羽毛，胡狼頭人身的Anubis在調整天秤的精確度，朱鷺頭人身的Thoth（智慧之神）等著紀錄測量結果，審判者終極命運的兩種表現：心臟跟羽毛取得平衡，則說明死者生前積德，可享永生；若心臟較重，則心臟會被鱷魚頭獅身河馬腿的怪獸阿米特Ammit吞噬，死者便不能進入來世。&lt;/p&gt;
&lt;p&gt;注意喔！古埃及人沒有地獄的概念。&lt;/p&gt;
&lt;p&gt;因為真心相信復活，法老王在登基就會開始蓋自己的陵墓（生前契約！？），活越久就蓋越深，死後就把木乃伊和陪葬品放入陵墓當中，其實放東西的方法就像是堆倉庫一樣，因為陵墓不是為了展示用的，也沒打算要再打開。最後用土把陵墓的洞補起來就收工了。&lt;/p&gt;
&lt;p&gt;剛剛有提到活越久就蓋越深，大名鼎鼎的圖坦卡門在位不過十年，所以他的墓室很小，也沒什麼壁畫，而且所有的陪葬品都搬到埃及博物館了，另外還要額外收費，所以導遊不推薦我們去看圖坦卡門的墓室。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2885" src="/media/EgyptTravel/IMG_2885.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2889" src="/media/EgyptTravel/IMG_2889.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
據說這座谷地之所以會雀屏中選，是因為阿爾巴赫里山有金字塔型的山頂
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2890" src="/media/EgyptTravel/IMG_2890.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
帝王谷門票
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="kv8"&gt;KV8（麥倫普塔赫）&lt;/h3&gt;
&lt;p&gt;麥倫普塔赫為拉美西斯二世的兒子，你會覺得奇怪，拉美西斯二世的兒子怎麼不叫拉美西斯三世？其實古埃及的一世二世...並沒有繼承關係，只是歷史上有重複就一路排下去。法老王在登基可以自己選擇一個名字，並一定要和父親一樣，另外每在位多十年就會再多加一個名字，所以有些法老是有很多名字的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2893" src="/media/EgyptTravel/IMG_2893.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
KV8的透視圖。值得一提，看板上有三種文字，英文就是從左向右讀，阿拉伯文則是從右向左讀，那麼埃及文呢？埃及文是上下左右都可以，什麼！？重點是要看鳥的方向，像上面的鳥是朝左，所以是從右向左讀，麥倫普塔赫在位剛好10年所以你可以看到他有兩個名字
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2899" src="/media/EgyptTravel/IMG_2899.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2900" src="/media/EgyptTravel/IMG_2900.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
死亡之書
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2901" src="/media/EgyptTravel/IMG_2901.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
象徵靈魂的鳥，在天花板上朝內飛，意味著當回歸時靈魂會飛過這個走道
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2902" src="/media/EgyptTravel/IMG_2902.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2903" src="/media/EgyptTravel/IMG_2903.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
正義女神 Maat。會用她頭上的羽毛進行秤心儀式
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2904" src="/media/EgyptTravel/IMG_2904.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2905" src="/media/EgyptTravel/IMG_2905.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2906" src="/media/EgyptTravel/IMG_2906.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2908" src="/media/EgyptTravel/IMG_2908.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2915" src="/media/EgyptTravel/IMG_2915.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2917" src="/media/EgyptTravel/IMG_2917.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
花崗岩石棺
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2918" src="/media/EgyptTravel/IMG_2918.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Horus用生命之鑰替法老開口
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2919" src="/media/EgyptTravel/IMG_2919.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
天堂門內有冥王Osiris鎮守
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2921" src="/media/EgyptTravel/IMG_2921.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
通道之書
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2922" src="/media/EgyptTravel/IMG_2922.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
右一：朱鷺頭人身的Thoth（智慧之神）  右二：Anubis
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2923" src="/media/EgyptTravel/IMG_2923.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2924" src="/media/EgyptTravel/IMG_2924.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
往天堂路途上的怪物
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2925" src="/media/EgyptTravel/IMG_2925.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2926" src="/media/EgyptTravel/IMG_2926.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2927" src="/media/EgyptTravel/IMG_2927.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
冥王Osiris和法老王
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2928" src="/media/EgyptTravel/IMG_2928.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
旭日的太陽神 Khepri，旭日隱含著復活
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2929" src="/media/EgyptTravel/IMG_2929.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Ra-Horakhty用生命之鑰替法老開口
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2931" src="/media/EgyptTravel/IMG_2931.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="kv6"&gt;KV6（拉美西斯九世）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="IMG_2936" src="/media/EgyptTravel/IMG_2936.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
KV6的透視圖
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2937" src="/media/EgyptTravel/IMG_2937.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2939" src="/media/EgyptTravel/IMG_2939.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
太陽神Ra
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2940" src="/media/EgyptTravel/IMG_2940.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2942" src="/media/EgyptTravel/IMG_2942.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2944" src="/media/EgyptTravel/IMG_2944.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2945" src="/media/EgyptTravel/IMG_2945.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
天花板佈滿星空圖
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2946" src="/media/EgyptTravel/IMG_2946.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2947" src="/media/EgyptTravel/IMG_2947.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2948" src="/media/EgyptTravel/IMG_2948.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2949" src="/media/EgyptTravel/IMG_2949.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2950" src="/media/EgyptTravel/IMG_2950.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
眾神乘著太陽船
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2951" src="/media/EgyptTravel/IMG_2951.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2953" src="/media/EgyptTravel/IMG_2953.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯九世手托著正義之神Maat，面向冥王Osiris
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2954" src="/media/EgyptTravel/IMG_2954.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2955" src="/media/EgyptTravel/IMG_2955.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2956" src="/media/EgyptTravel/IMG_2956.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2959" src="/media/EgyptTravel/IMG_2959.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
墓室
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2962" src="/media/EgyptTravel/IMG_2962.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
墓室。墓室的天花板記載著天空之神Nut的故事
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2963" src="/media/EgyptTravel/IMG_2963.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
來世之書
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2964" src="/media/EgyptTravel/IMG_2964.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
來世之書的內容
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2965" src="/media/EgyptTravel/IMG_2965.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯九世的名字
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2966" src="/media/EgyptTravel/IMG_2966.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2967" src="/media/EgyptTravel/IMG_2967.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
壁畫的顏色很鮮豔
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2968" src="/media/EgyptTravel/IMG_2968.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
公羊神Khnum
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="kv11"&gt;KV11（拉美西斯三世）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="IMG_2971" src="/media/EgyptTravel/IMG_2971.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2973" src="/media/EgyptTravel/IMG_2973.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2974" src="/media/EgyptTravel/IMG_2974.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2976" src="/media/EgyptTravel/IMG_2976.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2977" src="/media/EgyptTravel/IMG_2977.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
天花板有整排Ra的祈禱文，再加上兩旁的藍色星空
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2980" src="/media/EgyptTravel/IMG_2980.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Ra的祈禱文
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2982" src="/media/EgyptTravel/IMG_2982.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2983" src="/media/EgyptTravel/IMG_2983.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2984" src="/media/EgyptTravel/IMG_2984.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2985" src="/media/EgyptTravel/IMG_2985.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Nephthys和Anubis是亡者的守護神
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2988" src="/media/EgyptTravel/IMG_2988.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯三世奉獻給尼羅河之神
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2990" src="/media/EgyptTravel/IMG_2990.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯三世奉獻給冥王Osiris
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2993" src="/media/EgyptTravel/IMG_2993.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2995" src="/media/EgyptTravel/IMG_2995.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2996" src="/media/EgyptTravel/IMG_2996.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2997" src="/media/EgyptTravel/IMG_2997.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2999" src="/media/EgyptTravel/IMG_2999.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
來世之書
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3001" src="/media/EgyptTravel/IMG_3001.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
來世之書，許多怪物關卡
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3002" src="/media/EgyptTravel/IMG_3002.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3003" src="/media/EgyptTravel/IMG_3003.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3004" src="/media/EgyptTravel/IMG_3004.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3005" src="/media/EgyptTravel/IMG_3005.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3007" src="/media/EgyptTravel/IMG_3007.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3008" src="/media/EgyptTravel/IMG_3008.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3009" src="/media/EgyptTravel/IMG_3009.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3010" src="/media/EgyptTravel/IMG_3010.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
這麼墓穴是蓋到一半，沒有完全蓋完
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3011" src="/media/EgyptTravel/IMG_3011.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3012" src="/media/EgyptTravel/IMG_3012.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3014" src="/media/EgyptTravel/IMG_3014.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3016" src="/media/EgyptTravel/IMG_3016.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3021" src="/media/EgyptTravel/IMG_3021.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3022" src="/media/EgyptTravel/IMG_3022.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_3030" src="/media/EgyptTravel/IMG_3030.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_2"&gt;曼儂巨像&lt;/h3&gt;
&lt;p&gt;&lt;img alt="IMG_3055" src="/media/EgyptTravel/IMG_3055.jpg" /&gt;&lt;/p&gt;</content><category term="Life"></category><category term="遊記"></category><category term="埃及"></category></entry><entry><title>[入埃及記] Day5: 艾得夫 Edfu【艾得夫神殿】、伊斯納水匣門、路克索【路克索神殿、卡納克神殿】</title><link href="https://ycc.idv.tw/egypt-travel_4.html" rel="alternate"></link><published>2020-02-09T12:00:00+08:00</published><updated>2020-02-09T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-09:/egypt-travel_4.html</id><summary type="html"></summary><content type="html">&lt;p&gt;&lt;img alt="map" src="/media/EgyptTravel/egypt_map_d5.png" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Day5 行程： 艾得夫 Edfu【艾得夫神殿】&amp;gt; 🛳 &amp;gt; 伊斯納水匣門  &amp;gt; 🛳 &amp;gt; 路克索Luxor【路克索神殿、卡納克神殿】&amp;gt; 🛳😴&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="_1"&gt;古埃及神話&lt;/h3&gt;
&lt;p&gt;在開始今天的旅途前，我來說說古埃及的神話。&lt;/p&gt;
&lt;p&gt;古埃及人是多神信仰，除了重要的神祉以外，每個地區都會有自己的守護神，總共有600多個神祉，所以想要知道每個神祉是不可能的，這邊我們只聊最常見的幾個。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/egypt_god.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
重要神祉關係圖 (cite from: https://travelm.tw/egypt-god/)
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4 id="_2"&gt;上圖的第一層&lt;/h4&gt;
&lt;p&gt;最上面的就是大名鼎鼎的太陽神Ra，是創世之神，也是所有神祉的源頭。而太陽神在每日不同時段有不同形象，旭日為Khepri，白天運行為Ra，夕陽為阿圖Atum，阿頓Aton則為太陽光輪。另外埃及人很喜歡讓神祉之間合體（大概就是三隻青眼白龍可以召喚究極青眼白龍的概念），所以和 荷魯斯Horus 結合成為 Ra-Horakhty、和 阿蒙神Amun 結合成為Amun-Ra  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Khepri
        - 形象是糞金龜，推糞動作像是運行的太陽
        - 旭日隱含著復活，因此木乃伊身上常放著俗稱聖甲蟲的赫普立護身符，放在心臟的位置，以防心臟在秤心儀式中說出不利於主人的供詞&lt;/li&gt;
&lt;li&gt;Aton&lt;ul&gt;
&lt;li&gt;阿蒙霍特普四世 Amenhotep IV 曾試圖在古埃及進行宗教改革，試圖把紛亂的古埃及信仰改成阿頓神獨尊的一神教，不過這場宗教改革最後以失敗告終。&lt;/li&gt;
&lt;li&gt;&lt;img alt="" src="/media/EgyptTravel/aton.png" /&gt;&lt;ul&gt;
&lt;li&gt;那個是手喔！你沒看錯&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="_3"&gt;上圖的第二層&lt;/h4&gt;
&lt;p&gt;Ra 的兒子、女兒則分別是風神和空氣之神 Shu、雨神和生育之神 Tefnut&lt;/p&gt;
&lt;h4 id="_4"&gt;上圖的第三層&lt;/h4&gt;
&lt;p&gt;Shu與Tefnut的兒子是大地之神Geb、女兒是天空女神Nuit，Geb和Nuit生下了四個孩子，但是也因為小兩口成天相親相愛，都荒廢了職責，於是他們的父親 Shu 介入其中拆散他們  &lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/geb_nut_shu.png" /&gt;
&lt;center&gt;&lt;small&gt;
cite from: https://en.m.wikipedia.org/wiki/File:Geb_and_Nut03.png
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4 id="_5"&gt;上圖的第四層&lt;/h4&gt;
&lt;p&gt;Geb和Nuit生下了四個孩子分別為俄賽里斯 Osiris、伊西絲 Isis、賽特 Seth、奈弗蒂斯 Nephthys  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Osiris 是冥王，源自於一個埃及著名的神話，Osiris有一個壞弟弟Seth，Seth設了一個局準備了一副美麗的棺材，並說要送給能躺進去的人，眾人都塞不進去，只有Osiris剛好吻合，Seth馬上封印棺材，並把它沈入了尼羅河想要溺死Osiris (埃及人相信這造成了尼羅河的氾濫)，不過Osiris偉大的妹妹兼妻子Isis及妹妹兼弟媳兼一夜情對象Nephthys終究是把他找了出來。可惜又被Seth發現後迅速地把屍體大卸十四塊藏到整個埃及的各角落。之後Isis又花了很多很多年才找回這些碎塊，並委託 阿努比思Anubis把這些屍塊用繃帶綁在一起用魔法將它復活 (木乃伊的原型)，但是可惜悲慘的是Osiris的那話兒沒找到，這也影響了Osiris 的神力，同時間兒子 荷魯斯Horus 奇蹟似的出生了（不要問我為什麼），Osiris就將王位傳給了他，自己跑到陰間當王去了。也因此，他後半生主要的工作內容就剩下執行人死後是否能得到永生的審判&lt;/li&gt;
&lt;li&gt;Seth: 風暴之神、沙漠之神、外國之神，是埃及有名的惡神，若在描述和荷魯斯大戰時則會是河馬的形象&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="_6"&gt;上圖的第五層&lt;/h4&gt;
&lt;p&gt;從左至右分別是Anubis、Horus和Hathor。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;阿努比思 Anubis: 胡狼頭人身的死神，木乃伊的創造者與守護者&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;荷魯斯 Horus: 法老守護神，王權的象徵，俄塞里斯Orisis和伊西絲Isis的兒子，鷹頭人身的形象。剛剛的故事繼續說下去，Horus繼承王位後展開了與Seth的鬥爭，終於奪回王位（艾得夫神殿的壁畫有畫這個故事），在大戰的過程中Horus的一隻眼睛被傷，後來這隻眼睛就被稱為「荷魯斯之眼」，被埃及人視為護身符，其實這就是遊戲王中千年積木中間的符號&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img alt="" src="/media/EgyptTravel/horus_eye.jpg" /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hathor: Horus的老婆，形象特徵是牛角加上太陽，牠代表喜悅、音樂、多產和幸福&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="_7"&gt;上圖的第六層&lt;/h4&gt;
&lt;p&gt;荷魯斯與Hathor的四子，通常會是木乃伊內臟罐子的形象，人形的 Imset 負責保護肝臟，胡狼頭得 Duamutef負責保護胃，狒狒頭的 Hapi負責保護肺，獵鷹頭的 Qebshenuf負責保護腸。&lt;/p&gt;
&lt;h4 id="_8"&gt;底比斯三神&lt;/h4&gt;
&lt;p&gt;阿蒙(Amun)、其妻子 姆特(Mut)和兒子為孔斯(Chons)
&lt;img alt="" src="/media/EgyptTravel/three_gods.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;阿蒙Amun：生殖之神、造物之神&lt;/li&gt;
&lt;li&gt;孔蘇：月亮之神&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="_9"&gt;死亡之書之秤心儀式&lt;/h4&gt;
&lt;p&gt;古埃及人相信死後會進行審判，審判的儀式稱為「秤心儀式」。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/book_of_dead.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;右邊坐在王位的是冥王Osiris，牠的後面站著分別是Isis和Nephthys，他正在進行死後的審判，天秤兩端分別是審判者的心臟與正義女神 瑪特Maat 頭上的羽毛，胡狼頭人身的Anubis在調整天秤的精確度，朱鷺頭人身的Thoth等著紀錄測量結果，審判者終極命運的兩種表現：心臟跟羽毛取得平衡，則說明死者生前積德，可享永生；若心臟較重，則心臟會被鱷魚頭獅身河馬腿的怪獸阿米特Ammit吞噬，死者便不能進入來世，上方有一排神祇陪審團，最左邊的就是太陽神Ra&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;瑪特Maat：正義女神，特徵是頭頂上有羽毛&lt;/li&gt;
&lt;li&gt;Anubis&lt;/li&gt;
&lt;li&gt;Thoth：智慧之神，特徵是朱鷺頭人身&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_10"&gt;搭馬車前往艾得夫神殿&lt;/h3&gt;
&lt;p&gt;早上又得早起，昨天晚上一邊吃晚餐一邊遊輪就在往艾得夫前進，所以早上五點就直接從碼頭坐馬車前進艾得夫神殿。&lt;/p&gt;
&lt;p&gt;艾得夫是希臘統治時期建立的城市，希臘人稱之為「太陽城」，曾經是宗教、商業中心，盛產陶器和甘蔗，而今天要參觀的艾得夫神殿也是在希臘統治時期所建立的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2644" src="/media/EgyptTravel/IMG_2644.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2647" src="/media/EgyptTravel/IMG_2647.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_11"&gt;艾得夫神殿&lt;/h3&gt;
&lt;p&gt;艾得夫神殿（Temple of Edfu）又名為荷魯斯神殿，花300多年時間建，目的是獻給正義化身－鷹神荷魯斯（Horus）。&lt;/p&gt;
&lt;p&gt;艾得夫神殿可以說是全埃及保存最完整的神殿建築，至今仍維持當年的原貌，浮雕內容更涵蓋古埃及崇拜的諸神傳說，保存完整。&lt;/p&gt;
&lt;p&gt;原因有兩點：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因為興建於地勢較高、離尼羅河較遠的地方，躲過了尼羅河洪水的侵害&lt;/li&gt;
&lt;li&gt;羅馬人在破壞這個古蹟時意外的保護了這個古蹟，羅馬人用石膏蓋住浮雕然後在上面做其他的壁畫，把石膏去除掉，還是保存了原本的浮雕&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;昨天Hassan已經有說過古埃及的標準神廟配置，今天參觀的這個艾得夫神殿因為比較少被破壞，所以根本就是教科書般的活教材。見下圖，從城牆外牆開始向內算起隔7層會到神龕的位置，而且依循著門越往內越小、地板越往內越高、屋頂越往內越低的原則，並且在最外面進去神殿會是一個前庭，前庭再進去就會是多柱廳。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2659" src="/media/EgyptTravel/IMG_2659.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
艾得夫神殿的平面圖
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;另外，第一多柱廳和第二多柱廳所有柱子家起剛好是24根，意味著一天24小時。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/temple_of_horus_map.jpeg" /&gt;
&lt;center&gt;&lt;small&gt;
cite from: 埃及 / 墨刻編輯部
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2652" src="/media/EgyptTravel/IMG_2652.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
艾得夫神殿門票
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2713" src="/media/EgyptTravel/IMG_2713.JPG" /&gt;
&lt;center&gt;&lt;small&gt;
艾得夫神殿全景圖
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2471" src="/media/EgyptTravel/IMG_2471.JPG" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2459" src="/media/EgyptTravel/IMG_2459.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
戴著上下埃及皇冠的Horus
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2467" src="/media/EgyptTravel/IMG_2467.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2668" src="/media/EgyptTravel/IMG_2668.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2674" src="/media/EgyptTravel/IMG_2674.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2680" src="/media/EgyptTravel/IMG_2680.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Horus的形象雕像
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2447" src="/media/EgyptTravel/IMG_2447.JPG" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2683" src="/media/EgyptTravel/IMG_2683.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拿破崙曾率軍住在神廟這裡三個月，天花板上的黑漬是他們在這邊烤東西所造成的
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2686" src="/media/EgyptTravel/IMG_2686.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2689" src="/media/EgyptTravel/IMG_2689.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
神龕
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2695" src="/media/EgyptTravel/IMG_2695.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2698" src="/media/EgyptTravel/IMG_2698.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2699" src="/media/EgyptTravel/IMG_2699.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2700" src="/media/EgyptTravel/IMG_2700.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
右邊是Osiris
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2702" src="/media/EgyptTravel/IMG_2702.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
壁畫的色彩還維持的很好，他進奉的是香精，這個房間曾經是放香精的地方，而在同一個房間的壁上象形文字有記載著香精的製作流程
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2704" src="/media/EgyptTravel/IMG_2704.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
存放香精
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2706" src="/media/EgyptTravel/IMG_2706.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
放香精的地方的房間牆壁記載著香精的製作流程。當時拿破崙發現了這個房間，雖然他不知道上面的象形文字代表什麼意思，但是他把他給抄起來，回法國解譯之後才知道是做香精的流程，Hassan說難怪法國香水這麼有名
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2708" src="/media/EgyptTravel/IMG_2708.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2710" src="/media/EgyptTravel/IMG_2710.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="hassan"&gt;Hassan的古埃及小教室&lt;/h3&gt;
&lt;p&gt;在開往伊斯納水匣門的路途中有好一段時間，Hassan幫我們惡補了一些古埃及的知識。&lt;/p&gt;
&lt;p&gt;Q：為什麼會稱之為上下埃及呢？&lt;/p&gt;
&lt;p&gt;最直接的原因是尼羅河的上游和下游，還有另外一個原因是因為地勢，上埃及較高、下埃及較低，所以尼羅河從上埃及流到下埃及。&lt;/p&gt;
&lt;p&gt;當然，因為地勢的關係，下埃及氾濫較為嚴重，也因此雖然下埃及比上埃及古蹟更多，但是大多因為氾濫而埋在地底下了。&lt;/p&gt;
&lt;p&gt;Q：古埃及為什麼會相信復活呢？&lt;/p&gt;
&lt;p&gt;因為古埃及人看到大自然的一起都會經歷強盛和衰亡，但總是生生不息，例如，太陽東昇西落，古埃及人認為晚上太陽神Ra死掉了，隔天早上卻又復活了，尼羅河一年會乾涸一次，尼羅河死掉了，但是他總是會再復活氾濫，滋養著古埃及人。&lt;/p&gt;
&lt;p&gt;既然萬物都是會復活，那人也應該會經歷死亡和復活，他們相信人有靈魂，靈魂會回來找自己的身體，如果找不到就不能復活，於是乎有了木乃伊。&lt;/p&gt;
&lt;p&gt;Q：木乃伊怎麼製作？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;屍體放上手術台&lt;/li&gt;
&lt;li&gt;從側腹切一小洞，把內臟取出用尼羅河的水洗乾淨，並且加鹽曝曬14天&lt;/li&gt;
&lt;li&gt;肝臟、胃、肺和腸放到罐子，並且要寫上死者的名字 &lt;/li&gt;
&lt;li&gt;心臟要放回去身體裡面&lt;/li&gt;
&lt;li&gt;腦子要清除乾淨，有兩個方法：從鼻子鉤出來或是從後腦開刀取出&lt;/li&gt;
&lt;li&gt;眼鏡取出後，放假眼睛進去&lt;/li&gt;
&lt;li&gt;身體用尼羅河的水洗乾淨後，加鹽曝曬40天，為了保護木乃伊的皮膚不會曬壞，古埃及人會塗上黑色防曬塗料，所以木乃伊才會看起來黑黑的&lt;/li&gt;
&lt;li&gt;將身體纏上亞麻布，配戴零零總總的附身符&lt;/li&gt;
&lt;li&gt;放木乃伊的棺材要有7層，如果是法老王的話，最裡面一層會用純金的棺材，再向外一層是鍍金的棺材，在向外一層是花崗岩石棺，...。最重要的是棺木上要刻上死者的名字，要讓死者回來找的到&lt;/li&gt;
&lt;li&gt;因為古埃及人真的相信復活，所以通常在陵墓內會放許多祂身前使用的東西，讓祂復活後可以繼續使用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Q：古埃及人為什麼迷戀7這個數字？&lt;/p&gt;
&lt;p&gt;之前說過神廟從外向內共7層，並且越來越小，是不是很像棺木從外到裡共7層，也是越來越小。這源於古埃及人相信天空總共有7層。&lt;/p&gt;
&lt;p&gt;Q：為什麼要建金字塔？&lt;/p&gt;
&lt;p&gt;古埃及人崇拜太陽神 Ra，所以一切跟太陽有關的事物他們都很喜歡。&lt;/p&gt;
&lt;p&gt;在古王國時期，因為金字塔的形狀像是太陽灑落的光線，於是乎古埃及人建立金字塔，希望可以藉由金字塔把地和天連在一起。但是因為金字塔實在是很顯眼，而且又充滿著貴重的陪葬品，所以基本上所有陵墓都被盜光了。&lt;/p&gt;
&lt;p&gt;記取教訓，於是乎中王國時期，不把金字塔蓋太高、太顯眼，並且把陪葬藏到金字塔的地底下。但是道高一尺、魔高一丈，盜墓者仍然可以找到陪葬品。&lt;/p&gt;
&lt;p&gt;所以在新王國時期，法老王放棄使用金字塔，而是在底比斯的西岸的阿爾巴赫里山裡建陵寢，據說這座谷地之所以會雀屏中選，是因為阿爾巴赫里山有金字塔型的山頂，這裡稱為帝王谷。&lt;/p&gt;
&lt;p&gt;新王國時期沒有金字塔了，但是法老王還是想要和天連在一起，於是建了方尖碑，方尖碑的頭部正是一個金字塔形狀。方尖碑的工藝水準很高，整塊石材是用花崗岩一體成形，要知道花崗岩是很硬的，切割相當不容易。Hassan認為方尖碑比金字塔更偉大，因為金字塔是用堆砌的，而且石材不一定全是花崗岩，也可能用石灰岩，整體的製作難度不比方尖碑。&lt;/p&gt;
&lt;h3 id="_12"&gt;伊斯納水匣門&lt;/h3&gt;
&lt;p&gt;建築這個水閘是要調節尼羅河水量，預防尼羅河氾濫，因為尼羅河的河道高低起伏很大，下游的水位比上游來得低，等遊輪進入水閘後會將前面的閘門關閉，等遊輪進去後再將水位做些調整，之後再開啟閘門讓遊輪通行。&lt;/p&gt;
&lt;p&gt;很多小販會利用船隻在等候時間來販賣東西，他們就直接在底下用丟的把東西丟上來，並且和船上的人討價還價，如果你想要買，你可以把錢放在一個袋子裡再丟下去給他們。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2740" src="/media/EgyptTravel/IMG_2740.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2742" src="/media/EgyptTravel/IMG_2742.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="luxor"&gt;路克索Luxor（古都 底比斯）&lt;/h3&gt;
&lt;p&gt;新王國時期，古埃及首都從Memphis遷移到底比斯，也就是現在的路克索。&lt;/p&gt;
&lt;p&gt;在底比斯東岸，有佔地超過100公頃，多達20多座的古埃及神殿。&lt;/p&gt;
&lt;p&gt;這裡主要侍奉的是「底比斯三神」，也就是Amun-Ra、 他的妻子姆特（Mut）、兒子孔蘇（Khonsu）。在中王國時期，阿蒙神（Amun）僅止於本地的神祇，直到新王朝阿蒙神（Amun）開始被視為勝利的象徵，阿蒙神（Amun）的地位開始水漲船高，後來又與太陽神 Ra 結合同化為 Amun-Ra，躍升成為「眾神之王」。&lt;/p&gt;
&lt;p&gt;在底比斯西岸，有一定要看的帝王谷，這裡已經發現有62個帝王的陵墓，陵墓當中的壁畫保持良好。&lt;/p&gt;
&lt;p&gt;因為想防止盜墓者的緣故，到了新王國，法老王的陵墓不再選擇顯眼的金字塔，而是選擇在較隱密的帝王谷。&lt;/p&gt;
&lt;p&gt;在古埃及，所有的陵墓，包括：金字塔、帝王谷一定都在尼羅河的西岸。這是因為古埃及人依據太陽東升西落的自然定理，衍生出死亡與復活循環不息的信仰，繼而形成日出的東方代表重生、繁衍；日落的西方代表死亡、衰退的觀念。&lt;/p&gt;
&lt;p&gt;因而在底比斯這裡膜拜阿蒙神（太陽神）的神殿遍及東岸，而皇陵則是建於西岸。&lt;/p&gt;
&lt;h3 id="temple-of-karnak"&gt;卡納克神殿 Temple of Karnak&lt;/h3&gt;
&lt;p&gt;世界上最大之一的露天史前古蹟，神殿規模之大無所可及，神殿門前列著一排獅身羊面像，入門後還有一排，緊接著的則是十根高約二十一公尺的石柱，還可見到的二座方尖碑，高約二十九米，是埃及現今最高的方尖碑。&lt;/p&gt;
&lt;p&gt;卡納克神殿共有六座神殿:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Amun-Ra神殿 &lt;/li&gt;
&lt;li&gt;曼圖（Montu，底比斯當地的鷹頭戰神）神殿&lt;/li&gt;
&lt;li&gt;姆特（Mut）神殿 &lt;/li&gt;
&lt;li&gt;孔蘇（Khonsu）神殿&lt;/li&gt;
&lt;li&gt;Opet神殿 &lt;/li&gt;
&lt;li&gt;Ptah神殿 &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在古埃及時期，每年會有歐佩特迎神慶典（Feast of Opet），當每年尼羅河氾濫時，底比斯都會舉行這個迎神慶典，埃及人把底比斯三神從卡納克神殿迎往路克索神殿，讓他們家人團聚，並且住上一個月，去程和回程則是靠著尼羅河水路，或沿著一條連接卡納克神殿和路克索神殿的獅身人面大道。&lt;/p&gt;
&lt;p&gt;走到外牆會看到磚頭外露，埃及神殿的牆壁一定會有壁畫，如果不是被迫壞掉，那就是還沒有蓋完。卡納克神殿是從裡向外蓋的，裡面的部分已經有3800歷史，但是外面只有2600年歷史，而它的外牆確實是還沒有蓋完，因為蓋這個外牆的人正是新王國的最後一個法老。&lt;/p&gt;
&lt;p&gt;卡納克神殿總共有130根柱子，而且囊括兩種蓮花柱。&lt;/p&gt;
&lt;p&gt;埃及神廟會用到的三種柱子：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;綻放的蓮花柱&lt;/li&gt;
&lt;li&gt;含苞的蓮花柱&lt;/li&gt;
&lt;li&gt;蘆葦草柱&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中蓮花代表的是上埃及，蘆葦草代表下埃及。&lt;/p&gt;
&lt;p&gt;在這裡可以看一個埃及人的巧思，蓮花有個特性是早上綻放，晚上含苞，所以靠近太陽的東邊是使用綻放的蓮花柱，而另外一邊則使用含苞的蓮花柱。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2761" src="/media/EgyptTravel/IMG_2761.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
卡納克神殿群：&lt;br&gt;
1. Amun-Ra神殿  &lt;br&gt;
2. 曼圖（Montu，底比斯當地的鷹頭戰神）神殿 &lt;br&gt;
3. 姆特（Mut）神殿 &lt;br&gt;
4. 孔蘇（Khonsu）神殿 &lt;br&gt;
5. Opet神殿 &lt;br&gt;
6. Ptah神殿 &lt;br&gt;
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/temple_of_karnak_map.jpeg" /&gt;
&lt;center&gt;&lt;small&gt;
圖上的聖湖是人工湖，用來淨身用的，具有3800年歷史，有地底水道通尼羅河 (cite from: 埃及 / 墨刻編輯部)
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2751" src="/media/EgyptTravel/IMG_2751.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
椰棗。一年只需要澆一次水，可以活100多年，可以長50米高，果實可以製成椰棗乾。
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2753" src="/media/EgyptTravel/IMG_2753.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
卡納克神殿門票
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2762" src="/media/EgyptTravel/IMG_2762.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
因為外牆的部分還沒有蓋完，所以當初用沙子當作斜坡的部分還沒有拆掉，所以Hassan說：不要再說金字塔是外星人蓋的，這裡的證據可以顯示金字塔是靠著斜坡來蓋出來的
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2759" src="/media/EgyptTravel/IMG_2759.JPG" /&gt;
&lt;center&gt;&lt;small&gt;
羊頭獅身代表晚上的太陽，可以看到牠的前方有一個人，那個是法老，象徵著保護法老王
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2547" src="/media/EgyptTravel/IMG_2547.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2766" src="/media/EgyptTravel/IMG_2766.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
綻放的蓮花柱
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2772" src="/media/EgyptTravel/IMG_2772.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
綻放的蓮花柱
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2773" src="/media/EgyptTravel/IMG_2773.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
含苞的蓮花柱
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2774" src="/media/EgyptTravel/IMG_2774.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2530" src="/media/EgyptTravel/IMG_2530-2.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
後面的雕像是拉美西斯二世
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2517" src="/media/EgyptTravel/IMG_2517.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2777" src="/media/EgyptTravel/IMG_2777.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
刻畫Feast of Opet時，將Amun-Ra沿著一條連接卡納克神殿和路克索神殿的獅身人面大道運送的過程，底下的祭司戴上老鷹頭並且扛著太陽船，太陽船上有裝有Amun-Ra神像的盒子
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2779" src="/media/EgyptTravel/IMG_2779.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯二世正在送Amun-Ra蓮花香精和牲品。法老的特徵：上下埃及皇冠、眼鏡蛇和長直鬍子，而Amun-Ra則是翹鬍子
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2792" src="/media/EgyptTravel/IMG_2792.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯二世正跪拜底比斯三神，身後的朱鷺頭智慧之神正在記載著一切，難得看到拉美西斯二世跪下，並且沒帶法老王的裝飾
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2781" src="/media/EgyptTravel/IMG_2781.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
方尖碑是一整塊花崗岩做成的，上面刻有法老的名字，已經他的事蹟，順道一提路克索這裡是不產花崗岩的，這麼高的石材怎麼一體成形，又怎麼從亞斯文那裡把他運過來，真的很不簡單
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2787" src="/media/EgyptTravel/IMG_2787.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
曾經是最高的方尖碑，是有埃及武則天之稱的哈賽普蘇女王所建
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2789" src="/media/EgyptTravel/IMG_2789.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
聖甲蟲雕像。古埃及人觀察到，聖甲蟲會在白天出來，晚上則躲在窩裡，躲在窩裡時牠會清自己的大便，一旦太陽出來後，聖甲蟲會把大便推出來。然後，剛剛有講到天空女神Nuit，埃及人解釋太陽會消失原因是太陽進去Nuit的嘴巴，太陽會再出現是因為太陽從Nuit的子宮出去，而中間的過程就是靠著聖甲蟲來推動的。所以，據說在這個聖甲蟲雕像轉7圈，可以求子，讓聖甲蟲幫忙推一下XD
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2795" src="/media/EgyptTravel/IMG_2795.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2798" src="/media/EgyptTravel/IMG_2798.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2538" src="/media/EgyptTravel/IMG_2538.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2518" src="/media/EgyptTravel/IMG_2518.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Amun-Ra
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="temple-of-luxor"&gt;路克索神殿 Temple of Luxor&lt;/h3&gt;
&lt;p&gt;阿曼侯提普三世（Amenhotep III）在3000年前建造了這座神殿供奉「底比斯三神」：阿蒙（Amun）、姆特（Mut）和孔蘇（Khonsu），而它最重要的目的，是為了舉辦當地一年一度的歐佩特迎神慶典，強化法老王與神之間的關係。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/luxor_temple_map.jpeg" /&gt;
&lt;center&gt;&lt;small&gt;
cite from: 埃及 / 墨刻編輯部
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2804" src="/media/EgyptTravel/IMG_2804.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
路克索神殿門票
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2807" src="/media/EgyptTravel/IMG_2807.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
路克索神殿第一塔門。方尖碑原本有兩個，對稱的放在塔門前，不過在被土耳其佔領時期將其中一個方尖碑送給法國
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2549" src="/media/EgyptTravel/IMG_2549-2.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2839" src="/media/EgyptTravel/IMG_2839.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2837" src="/media/EgyptTravel/IMG_2837.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2836" src="/media/EgyptTravel/IMG_2836.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2806" src="/media/EgyptTravel/IMG_2806.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
長達三公里長的獅身人面大道，連接卡納克神殿，在歐佩特迎神慶典時，底比斯三神會搭著太陽船來到這裡
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2842" src="/media/EgyptTravel/IMG_2842.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2810" src="/media/EgyptTravel/IMG_2810.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
中間為了祭祀Amun-Ra，兩旁則供奉姆特（Mut）和孔蘇（Khonsu），這邊的柱子是蘆葦草柱
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2811" src="/media/EgyptTravel/IMG_2811.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
為什麼有一個清真寺出現在這裡呢？而且怎麼門口會在這麼高的地方？原因是經過好幾年的風沙，沙子已經把路克索神殿淹沒了，阿拉伯人來這邊並不知道，所以就蓋了這個清真寺，直到後來考古學家看到方尖碑的頭，研判地底下有神殿，所以花了十年的挖掘，才讓路克索神殿重現光明
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2812" src="/media/EgyptTravel/IMG_2812.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2815" src="/media/EgyptTravel/IMG_2815.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2818" src="/media/EgyptTravel/IMG_2818.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
壁畫上的兩位面對面的是尼羅河之神Hapy，一個手拿代表上埃及的蓮花，一個手拿代表下埃及的蘆葦草，兩個繫在一起代表上下埃及的一統
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2820" src="/media/EgyptTravel/IMG_2820.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
右邊並肩而坐的是Amun-Ra和他的妻子Mut，左邊坐著的是拉美西斯二世（居然還比神大尊，真是的...）
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2821" src="/media/EgyptTravel/IMG_2821.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
圖坦卡門和他的妻子
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2830" src="/media/EgyptTravel/IMG_2830.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2562" src="/media/EgyptTravel/IMG_2562.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2567" src="/media/EgyptTravel/IMG_2567.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
蘆葦草柱
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2823" src="/media/EgyptTravel/IMG_2823.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
羅馬人把原本壁畫抹掉，把這裡建成教堂，這裡不像艾得夫神殿那樣幸運，羅馬人沒有先塗上石灰，所以就被破壞掉了
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2825" src="/media/EgyptTravel/IMG_2825.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2826" src="/media/EgyptTravel/IMG_2826.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
&lt;a href="https://zh.wikipedia.org/wiki/阿蒙霍特普二世"&gt;阿蒙霍特普二世&lt;/a&gt;與Amun-Ra。阿蒙霍特普二世是圖坦卡門的阿公，他登基時有一段小故事，他的爸爸 圖特摩斯三世 的正妻生的是女兒，而阿蒙霍特普二世的媽媽是妾，而且是血統是努比亞人，如果讓阿蒙霍特普二世當法老，大祭司們是不同意的，此時大祭司跟阿蒙霍特普二世說：「如果你想要當法老，你就娶正妻生的女兒吧！比較名正言順」，不過阿蒙霍特普二世已經有喜歡的人了，他並不答應，於是他編了一個故事，說他自己是他母親與Amun-Ra生下的兒子（所以圖特摩斯三世被帶綠帽了！？），這個說法大祭司們接受了，於是他登基成為法老，而後統治55年的時間，成就了第十八王朝最黃金的時期
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2876" src="/media/EgyptTravel/IMG_2876.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
晚上遊輪有旋轉舞表演
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;</content><category term="Life"></category><category term="遊記"></category><category term="埃及"></category></entry><entry><title>[入埃及記] Day4: 阿布辛貝Abu Simbel【阿布辛貝雙神殿】、康孟波Kom Ombo【康孟波雙神殿】</title><link href="https://ycc.idv.tw/egypt-travel_3.html" rel="alternate"></link><published>2020-02-08T12:00:00+08:00</published><updated>2020-02-08T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-08:/egypt-travel_3.html</id><summary type="html"></summary><content type="html">&lt;p&gt;&lt;img alt="map" src="/media/EgyptTravel/egypt_map_d4.png" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Day4 行程：亞斯文Aswan &amp;gt; 🚐(285KM) &amp;gt; 阿布辛貝Abu Simbel【阿布辛貝雙神殿】&amp;gt; 🚐(285KM) &amp;gt; 亞斯文Aswan &amp;gt; 🛳 &amp;gt; 康孟波Kom Ombo【康孟波雙神殿】 &amp;gt; 🛳😴 &amp;gt; 艾得夫 Edfu&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="_1"&gt;早起拉車前往阿布辛貝&lt;/h3&gt;
&lt;p&gt;因為從亞斯文驅車前往阿布辛貝需要3到4小時，所以我們起了個大早，清晨4點就開始拉車。&lt;/p&gt;
&lt;p&gt;在前往阿布辛貝的路途中幾乎都是行經在整片的沙漠當中，中途我們有短暫停靠在一處的休息站，也就順手拍了好幾張的沙漠早晨。&lt;/p&gt;
&lt;p&gt;阿布辛貝嚴格說起來並不是上埃及，當然不可能是下埃及，因為原本這片土地是由努比亞人所佔領，直到拉美西斯二世攻佔這個地方後，阿布辛貝才成為埃及的一部分。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2299" src="/media/EgyptTravel/IMG_2299.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2300" src="/media/EgyptTravel/IMG_2300.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2306" src="/media/EgyptTravel/IMG_2306.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
品種好像跟台灣不大一樣
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2314" src="/media/EgyptTravel/IMG_2314.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2315" src="/media/EgyptTravel/IMG_2315.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_2"&gt;古埃及歷史&lt;/h3&gt;
&lt;p&gt;遠在5500年前，那個時候有上埃及和下埃及，上下埃及都有各自的國王，注意喔！是國王不是法老王，法老王指的是統一上下埃及後的國王。&lt;/p&gt;
&lt;p&gt;BC3200，上埃及 曼尼斯國王統一上、下埃及，定都於Memphis（孟斐斯，現開羅西郊），成為古埃及第一個法老，古埃及從此開始了王朝時期。&lt;/p&gt;
&lt;p&gt;從古自今，埃及一共有四個首都，第一個首都是Memphis，第二個首都是底比斯，第三個首都是亞歷山大，第四個首都在開羅，也是目前埃及的首都。&lt;/p&gt;
&lt;p&gt;古埃及有31個王朝，一共325個法老王。王朝指的是有血緣關係的王位繼承，如果爸爸逝世就兒子繼位，如果沒有兒子就找親戚，如果還有親戚，這個王朝就結束了，換下一個王朝。&lt;/p&gt;
&lt;p&gt;第1到第7王朝稱為古王國時期，就是從法老王 曼尼斯開始，首都在Memphis，其中第3和第4王朝是強盛時期，在這段時間金字塔逐漸成熟，吉薩金字塔集之大成。&lt;/p&gt;
&lt;p&gt;第8到第16王朝稱為中王國時期，這段時間埃及不強大，經常為了奪位發動內戰。&lt;/p&gt;
&lt;p&gt;第17到第30王朝稱為新王國時期，首都遷到 底比斯，此時的法老王不建金字塔，他們選擇在底比斯（也就是現在的路克索）的西岸建立了帝王谷當作陵墓。新王國時期最強盛的時期是第18王朝、19王朝和20王朝，18王朝有知名的 圖坦卡門，19王朝有接下來要提到的 拉美西斯二世。&lt;/p&gt;
&lt;p&gt;拉美西斯二世 可以說是古埃及最偉大的法老王，那個時候可以說是全世界最偉大的帝國，稱之為古埃及帝國，拉美西斯二世統治埃及長達六十七年，支配富裕帝國的黃金年代，前半生四處征戰，後半生積極修建神廟和陵墓，他善用自我行銷，深諳能見度是統治成功的核心，自我行銷到接近吹噓和自大的程度，但他的確有資格驕傲。&lt;/p&gt;
&lt;h3 id="_3"&gt;阿布辛貝雙神殿&lt;/h3&gt;
&lt;p&gt;剛剛有提到過，拉美西斯二世 趕走了努比亞人，阿布辛貝變成埃及的一部分，而後拉美西斯二世 蓋了阿布辛貝雙神殿，也就是我們今天要參觀的，阿布辛貝雙神殿目的是想塑造 拉美西斯二世 為神的形象，並且吹噓自己的偉大和勇敢。&lt;/p&gt;
&lt;p&gt;仔細想想他這樣做是有政治意義的，神殿座落的這個地方當時就居住著努比亞人，而此地也是古埃及黃金的主要採集地點，這座神廟就是想彰顯拉美西斯二世的偉大，並且在神廟的浮雕上雕刻著埃及征戰的勝利與戰爭俘虜的慘狀，恫嚇著努比亞人不准輕舉妄動，這是 拉美西斯二世 一貫的風格。&lt;/p&gt;
&lt;p&gt;阿布辛貝雙神殿共由「拉美西斯二世大神殿」（Great Temple of Ramesses II, 或稱大阿布辛貝神殿）和「娜菲塔莉神殿」（Temple of Nefertari, 或稱小阿布辛貝神殿）兩座神殿組成，比鄰而坐，耗時30年才建立完成。其中大阿布辛貝神殿更是與金字塔和人面獅身像齊名，而小阿布辛貝神殿則是拉美西斯二世為其妻子興建的神殿，也是古埃及唯一一座法老王為妻子興建的神殿。&lt;/p&gt;
&lt;p&gt;阿布辛貝雙神殿原本被沙土給埋沒，直到1813年才由瑞士歷史學家 Burckhardt 發現，並在1817年，由另一位探險家 Belzoni 清除了入口的沙土，沈睡了11個世紀的拉美西斯二世終於被歐洲人喚醒。&lt;/p&gt;
&lt;p&gt;阿布辛貝雙神殿不是像金字塔是用石頭鍥出來的，它是由一座山雕刻而成的，而後美國總統山的靈感也來自於阿布辛貝雙神殿。&lt;/p&gt;
&lt;p&gt;因為建了亞斯文大水壩的關係，阿布辛貝神殿會被漲高的湖水淹沒，為了避免憾事發生，聯合國教科文組織與埃及政府合力用了4年的時間，將大神殿鋸成大石塊，搬運到現在的位置，實為現代拯救古蹟的一個大工程。&lt;/p&gt;
&lt;p&gt;順道一提，原本進去拍照有很多限制，但就在幾個月前已經鬆綁了入內拍照的規定，現在只要用手機拍照一律不用錢。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2329" src="/media/EgyptTravel/IMG_2329.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
阿布辛貝雙神殿的門票
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2330" src="/media/EgyptTravel/IMG_2330.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2332" src="/media/EgyptTravel/IMG_2332.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
阿布辛貝雙神殿舊址是在這湖泊下
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2337" src="/media/EgyptTravel/IMG_2337.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Hassan大哥講解阿布辛貝雙神殿
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="_4"&gt;小阿布辛貝神殿（娜菲塔莉神殿）&lt;/h3&gt;
&lt;p&gt;小阿布辛貝神殿的前面有六尊巨像，其中四個是 拉美西斯二世，另外兩個是他的老婆 娜菲塔莉。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2371" src="/media/EgyptTravel/IMG_2371.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
小阿布辛貝神殿全景圖
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2338" src="/media/EgyptTravel/IMG_2338.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
小阿布辛貝神殿
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2349" src="/media/EgyptTravel/IMG_2349.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2350" src="/media/EgyptTravel/IMG_2350.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2373" src="/media/EgyptTravel/IMG_2373.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2341" src="/media/EgyptTravel/IMG_2341.JPG" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2356" src="/media/EgyptTravel/IMG_2356.JPG" /&gt;&lt;/p&gt;
&lt;p&gt;走入內部，神殿內四壁佈滿 拉美西斯二世 和 娜菲塔莉 向 Horus、Hathor、Maat、Mut、Satis、Horus、Isis、Khnum、Khonsu、Thoth 諸神敬奉鮮花及燃香的情景，古埃及人很喜歡用壁畫說故事。&lt;/p&gt;
&lt;p&gt;他們獻上的花通常是古埃及的國花—蓮花。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2363" src="/media/EgyptTravel/IMG_2363.JPG" /&gt;
&lt;center&gt;&lt;small&gt;
生命之匙，其形狀為尼羅河的形狀，尾端像是上游，手提端有如下游的三角洲，所以生命之匙表示的是埃及
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2377" src="/media/EgyptTravel/IMG_2377.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2378" src="/media/EgyptTravel/IMG_2378.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2379" src="/media/EgyptTravel/IMG_2379.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
娜菲塔莉（右） 獻奉給 Hathor（左），牠是鷹頭人身的Horus的妻子，形象特徵是牛角加上太陽，牠代表喜悅、音樂、多產和幸福
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2381" src="/media/EgyptTravel/IMG_2381.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯二世（右） 獻奉給 Ra-Horakhty（左），Ra-Horakhty 是由太陽神Ra和鷹頭人身的Horus合體。太陽神Ra是創世之神，也是所有神祉的源頭，形象特徵就是頭頂上的大太陽。而Horus是代表法老王的神，形象特徵就是鷹頭人身。所以Ra-Horakhty兼具鷹頭人身與大太陽。
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2382" src="/media/EgyptTravel/IMG_2382.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯二世（中）和 娜菲塔莉（右）獻奉給 Hathor（左），奉獻的正是古埃及的國花—蓮花。
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2384" src="/media/EgyptTravel/IMG_2384.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
Hathor
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2385" src="/media/EgyptTravel/IMG_2385.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
鷹頭人身的Horus頭戴象徵上埃及的白冠
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="_5"&gt;大阿布辛貝神殿（拉美西斯二世大神殿）&lt;/h3&gt;
&lt;p&gt;大阿布辛貝神殿的前面有四尊坐姿巨像，其實這四座都是 拉美西斯二世（看他有多自戀，跟法國太陽王路易十四有得拼），當然，這些雕像跟真實的 拉美西斯二世 長相是有落差的，因為 拉美西斯二世 就是想要讓人民看到最好的一面，讓人民覺得被這樣強壯、英俊的法老王統治是理所當然的，是有安全感的。&lt;/p&gt;
&lt;p&gt;大阿布辛貝神殿的門上方有 Ra-Horakhty 的雕像，剛剛說過，Ra-Horakhty 是由太陽神Ra和鷹頭人身的Horus合體。&lt;/p&gt;
&lt;p&gt;大阿布辛貝神殿的內部深度為65米深。&lt;/p&gt;
&lt;p&gt;值得一提的 2/22 和10/22是阿布辛貝太陽節，當天太陽光會照進大阿布辛貝神殿的聖壇，據說這兩天分別為拉美西斯二世的生日和登基紀念日，時間點位於早上6:30，大概持續30分鐘太陽光照在法老王的臉上，不過因為搬遷的緣故，現在陽光照進的時間推遲一天。&lt;/p&gt;
&lt;p&gt;仔細想想這有多神奇，埃及人怎麼知道要如何設計神殿入口讓這兩天陽光射入的角度一致，而且就算是現代人也很難去精確的做到，真是令人驚嘆。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/great_temple_of_ramessessII_map.jpeg" /&gt;
 &lt;center&gt;&lt;small&gt;
cite from: 埃及 / 墨刻編輯部
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2453" src="/media/EgyptTravel/IMG_2453.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
大阿布辛貝神殿全景圖
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2536" src="/media/EgyptTravel/IMG_2536.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
四座拉美西斯二世，再加上中間的Ra-Horakhty
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2391" src="/media/EgyptTravel/IMG_2391.JPG" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2409" src="/media/EgyptTravel/IMG_2409.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2410" src="/media/EgyptTravel/IMG_2410.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2448" src="/media/EgyptTravel/IMG_2448.JPG" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2413" src="/media/EgyptTravel/IMG_2413.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
前面的老鷹是Horus的形象
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2414" src="/media/EgyptTravel/IMG_2414.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2531" src="/media/EgyptTravel/IMG_2531.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
四座巨像的腳附近的小型雕刻是拉美西斯二世的家人
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2532" src="/media/EgyptTravel/IMG_2532.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2415" src="/media/EgyptTravel/IMG_2415.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
俘虜
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2454" src="/media/EgyptTravel/IMG_2454.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
中間被橢圓框框住的是法老的名字
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2455" src="/media/EgyptTravel/IMG_2455.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2456" src="/media/EgyptTravel/IMG_2456.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
中間被橢圓框框住的是法老的名字
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2530" src="/media/EgyptTravel/IMG_2530.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
中間被橢圓框框住的是法老的名字
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2533" src="/media/EgyptTravel/IMG_2533.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2534" src="/media/EgyptTravel/IMG_2534.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;走入內部，到了第一多柱廳，有八座立姿的雕像，兩排面對面，不要懷疑，這八座也是拉美西斯二世，到底是...，而且這八座立姿的拉美西斯二世都手拿權杖，比出冥神Osiris的姿勢，更強化自己的神性。&lt;/p&gt;
&lt;p&gt;在繼續往裡面走到底，就會到神龕的位置，敬奉的神有（順序從左至右）：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Memphis的守護神—Ptah&lt;/li&gt;
&lt;li&gt;底比斯的守護神—Amun-Ra&lt;/li&gt;
&lt;li&gt;拉美西斯二世&lt;/li&gt;
&lt;li&gt;Heliopolis（開羅的古地名）的守護神—Ra-Horakhty&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有注意到了嗎？拉美西斯二世 已經把自己當作神讓人民供奉。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2461" src="/media/EgyptTravel/IMG_2461.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
第一多柱廳
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2464" src="/media/EgyptTravel/IMG_2464.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯二世 驅馬當頭作戰
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2465" src="/media/EgyptTravel/IMG_2465.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
拉美西斯二世 一個打兩個
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2498" src="/media/EgyptTravel/IMG_2498.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2499" src="/media/EgyptTravel/IMG_2499.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2501" src="/media/EgyptTravel/IMG_2501.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2502" src="/media/EgyptTravel/IMG_2502.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2507" src="/media/EgyptTravel/IMG_2507.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
大阿布辛貝神殿神龕，從左至右分別是Ptah、Amun-Ra、拉美西斯二世 和 Ra-Horakhty，今天是2/8，所以再過幾天的時間陽光就會打在 拉美西斯二世 的身上，並且壟罩在右邊的三尊，只有象徵冥神的Ptah不會被照到。
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2510" src="/media/EgyptTravel/IMG_2510.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2522" src="/media/EgyptTravel/IMG_2522.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2527" src="/media/EgyptTravel/IMG_2527.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2528" src="/media/EgyptTravel/IMG_2528.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
大阿布辛貝神殿的天花板
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2529" src="/media/EgyptTravel/IMG_2529.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2483" src="/media/EgyptTravel/IMG_2483.JPG" /&gt;&lt;/p&gt;
&lt;h3 id="_6"&gt;海市蜃樓奇景&lt;/h3&gt;
&lt;p&gt;在回遊輪的路途中，有看到海市蜃樓的奇景，彷彿就能看到在不遠的地方有一攤水，這不知道會害到多少古代在沙漠的人。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2549" src="/media/EgyptTravel/IMG_2549.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="temple-of-kom-ombo"&gt;康孟波神殿 Temple of Kom Ombo&lt;/h3&gt;
&lt;p&gt;吃了午餐，休息一下，又吃了下午茶，遊輪也漸漸往北行駛，最後抵達康孟波。&lt;/p&gt;
&lt;p&gt;名字原意為「大量(Kom) 黃金(Ombo)」的康孟波，是古埃及黃金之都的舊址，從史前時代開始，這邊就一直是熱鬧的城鎮，當地居民種植甘蔗和小麥。&lt;/p&gt;
&lt;p&gt;就在遊輪停靠的地方不遠處就是康孟波神殿了，走路不到2分鐘就到了。&lt;/p&gt;
&lt;p&gt;古埃及各地區都會有守護神，大部分只有一個，但是唯一只有在康孟波這裡有兩位守護神。&lt;/p&gt;
&lt;p&gt;康孟波神殿分別敬奉二位守護神，北邊祭鷹神Haroeris（荷魯斯的其中一個化身），南邊則是鱷魚神索貝克（Sobek），在埃及絕無僅有的，不僅僅是因為它是少數獻給惡神的神殿，更因為它採用一殿敬奉雙神信仰與兩兩相對的建築結構之神殿，它也是全埃及保存最完整的建築，至今幾乎還維持當年的原貌，細密的浮雕，充滿著原始藝術之美。&lt;/p&gt;
&lt;p&gt;昔日這裡地區常出現鱷魚爬上沙質河岸曬太陽，對這裡的居民來說，除了崇拜象徵正向力量的神祉外，他們也膜拜害怕的事物，過去常常把村民當作食物的鱷魚，也成為當地敬畏的對象。&lt;/p&gt;
&lt;p&gt;這一座神廟是已經脫離古埃及時代之後所建的，迄今有2000年的歷史，它是在希臘時期的托勒密王朝誕生的，亞歷山大帝攻佔埃及時答應當地人可以保有自己的文化，並且自稱是阿蒙神的兒子，所以贏得了尊重，所以接下來的希臘國王都有兩個身分，一個是希臘的國王，另外一個是埃及的法老王，亞歷山大帝後，托勒密一世繼位，再來是托勒密二世，...，最後一任是Cleopatra女王，也就是台灣人說的埃及艷后，她與凱薩結婚，結果凱薩大帝因為政變而遭刺殺。&lt;/p&gt;
&lt;p&gt;順道一提，繼任者羅馬就沒這麼尊重埃及的文化，任意的破壞。&lt;/p&gt;
&lt;p&gt;康孟波神殿被摧殘的很嚴重，但是和它同時也在托勒密王朝興建的艾得夫神殿卻是保存良好，原因是因為康孟波神殿地勢較低，遭受尼羅河氾濫的破壞也比較嚴重，而艾得夫神殿地勢比較高，也因此保存比較良好。&lt;/p&gt;
&lt;p&gt;埃及神廟有個基本的標準，外面有城牆，城牆上有一個大門，大門進去是前庭，前庭進去就會是多柱廳，加上大門往內共七層的隔間才到神龕的位置，而且依循著門越往內越小、地板越往內越高、屋頂越往內越低的原則。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/temple_of_kom_ombo_map.jpeg" /&gt;
&lt;center&gt;&lt;small&gt;
cite from: 埃及 / 墨刻編輯部
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2583" src="/media/EgyptTravel/IMG_2583.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2591" src="/media/EgyptTravel/IMG_2591.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2595" src="/media/EgyptTravel/IMG_2595.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
從左自右分別是朱鷺頭人身的Thoth（智慧之神）、Hathor、法老王、獅面人身的Sekhmet（戰爭之神）、頭戴上下埃及王冠的Horus和頭戴下埃及王冠的Horus。諸神手拿生命之匙，代表他們保護埃及人。特別注意，Sekhmet手拿生命之匙放進法老的嘴巴，這是所謂的開口儀式，意味著復活。
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2596" src="/media/EgyptTravel/IMG_2596.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
在剛剛上面的壁畫對面，有鱷魚神索貝克（Sobek）的壁雕
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2597" src="/media/EgyptTravel/IMG_2597.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
法老王的受洗儀式，在過去大祭司會為新任法老舉辦受洗儀式，大祭司一位戴上朱鷺頭象徵Thoth（智慧之神），另外一位大祭司會戴上老鷹頭象徵Horus，雙雙手拿生命之水受洗準法老，仔細看那個水是許多生命之匙串起來的
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2598" src="/media/EgyptTravel/IMG_2598.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
法老王的受洗儀式結束之後，法老王受到左右分別代表上、下埃及的女神的擁戴，並且頭上一隻老鷹飛過，代表的是Horus正在保護埃及，最右邊的是Ra-Horakhty
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2599" src="/media/EgyptTravel/IMG_2599.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2600" src="/media/EgyptTravel/IMG_2600.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2601" src="/media/EgyptTravel/IMG_2601.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
因為被破壞過，才可以看到隱藏於石塊裡的卡榫，一個卡榫會卡住四個石塊
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2602" src="/media/EgyptTravel/IMG_2602.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
祭壇，擺的東西可不是隨便擺，有日曆紀錄聖壇每天要擺什麼
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2603" src="/media/EgyptTravel/IMG_2603.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2604" src="/media/EgyptTravel/IMG_2604.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2574" src="/media/EgyptTravel/IMG_2574.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
古埃及人醫學水平是很高的，第一排是手術刀，第三排還出現剪刀，第四排最左邊球狀的是拔罐的工具。其實在古埃及不管是醫生還是老師都是由祭司來擔任。
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2575" src="/media/EgyptTravel/IMG_2575.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
這兩位女生坐姿正在生小孩，這是古埃及人的作法，跟現在美國部分作法不謀而合
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2577" src="/media/EgyptTravel/IMG_2577.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2579" src="/media/EgyptTravel/IMG_2579.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
俘虜，連臉都不屑畫了，當然下面那個不是人名，而是他的出生地
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2580" src="/media/EgyptTravel/IMG_2580.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
動物的木乃伊棺材
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2629" src="/media/EgyptTravel/IMG_2629.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
鱷魚的木乃伊
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2633" src="/media/EgyptTravel/IMG_2633.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
鱷魚的木乃伊
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2606" src="/media/EgyptTravel/IMG_2606.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2617" src="/media/EgyptTravel/IMG_2617.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2627" src="/media/EgyptTravel/IMG_2627.jpg" /&gt;&lt;/p&gt;</content><category term="Life"></category><category term="遊記"></category><category term="埃及"></category></entry><entry><title>[入埃及記] Day3: 亞斯文【亞斯文大水壩、騎駱駝拜訪努比亞人家、三角風帆船】</title><link href="https://ycc.idv.tw/egypt-travel_2.html" rel="alternate"></link><published>2020-02-07T12:00:00+08:00</published><updated>2020-02-07T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-07:/egypt-travel_2.html</id><summary type="html"></summary><content type="html">&lt;p&gt;&lt;img alt="map" src="/media/EgyptTravel/egypt_map_d3.png" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Day3 行程：亞斯文Aswan【亞斯文大水壩、騎駱駝拜訪努比亞人家、三角風帆船】&amp;gt; 遊輪🛳😴&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;火車上的早餐還可以，吃完早餐不久火車就抵達亞斯文，接下來我們搭著小巴前往亞斯文大水壩。&lt;/p&gt;
&lt;h3 id="_1"&gt;亞斯文大水壩&lt;/h3&gt;
&lt;p&gt;亞斯文的名稱源自女神Swenet，因為至關重要的地理位置而備受矚目。這裡是古埃及時期最南端的邊防屏障，埃及通往蘇丹的重鎮，對抗努比亞人的軍事前哨；這裡還是駱駝商旅聚集的大市場，來自各地的特產與當地黃金、岩礦和象牙轉換交易。&lt;/p&gt;
&lt;p&gt;努比亞人跟埃及人是不同種族，努比亞人是屬於黑人的種族。&lt;/p&gt;
&lt;p&gt;亞斯文位於上埃及，上埃及雖然土地面積比下埃及來得大，但是人口大部分卻都集中在下埃及，原因是上埃及有半年的時間是處於夏天，而且夏天經常超過45度，非常不宜人居住。&lt;/p&gt;
&lt;p&gt;而亞斯文這邊的土層大多是花崗岩，這會造成這裡種植作物相當不方便，而且蓋房子也不容易挖地基，要知道花崗岩是很硬的。&lt;/p&gt;
&lt;p&gt;大家一定有想到，難道這裡的花崗岩就是金字塔的建材來源，沒有錯！金字塔和方尖碑的石材就是源於這裡，並且藉由尼羅河將石材運到下埃及去。&lt;/p&gt;
&lt;p&gt;尼羅河的氾濫才造就了古埃及文明，埃及一年約乾涸兩個月、約氾濫兩個月，乾涸時沒有水可用，氾濫時又會破壞建物，對於現今埃及來說尼羅河反而是一個麻煩的點。&lt;/p&gt;
&lt;p&gt;於是英國統治時期開始建了水壩，這個水壩並不是我們現在所說的亞斯文大水壩，是一個規模比較小的水壩，當然蓄水能力就比較有限，這個水壩後來又加高了兩次，為了就是增加蓄水能力，但是尼羅河一氾濫起來這個舊水壩根本是擋不住的。&lt;/p&gt;
&lt;p&gt;埃及獨立後，&lt;a href="https://zh.wikipedia.org/wiki/贾迈勒·阿卜杜-纳赛尔"&gt;納瑟&lt;/a&gt;做為第一任總統，同時他也是上埃及人，所以他相當了解埃及的最大的問題是水資源問題，所以決定要蓋現在的亞斯文水壩，但是當時才剛剛獨立不久，沒有機器和錢可以去做這件事，也不可能找英國和法國，因為這些國家都希望從蘇伊士運河中獲得利益，所以他先去找了曾幫助埃及獨立的美國，可是美國並不打算幫這個忙，後來納瑟去找了蘇聯，結果蘇聯居然同意合作建水壩，當然是有條件的，條件是埃及必須在未來只能跟蘇聯買機器和武器。&lt;/p&gt;
&lt;p&gt;於是乎兩國開始合作蓋亞斯文大水壩，總共蓋了11年，終於在1971年，經歷了兩任總統，以就地取材的花崗岩建出了高度約120米、長度約300公里的亞斯文水壩，建造此水壩所用掉的花崗岩石塊是古夫金字塔的十幾倍。&lt;/p&gt;
&lt;p&gt;同時，因為亞斯文大水壩可以用來發電，也一起解決了缺電問題。&lt;/p&gt;
&lt;p&gt;為了感謝俄羅斯的幫忙，在亞斯文大水壩附近埃及政府蓋了一座「蓮花塔」來做為合作的紀念。&lt;/p&gt;
&lt;p&gt;但是建水庫也不僅僅有好處也有一些缺點，第一，不氾濫，就不會出現像以前一樣的黑色肥沃的泥土，第二，因為建了水庫，上游的水位線大大的提高，也造成一些土地會被淹沒，其中也包括一些古蹟，其實現在的納賽爾湖正是因為建了亞斯文水壩才多出來的，而也因為建了這個水庫，鱷魚的棲息地被限制在上游，所以納賽爾湖可以看到鱷魚，但是過了水壩的下游就再也看不到了（也是一種優點）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_1904" src="/media/EgyptTravel/IMG_1904.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
亞斯文大水壩的票
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_1906" src="/media/EgyptTravel/IMG_1906.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
亞斯文大水壩
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_1917" src="/media/EgyptTravel/IMG_1917.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
亞斯文大水壩
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_1962" src="/media/EgyptTravel/IMG_1962.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
台埃友好
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2007" src="/media/EgyptTravel/IMG_2007.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
蓮花塔（古埃及的國花是蓮花）
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_1997" src="/media/EgyptTravel/IMG_1997.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_1986" src="/media/EgyptTravel/IMG_1986.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_1987" src="/media/EgyptTravel/IMG_1987.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_1989" src="/media/EgyptTravel/IMG_1989.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
建亞斯文大水壩時的兩任總統
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_1992" src="/media/EgyptTravel/IMG_1992.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
埃及（右）與俄羅斯（左）的國徽
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="_2"&gt;搭船前往騎駱駝的地方&lt;/h3&gt;
&lt;p&gt;參觀完水壩，我們就要去check in接下來要住四天兩夜的遊輪，遊輪看起來舒適多了，餐點也相當的不錯，至少這四天不會有腸胃炎的風險。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2029" src="/media/EgyptTravel/IMG_2029.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
我們要住四天三夜的遊輪
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2030" src="/media/EgyptTravel/IMG_2030.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2038" src="/media/EgyptTravel/IMG_2038.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;用完餐後，直接在遊輪停靠的地方，搭著汽船前往要去騎駱駝的地方，沿途的風景很好，只不過河裡礁石相當多，掌舵的人需要很小心的不讓船去撞到。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2042" src="/media/EgyptTravel/IMG_2042.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2048" src="/media/EgyptTravel/IMG_2048.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2049" src="/media/EgyptTravel/IMG_2049.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2058" src="/media/EgyptTravel/IMG_2058.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2059" src="/media/EgyptTravel/IMG_2059.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_3"&gt;騎駱駝拜訪努比亞人家&lt;/h3&gt;
&lt;p&gt;靠岸之後眼前就是一片的沙漠，沙漠上面蹲了好幾隻的駱駝，不要看牠蹲下去好像很矮，牠站起來真他媽的有夠高，好像還比我高...&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2064" src="/media/EgyptTravel/IMG_2064.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2065" src="/media/EgyptTravel/IMG_2065.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2066" src="/media/EgyptTravel/IMG_2066.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;選定了一隻駱駝後，連一開始要跨上去都有點困難，就這樣在那邊蹬了好多次才上駱，上去後服務人員幫忙拍了幾張相片，然後把手機還給我，然後我就天真的把它裝上去自拍棒，想說待會在行進間可以一邊拍。&lt;/p&gt;
&lt;p&gt;服務人員說了一些口令，駱駝就開始起立，不是啊！超晃的，我的手機險些掉下去，而且為了要不讓自己摔下去，我必須緊緊的抓住前面難抓的小把手，但是我手上拿著自拍棒，更是抓不住。而且我一開始根本沒想到牠的腿這麼長，整個超高，而且駱駝不是站起來很晃以外，連走路時都超晃。&lt;/p&gt;
&lt;p&gt;後來我漸漸抓到Tempo，比較穩了就在駱駝上面拍了幾張照片，服務小弟看我穩妥，居然就把牽繩遞給我，我一臉傻眼的看著他，是認真的嗎？我拿著牽繩後，他就開始在後面趕我的駱駝衝刺，就在驚慌當中我從原本在第六位衝到第一位，還險些走到懸崖旁（不知道為什麼駱駝總是喜歡貼著懸崖走路），就在這過程中我的自拍棒就壞了，只好趕緊把手機收進口袋裡。&lt;/p&gt;
&lt;p&gt;路途比想像中的長，很新鮮的一個體驗。我的這隻駱駝有點不安分，看到其他駱駝就會一直靠過去，還去偷吃別人的草，而且他是很興奮的往前衝，完全沒在管牠上面的這個人，一直深怕牠會大暴走啊～&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2109" src="/media/EgyptTravel/IMG_2109.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2136" src="/media/EgyptTravel/IMG_2136.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2146" src="/media/EgyptTravel/IMG_2146.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;駱駝最後停在努比亞的村落裡，努比亞的種族是黑人，他們是曾經也建立過高度文明的古老民族，這個國家在埃及的南方，在蘇丹的北方，後來古埃及新王國時期的戰神法老王—拉美西斯二世 發動戰爭滅掉了努比亞人所建立的國家，從此失去了自己的國家達三千年之久。&lt;/p&gt;
&lt;p&gt;目前努比亞人大多前往蘇丹的領土定居，只有少部份的人在埃及，聽Hassan大哥說，他們有些時候會抗議政府想要獨立。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2177" src="/media/EgyptTravel/IMG_2177.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
努比亞人家自己畫的壁畫
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2178" src="/media/EgyptTravel/IMG_2178.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
努比亞人家
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2180" src="/media/EgyptTravel/IMG_2180.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
努比亞人家的小寵物
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2185" src="/media/EgyptTravel/IMG_2185.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2188" src="/media/EgyptTravel/IMG_2188.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
努比亞人家的廣場屋頂是不太能遮雨的，不過如果真的下雨了，他們反而會很開心，因為一年可能只下個一兩場雨而已
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2189" src="/media/EgyptTravel/IMG_2189.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
牠是活的，努比亞人家的大寵物
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2190" src="/media/EgyptTravel/IMG_2190.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2191" src="/media/EgyptTravel/IMG_2191.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_4"&gt;三角風帆船&lt;/h3&gt;
&lt;p&gt;三角風帆船 Felucca 是在尼羅河上所使用的交通工具之一，據說最早以前是給法老王到各處巡視時使用的，後來才慢慢的普遍起來。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2204" src="/media/EgyptTravel/IMG_2204.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2212" src="/media/EgyptTravel/IMG_2212.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
努比亞人的迎賓曲
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2214" src="/media/EgyptTravel/IMG_2214.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2220" src="/media/EgyptTravel/IMG_2220.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="hassan"&gt;Hassan大哥的特別叮嚀&lt;/h3&gt;
&lt;p&gt;最後回到遊輪，當地導遊Hassan大哥跟我們說晚上可以自由在附近逛，但是要特別注意很多小販賣的東西品質並不好。&lt;/p&gt;
&lt;p&gt;例如，香精，埃及很多小販賣得並不是香精，而是精油，Hassan大哥說未來會參觀香精工廠，到了那邊在買就可以了。&lt;/p&gt;
&lt;p&gt;另外，紙莎草，好的紙莎草在埃及一張A4大小大概要數百美元，但是有一些店卻是可以賣到只有1美元，那些大部分是用香蕉製成的紙，可能差不多放三個月就會開始脫落，好的紙莎草是可以存放千年之久的，Hassan大哥也說未來會參觀紙莎草工廠，所以也不急著現在買。因為我在背包客棧看到很多人會推路克索的 LOTUS PAPYRUS，那邊老闆會手繪紙莎草書籤，我趕緊拿這家店去問Hassan大哥，他說他知道這家店，因為很多台灣人問過他，他說這家的品質不是很好，不建議我去購買，果然有些事情還是當地人清楚，這也算是跟團的好處。&lt;/p&gt;
&lt;p&gt;&lt;img alt="IMG_2221" src="/media/EgyptTravel/IMG_2221.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
遊輪的頂樓空間很適合放空
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;</content><category term="Life"></category><category term="遊記"></category><category term="埃及"></category></entry><entry><title>[入埃及記] Day1-2: 開羅 穆罕默德阿里清真寺</title><link href="https://ycc.idv.tw/egypt-travel_1.html" rel="alternate"></link><published>2020-02-05T12:00:00+08:00</published><updated>2020-02-05T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-05:/egypt-travel_1.html</id><summary type="html"></summary><content type="html">&lt;p&gt;&lt;img alt="map" src="/media/EgyptTravel/egypt_map_d2.png" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Day1 行程：桃園機場 &amp;gt; ✈️(10 hrs) &amp;gt; 杜拜機場&lt;br /&gt;
Day2 行程：杜拜 &amp;gt; ✈️(4 hrs) &amp;gt; 開羅Cairo【穆罕默德阿里清真寺】 &amp;gt; 🚐 &amp;gt; 旅館盥洗 &amp;gt; 🚆(夜臥火車😴)  &amp;gt; 亞斯文Aswan&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="_1"&gt;啟程&lt;/h3&gt;
&lt;p&gt;從小我就對埃及有種嚮往，到埃及旅遊也是人生清單的其中之一，所以這次旅行可以算是我們圓夢之旅吧！&lt;/p&gt;
&lt;p&gt;原本打算自由行，不過後來做了一些功課發現埃及要自由行不是那麼容易，所以這次就選擇跟雄獅的十日團，雖然可能會貴一些，也比較不自由一點，但是至少旅程過程有人幫忙打點。&lt;/p&gt;
&lt;p&gt;2020年2月5日晚上11點從桃園機場出發，搭阿聯酋一路10小時到杜拜機場轉機，再搭4小時的飛機到開羅機場。&lt;/p&gt;
&lt;p&gt;2月的埃及早上的體感溫度大概就像是台北的秋天，短袖配長袖襯衫外加薄外套一件就夠了，埃及因為在沙漠地區，日夜溫差大，晚上的體感溫度大概像是台北的冬天，但是又不是像是寒流來的時候的那樣冷。&lt;/p&gt;
&lt;p&gt;要出境開羅機場前會有助理幫忙打點，要換埃磅、買Sim卡他都會幫忙，甚至還有賣口罩，然後領隊小綠大哥跟我們說一些當地的淺規則，出境機場時不要用推車，不然機場人員會「細細」查驗；如果有人塞東西到你手上，馬上把東西放到地板上，然後快速離開；如果遇到路邊的移動小販，沒有想要購買的話，千萬不要和他們有眼神接觸，不然他就會纏上來；還有在埃及買東西，他們會看你是外國人就隨意喊價，所以殺價可以從三折殺起（你沒看錯，就是三折），如果真的殺不下來，拿出你在台灣準備好的10元原子筆，通常效果會不錯，等等之類的，在埃及旅遊有一堆的眉眉角角。&lt;/p&gt;
&lt;p&gt;出境後我們就準備搭小巴，當地安檢很嚴格，小巴在搭乘前會整個檢查一遍，可能怕有炸彈之類的，我們的小巴在開羅也有警察壓車護送。&lt;/p&gt;
&lt;p&gt;開羅是埃及的首都，1979年入選聯合國世界文化遺產，世界最古老的伊斯蘭城市，隱身於繁華現代化的開羅城中，區內擁有許多著名的清真寺、浴池、 噴泉及宗教學院，自969年法瑪蒂王朝建都於此之後，開羅便發展為伊斯蘭世界新興之中心都市，並於14世紀時達於黃金時期。踏入此區，映入眼簾的盡是典雅的清真寺尖塔，因此自古以來開羅便有「千塔之城」之美譽。&lt;/p&gt;
&lt;p&gt;同時，古埃及的古都 孟斐斯(Memphis) 也在開羅附近。公元前3100年左右，傳說上埃及國王&lt;a href="https://zh.wikipedia.org/wiki/美尼斯"&gt;美尼斯&lt;/a&gt;統一上、下埃及，建立&lt;a href="https://zh.wikipedia.org/wiki/第一王朝"&gt;第一王朝&lt;/a&gt;，開啟了古王朝時代，定都&lt;a href="https://zh.wikipedia.org/wiki/孟斐斯_(埃及)"&gt;孟斐斯&lt;/a&gt;（今&lt;a href="https://zh.wikipedia.org/wiki/开罗"&gt;開羅&lt;/a&gt;西郊），成為古埃及第一個&lt;a href="https://zh.wikipedia.org/wiki/法老"&gt;法老&lt;/a&gt;，古埃及從此開始了王朝時期。此時的埃及已經具備了&lt;a href="https://zh.wikipedia.org/wiki/文明"&gt;文明&lt;/a&gt;的幾個基本特徵，比如有行政官員、士兵、宗教、文字等。&lt;/p&gt;
&lt;p&gt;而古代的上、下埃及也大概是以 孟斐斯(Memphis) 來劃分，Memphis 以北也就是尼羅河的下游稱為「下埃及」，Memphis 以南也就是尼羅河的上游稱為「上埃及」。順道一提，尼羅河的東岸稱為「東岸」，尼羅河的西岸稱為「西岸」。&lt;/p&gt;
&lt;p&gt;古埃及的歷史悠久，是世界文明古國之一，但是古埃及結束之後，埃及長時間的受外來政權統治，分別經歷希臘、羅馬、阿拉伯、鄂圖曼土耳其帝國、法國、英國，終於在1922年2月28日，英國被迫承認埃及獨立，才結束了長時間的外來政權統治。也因此古埃及文明和知識並沒有被傳承下來，直到現代才被考古學發現後重新被世人認知。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1695.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
埃及領隊 Hassan
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;導遊Hassan大哥跟我們說自古因為埃及的戰略地理位置好，北有地中海、東有紅海，所以埃及成為了必爭之地，希臘統治300多年、羅馬統治700多年、阿拉伯統治500多年、鄂圖曼土耳其統治400多年、法國統治3年、英國統治80年、以色列統治10年，超過兩千年的時間埃及才獨立自治。&lt;/p&gt;
&lt;p&gt;也因此目前蘇伊士運河為埃及帶來豐富的收入，蘇伊士運河連接紅海與地中海，也就等同於連接了亞洲和歐洲，埃及每天可以受到大約1000萬美金的過路費。&lt;/p&gt;
&lt;p&gt;接下來Hassan講了一下埃及的地理，首先埃及不純然是個非洲國家，它的腹地包含位在亞洲的西奈半島，全埃及佔地約100萬平方公里。另外一個需要正名的是，在埃及不存在撒哈拉沙漠，因為撒哈拉在阿拉伯文就是沙漠的意思，埃及有三大沙漠佔地64%，尼羅河東邊的阿拉伯沙漠、尼羅河西邊的大沙漠、還有西奈半島也是沙漠。&lt;/p&gt;
&lt;p&gt;埃及人口9000萬人，埃及是一個阿拉伯國家，但是不是所有埃及人都是阿拉伯人。&lt;/p&gt;
&lt;h3 id="_2"&gt;穆罕默德阿里清真寺@開羅&lt;/h3&gt;
&lt;p&gt;在開羅的第一站就是在「大城堡」裡頭的穆罕默德阿里清真寺。&lt;/p&gt;
&lt;p&gt;大城堡位於開羅，大城堡曾經是埃及的統治中心，時間長達700年之久，歷經多個王朝，於1176年開始興建，當初的目的是在於抵抗十字軍，後來，在土耳其、拿破崙和法國統治時期，都是一個重要的據點。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/citadel_map.jpeg" /&gt;
&lt;center&gt;&lt;small&gt;
cite from: 埃及 / 墨刻編輯部
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1697.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
大城堡入場券
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1698.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
穆罕默德阿里清真寺
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;穆罕默德阿里清真寺是穆罕默德•阿里於19世紀統治埃及時，認為有必要在城堡中建築一座清真寺以供祈禱之用，歷經了18年才建成此宏偉建築，這座宏偉的建築迄今已有200年歷史。&lt;/p&gt;
&lt;p&gt;穆罕默德•阿里在埃及近代史上享譽盛名，有現代埃及之父的美稱，他積極推廣教育、改良耕作，帶領埃及走向現代化。&lt;/p&gt;
&lt;p&gt;外部是雪花石建材，內部華麗的祈禱室，由高達52公尺的中心圓頂支撐，遼闊的庭院，還保留有以路克索神殿方尖碑和法國換來的時鐘，可媲美伊斯坦堡的藍色清真寺，穆罕默德阿里清真寺乃埃及現代化的最佳表徵。&lt;/p&gt;
&lt;p&gt;進到清真寺不能穿短褲，也不宜穿的太清涼，入內需要脫鞋，建議鞋子不要放外面，提著鞋子入內，Hassan大哥帶我們把鞋子放到裡頭的窗台前，果然是內行的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1775.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
鞋子可以放在裡面的窗台前
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1791.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;就這樣入內席地而坐，Hassan大哥開始介紹埃及的宗教，羅馬統治時引入了基督教，鄂圖曼土耳其帝國時引入了伊斯蘭教，目前埃及信仰中基督教佔 8%、伊斯蘭教佔 92%。&lt;/p&gt;
&lt;p&gt;因為Hassan大哥本身是埃及人，也是伊斯蘭教的信徒，他開始仔細的介紹伊斯蘭教的禱告，虔誠的信徒一天要禱告五次，禱告前要將手、頭、臉、鼻內、口和腳全部洗過三次，然後朝著麥加的方向跪拜真主阿拉，並且要唸一段約5分鐘的經文。他開玩笑的說，埃及人一天要洗15次手，難怪沒有武漢肺炎。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1776.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
階梯面向麥加的方向
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1790.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1788.jpg" /&gt;
&lt;center&gt;&lt;small&gt;
四個柱子撐起的圓拱式建築
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1824.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1839.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_3"&gt;夜晚的臥舖火車&lt;/h3&gt;
&lt;p&gt;參觀完清真寺晚上要驅車前往亞斯文，在那之前旅行社有安排盥洗的飯店，因為火車上面沒辦法洗澡。&lt;/p&gt;
&lt;p&gt;順道一提，埃及的交通真的蠻亂的，鮮少看到紅綠燈，車子基本上都是用擠的，逆向也是很常見的，馬路上充斥著喇叭聲，還好我不是自助旅行。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1877.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1880.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;臥舖火車的品質跟想像中的一樣差，餐點也是不好吃，當然有做功課的我已經準備好泡麵了，一個房間擠兩個人，空間相當的狹小，這個倒是還好，但是環境真的不是很乾淨，空氣中有滿滿的發霉味。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1889.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1891.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1894.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;廁所也是很嚇人的髒，馬桶是直通火車外面的地板，所以車站可能會飄來大便味是不意外的，反正都是預期內的事，來埃及本來就不是來享受度假的（煙～&lt;/p&gt;
&lt;p&gt;吃完晚餐鋪好上下鋪，就準備睡覺囉！&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1898.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="/media/EgyptTravel/IMG_1899.jpg" /&gt;&lt;/p&gt;</content><category term="Life"></category><category term="遊記"></category><category term="埃及"></category></entry><entry><title>[Paper] Wide &amp; Deep Learning for Recommender Systems</title><link href="https://ycc.idv.tw/wide-and-deep-learning.html" rel="alternate"></link><published>2019-06-01T12:00:00+08:00</published><updated>2019-06-01T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2019-06-01:/wide-and-deep-learning.html</id><summary type="html">&lt;p&gt;以往認為deep learning有辦法完全取代feature engineering，Google在2016年寫下的這篇paper，指出在數據相對稀疏（sparse）的情況下feature engineering仍然有其重要性&lt;/p&gt;</summary><content type="html">&lt;h3 id="_1"&gt;結論寫在前面&lt;/h3&gt;
&lt;p&gt;以往認為deep learning有辦法完全取代feature engineering，Google在2016年寫下的這篇paper，指出在數據相對稀疏（sparse）的情況下feature engineering仍然有其重要性，此篇paper使用聯合訓練（jointly train）deep和wide結構的方法，得到比純粹deep或純粹wide的效果還好的成果，這裡的wide就是我們一般所說的feature engineering。&lt;/p&gt;
&lt;h3 id="memorization-and-generalization"&gt;Memorization and Generalization&lt;/h3&gt;
&lt;p&gt;Memorization指的就是wide learning，此部分的產生需要較多的人為參與，也就是feature engineering，通常我們會依照人為的認知、或是統計數據、或是反覆實驗的結果，將一些有關聯性的變數進行cross-product以讓model得以「記住」這些相關性，通常這種基於memorization的結構會比較具有解釋性。&lt;/p&gt;
&lt;p&gt;Generalization指的就是deep Learning，此部份與wide learning相反，結構本身因為它的深度結構，等效於多次的non-linear transformation，也因此model自身在學習的過程就會將隱藏的feature組合給找出來，所以不用太多的人為參與，所以它會比較「一般化」。&lt;/p&gt;
&lt;p&gt;近年來deep learning大行其道，所以人們往往認為已經沒有必要再去做feature engineering了，只需要設定好深度結構，機器自動會去學出我們人類已知的feature組合，甚至學出隱藏的feature組合，但這篇paper指出這樣的想法是錯誤的，Wide結構有其重要性。&lt;/p&gt;
&lt;h3 id="_2"&gt;推薦問題&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Overview of the recommender system" src="/media/Papers/WideAndDeepLearning-001.png" /&gt;&lt;/p&gt;
&lt;p&gt;這篇paper想要解決的是Google Play上的app推薦問題，如上圖所示，我們透過Learner從Logs中學習出一個Model，接下來使用這個Model為app打上推薦的分數，再進行排序，不過因為我們要評分的app超過一百萬個以上，在幾十 miliseconds 的限制推薦時間之下，根本是來不及的，所以這邊需要先進行Retrieval，我們並不是把所有的app都評過一次分數，而是Retrieval事先挑選出一些app再進行評分，挑選方式是使用其他的machine learning方法或是人為規則限定。&lt;/p&gt;
&lt;h3 id="_3"&gt;精神&lt;/h3&gt;
&lt;p&gt;基於memorization的推薦系統，通常比較容易推薦出過去曾經被使用者作用過的app；相反的，基於generalization的推薦系統，它的多樣性（diversity ）會更好一點，更可能去推薦一些不曾使用過或很少被使用過的app，以下詳細解釋。&lt;/p&gt;
&lt;p&gt;對於大型推薦系統而言，我們經常使用wide learning搭配logistic regression，因為它簡單、可擴充和可解釋。舉個cross-product的例子： &lt;code&gt;AND(user_installed_app=netflex, impression_app=pandora)&lt;/code&gt; ，這個feature組合就相當有可解釋性，它指的是同時滿足使用者曾經安裝過netflex和接下來會顯示pandora，相當直觀。但也因為這種feature組合是基於過去資料，造成model難以學出過去沒出現過的組合，所以多樣性會較差。&lt;/p&gt;
&lt;p&gt;另一方面，embedding-base models，例如：factorization machines或deep neural networks，能利用低維度的embedding vector去學出更一般的行為，讓過去沒出現過的組合更有可能出線，進而增加多樣性。其實說穿了，embedding的概念就是降維，當今天維度降低、輸入參數變少，就會迫使系統去學更一般、更重要的規則。&lt;/p&gt;
&lt;p&gt;不過embedding-base models如果遇到稀疏（sparse）的情形就會很慘，因為資料稀疏，一般化的規則出現次數不怎麼多，所以容易過度一般化（over-generalize），然後就學出完全不相關的東西。&lt;/p&gt;
&lt;p&gt;綜上所述，在稀疏情況下的推薦系統，最好要同時考慮memorization和generalization，作者這裡是使用聯合訓練（jointly train），也就是將兩種系統綁在一起優化，這和ensemble models不一樣，ensemble models是不同model各自訓練再結合，這樣做的缺點是需要更多的model參數。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Apps recommendation pipeline overview" src="/media/Papers/WideAndDeepLearning-002.png" /&gt;&lt;/p&gt;
&lt;h3 id="wide-deep-learning"&gt;Wide &amp;amp; Deep Learning: 資料處理&lt;/h3&gt;
&lt;p&gt;如上圖所示，我們採用User Data和App Impression Data，接下來利用Vocabulary Generator把categorical feature轉成整數ID，categorical feature的labels不是0就是1，另外continuous features則會進行normalization，將labels映射到 [0, 1] 之間，normalization的方法是採用分位數（quantiles）方法，將feature的數據分成 &lt;span class="math"&gt;\(n_q\)&lt;/span&gt; 位數，每個級距內的數值映射到 &lt;span class="math"&gt;\((i-1)/(n_q-1)\)&lt;/span&gt; ，&lt;span class="math"&gt;\(i\)&lt;/span&gt; 指的是它的分位數落在哪裡。&lt;/p&gt;
&lt;h3 id="wide-deep-learning_1"&gt;Wide &amp;amp; Deep Learning: 模型訓練&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Wide &amp;amp; Deep model structure for apps recommendation" src="/media/Papers/WideAndDeepLearning-003.png" /&gt;&lt;/p&gt;
&lt;p&gt;上圖是model structure，左側是Deep的部分，右側是Wide的部分。&lt;/p&gt;
&lt;p&gt;Wide &amp;amp; Deep Learning是採用back-propagation來優化「評分」，而Wide的部分，使用FTRL (Follow-the-regularized-leader) algorithm搭配L1 regularization當作optimizer來優化，這種算法會使得model weights更稀疏，詳細解釋未來需要整整一篇來介紹；Deep的部分，則是使用AdaGrad當作optimizer來優化。&lt;/p&gt;
&lt;p&gt;來更仔細的看Deep部份的結構，從下而上看起，每個categorical features都會經過一個降維轉化，轉化成32維的embedding vector，降維迫使系統學習更一般的規則。接下來把所有的feature包含continuous feature和categorical features連結在一起，成為Concatenated Embeddings，這大概有1200維左右。這個Concatenated Embeddings接下來會餵進去三層的ReLU，就完成了。&lt;/p&gt;
&lt;h3 id="_4"&gt;實驗結果&lt;/h3&gt;
&lt;p&gt;Offline實驗的結果，純粹Wide當作控制組它的AUC為&lt;span class="math"&gt;\(0.726\)&lt;/span&gt;，純粹Deep的AUC為&lt;span class="math"&gt;\(0.722\)&lt;/span&gt;，稍差於控制組，Wide &amp;amp; Deep的AUC為&lt;span class="math"&gt;\(0.728\)&lt;/span&gt;，稍好於控制組。Online AB分流實驗差異比較顯著，與控制組（純粹Wide）相比，純粹Deep增加了&lt;span class="math"&gt;\(2.9\%\)&lt;/span&gt; 的獲取率，Wide &amp;amp; Deep增加了 &lt;span class="math"&gt;\(3.9\%\)&lt;/span&gt; 的獲取率，所以確實Wide &amp;amp; Deep是效果最好的。&lt;/p&gt;
&lt;p&gt;眼尖的讀者應該發現一個奇怪的地方，純粹Deep在offline明明比控制組差，為何在online會比控制組好，可能的原因是offline實驗數據是固定的，所以當我增加更多的多樣性，對於offline是沒什麼幫助的，但是對於online而言多樣性可能造成用戶更多的獲取，從這裡我們也看到多樣性的重要性。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Papers"></category></entry><entry><title>尾牙表演 - Sweet Child Oh Mine</title><link href="https://ycc.idv.tw/sweet-child-oh-mine.html" rel="alternate"></link><published>2019-03-30T12:00:00+08:00</published><updated>2019-03-30T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2019-03-30:/sweet-child-oh-mine.html</id><summary type="html">&lt;p&gt;尾牙吼起來&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;div class="video-container"&gt;
    &lt;iframe src="https://www.youtube.com/embed/XNJibRL6UYE" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</content><category term="Life"></category><category term="吉他"></category></entry><entry><title>物件導向武功秘笈（3）：內功篇 — 物件導向指導原則SOLID</title><link href="https://ycc.idv.tw/introduction-object-oriented-programming_3.html" rel="alternate"></link><published>2018-04-14T12:00:00+08:00</published><updated>2018-04-14T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2018-04-14:/introduction-object-oriented-programming_3.html</id><summary type="html">&lt;p&gt;物件導向怎麼用才能成就好的程式碼？ / UML類別圖 / 單一職責原則(Single Responsibility Principle, SRP) / 開閉原則(Open-Closed Principle, OCP) / 里氏替換原則(Liskov Subsititution Principle, LSP) / 迪米特法則(Law of Demeter, LoD) / 依賴倒置原則(Dependence Inversion Principle, DIP) / 接口分隔原則(Interface Segregation Principle, ISP)&lt;/p&gt;</summary><content type="html">&lt;div class="toc"&gt;&lt;span class="toctitle"&gt;Table of Contents&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#_1"&gt;物件導向怎麼用才能成就好的程式碼？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#uml"&gt;UML類別圖&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#single-responsibility-principle-srp"&gt;單一職責原則(Single Responsibility Principle, SRP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#open-closed-principle-ocp"&gt;開閉原則(Open-Closed Principle, OCP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#liskov-subsititution-principle-lsp"&gt;里氏替換原則(Liskov Subsititution Principle, LSP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#law-of-demeter-lod"&gt;迪米特法則(Law of Demeter, LoD)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#dependence-inversion-principle-dip"&gt;依賴倒置原則(Dependence Inversion Principle, DIP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#interface-segregation-principle-isp"&gt;接口分隔原則(Interface Segregation Principle, ISP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#solid"&gt;總結：物件導向的指導原則—SOLID&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reference"&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3 id="_1"&gt;物件導向怎麼用才能成就好的程式碼？&lt;/h3&gt;
&lt;p&gt;一個好的工具，也要配合對於工具的理解，才能發揮效用。&lt;a href="/introduction-object-oriented-programming_2.html"&gt;在上一回中&lt;/a&gt;，我們完整介紹了Java和Python的物件導向實現方式，我們講到了「封裝」、「繼承」、「多型」等等物件導向的特色，也講了「抽象類別」、「接口」等抽象化的方法，不過我並沒有告訴大家該怎麼用這些工具？使用這些工具是不是有什麼樣的法則？&lt;/p&gt;
&lt;p&gt;在接下來的這一篇，我將會介紹物件導向的使用方式，我會提到物件導向著名的六大法則SOLID：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;單一職責原理&lt;/li&gt;
&lt;li&gt;開閉原理&lt;/li&gt;
&lt;li&gt;里氏替換原則&lt;/li&gt;
&lt;li&gt;迪米特法則&lt;/li&gt;
&lt;li&gt;依賴倒置原則&lt;/li&gt;
&lt;li&gt;接口分隔原則&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在這之前我們先來介紹描述類別關係的UML類別圖。&lt;/p&gt;
&lt;h3 id="uml"&gt;UML類別圖&lt;/h3&gt;
&lt;p&gt;開始介紹各種原則之前，先來介紹UML類別圖，UML全名稱為Unified Modeling Language，是一種使用圖形來描繪軟體工程架構的方法，這邊準備介紹的是它的類別圖，這個工具有助於我們快速的了解物件與物件之間的關係。&lt;/p&gt;
&lt;p&gt;首先先來看一下UML類別圖的節點，共有三種：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;類別(Class): 其中第一個 block 表示名稱、第二個 block 表示變數、第三個 block 表示方法。而 &lt;code&gt;-&lt;/code&gt; 代表 &lt;code&gt;private&lt;/code&gt;，&lt;code&gt;+&lt;/code&gt; 代表&lt;code&gt;public&lt;/code&gt;，&lt;code&gt;#&lt;/code&gt; 代表&lt;code&gt;protected&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Class" src="/media/SOLID_Introduction/Class.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;抽象類別(Abstract Class)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="AbstractClass" src="/media/SOLID_Introduction/AbstractClass.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;接口(Interface)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Interface" src="/media/SOLID_Introduction/Interface.png" /&gt;&lt;/p&gt;
&lt;p&gt;接下來來了解UML類別圖的連接關係，從連接關係的強到弱依序介紹：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;繼承關係(Inheritance)和抽象類、接口實現：在UML類別圖中，箭頭方向代表依賴方向，A箭頭指向B，代表A依賴B，代表B的改變將連同改變A，而A的改變不影響B。因此在繼承關係中，子類箭頭指向父類，意味著子類依賴父類。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Inheritance" src="/media/SOLID_Introduction/Inheritance.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合成（組合）關係(Composition)：指的是 "is-part-of" 的關係，是一個強的「擁有」關係。實心菱形指向整體、箭頭指向部件（代表整體依賴部件），整體不可以脫離部件而存在，例如下面範例中飛機不能沒有引擎。在程式碼中，通常部件會放在「成員變數」中，並且在實例化時就產生。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Composition" src="/media/SOLID_Introduction/Composition.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;PlaneEngine&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Plane&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_engine&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PlaneEngine&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;聚合關係(Aggregation)：指的是 "owns-a" 的關係，是一個弱的「擁有」關係。空心菱形指向整體、箭頭指向部件（代表整體依賴部件），整體可以脫離部件而存在，整體和部件擁有各自的生命週期，例如下面範例中飛機場有停放飛機，但是除去飛機，飛機場仍可以正常運作。在程式碼中，通常部件會放在「成員變數」中。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Aggregation" src="/media/SOLID_Introduction/Aggregation.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Plane&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Airport&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_inplace_planes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;關聯關係(Association)：指的是 "has-a" 的關係，是個「有個」的關係。兩種類別擁有各自的生命週期，且兩者並不具備整體與部件的關係，我們使用 Association 來連接，箭頭代表依賴的方向，例如下面範例中飛機和排程不具有整體與部件的關係，但飛機有個排程。在程式碼中，會放在「成員變數」中。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Associatione" src="/media/SOLID_Introduction/Association.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Schedule&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Plane&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_schedule&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Schedule&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;依賴關係(Dependency)：指的是 "uses-a" 的關係，是個「使用」的關係。A類中使用到B類，但僅僅是弱連結（在程式碼中，不放在「成員變數」中），譬如：B類作為A類方法的參數、B類作為A類的局域變數、A類調用B類的靜態方法、B類作為A類方法的回傳值，就稱為：A依賴B。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Dependency" src="/media/SOLID_Introduction/Dependency.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Plane&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;PlanePilot&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;pilot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;plane&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;pass&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="single-responsibility-principle-srp"&gt;單一職責原則(Single Responsibility Principle, SRP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定義：There should never be more than one reason for a class to change.（一個類別中不要有多於一個以上的變化理由）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;簡單的說，就是一個類別中不要做超過一件事，要去切分直到不能再分割為止，如此一來可以提高內聚性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;乍看之下，這樣的原則很容易實現，但是魔鬼藏在細節裡，我們常常會沒注意到其實還可以繼續的切分。舉個例子，假設我想設計一個電話的接口，我可能是這樣設計的&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="phone_1" src="/media/SOLID_Introduction/phone_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;乍看之下沒有問題，一個電話擁有撥號、掛號、數據傳送和接收，但是等等！連接的過程和數據的傳輸其實是兩個職責啊！它們之間沒有強烈的關聯性，完全是可以分開處理的，因此這個配置不符合「單一職責原則」，可以繼續切分下去，修改如下。&lt;/p&gt;
&lt;p&gt;&lt;img alt="phone_2" src="/media/SOLID_Introduction/phone_2.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;「單一職責原則」原文指的是類別的單一職責，但是務實上，類別如果切分到如此程度，程式碼會變得細碎不堪，這違反了程式碼的「可讀性」，所以我們一般只要求「接口必須保持單一原則」，而類別去套用接口，類別就盡量達成少的職責就好。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="open-closed-principle-ocp"&gt;開閉原則(Open-Closed Principle, OCP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定義：Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification.（軟體中的實體，例如：類、模組、函數等等，都必須對延伸開放，但對修改封閉）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對延伸開放：實體在因應新的改變時，必須是可以靈活擴充的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;對修改封閉：實體一旦完成，就盡量不要再去修改它了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;綜合以上兩點，我們可以總結出：實體本身的內聚性要高，可以讓我們未來不需要再做修改，單一職責可以做到增強內聚性；實體間的耦合性要低，所以實體像是積木一樣可以因應各種需求去任意組合、擴充。所以「開閉原則」只是進一步的把「低耦合高內聚」再說的更清楚一點，實現「開閉原則」將有利於單元測試、提高維護和擴充能力。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="liskov-subsititution-principle-lsp"&gt;里氏替換原則(Liskov Subsititution Principle, LSP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;定義：What is wanted here is something like the following substitution property: If for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T.（簡言之：子類對象能夠替換其父類對象，使用父類方法而不會有問題）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;「里氏替換原則」用於規範繼承，子類繼承自父類的方法是保有彈性可以覆寫(Overriding)和多載(Overloading)的，但是應該怎麼做，程式碼才不會髒掉？「里氏替換原則」告訴我們一個簡單的法則，就是先寫一段父類的執行代碼，然後把父類替換成子類，然後再跑跑看能不能正常執行，如果正常執行代表這個繼承關係是健康的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;為什麼要這樣檢查？之前我們提過繼承主要是為了要避免Repeat Yourself而生，我們找出各種類別共享的屬性和方法，把它獨立出來，然後大家再一起繼承自它，所以我們要盡可能的避免父類出現不是共享的性質。也就是說在理想情況下「父類必須等於子類們的交集」，所以「父類必定是任一子類的子集合」，因此「使用子類來執行父類是不應該有問題的」，這就是「里氏替換原則」。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;為了遵循「里氏替換原則」，則子類必須完全實現父類的方法。如果子類不能完整地實現父類的方法，或者父類的某些方法在子類中已經發生了「畸變」，則建議斷開父子繼承關係，採用依賴、聚集、組合等關係替代。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;有了「里氏替換原則」，我們終於可以談談一個上一章沒提到的重要問題：什麼情況可以做繼承？有一些書籍會告訴你，繼承為"is-a"的關係，例如：瑪爾濟斯(B) is-a 狗(A)，所以瑪爾濟斯(B)可以繼承狗(A)，乍看之下沒問題，但這樣的說法存在缺陷，舉個例子，假設今天我先有了類別&lt;code&gt;Retangle&lt;/code&gt;，也就是長方形，然後我想要弄一個新的類別&lt;code&gt;Square&lt;/code&gt;，也就是正方形，我可以讓&lt;code&gt;Square&lt;/code&gt;繼承自&lt;code&gt;Retangle&lt;/code&gt;嗎？我們用"is-a"來檢視：正方形是一個長方形？答案是Yes，但是「里氏替換原則」持相反意見，來看一下，&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="square_1" src="/media/SOLID_Introduction/square_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;依照「里氏替換原則」，&lt;code&gt;Square&lt;/code&gt;不能繼承自&lt;code&gt;Retangle&lt;/code&gt;，因為&lt;code&gt;Square&lt;/code&gt;只需要&lt;code&gt;width&lt;/code&gt;的成員變數，而&lt;code&gt;Retangle&lt;/code&gt;則需要&lt;code&gt;width&lt;/code&gt;和&lt;code&gt;height&lt;/code&gt;兩個成員變數，當我們將子類&lt;code&gt;Square&lt;/code&gt;放到父類&lt;code&gt;Retangle&lt;/code&gt;的方法中，因為缺少&lt;code&gt;height&lt;/code&gt;變數，必然會出錯，所以違反「里氏替換原則」，因此這兩類不適合作為「繼承」關係。我們可以這樣改善，讓&lt;code&gt;Square&lt;/code&gt;應用&lt;code&gt;Retangle&lt;/code&gt;來幫忙計算，使用「關聯」關係取代「繼承」關係。&lt;/p&gt;
&lt;p&gt;&lt;img alt="square_2" src="/media/SOLID_Introduction/square_2.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Retangle&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_area&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_width&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_height&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Square&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_retangle&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Retangle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_area&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_retangle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_area&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;下面這一張集合圖是我自創的，圖中清楚的指出「繼承」中的父類和子類應該是什麼樣的關係。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Inheritance Principle.jpeg" src="/media/SOLID_Introduction/inheritance_principle.jpeg" /&gt;&lt;/p&gt;
&lt;h3 id="law-of-demeter-lod"&gt;迪米特法則(Law of Demeter, LoD)&lt;/h3&gt;
&lt;p&gt;又稱為「最少知識原則」，它規定物件應該要對其他物件有最少的了解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;規則1：一個物件應該與它「朋友」互動，而不應該與陌生對象互動。&lt;/strong&gt;這樣可以減少耦合，提高物件之間的低耦合性，使得物件與物件之間的關係更加簡單易懂。「朋友」的定義：對於類別 C 的其中一個方法 M 而言，在 M 的方法中僅能訪問以下物件：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;self&lt;/code&gt;，類別 C 自身&lt;/li&gt;
&lt;li&gt;C 的成員變數&lt;/li&gt;
&lt;li&gt;M 的輸入參數&lt;/li&gt;
&lt;li&gt;M 的輸出物件&lt;/li&gt;
&lt;li&gt;全域變數的物件&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;舉個例子：假設今天一名老師給了學生名條想叫班長幫忙點名&lt;/p&gt;
&lt;p&gt;錯誤示範：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Student&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Leader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;countStudents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;student_list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Total number of students is &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;student_list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Teacher&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;command&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;student_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Student&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# `Student` is not a friend&lt;/span&gt;
        &lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;countStudents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;student_list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;teacher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Teacher&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;leader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Leader&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;name_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;D&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;E&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;teacher&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="teacher-leader-student_1" src="/media/SOLID_Introduction/teacher-leader-student_1.png" /&gt;&lt;/p&gt;
&lt;p&gt;我們來使用「迪米特法則」來檢驗一下，&lt;code&gt;Teacher.command&lt;/code&gt; 的「朋友」有 &lt;code&gt;self&lt;/code&gt;、輸入參數 &lt;code&gt;name_list&lt;/code&gt; (&lt;code&gt;List[String]&lt;/code&gt;) 和 &lt;code&gt;leader&lt;/code&gt; (&lt;code&gt;Leader&lt;/code&gt;)，但在上面這個例子它使用到了不是「朋友」的 &lt;code&gt;Student&lt;/code&gt;，這會使得 &lt;code&gt;Teacher&lt;/code&gt; 和 &lt;code&gt;Student&lt;/code&gt; 會產生不必要的耦合。解法是，我們可以將創造 &lt;code&gt;student_list&lt;/code&gt; 的權責轉移到 &lt;code&gt;Leader&lt;/code&gt; 上，如此一來就可以斷開 &lt;code&gt;Teacher&lt;/code&gt; 和 &lt;code&gt;Student&lt;/code&gt; 的耦合。&lt;/p&gt;
&lt;p&gt;正確示範：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Student&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Leader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;giveNameList&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_student_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Student&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;countStudents&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Total number of students is &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_student_list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Teacher&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;command&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;giveNameList&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;countStudents&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;teacher&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Teacher&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;leader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Leader&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;name_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;B&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;D&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;E&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;teacher&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;leader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="teacher-leader-student_2" src="/media/SOLID_Introduction/teacher-leader-student_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;為什麼這樣規範呢？先來想想「朋友」有什麼共通之處，其實它們都是類別本身無法斷開耦合的物件，既然無法斷開耦合，何不運用到底，運用這些「朋友」來完成任務，不要再去增加其他的耦合性，也同時幫助提升類別的內聚性，這就是「迪米特法則」想做的事。以這樣的方式去寫程式，也可以避免寫出像是&lt;code&gt;A.getB().getC()&lt;/code&gt;的程式碼（A和C不是朋友），這樣冗長的程式碼不僅增加了無益的耦合，也讓程式變得不利於可讀性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;規則2：減少類別的對外方法，將沒必要對外公布的方法隱藏起來。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;例子: 安裝程式。&lt;/p&gt;
&lt;p&gt;錯誤範例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Wizard&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;first&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install first step of wizard at mode&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;second&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install second step of wizard at mode &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;third&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install third step of wizard&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Install&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;second&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;third&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;有太多沒必要對外公布的細節了，依照「迪米特法則」，我們應該將盡量減少對外公布的資訊，把不必要公布的細節私有化。&lt;/p&gt;
&lt;p&gt;正確範例：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Wizard&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_first&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_second&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_third&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_first&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install first step of wizard&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_second&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install second step of wizard at mode &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_third&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Install third step of wizard&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Install&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;wizard&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="dependence-inversion-principle-dip"&gt;依賴倒置原則(Dependence Inversion Principle, DIP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;定義：High level modules should not depend upon low level modules. Both should depend upon abstractions. Abstractions should not depend upon details. Details should depend upon abstractions.（高階模組不應該依賴低階模組，兩者都應該依賴抽象。而抽象不應該依賴細節，反之細節應該要依賴抽象。）&lt;/li&gt;
&lt;li&gt;它要求高階模組不應該依賴低階模組，而是應該依賴抽象。這意味著在設計類之間的關係時，應該避免直接依賴具體類，而是應該依賴抽象類。這樣可以使得高階模組不受低階模組的影響，並且可以更容易地更換和修改低階模組。依賴倒置原則可以通過使用介面和抽象類來實現。&lt;/li&gt;
&lt;li&gt;舉個例子，假設我們有一個高階模組，它負責讀取和顯示數據。我們有一個低階模組，它負責從文件中讀取數據。如果高階模組直接依賴低階模組，那麼如果低階模組的實現發生變化，高階模組也必須作出相應的修改。這樣不符合依賴倒置原則，我們應該抽象出一個抽象類，讓高階模組依賴於這個抽象類，而低階模組實現這個抽象類。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="/media/SOLID_Introduction/dip.png" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DataReader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;CsvDataReader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DataReader&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# code to read data from csv&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;HighLevelModule&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data_reader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;DataReader&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data_reader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_reader&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_and_display_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_data_reader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;file_data_reader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CsvDataReader&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;high_level_module&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;HighLevelModule&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file_data_reader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;high_level_module&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_and_display_data&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;在上面程式碼中，&lt;code&gt;DataReader&lt;/code&gt;是一個抽象類，它定義了一個&lt;code&gt;read_data()&lt;/code&gt;方法。&lt;code&gt;CsvDataReader&lt;/code&gt;是一個具體類，它實現了&lt;code&gt;DataReader&lt;/code&gt;並從文件中讀取數據。&lt;code&gt;HighLevelModule&lt;/code&gt;是一個高階模組，它依賴於&lt;code&gt;DataReader&lt;/code&gt;接口而不是具體類。這樣，當低階模組的實現發生變化時，高階模組不需要作出任何修改，只需要更換實現了&lt;code&gt;DataReader&lt;/code&gt;抽象類的具體類即可。這樣可以使得高階模組更穩定，並且可以更容易地更換和修改低階模組。同時，由於高階模組依賴的是抽象類，而不是具體類，我們可以更輕鬆地對高階模組進行測試，因為我們可以使用模擬數據來模擬低階模組的行為。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;依賴倒置原則又稱為「面向接口原則」，這裡的接口應該想的更廣義一點，不侷限在interface上，我認為只要藉由抽象化將架構擬定出來的這些抽象單元都可以稱作接口，「廣義的接口」可以是指：&lt;ol&gt;
&lt;li&gt;客戶端和業務邏輯的分離介面&lt;/li&gt;
&lt;li&gt;物件的開放方法&lt;/li&gt;
&lt;li&gt;抽象類別&lt;/li&gt;
&lt;li&gt;定義行為的interface&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="interface-segregation-principle-isp"&gt;接口分隔原則(Interface Segregation Principle, ISP)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;定義：Clients should not be forced to depend uponn interfaces that they don't use. The dependency of one class to another one should depend on the smallest possible interface.（客戶類不應該被強迫依賴那些它不需要的接口，類別間的彼此依賴應該建立在盡可能小的接口上）&lt;/li&gt;
&lt;li&gt;這裡說的接口同樣的是剛剛所說的「廣義接口」，可以是客戶端和業務邏輯的分離介面、物件的開放方法、抽象類別和Interface。&lt;/li&gt;
&lt;li&gt;它要求將較大的接口分解成較小的接口，以適應客戶端需求。這樣做可以避免客戶端被迫使實現未使用的方法，並減少程序的耦合性。接口分隔原則建議我們要讓這些廣義接口盡可能的細切，但在實務上，切的過細會導致程式碼非常零碎難以閱讀，所以YC的建議是切到遵守「單一職責原理」就足夠了。&lt;/li&gt;
&lt;li&gt;舉個例子：錯誤範例如下&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;close&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;encrypt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decrypt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SimpleDocument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document opened&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;close&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document closed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document saved&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ComplexDocument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Document&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document opened&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;close&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document closed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document saved&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;encrypt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document encrypted&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decrypt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document decrypted&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;上述程式碼中，&lt;code&gt;SimpleDocument&lt;/code&gt; 類可能只需要繼承中的一部分方法，這樣的設計違反了 ISP 原則，客戶端 (&lt;code&gt;SimpleDocument&lt;/code&gt; 和 &lt;code&gt;ComplexDocument&lt;/code&gt; 類) 被迫使實現未使用的方法。更好的設計方法是將這些方法分為兩個不同的接口，例如 &lt;code&gt;DocumentHandler&lt;/code&gt; 和 &lt;code&gt;DocumentEncoder&lt;/code&gt;，並讓客戶端只實現需要的接口。修改如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DocumentHandler&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;close&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DocumentEncoder&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;encrypt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decrypt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SimpleDocument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DocumentHandler&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document opened&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;close&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document closed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document saved&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ComplexDocument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DocumentHandler&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DocumentEncoder&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document opened&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;close&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document closed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document saved&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;encrypt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document encrypted&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decrypt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Document decrypted&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="" src="/media/SOLID_Introduction/isp.png" /&gt;&lt;/p&gt;
&lt;p&gt;這樣的設計更符合 ISP 原則，因為客戶端 (&lt;code&gt;SimpleDocument&lt;/code&gt; 和 &lt;code&gt;ComplexDocument&lt;/code&gt; 類) 只實現了需要的接口，並沒有被迫使實現未使用的方法。這樣可以減少程序的耦合性，並更容易維護和擴展。&lt;/p&gt;
&lt;h3 id="solid"&gt;總結：物件導向的指導原則—SOLID&lt;/h3&gt;
&lt;p&gt;上面介紹的六大原理：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Single Responsibility Principl&lt;/li&gt;
&lt;li&gt;Open-Closed Principle&lt;/li&gt;
&lt;li&gt;Liskov Subsititution Principle&lt;/li&gt;
&lt;li&gt;Law of Demeter&lt;/li&gt;
&lt;li&gt;Interface Segregation Principle&lt;/li&gt;
&lt;li&gt;Dependence Inversion Principle&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;剛剛好組成SOLID這個單字，所以又被統稱SOLID原則。&lt;/p&gt;
&lt;p&gt;事實上，這些原則所要達到的目的，不外乎就是我們&lt;a href="/introduction-object-oriented-programming_1.html"&gt;第一篇&lt;/a&gt;當中所介紹的好的程式碼特性：「正常執行」、「穩健」、「不重複撰寫」、「可讀性」、「可擴展」，或者是「低耦合、高內聚」，所以寫程式時如果能時時注意，說不定你也可以自己領會這六大法則。&lt;/p&gt;
&lt;p&gt;我來快速的總結這六大法則告訴我們的事：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;在開發程式的初期，先定義好抽象架構，也就是廣義的接口，徹底的使客戶端與業務邏輯分離，將「行為」定義成Interface，將「類別的泛化」定義成Abstract Class。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;所有的實體類別都依賴於抽象，細節依賴於抽象。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;每個單元盡量達到：單一權責、對延伸開放但對修改封閉、盡可能少的對外方法。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;牽涉「繼承」，必須要問自己：子類可以替換父類執行嗎？父類是不是為子類的交集？&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;類別中的方法僅能訪問它的「朋友」們。&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如此一來，我們心中就有一個準則去使用物件導向。&lt;/p&gt;
&lt;p&gt;在一般情形下，這三篇的內容應該就足夠讓你寫出好的程式碼，但是實際面上使用仍然會碰到許多問題，於是乎有人將問題整理並總結出一些套路，這就是「設計模式」，我們以後再來談談吧！今天就先到這。&lt;/p&gt;
&lt;h3 id="reference"&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.tenlong.com.tw/products/9789866761799"&gt;大話設計模式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tenlong.com.tw/products/9787111437871"&gt;設計模式之禪&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;use &lt;a href="https://plantuml.com"&gt;plantuml&lt;/a&gt; in hackmd&lt;/li&gt;
&lt;/ol&gt;</content><category term="CS"></category><category term="軟體設計"></category></entry><entry><title>物件導向武功秘笈（2）：招式篇 — Python與Java的物件導向編程介紹</title><link href="https://ycc.idv.tw/introduction-object-oriented-programming_2.html" rel="alternate"></link><published>2018-04-10T12:00:00+08:00</published><updated>2018-04-10T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2018-04-10:/introduction-object-oriented-programming_2.html</id><summary type="html">&lt;p&gt;物件導向編程 / 類別(Class)與物件(Object) / 方法多載（Method Overloading） / 物件導向三大特性—封裝(Encapsulation) / 物件導向三大特性—繼承(Inheritance) / 抽象化：抽象類別(Abstract Class)、抽象方法(Abstract Method)和接口(Interface) / 物件導向三大特性—多型(Polymorphism) /&lt;/p&gt;</summary><content type="html">&lt;div class="toc"&gt;&lt;span class="toctitle"&gt;Table of Contents&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#_1"&gt;物件導向編程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#classobject"&gt;類別(Class)與物件(Object)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#method-overloading"&gt;方法多載（Method Overloading）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#encapsulation"&gt;物件導向三大特性—封裝(Encapsulation)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#inheritance"&gt;物件導向三大特性—繼承(Inheritance)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#abstract-classabstract-methodinterface"&gt;抽象化：抽象類別(Abstract Class)、抽象方法(Abstract Method)和接口(Interface)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#polymorphism"&gt;物件導向三大特性—多型(Polymorphism)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#_2"&gt;總結：物件導向使用方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3 id="_1"&gt;物件導向編程&lt;/h3&gt;
&lt;p&gt;&lt;a href="/introduction-object-oriented-programming_1.html"&gt;在上一章當中&lt;/a&gt;，我們藉由好的程式碼的特性：「正常執行」、「穩健」、「不重複撰寫」、「可讀性」、「可擴展」，自然而然引出物件導向的概念。在這一章當中YC會接續介紹完整的物件導向要如何實現，包括物件導向三大特性：封裝、繼承和多型。&lt;/p&gt;
&lt;p&gt;在本章我會採用兩種語言交叉作說明，一種是靜態型別的語言Java，另一種是動態型別的語言Python，這兩種語言都是可以實現物件導向的語言，而所謂型別的動態與靜態可以用一個簡單的方法來區分：型別檢查(Type Checking)發生在什麼時候？像Java這類的靜態型別語言，它的型別檢查是在編譯時期(Compile Time)完成的，而像是Python這類的動態型別語言，它的型別檢查則是在執行時期(Runtime)才去做，所以Python可以不事先宣告變數型別，這點使得Python在開發上方便許多。&lt;/p&gt;
&lt;p&gt;雖然Python和Java都是支援物件導向的語言，但在使用上有所差異。Java是一套對物件導向支援非常完整的語言，甚至你可以說Java是物件導向的原生種。而Python為了使程式語言更為簡潔，做出了許多改變，而這也造成在Python使用物件導向要跟著做出相應的改變，如果還守著嚴格的定義，使用起來會非常的彆扭。使用兩種語言說明物件導向是為了讓讀者更能了解物件導向的本質，而非語言本身。&lt;/p&gt;
&lt;p&gt;本篇採用『&lt;a href="https://www.tenlong.com.tw/products/9789866761799"&gt;大話設計模式&lt;/a&gt;』書中的物件導向篇範例。&lt;/p&gt;
&lt;h3 id="classobject"&gt;類別(Class)與物件(Object)&lt;/h3&gt;
&lt;p&gt;首先來看物件導向的基本組成，類別(Class)與物件(Object)。物件導向程式設計中，類別(Class)是一種模板，它描述了物件的共同屬性和行為。物件(Object)是類別的實例，它具有類別定義的屬性和行為。物件是程式中實際存在的東西，而類別只是用來定義物件的模板。 &lt;/p&gt;
&lt;p&gt;舉個例子，我想要創造一隻有名字的貓，她有喵喵叫的能力，在Java中可以寫成&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;//{1}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;private&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{2}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{3}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{4}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{5}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. meow~&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{6}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Test&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;static&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{7}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{8}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{9}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// output:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// My name is May. meow~&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} 建構一個&lt;code&gt;Cat&lt;/code&gt;的類別，類別不是物件，類別只是物件的藍圖。&lt;/p&gt;
&lt;p&gt;{2} 建立一個私有變數&lt;code&gt;name&lt;/code&gt;，用來代表貓的名字，我們使用&lt;code&gt;private&lt;/code&gt;的修飾詞讓它是私有的，也就是說外部環境沒辦法去讀取到這個變數，只有物件內部才可以讀取的到&lt;/p&gt;
&lt;p&gt;{3} 提供建造方法(constructor)來初始化這一個物件，初始化需要&lt;code&gt;name&lt;/code&gt;的參數。&lt;/p&gt;
&lt;p&gt;{4} 在初始化的過程中，我們會將從外部讀取的&lt;code&gt;name&lt;/code&gt;存入私有變數&lt;code&gt;this.name&lt;/code&gt;裡，在Java裡頭，如果外部變數名稱與本地變數名稱相同，需要使用&lt;code&gt;this&lt;/code&gt;來特別區分。&lt;/p&gt;
&lt;p&gt;{5} 創造一個公開的類別方法&lt;code&gt;shout()&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;{6} 使用私有變數&lt;code&gt;name&lt;/code&gt;讓貓可以自我介紹，再發出喵喵叫的聲音。&lt;/p&gt;
&lt;p&gt;{7} Java只要遇到&lt;code&gt;main&lt;/code&gt;就會去執行，方法&lt;code&gt;main&lt;/code&gt;具有靜態方法的修飾詞&lt;code&gt;static&lt;/code&gt;，也就是說&lt;code&gt;Test&lt;/code&gt;不需要被實體化也能執行&lt;code&gt;main&lt;/code&gt;這個方法。&lt;/p&gt;
&lt;p&gt;{8} 使用&lt;code&gt;new&lt;/code&gt;來創造一個物件，在創造的過程會執行初始化，所以必須放入初始化需要的參數&lt;code&gt;name&lt;/code&gt;，所以上面的新的物件有了&lt;code&gt;"May"&lt;/code&gt;的名字。&lt;/p&gt;
&lt;p&gt;{9} 接下來使用&lt;code&gt;cat.shout()&lt;/code&gt;去執行喵喵叫的動作，這個方法會回傳字串，再利用&lt;code&gt;System.out.println&lt;/code&gt;的方法將字串顯示出來。注意！在物件導向的習慣中，會用&lt;code&gt;.&lt;/code&gt;來表示在那物件中的方法或屬性，所以&lt;code&gt;cat.shout()&lt;/code&gt;就是執行在物件&lt;code&gt;cat&lt;/code&gt;中的方法&lt;code&gt;shout()&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;再來看Python怎麼表示，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### Python3&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#{1}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{2}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="c1"&gt;#{3}&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{4}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;. meow~&amp;quot;&lt;/span&gt; &lt;span class="c1"&gt;#{5}&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;#{6}&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="c1"&gt;#{7}&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;#{8}&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# output:&lt;/span&gt;
&lt;span class="c1"&gt;# My name is May. meow~&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} 建構&lt;code&gt;Cat&lt;/code&gt;的類別，這是Python3的表示方法，如果是使用Python2.7的話，要寫成&lt;code&gt;class Cat(object):&lt;/code&gt;才可以。&lt;/p&gt;
&lt;p&gt;{2} Python的初始化方法，Python在初始化之前會先自行執行&lt;code&gt;__new__&lt;/code&gt;的方法，這個過程會產生一個新的物件，也就是實體化，而這個新的物件會以第一個參數的方法被帶入&lt;code&gt;__init__&lt;/code&gt;的方法裡進行初始化，我們通常會命名這個變數為&lt;code&gt;self&lt;/code&gt;，這裡的&lt;code&gt;self&lt;/code&gt;已經是個物件而不是類別，那初始化的過程需要引入外部資訊&lt;code&gt;name&lt;/code&gt;的參數來進行命名，所以第二個參數就要設&lt;code&gt;name&lt;/code&gt;，記住喔！第一個參數是Python自動產生的，不是由外部帶入的，所以外部只要給&lt;code&gt;name&lt;/code&gt;一個參數就足夠了。&lt;/p&gt;
&lt;p&gt;{3} 創造一個私有本地變數&lt;code&gt;_name&lt;/code&gt;來將&lt;code&gt;name&lt;/code&gt;存入，在Python當中以雙底線&lt;code&gt;__&lt;/code&gt;或底線&lt;code&gt;_&lt;/code&gt;開頭的變數會被視為是「私有的」，效果和Java的&lt;code&gt;private&lt;/code&gt;接近，不過Python並沒有這麼嚴格禁止外部去讀取私有變數，所以需要配合工程師的自我規範。&lt;/p&gt;
&lt;p&gt;{4} 類別方法&lt;code&gt;shout()&lt;/code&gt;，只要你不是靜態的類別方法，Python都會自動幫你帶入物件資訊當作第一個參數，通常命名為&lt;code&gt;self&lt;/code&gt;，那為什麼靜態方法沒有自動帶入，因為靜態方法不用實體化，所以根本不擁有物件的資訊。&lt;/p&gt;
&lt;p&gt;{5} 使用到本地的&lt;code&gt;self._name&lt;/code&gt;變數&lt;/p&gt;
&lt;p&gt;{6} 創造一個物件，在創造的過程會執行初始化，所以必須放入初始化需要的參數&lt;code&gt;name&lt;/code&gt;，所以上面的新的物件有了&lt;code&gt;"May"&lt;/code&gt;的名字。&lt;/p&gt;
&lt;p&gt;{7} 接下來使用&lt;code&gt;cat.shout()&lt;/code&gt;去執行喵喵叫的動作，這個方法會回傳字串，再利用&lt;code&gt;print&lt;/code&gt;的方法將字串顯示出來。注意！在物件導向的習慣中，會用&lt;code&gt;.&lt;/code&gt;來表示在那物件中的方法或屬性，所以&lt;code&gt;cat.shout()&lt;/code&gt;就是執行在物件&lt;code&gt;cat&lt;/code&gt;中的方法&lt;code&gt;shout()&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;{8} 在Python程式執行時，它的&lt;code&gt;__name__&lt;/code&gt;會是&lt;code&gt;"__main__"&lt;/code&gt;，也就是說會去執行這個&lt;code&gt;if&lt;/code&gt;判斷式下面的程式。&lt;/p&gt;
&lt;h3 id="method-overloading"&gt;方法多載（Method Overloading）&lt;/h3&gt;
&lt;p&gt;物件導向的方法多載（Method Overloading）是指在同一類別中定義多個名稱相同的方法，但傳入參數的型別或數量不同。這可以讓同一個方法名稱能夠支援不同的參數，並讓程式碼更具彈性和易讀性。&lt;/p&gt;
&lt;p&gt;來延伸剛剛的貓的例子，假設今天我們允許用戶不去設定貓咪的名字，而程式會預先給貓咪No-Name的預設值，所以我們需要另外一個初始化方法是不用貓咪名字的參數形式。&lt;/p&gt;
&lt;p&gt;Java的實現程式碼如下所示，如此一來只要碰到沒有參數的形式，程式會給予&lt;code&gt;"No-Name"&lt;/code&gt;的名字去當作貓咪的名字，並進行初始化。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;private&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;// method overloading&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;// given &amp;quot;No-Name&amp;quot; as its name&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. meow~&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Test&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;static&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;// no-argumant format&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// output:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// My name is No-Name. meow~&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;但在Python當中，不允許這種「相同方法名稱，卻又不同參數形式」，Python採用其他的方式來產生同樣的方法多載效果，如以下所示，我們可以看到Python使用default方法來實現多載，只要我們不給予&lt;code&gt;name&lt;/code&gt;，它的default就是&lt;code&gt;"No-Name"&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### Python3&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;# name&amp;#39;s default is &amp;quot;No-Name&amp;quot;&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; 

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; 
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;. meow~&amp;quot;&lt;/span&gt; 

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# no-argument format&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; 

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# output:&lt;/span&gt;
&lt;span class="c1"&gt;# My name is No-Name. meow~&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="encapsulation"&gt;物件導向三大特性—封裝(Encapsulation)&lt;/h3&gt;
&lt;p&gt;還記得「低耦合，高內聚」的原則嗎？為了符合這原則，每個物件都要盡可能的去包含需要用到的屬性和方法，並且使得外部不能以不合理的方法去影響物件，這就稱之為「封裝」。&lt;/p&gt;
&lt;p&gt;我們來看看上次的成果，我們就用這個例子來說明「封裝」。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### Python3&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Calculation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;  &lt;span class="c1"&gt;# {1}&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  &lt;span class="c1"&gt;# {2}&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;invalid positive integer: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  &lt;span class="c1"&gt;# {3}&lt;/span&gt;
        &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
            &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findLCM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;lcm&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} 使用私有變數，讓外部不能任意的改變&lt;code&gt;self._nums&lt;/code&gt;變數，在這個例子當中，如果&lt;code&gt;self._nums&lt;/code&gt;被任意改變，它將會逃過&lt;code&gt;_checkPositiveInteger&lt;/code&gt;的檢查。
{2}&amp;amp;{3} 不需要由外部讀取的方法，就盡量讓它是私有的。&lt;/p&gt;
&lt;p&gt;再回到貓咪的這個例子，如果我們想要可以調控「叫聲次數」的話，可以這樣實現。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;private&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;private&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{1}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;getShoutNum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{2}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;setShoutNum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{3}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;throw&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;IllegalArgumentException&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;meow~ &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Test&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;static&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;setShoutNum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{4}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// output:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// My name is May. meow~ meow~ meow~ meow~ meow~&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} 設置私有變數&lt;code&gt;shout_num&lt;/code&gt;來決定叫聲次數。&lt;/p&gt;
&lt;p&gt;{2} 為了做到封裝，外部不能直接去讀取&lt;code&gt;shout_num&lt;/code&gt;，而是經由&lt;code&gt;getShoutNum&lt;/code&gt;的外部方法來得到叫聲次數，這個&lt;code&gt;getXXX&lt;/code&gt;的形式在物件導向裡頭被稱為Getter。&lt;/p&gt;
&lt;p&gt;{3} 為了做到封裝，外部不能直接去改變&lt;code&gt;shout_num&lt;/code&gt;，而是經由&lt;code&gt;setShoutNum&lt;/code&gt;的外部方法來以合理的方法改變叫聲次數，在這個方法中，我們會先檢查再設定，如果是直接的改變&lt;code&gt;shout_num&lt;/code&gt;將會少了這一份檢查，這個&lt;code&gt;setXXX&lt;/code&gt;的形式在物件導向裡頭被稱為Setter。&lt;/p&gt;
&lt;p&gt;{4} 使用Setter方法由外部來改變叫聲次數。&lt;/p&gt;
&lt;p&gt;在Python中，Getter和Setter被簡化了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### Python3&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;  &lt;span class="c1"&gt;#{1}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;

    &lt;span class="nd"&gt;@shout_num&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setter&lt;/span&gt;  &lt;span class="c1"&gt;#{2}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;meow~ &amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;. &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;  &lt;span class="c1"&gt;#{3}&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# output:&lt;/span&gt;
&lt;span class="c1"&gt;# My name is May. meow~ meow~ meow~ meow~ meow~&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} Python使用Decorator&lt;code&gt;@property&lt;/code&gt;來創造Getter，一旦加上了&lt;code&gt;@property&lt;/code&gt;，當下的函數方法就會變成一種性質。&lt;/p&gt;
&lt;p&gt;{2} 再使用&lt;code&gt;shout_num.setter&lt;/code&gt;來替&lt;code&gt;shout_num&lt;/code&gt;這個特性加上Setter。&lt;/p&gt;
&lt;p&gt;{3} 然後我們就可以以像是修改一般變數的方式來修改&lt;code&gt;shout_num&lt;/code&gt;，但實際上&lt;code&gt;shout_num&lt;/code&gt;是有被封裝的，如此一來就可以更為簡潔，不用去寫&lt;code&gt;getXX&lt;/code&gt;和&lt;code&gt;setXXX&lt;/code&gt;等囉唆的寫法。&lt;/p&gt;
&lt;h3 id="inheritance"&gt;物件導向三大特性—繼承(Inheritance)&lt;/h3&gt;
&lt;p&gt;還記得「Don't Repeat Yourself」原則嗎？物件導向同樣提供了這個選項，「繼承」可以讓子類擁有父類的屬性和方法，避免不必要的重寫，但同時也會增加父類和子類之間的耦合，所以使用時要去評估它影響了多少耦合性。&lt;/p&gt;
&lt;p&gt;子類可以先繼承父類的屬性和方法，再去新增屬於子類自己的屬性和方法，甚至還可以去覆寫父類的方法，這稱之為方法覆寫(Method Overriding)，有了這些方法，子類可以在不重複撰寫父類方法的情況下，去增加自己的特色和自己的功能。&lt;/p&gt;
&lt;p&gt;依循著剛剛的例子，如果我們今天想要增加狗的類別，但是又不想重複撰寫相同的部分，所以我們可以選擇創造動物的類別，再讓貓和狗繼承自動物。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{1}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;getShoutNum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;setShoutNum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;throw&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;IllegalArgumentException&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;extends&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="kd"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{2}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="kd"&gt;super&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;meow~ &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;extends&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;woof~ &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} &lt;code&gt;protected&lt;/code&gt;的效果和&lt;code&gt;private&lt;/code&gt;一樣，讓外部無法讀取到內部的私有化，但是&lt;code&gt;private&lt;/code&gt;無法被「繼承」，而&lt;code&gt;protected&lt;/code&gt;可以被繼承，所以如果希望可以被繼承的私有變數或方法，就使用&lt;code&gt;protected&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;{2} &lt;code&gt;super&lt;/code&gt;指的是父類，這裡我們使用父類來做初始化，事實上這邊可以不用再初始化一次，子類本身就會繼承父類的初始化方法，所以可以像&lt;code&gt;Dog&lt;/code&gt;一樣省略不寫。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### Python3&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;

    &lt;span class="nd"&gt;@shout_num&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setter&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No_Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;#{1}&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;meow~ &amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;. &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;woof~ &amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;. &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} &lt;code&gt;super&lt;/code&gt;指的是父類，這裡我們使用父類來做初始化，事實上這邊可以不用再初始化一次，子類本身就會繼承父類的初始化方法，所以可以像&lt;code&gt;Dog&lt;/code&gt;一樣省略不寫。&lt;/p&gt;
&lt;h3 id="abstract-classabstract-methodinterface"&gt;抽象化：抽象類別(Abstract Class)、抽象方法(Abstract Method)和接口(Interface)&lt;/h3&gt;
&lt;p&gt;事實上，剛剛使用&lt;code&gt;Animal&lt;/code&gt;的方法並不是很完整，我們將&lt;code&gt;Animal&lt;/code&gt;當作一個類別處理，所以&lt;code&gt;Animal&lt;/code&gt;其實是可以被實例化的，但是&lt;code&gt;Animal&lt;/code&gt;根本沒有什麼有用的方法，它必須被繼承後再添加方法才有用處，所以我們其實可以把&lt;code&gt;Animal&lt;/code&gt;抽象化，將&lt;code&gt;Animal&lt;/code&gt;視為抽象類別，其中擁有一些方法需要在子類實現的，稱為抽象方法，我們直接看怎麼做。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;abstract&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{1}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;getShoutNum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;setShoutNum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;throw&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;IllegalArgumentException&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{2}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;getShoutSound&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{3}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;abstract&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{4}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;extends&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{5}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;meow~&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;extends&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;woof~&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} 使用&lt;code&gt;abstract class&lt;/code&gt;修飾詞來創建抽象類別，只有在抽象類別中才可以擁有抽象方法，抽象類別不能直接被實例化。&lt;/p&gt;
&lt;p&gt;{2} 抽象類別也可以有一般的具體方法。&lt;/p&gt;
&lt;p&gt;{3} 這裡使用的&lt;code&gt;getShoutSound()&lt;/code&gt;方法要等在子類才會被實現。&lt;/p&gt;
&lt;p&gt;{4} 使用&lt;code&gt;abstract&lt;/code&gt;設置抽象方法，繼承自抽象類別的子類必須要完全實現所有的抽象方法。&lt;/p&gt;
&lt;p&gt;{5} 實現抽象方法。&lt;/p&gt;
&lt;p&gt;在 Python 中沒有原生的抽象類別和方法，過去可能會使用 &lt;code&gt;abc&lt;/code&gt; ，但是 YC 發現其實幾乎很少人使用這樣的方法，大部分的作法是使用 &lt;code&gt;raise NotImplementedError&lt;/code&gt; ，如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### Python3&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;

    &lt;span class="nd"&gt;@shout_num&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setter&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;. &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  &lt;span class="c1"&gt;#{1}&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{2}&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;meow~&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;woof~&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} &lt;code&gt;_getShoutSound&lt;/code&gt; 是一個未實現的方法，如果繼承之後沒有去Override這個方法，將會報錯。&lt;/p&gt;
&lt;p&gt;{2} Override這個方法。&lt;/p&gt;
&lt;p&gt;還有一種類型抽象化的更為徹底，稱之為「接口」，「接口」上的所有方法都是抽象未實現的，「接口」不能擁有任何具體的方法。雖然「接口」很像是完全抽象化的「抽象類別」，也確實可以利用「抽象類別」來創造「接口」，但是兩者的意義是不同的，「抽象類別」是從子類中發現共通的東西，而泛化出現的，但是「接口」可以根本不預先知道子類是什麼，而僅僅事先定義行為本身，換句話說，「抽象類別」是類別的抽象化，而「接口」則是行為的抽象化。&lt;/p&gt;
&lt;p&gt;例如，我想要讓某些動物擁有「飛」的能力，這是一個行為，而不會事先知道它會套用到哪一個類別上面。&lt;/p&gt;
&lt;p&gt;在Java之中只允許單一繼承，但是卻可以有多個「接口」；而Python沒有現成的「接口」可以使用，我們必須使用「抽象方法」來創造「接口」，所以開發者要謹記「接口」的限制：不能有任何的具體方法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;interface&lt;/span&gt; &lt;span class="nc"&gt;IFly&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{1}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;flyTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{2}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;FlyingCat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;extends&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;implements&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;IFly&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{3}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;flyTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{4}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot; I&amp;#39;m going to fly to &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Test&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;static&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;FlyingCat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;FlyingCat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;flyTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Taiwan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// output: &lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// My name is May. meow~ meow~ meow~ I&amp;#39;m going to fly to Taiwan.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} 使用&lt;code&gt;interface&lt;/code&gt;創建「接口」，我們習慣會使用開頭大寫I來表示Interface(接口)。&lt;/p&gt;
&lt;p&gt;{2} 「接口」定義未實現的抽象方法&lt;code&gt;flyTo&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;{3} &lt;code&gt;FlyingCat&lt;/code&gt;繼承自&lt;code&gt;Cat&lt;/code&gt;並且裝上&lt;code&gt;IFly&lt;/code&gt;的「接口」。&lt;/p&gt;
&lt;p&gt;{4} 必須實現「接口」上所有的抽象方法。&lt;/p&gt;
&lt;p&gt;因為Python允許多重繼承，所以就可以直接將模擬「接口」的抽象類別直接疊加上去。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### Python3&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;IFly&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;flyTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; 
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;FlyingCat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IFly&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; &lt;span class="c1"&gt;#{1}&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;flyTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;place&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; I&amp;#39;m going to fly to &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;place&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;cat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FlyingCat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flyTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Taiwan&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# output: &lt;/span&gt;
&lt;span class="c1"&gt;# My name is May. meow~ meow~ meow~ I&amp;#39;m going to fly to Taiwan.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} 直接使用多重繼承，將&lt;code&gt;IFly&lt;/code&gt;安裝上去。&lt;/p&gt;
&lt;h3 id="polymorphism"&gt;物件導向三大特性—多型(Polymorphism)&lt;/h3&gt;
&lt;p&gt;最後，來講講物件導向的最後一個特性，那就是「多型」。「多型」的涵義是指「子類可以以父類的身分出現」，而因為是以父類的角色出現，所以只能執行父類擁有的方法，也就是只能執行這些子類共同泛化分享的方法，當然不同的子類實現後的效果會不一樣，不然使用「多型」的意義就不大了，至於子類自己的特殊方法則不可以使用「多型」去執行。&lt;/p&gt;
&lt;p&gt;直接來看範例，假設今天我要邀請三隻貓貓狗狗來參加叫聲比賽，分別請他們叫個幾聲來聽聽，此時就需要使用到「多型」的方法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;
&lt;span class="normal"&gt;53&lt;/span&gt;
&lt;span class="normal"&gt;54&lt;/span&gt;
&lt;span class="normal"&gt;55&lt;/span&gt;
&lt;span class="normal"&gt;56&lt;/span&gt;
&lt;span class="normal"&gt;57&lt;/span&gt;
&lt;span class="normal"&gt;58&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="cm"&gt;/* Java */&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;abstract&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;getShoutNum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;setShoutNum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;throw&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;IllegalArgumentException&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;getShoutSound&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;. &amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;abstract&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;extends&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;meow~&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;extends&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;protected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;getShoutSound&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;woof~&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ShoutGame&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;public&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;static&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{1}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="c1"&gt;// polymorphism&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{2}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Dog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Linda&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Joy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="na"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;//{3}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// output: &lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// My name is May. meow~ meow~ meow~&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// My name is Linda. woof~ woof~ woof~&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="c1"&gt;// My name is Joy. meow~ meow~ meow~&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} 創建&lt;code&gt;Animal&lt;/code&gt;的Array，Animal是抽象類別不能實體化，這裡預定要放的是它的繼承實現類別。&lt;/p&gt;
&lt;p&gt;{2} 將&lt;code&gt;Cat&lt;/code&gt;和&lt;code&gt;Dog&lt;/code&gt;任意放到&lt;code&gt;Animal&lt;/code&gt;的Array是可以的，此時就套用「多型」，不管是&lt;code&gt;Cat&lt;/code&gt;和&lt;code&gt;Dog&lt;/code&gt;都是以&lt;code&gt;Animal&lt;/code&gt;的形式出現，只能執行&lt;code&gt;Animal&lt;/code&gt;有的方法。&lt;/p&gt;
&lt;p&gt;{3} &lt;code&gt;shout()&lt;/code&gt;是父類&lt;code&gt;Animal&lt;/code&gt;有的方法，可以被執行。&lt;/p&gt;
&lt;p&gt;而在Python當中，「多型」就沒這麼重要了，因為Python具有「鴨子型別」（Duck Typing），什麼是「鴨子型別」呢？有一句話道出它的意義：「當看到一隻鳥走起來像鴨子、游泳起來像鴨子、叫起來也像鴨子，那麼這隻鳥就可以被稱為鴨子」，因為Python是動態型別的語言，變數型態不需要事先宣告，所以一個變數具有彈性可以放入任意型別，直到出現不合適的使用方法，才會報錯，所以在Python中變數可以任意放入不同的類別的物件，只要確保這些類別都具有這些變數所需要用到的方法，就可以了，這不正是接近「多型」的概念。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### Python3&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No-Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt;

    &lt;span class="nd"&gt;@shout_num&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setter&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_shout_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout_num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; &amp;quot;&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My name is &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_name&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;. &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;  

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;meow~&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Dog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_getShoutSound&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;woof~&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# Shout Game&lt;/span&gt;
    &lt;span class="n"&gt;arrayAnimal&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="c1"&gt;#{1}&lt;/span&gt;
    &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;May&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; 
    &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Dog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Linda&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Cat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Joy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;animal&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;arrayAnimal&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Animal&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt; 
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;TypeError&lt;/span&gt; &lt;span class="c1"&gt;#not necessary #{2}&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;animal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shout&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# output: &lt;/span&gt;
&lt;span class="c1"&gt;# My name is May. meow~ meow~ meow~&lt;/span&gt;
&lt;span class="c1"&gt;# My name is Linda. woof~ woof~ woof~&lt;/span&gt;
&lt;span class="c1"&gt;# My name is Joy. meow~ meow~ meow~&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;{1} 要存入多型的List不需要特別處理。&lt;/p&gt;
&lt;p&gt;{2} 可以檢查是不是繼承自&lt;code&gt;Animal&lt;/code&gt;，以確保多型的嚴格定義，但這個過程是非必要的。&lt;/p&gt;
&lt;h3 id="_2"&gt;總結：物件導向使用方法&lt;/h3&gt;
&lt;p&gt;好！我們花了好大的力氣，終於了解如何在Java和Python中使用物件導向。從一開始的「類別」和「物件」講起，再來談到物件導向的三大特性：「封裝」、「繼承」和「多型」，還談到方法可以「多載」也可以「覆寫」，以及一些抽象化的東西，包括「抽象類別」、「抽象方法」和「接口」。&lt;/p&gt;
&lt;p&gt;但是等等！你知道該怎麼運用這些技巧嗎？沒錯，僅僅是了解這些招式不足以讓你寫出卓越的程式碼，你還需要了解如何使用，就像是外功招式還得配合內功才可以是一套完整的功夫，否則只是半吊子而已，我們將在下一章節中來帶大家了解如何去使用這些招式。&lt;/p&gt;</content><category term="CS"></category><category term="軟體設計"></category><category term="Python"></category><category term="Java"></category></entry><entry><title>物件導向武功秘笈（1）：認知篇 — 什麼是好的程式？</title><link href="https://ycc.idv.tw/introduction-object-oriented-programming_1.html" rel="alternate"></link><published>2018-04-05T12:00:00+08:00</published><updated>2018-04-05T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2018-04-05:/introduction-object-oriented-programming_1.html</id><summary type="html">&lt;p&gt;物件導向為何重要？ / 程式的好壞？ / 低耦合、高內聚 / 程式碼精練之旅 / 形塑出物件導向&lt;/p&gt;</summary><content type="html">&lt;div class="toc"&gt;&lt;span class="toctitle"&gt;Table of Contents&lt;/span&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#_1"&gt;物件導向為何重要？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#_2"&gt;程式的好壞？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#_3"&gt;低耦合、高內聚&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#_4"&gt;程式碼精練之旅&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#_5"&gt;形塑出物件導向&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#_6"&gt;總結：程式碼鑑賞能力&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3 id="_1"&gt;物件導向為何重要？&lt;/h3&gt;
&lt;p&gt;我相信很多朋友一定像YC我一樣，想要學某個程式語言，就去買那個程式語言的簡介書籍，然後一章一章的唸下去，這種書通常會先教變數怎麼設定？然後再教if、while、for、function等程式邏輯。&lt;/p&gt;
&lt;p&gt;那如果你學的是「物件導向」的語言，譬如：Java、C++、Python，接下來的章節就會開始介紹「類別」、「物件」等等難懂的東西，然後就會陷入一種霧煞煞的狀態，然後心中就會出現一種聲音：為什麼寫個程式你跟我扯什麼「物件」？我原本用前面所學的方法就可以完成所有事情啦！為何要把事情弄的這麼複雜？這東西到底有什麼好處啊？&lt;/p&gt;
&lt;p&gt;YC一開始也是充滿著疑惑，然後一知半解的就把這些定義記在心中，然後天真的認為「物件導向」只是讓程式比較整齊的方法罷了！直到後來學了資料結構與演算法，然後又學了一點設計模式，然後又有過幾個大型軟體開發的經驗，一路走過才漸漸的了解「物件導向」是怎麼一回事？&lt;/p&gt;
&lt;p&gt;所以我打算把這些收穫用三篇文章來說明，好讓讀者們可以少走一點冤枉路，在第一篇中，也就是本篇，我會帶大家認識好的程式是長什麼樣子的，它擁有什麼樣的特點，有了正確的認知，除了可以讓我們避免寫出糟糕的程式之外，我們也才能漸漸的認識到「物件導向」為何重要。&lt;/p&gt;
&lt;h3 id="_2"&gt;程式的好壞？&lt;/h3&gt;
&lt;p&gt;一開始，我們必須要對程式培養出鑑賞能力，我曾經聽過電視上有一位歌唱老師說過：「好的歌手必須先練他的聽力」，我覺得相同的，一個好的Programmer要先培養出對於程式的鑑賞能力。&lt;/p&gt;
&lt;p&gt;首先，一個好的程式當然要「能正常執行」，要能滿足客戶的需求，這是基本款，所以一般而言我們會使用很多的測試去看看程式是否可以正常運作，我們會找一些一般的條件來測試，我們也會找一些合法但是位於極端條件的例子，也就是邊界條件（Edge Case）來測試，或者找一些不合法的例子試試程式是否可以排除錯誤條件。&lt;/p&gt;
&lt;p&gt;測試可以即早的發現Bug，即早的治療，如果真的發現有Bug的話，接下來就是去找出Bug的源頭，這可就相當的困難，這裡想像一下喔！如果你的程式總共有1000行，而當你測試時發現有Bug，那想從這麼多行當中找出Bug的來源是相當困難的，所以好的方法是這樣的，先將一個大任務分解成為幾個小任務，然後完成這幾個小任務後，逐一的進行測試，稱之為「單元測試」，最後在將這些測試完成的小任務組合成為大任務，然後再做最後的總測試，這麼一來就可以避免在大範圍中找尋Bug，又可以做到對程式從裡到外的完整測試以達到程式「能正常執行」的目的。&lt;/p&gt;
&lt;p&gt;這裡提出一個問題讓大家思考，究竟要使用什麼方法去解析問題？讓我們可以有條理的拆解出小「單元」，來組合出最後的目標，有沒有一個系統化的思考方法？&lt;/p&gt;
&lt;p&gt;第二點，一個好的程式必須是「穩健的」(Robust)，程式原本能用的功能，不會因為更新、不會因為添加新功能，就出現錯誤！要做到這一點，除了剛剛說的「單元」拆分以外，還要讓「單元」和「單元」之間不會有太多的彼此影響，這麼一來在原先的功能所調用的「單元」不被動到的前提下，我還可以新增新的功能，才能做到「穩健的」特質。&lt;/p&gt;
&lt;p&gt;第三點，一個好的程式必須具備「不重複撰寫」的特性，有一句經典的法則叫做「Don't Repeat Yourself」，不要去重複寫已經寫過的程式碼，如果是重複需要用到的「單元」我們就把它獨立出來，讓其他程式去調用它，對於工程師來說，「不重複撰寫」意味著可以少寫一點程式碼，增加開發的速度，更重要的是，調用公享的程式碼可以讓程式更有邏輯，更具一致性，能夠減少出錯的可能性。&lt;/p&gt;
&lt;p&gt;第四點，好的程式要具有「可讀性」，軟體開發常常是長時間、多人合作、龐大的程式碼，如果程式碼沒有具備清晰的邏輯、沒有在該註解的部分寫清楚、沒有一個統一的規範，這樣的開發終就會陷入泥坑，永遠解不完的Bug會不斷的出現，解了一個又產生一個，永無止盡的輪迴，而且最慘的是完全不清楚真正的源頭在哪裡，這可是軟體工程師的夢魘啊！&lt;/p&gt;
&lt;p&gt;第五點，一個好的程式要具備「可擴展」，工程師最討厭的一句話應該就是客戶說：「我突然想到我還需要XXX功能，這只是在這邊再多一點而已，應該不難吧！」呵呵～通常「這多一點」就要大大的修改整個程式碼，弄不好還可能把原本的功能給搞壞，所以工程師應該在設計的一開始就考慮到會有什麼潛在需要更改的部分，而先採取因應措施，好讓程式易於擴展，好讓自己不會因此而加班！&lt;/p&gt;
&lt;h3 id="_3"&gt;低耦合、高內聚&lt;/h3&gt;
&lt;p&gt;再重複一次，一個好的程式要具備「正常執行」、「穩健」、「不重複撰寫」、「可讀性」、「可擴展」的特性，請將這些原則記在心裡，隨時的檢視自己的程式是不是有違反這些規則。&lt;/p&gt;
&lt;p&gt;而剛剛我們有了一個大致的想法：將任務分成幾個小的「單元」是一個很好的策略，而為了讓程式「穩健」，這些「單元」之間不能有太多的相依性；但是站在另外一個角度看，為了讓程式「不重複撰寫」，我們需要讓一個「單元」使用另外一個「單元」，好讓工程師可以做到「Don't Repeat Yourself」，如此一來則是增加了「單元」間的相依性，這兩者是一個Trade-off。&lt;/p&gt;
&lt;p&gt;有關「單元」的相依性有兩個重要術語—耦合性(Coupling)和聚合性(Cohesion)，耦合性指的是「單元」和「單元」之間資訊或參數依賴的程度，所以我們要追求「低耦合」。聚合性指的是「單元」內使用到自身資訊或參數的程度，所以我們要追求「高內聚」，通常「低耦合」都會伴隨著「高內聚」。&lt;/p&gt;
&lt;h3 id="_4"&gt;程式碼精練之旅&lt;/h3&gt;
&lt;p&gt;來看個例子，假設今天我想要實現一個求最大公因數的計算機，使用&lt;strong&gt;Python&lt;/strong&gt;隨便寫一段程式碼可能是這樣的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;str_numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer A: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;str_numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer B: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;    

    &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Greatest Common Factor: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;好！那接下來用剛剛的規則來檢視看看這個程式，第一點，有沒有「可正常執行」？上述的例子，沒有考慮到一些Edge Case，當輸入的值不是正整數，必須要報錯，所以我們將程式修改一下。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="w"&gt;def main():&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;   str_numA = input(&amp;quot;Positive Integer A: &amp;quot;)&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;   str_numB = input(&amp;quot;Positive Integer B: &amp;quot;)&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt; &lt;/span&gt;   numA = int(str_numA)&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="gi"&gt;+   if numA &amp;lt;= 0: &lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="gi"&gt;+       raise ValueError(&amp;quot;invalid positive integer: &amp;quot;+str(numA))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;   numB = int(str_numB)&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="gi"&gt;+   if numB &amp;lt;= 0: &lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="gi"&gt;+       raise ValueError(&amp;quot;invalid positive integer: &amp;quot;+str(numB))&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt; &lt;/span&gt;   prime_factorize_A = dict()&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;   i = 2&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;   while(numA &amp;gt; 1):&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="gu"&gt;@@ ... omit ... @@&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;再來檢查一下是不是具有「不重複撰寫」的特性？也就是Don't Repeat Yourself，顯然是沒有遵守，&lt;code&gt;numA&lt;/code&gt;和&lt;code&gt;numB&lt;/code&gt;處理方法幾乎一模一樣，這會造成程式碼很冗長，來稍做修改。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;invalid positive integer: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;str_numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer A: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;str_numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer B: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;prime_factorize_A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize_A&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;prime_factorize_B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Greatest Common Factor: &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;接下來來檢查一下「穩健度」和「可擴展」，也就是程式是否符合：低耦合、高內聚，其實上面的程式碼有一個大問題，客戶端邏輯和業務邏輯混為一談，客戶端邏輯就是實現功能的部分，而業務邏輯就是實作的細節，所以上面的程式碼把所有的實作的細節全部攤在客戶端，這是相當不好的，這會造成不易更改，因此我們將程式作單元的拆分，讓業務邏輯和客戶端邏輯相分離，讓不直接實現客戶端的程式碼可以隱藏起來，減少客戶端和業務邏輯的耦合。然後順道加入求取最小公倍數的功能。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# ... same as above, omit ...&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# ... same as above, omit ...&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
        &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findLCM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;lcm&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;str_numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer A: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;str_numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer B: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;numB&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;    
    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;findLCM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Greatest Common Factor: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Lowest Common Multiple: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lcm&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;如此一來程式碼就看起來乾淨很多，function和function之間的耦合性被降低了，而function本身的內聚性提高了，程式碼達到了低耦合、高內聚，但是似乎還可以更好。&lt;/p&gt;
&lt;h3 id="_5"&gt;形塑出物件導向&lt;/h3&gt;
&lt;p&gt;剛剛我們已經完成了一個看起來很乾淨的程式碼了，但是其實還可以更好，在這裡我們就必須形塑出物件導向，才有辦法再前進一步。&lt;/p&gt;
&lt;p&gt;剛剛的程式碼當中的&lt;code&gt;checkPositiveInteger(num)&lt;/code&gt;, &lt;code&gt;primeFactorize(num)&lt;/code&gt;, &lt;code&gt;findGCF(nums)&lt;/code&gt;, &lt;code&gt;findLCM(nums)&lt;/code&gt;函數其實都是實現同一個目標—因式計算，但卻是被寫成一個一個獨立的函數，這裡的內聚性還可以再更好。&lt;/p&gt;
&lt;p&gt;而且&lt;code&gt;checkPositiveInteger(num)&lt;/code&gt;, &lt;code&gt;primeFactorize(num)&lt;/code&gt;並不是用來實現主要的目的，而只是實現目的過程中，為了避免重複而產生的，這樣寫很容易讓人不清楚什麼是重要的函數，而什麼只是中繼的函數，這裡的「可讀性」應該還可以再提升。&lt;/p&gt;
&lt;p&gt;輸入的數字&lt;code&gt;nums&lt;/code&gt;對於&lt;code&gt;findGCF&lt;/code&gt;和&lt;code&gt;findLCM&lt;/code&gt;，應該是一模一樣的，有沒有一個方法可以讓&lt;code&gt;nums&lt;/code&gt;避免重複呢？以增強「不要重複撰寫」的原則。&lt;/p&gt;
&lt;p&gt;要擁有以上的功能，我們需要一個「物件」，這個「物件」能夠保有屬於它的變數，才可以儲存&lt;code&gt;nums&lt;/code&gt;等參數，變數可以是對外公布的，也可以是私有的。另外,這個「對象」擁有屬於它的函數方法，而方法一樣可以是對外公布的，也可以是私有的，所以我們可以公布&lt;code&gt;findGCF(nums)&lt;/code&gt;, &lt;code&gt;findLCM(nums)&lt;/code&gt;，而私有化
&lt;code&gt;checkPositiveInteger(num)&lt;/code&gt;, &lt;code&gt;primeFactorize(num)&lt;/code&gt;。我們使用「藍圖」去建構「物件」的模版，再由「藍圖」配合不同的輸入參數去生成一個一個獨立的「物件」，以因應不同的狀況。&lt;/p&gt;
&lt;p&gt;這就是物件導向！&lt;/p&gt;
&lt;p&gt;接下來，我將上面程式碼引入物件導向改寫如下。（看不懂～沒關係！未來會詳述）&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;
&lt;span class="normal"&gt;53&lt;/span&gt;
&lt;span class="normal"&gt;54&lt;/span&gt;
&lt;span class="normal"&gt;55&lt;/span&gt;
&lt;span class="normal"&gt;56&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Calculation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_checkPositiveInteger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
              &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;invalid positive integer: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;prime_factorize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_primeFactorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
            &lt;span class="n"&gt;common_prime&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;common_prime&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;pf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;pf&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prime_factorize&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prime&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;findLCM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_nums&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;lcm&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;str_numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer A: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;str_numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Positive Integer B: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;numA&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;numB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;str_numB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;nums&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;numB&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;calc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Calculation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nums&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;gcf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findGCF&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;lcm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findLCM&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Greatest Common Factor: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Lowest Common Multiple: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lcm&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="_6"&gt;總結：程式碼鑑賞能力&lt;/h3&gt;
&lt;p&gt;本章YC帶大家建立一種品味，像是藝術評論家一樣，我們學會了如何鑑賞好的程式碼，我們提到了好的程式碼須要符合「正常執行」、「穩健」、「不重複撰寫」、「可讀性」、「可擴展」的特性，並且提到我們要追求低耦合、高內聚，但是「不重複撰寫」的這個原則會和低耦合相互違和，所以工程師要小心拿捏！有了鑑賞能力，我們開始精練我們的程式，而自然而然就可以引出物件導向的概念。當然，物件導向不只如此啦！我們下章就會看到物件導向還有什麼花拳繡腿。&lt;/p&gt;</content><category term="CS"></category><category term="軟體設計"></category></entry><entry><title>自私的基因：基因觀點下的天擇</title><link href="https://ycc.idv.tw/the-selfish-gene.html" rel="alternate"></link><published>2018-02-03T12:00:00+08:00</published><updated>2018-02-03T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2018-02-03:/the-selfish-gene.html</id><summary type="html">&lt;p&gt;物競天擇？ / 生命源自於複製 / 基因的代理人—神經網絡 / 再談利他行為 / 文化的複製者—迷因（Meme）&lt;/p&gt;</summary><content type="html">&lt;h3 id="_1"&gt;物競天擇？&lt;/h3&gt;
&lt;p&gt;『自私的基因』是當代相當重要的一本書，它在生物學上的地位等同於在物理學上的『時間簡史』。作者理察·道金斯（Richard Dawkins）是一個跨領域的通才，曾獲得動物學學士學位、文學碩士、哲學博士以及科學博士，我認為也是因為這樣的跨領域學習，才能讓他完成這樣一本創作俱佳的好書。&lt;/p&gt;
&lt;p&gt;達爾文的天擇說提出後，生物學開始有了一個思考的脈絡來描述生物的演化，天擇說告訴我們「物競天擇，適者生存、不適者淘汰」，地球上目前存在的物種是多年來環境盲目篩選後的結果，但我們會發現如果以物種為單位來說明天擇會有一些說不清的地方。&lt;/p&gt;
&lt;p&gt;舉個例子，物種的利他行為，例如許多小型鳥類遇到老鷹時會發出警訊給同伴，通知同伴趕快逃跑，這毫無疑問的是一種利他行為，發出警訊將會使自己暴露在危險之中，而換取到的是其他同伴的安全，針對這樣的利他行為，「群體選擇」理論會說生物會因演化而做出對種族有利的事，作者認為這是一種謬誤，生物間應該無法輕易的區分種族，就算可以，那我們又如何去劃分層次，要從界、門、綱、目、科、屬、種哪個層面下手去有意識的幫助自己的種族呢？以物種為單位的天擇說僅能說明生物本身的自利行為，但是解釋不清楚利他行為如何形成。&lt;/p&gt;
&lt;p&gt;作者道金斯受到魏斯曼（A. Weismann）的學說「生殖細胞的延續性」所啟蒙，提出了天擇利己主義的基本單位，既非種，也非群體，亦非個體，而是「基因」（Gene）這個遺傳的基本單位。而也正是基因的自私行為才造就個體的利他行為，基因傾向於保全與自己基因相似的個體，以獲得基因本身的延續，基因是自私的，所以本書的書名才會叫做『自私的基因』。&lt;/p&gt;
&lt;h3 id="_2"&gt;生命源自於複製&lt;/h3&gt;
&lt;p&gt;為了說清楚基因觀點下的演化，我們回到四十億年前的地球，那時的地球海洋已經形成，稱之為太古混湯（primeval soup），海洋中存在著大量的基本化合物，譬如：水、二氧化碳、甲烷和氮。這些基本化合物會因為化學作用，而可能有機會合併形成更大、更複雜的化合物，在那個時候大的有機化合物因為沒有存在細菌可以分解它，所以是可以被完整保存下來的，直到有一天出現了一種可以自我複製的化合物，它就開始大量形成相同的化學結構，而且方便的是太古混湯擁有大量的垂手可得的建材，所以可以大量的複製出更多的複製者，這樣的複製就開始擴張開來。&lt;/p&gt;
&lt;p&gt;當然像這類的複製者不只一種，太古混湯中夾雜許多可以自我複製的化合物種類，而且就連複製者在複製的過程當中都可能會出錯而產生出其他形式的複製者，當複製者越來越多，造成太古混湯開始缺乏素材而無法供養所有的複製者，競爭就開始了，天擇就開始了！而能存活的複製者必須符合長命、生產力大、複製正確度高等特性，否則將無法在歷史的長河中留下。&lt;/p&gt;
&lt;p&gt;競爭只會隨著時間日益增加而不會減少，為了存活（不是有意識的，而是天擇的結果）複製者開始改良自身，增加自身的穩定性，瓦解對手的穩定度，譬如：形成蛋白質保護自己、形成可以分解對手分子鍵結的能力以得到更多的建材，為了生存，複製者走向發展出更精細的、更複雜的求生機器。&lt;/p&gt;
&lt;p&gt;四十億年過去了，這些複製者在今日被稱之為基因，多個基因組成DNA，DNA可以轉譯成蛋白質分子來打造出求生機器，求生機器是承載基因的「工具」，承載數千、數萬個基因，每具肉體都是基因精巧的共同傑作，想要區分這個基因和另一個基因的貢獻，幾乎是做不到的。&lt;/p&gt;
&lt;p&gt;書中作者用划船來作比喻，划船人員就像是基因，而這艘船就像是個體，當個體無法繁殖，這組團隊就像是輸了比賽就會被淘汰，而獲勝的團隊將可能在未來與其他的基因合作繼續的比賽下去，這艘船的每個船員都有適合他的位置，可能有船首、舵手或尾槳，把人放在適合的位置才可以發揮最好的實力，那麼最後存活下來的物種就是擁有好基因彼此之間合作無間的成果。&lt;/p&gt;
&lt;h3 id="_3"&gt;基因的代理人—神經網絡&lt;/h3&gt;
&lt;p&gt;動物和植物不同之處，動物擁有快速的行為，譬如遇到敵人就要拔腿就跑、看到獵物就要採取攻擊，這一些快速的行為不可能由基因直接控制，因為基因的影響速度太慢了，可能需要幾個月的時間，所以基因依造它們的需求打造出了神經網絡，然後神經網絡就接管了個體每時每刻的行為，才能因應各種環境的變化去做出相應的行為，使得個體得以生存，幫助基因可以繼續繁衍下去，如果用電腦來比喻的話，圍棋程式AlphaGo在比賽之前是由人類打造而成的，但是在真正比賽時，它會依照當下狀況去自行下決定，基因也是一樣的，基因先打造好神經網絡，接下來個體的行為就由這些神經網絡來代理了。&lt;/p&gt;
&lt;h3 id="_4"&gt;再談利他行為&lt;/h3&gt;
&lt;p&gt;好的基因產生的神經網絡所造就的動物行為，是要可以幫助基因複製自身的，基因是自私的，但不是有意識的，而是天擇造就了自私的基因得以存活下來，也正是這一層關係，物種的利他行為仍然是基因自私的結果。&lt;/p&gt;
&lt;p&gt;像是一開始舉例的，小鳥會發出叫聲來提醒其他同伴，這樣的行為對個體是不利的，但是犧牲的個體可以保全其他相似的基因存活，這個基因就會延續下去。在人類社會，父母親給予子女的愛是無私的，但是這還是基因自私的結果，對子女無私的基因傾向讓子女更容易活下去，所以這樣的基因存活下來，相反的會拋下子女的基因，如果不能配合嬰兒時期可以自行生存的基因，這些子女也就會提早夭折，這樣的基因就會消失在歷史的長河裡頭。&lt;/p&gt;
&lt;p&gt;因此，所有的生物行為都只有一個目的—讓基因複製下去，而環境會做出審判，作者還進一步的使用賽局理論來解釋哪些行為會是有利的、哪些行為會造成不利，譬如說某個族群50%的個體不喜歡起衝突而50%的個體喜歡追殺到底，想當然爾，如果這個種族在競爭食物時，喜歡追殺到底的個體比較有利，所以不喜歡起衝突的個體會數量減少，所以接下來可能是存在20%的個體不喜歡起衝突而80%的個體喜歡追殺到底，因此喜歡追殺到底的個體很容易遇到一樣喜歡追殺到底的個體，這些喜歡衝突的個體可能會弄的兩拜俱傷，而不喜歡衝突的個體可能使用逃跑的方式躲避反而是存活下來了，此時就會出現數量上的反轉，最後會形成一種平衡，稱為「演化穩定策略」（evolutionary stable strategy, ESS），那當然不會有全然不喜歡衝突和喜歡衝突的個體，所以這樣的平衡是反應在基因上面的，基因在編寫神經網絡的時候就會擬定一個對它最有利的穩定策略，當這個策略奏效，這樣的基因就得以存活。&lt;/p&gt;
&lt;p&gt;這個部分讓我想起一個遊戲叫做&lt;a href="https://audreyt.github.io/trust-zh-TW/"&gt;信任的演化&lt;/a&gt;，這個遊戲是一個欺騙和合作的遊戲，當雙方都彼此合作時，會出現雙贏，而一方欺騙的情況下，欺騙者得利，另一方損失，如果雙方都欺騙則雙方都得不到好處。遊戲中有多個角色採取不同的策略，其中：「紅嬰仔」會一直信任對方，「黑到底」則會一直欺騙，「模仿貓」則是一開始採取合作的態度，當對方欺騙時在下一回合欺騙他，當對方合作時在下一回合就與他合作。當我們將這些角色丟進去進行多次的賽局，失敗者淘汰，成功者複製，在多次賽局之後你會發現「模仿貓」會勝出，利用這個遊戲你就能了解有限度的利他行為是如何勝出的，而基因所產生行為也就是在這樣多次的賽局下修正成有限度的利他行為。&lt;/p&gt;
&lt;h3 id="meme"&gt;文化的複製者—迷因（Meme）&lt;/h3&gt;
&lt;p&gt;基因為了自己的利益編寫出了神經網絡來幫助自己存活下去，但基因萬萬沒想到它所開發的東西居然會反過來對它不利！&lt;/p&gt;
&lt;p&gt;人類的基因產生了非常厲害的人腦，的確是有助於基因的繁衍，你看地球上的人口數量就知道了，但是大腦複雜到一個程度，就產生了思想，人類發展出了語言作溝通，文化得以使用語言或文字傳承下去，這在定義上也是一種複製者，這種文化的複製因子，作者將它稱之為迷因（Meme）。&lt;/p&gt;
&lt;p&gt;迷因（Meme）也是彼此競爭著，它們競爭的是你有限的大腦，如果你的大腦有資本主義，可能就容不下共產主義，而迷因也會不斷的改善自己，讓它們可以贏得競爭，然後傳播出去並大量的被複製。而有些時候迷因的競爭對手可能不只是其他迷因，也有可能會和基因作對，譬如說使用保險套的迷因，正阻止了基因的複製，激烈的宗教信仰的迷因，使人走向自殺式攻擊，同樣也是不利於基因的繁衍。&lt;/p&gt;
&lt;p&gt;這讓我想起最近人工智慧的興起，很多人對人工智慧開始產生擔憂，擔心機器人會反撲人類，如果依照上述的觀點看，這個可能是有機會發生的，程式設計師就像是基因，為了我們自身的目的去創造人工智慧來為我們服務，但是如果人工智慧越來越聰明，而且它的運算速度又比人類快，我們會根本無法理解人工智慧在想什麼，但是它們也確實是幫助人類解決很多問題，所以就算無法理解也就繼續的使用著，直到有一天，這些人工智慧在運作中突變出一種可以複製自己的複製者，也許是一種病毒，而這樣的複製者又能快速在人工智慧溝通中被傳遞出去，它就有可能回頭做出不利於人類的事。&lt;/p&gt;
&lt;h3 id="_5"&gt;結語：複製與競爭&lt;/h3&gt;
&lt;p&gt;本書讓我們重新認識了演化論，有了可以自己複製自己的複製者，就會有擴張，擴張的結果就是資源的稀缺，因此就自然產生競爭，在競爭的過程中，成功的策略符合ESS使得複製者得以存活下來，而失敗的複製者則會被淘汰、無法複製下去，這就是天擇的適者生存。從這一層的思想讓我們對於演化有了更深的認識，如果是生物的演化這個複製者就是基因，如果是文化的演化這個複製者就是迷因，也許你能想到其他的複製者，歡迎在下面留言讓大家知道。&lt;/p&gt;</content><category term="Reading"></category></entry><entry><title>實作Tensorflow (6)：Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)</title><link href="https://ycc.idv.tw/tensorflow-tutorial_6.html" rel="alternate"></link><published>2017-11-25T12:00:00+08:00</published><updated>2017-11-25T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-25:/tensorflow-tutorial_6.html</id><summary type="html">&lt;p&gt;概論RNN / 梯度消失與梯度爆炸 / Long Short-Term Memory (LSTM) / 使用LSTM實作文章產生器&lt;/p&gt;</summary><content type="html">&lt;p&gt;如果我們想要處理的問題是具有時序性的，該怎麼辦呢？本章將會介紹有時序性的Neurel Network。&lt;/p&gt;
&lt;p&gt;本單元程式碼LSTM部分可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/06_LSTM.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;h3 id="rnn"&gt;概論RNN&lt;/h3&gt;
&lt;p&gt;當我們想使得Neurel Network具有時序性，我們的Neurel Network就必須有記憶的功能，然後在我不斷的輸入新資訊時，也能同時保有歷史資訊的影響，最簡單的作法就是將Output的結果保留，等到新資訊進來時，將新的資訊和舊的Output一起考量來訓練Neurel Network。&lt;/p&gt;
&lt;p&gt;&lt;img alt="unrolling" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.010.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;這種將舊有資訊保留的Neurel Network統稱為Recurrent Neural Networks (RNN)，這種不斷回饋的網路可以攤開來處理，如上圖，如果我有5筆數據，拿訓練一個RNN 5個回合並做了5次更新，其實就等效於攤開來一次處理5筆數據並做1次更新，這樣的手法叫做Unrolling，我們實作上會使用Unrolling的手法來增加計算效率。&lt;/p&gt;
&lt;p&gt;&lt;img alt="RNN" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.011.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;接下來來看RNN內部怎麼實現的，上圖是最簡單的RNN形式，我們將上一回產生的Output和這一回的Input一起評估出這一回的Output，詳細式子如下：&lt;/p&gt;
&lt;div class="math"&gt;$$
o_{new}=tanh(i \times W_i + o \times W_o + B)
$$&lt;/div&gt;
&lt;p&gt;如此一來RNN就具有時序性了，舊的歷史資料將可以被「記憶」起來，你可以把RNN的「記憶」看成是「短期記憶」，因為它只會記得上一回的Output而已。&lt;/p&gt;
&lt;h3 id="_1"&gt;梯度消失與梯度爆炸&lt;/h3&gt;
&lt;p&gt;但這種形式的RNN在實作上會遇到很大的問題，還記得第二章當中，我們有講過像是tanh這類有飽和區的函數，會造成梯度消失的問題，而我們如果使用Unrolling的觀點來看RNN，將會發現這是一個超級深的網路，Backpapagation必須一路通到t0的RNN，想當然爾，有些梯度將會消失，部分權重就更新不到了，那有一些聰明的讀者一定會想到，那就使用Relu就好啦！不過其實還有一個重要的因素造成梯度消失，同時也造成梯度爆炸。&lt;/p&gt;
&lt;p&gt;注意喔！雖然我們使用Unrolling的觀點，把網路看成是一個Deep網路的連接，但是和之前DNN不同之處，這些RNN彼此間是共享同一組權重的，這會造成梯度消失和梯度爆炸兩個問題，在RNN的結構裡頭，一個權重會隨著時間不斷的加強影響一個單一特徵，因為不同時間之下的RNN Cell共用同一個權重，這麼一來若是權重大於1，影響將會隨時間放大到梯度爆炸，若是權重小於1，影響將會隨時間縮小到梯度消失，就像是蝴蝶效應一般，微小的差異因為回饋的機制，而不合理的放大或是消失，因此RNN的Error Surface將會崎嶇不平，這會造成我們無法穩定的找到最佳解，難以收斂。這才是RNN難以使用的重要原因，把Activation Function換成Relu不會解決問題，文獻上反而告訴我們會變更差。&lt;/p&gt;
&lt;p&gt;解決梯度爆炸有一個聽起來很廢但廣為人們使用的方法，叫做Gradient Clipping，也就是只要在更新過程梯度超過一個值，我就切掉讓梯度維持在這個上限，這樣就不會爆炸啦，待會會講到的LSTM只能夠解決梯度消失問題，但不能解決梯度爆炸問題，因此我們還是需要Gradient Clipping方法的幫忙。&lt;/p&gt;
&lt;p&gt;在Tensorflow怎麼做到Gradient Clipping呢？作法是這樣的，以往我們使用&lt;code&gt;optimizer.minimize(loss)&lt;/code&gt;來進行更新，事實上我們可以把這一步驟拆成兩部分，第一部分計算所有參數的梯度，第二部分使用這些梯度進行更新。因此我們可以從中作梗，把gradients偷天換日一番，一開始使用&lt;code&gt;optimizer.compute_gradients(loss)&lt;/code&gt;來計算出個別的梯度，然後使用&lt;code&gt;tf.clip_by_global_norm(gradients, clip_norm)&lt;/code&gt;來切梯度，最後再使用&lt;code&gt;optimizer.apply_gradients&lt;/code&gt;把新的梯度餵入進行更新。&lt;/p&gt;
&lt;h3 id="long-short-term-memory-lstm"&gt;Long Short-Term Memory (LSTM)&lt;/h3&gt;
&lt;p&gt;LSTM是現今RNN的主流，它可以解決梯度消失的問題，我們先來看看結構，先預告一下，LSTM是迄今為止這系列課程當中看過最複雜的Neurel Network。&lt;/p&gt;
&lt;p&gt;&lt;img alt="LSTM" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.012.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;最一開始和RNN一樣，Input會和上一回的Output一起評估一個「短期記憶」，&lt;/p&gt;
&lt;div class="math"&gt;$$
f_m = tanh (i \times W_{mi} + o \times W_{mo} + B_m)
$$&lt;/div&gt;
&lt;p&gt;但接下來不同於RNN直接輸出，LSTM做了一個類似於轉換成「長期記憶」的機制，「長期記憶」在這裡稱為State，State的狀態由三道門所控制，Input Gate負責控管哪些「短期記憶」可以進到「長期記憶」，Forget Gate負責調配哪一些「長期記憶」需要被遺忘，Output Gate則負責去決定需要從「長期記憶」中輸出怎樣的內容，先不要管這些Gate怎麼來，我們可以把這樣的記憶機制寫成以下的式子，假設State為&lt;span class="math"&gt;\(f_{state}\)&lt;/span&gt;、Input Gate為&lt;span class="math"&gt;\(G_i\)&lt;/span&gt;、Forget Gate為&lt;span class="math"&gt;\(G_f\)&lt;/span&gt;和Output Gate為&lt;span class="math"&gt;\(G_o\)&lt;/span&gt;。&lt;/p&gt;
&lt;div class="math"&gt;$$
f_{state,new} = G_i \times f_m + G_f \times f_{state}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
o_{new} = G_o \times tanh(f_{state,new})
$$&lt;/div&gt;
&lt;p&gt;如果我們要使得上面中Gates的部分具有開關的功能的話，我們會希望Gates可以是0到1的值，0代表全關，1代表全開，sigmoid正可以幫我們做到這件事，那哪些因素會決定Gates的關閉與否呢？不妨考慮所有可能的因素，也就是所有輸入這個Cell的資訊都考慮進去，但上一回的State必須被剔除於外，因為上一回的State來決定下一個State的操作是不合理的，因此我們就可以寫下所有Gates的表示式了。&lt;/p&gt;
&lt;div class="math"&gt;$$
G_i = Sigmoid (i \times W_{ii} + o \times W_{io} + B_i)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
G_f = Sigmoid (i \times W_{fi} + o \times W_{fo} + B_f)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
G_o = Sigmoid(i \times W_{oi} + o \times W_{oo} + B_o)
$$&lt;/div&gt;
&lt;p&gt;這就是LSTM，「長期記憶」的出現可以解決掉梯度消失的問題，RNN只有「短期記憶」，所以一旦認為一個特徵不重要，經過幾回連乘，這個特徵的梯度就會消失殆盡，但是LSTM保留State，並且使用「加」的方法更新State，所以有一些重要的State得以留下來持續影響著Output，解決了梯度消失的問題。但是，不幸的LSTM還是免不了梯度爆炸，為什麼呢？如果一個特徵真的很重要，State會記住，Input也會強調，所以幾輪下來還是有可能出現爆炸的情況，這時候我們就需要Gradient Clipping的幫忙。&lt;/p&gt;
&lt;h3 id="lstm"&gt;使用LSTM實作文章產生器&lt;/h3&gt;
&lt;p&gt;接下來我們來實作LSTM，目標是做一個文章產生器，我們希望機器可以不斷的根據前文猜測下一個「字母」(Letters)應該要下什麼，如此一來我只要給個開頭字母，LSTM就可以幫我腦補成一篇文章。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;string&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;zipfile&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;urllib.request&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;urlretrieve&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;
&lt;span class="normal"&gt;53&lt;/span&gt;
&lt;span class="normal"&gt;54&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ascii_lowercase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# [a-z] + &amp;#39; &amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FIRST_LETTER_ASCII&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;ord&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ascii_lowercase&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maybe_download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_bytes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Download a file if not present, and make sure it&amp;#39;s the right size.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urlretrieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;statinfo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;statinfo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;st_size&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;expected_bytes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Found and verified &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;statinfo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;st_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Failed to verify &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;. Can you get to it with a browser?&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;zipfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ZipFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;namelist&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;char2id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;char&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ascii_lowercase&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;ord&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;FIRST_LETTER_ASCII&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;char&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Unexpected character: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;id2char&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictid&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;dictid&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictid&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;FIRST_LETTER_ASCII&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;


&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Downloading text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;maybe_download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;http://mattmahoney.net/dc/text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;./text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;31344016&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;=====&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Data size &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; letters&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;=====&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;valid_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;valid_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;valid_size&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;train_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;valid_size&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;train_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Train Dataset: size:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;letters,&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;  first 64:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_text&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Validation Dataset: size:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;letters,&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;  first 64:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_text&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;
&lt;span class="normal"&gt;9&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Downloading text8.zip&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Found and verified ./text8.zip&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=====&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Data size 100000000 letters&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=====&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Train Dataset&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;99999000 letters,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;first 64&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ons anarchists advocate social relations based upon voluntary as&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Validation Dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;1000 letters,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;first 64&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;anarchism originated as a term of abuse first used against earl&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;上面操作我們建制完成了字母庫，接下來就可以產生我們訓練所需要的Batch Data，所以我們來看看究竟要產生怎樣格式的資料。&lt;/p&gt;
&lt;p&gt;&lt;img alt="LSTM Implement" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.013.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;如上圖所示，有點小複雜，假設我要設計一個LSTM Model，它的Unrolling Number為3，Batch Size為2，然後遇到的字串是"abcde fghij klmno pqrst"，接下來就開始產生每個Round要用的Data，產生的結果如上圖所示，你會發現產生的Data第0軸表示的是考慮unrolling需要取樣的資料，總共應該會有(Unrolling Number+1)筆，如上圖例，共有4筆，3筆當作輸入而3筆當作Labels，中間有2筆重疊使用，另外還有一點，我們會保留最後一筆Data當作下一個回合的第一筆，這是為了不浪費使用每一個字母前後的組合。而第1軸則是餵入單一LSTM需要的資料，我們一次可以餵多組不相干的字母進去，如上圖例，Batch Size=2所以餵2個字母進去，那這些不相干的字母在取樣的時候，我們會盡量讓它平均分配在文字庫，才能確保彼此之間不相干，以增加LSTM的訓練效率和效果。&lt;/p&gt;
&lt;p&gt;因此，先產生Batch Data吧！&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;
&lt;span class="normal"&gt;53&lt;/span&gt;
&lt;span class="normal"&gt;54&lt;/span&gt;
&lt;span class="normal"&gt;55&lt;/span&gt;
&lt;span class="normal"&gt;56&lt;/span&gt;
&lt;span class="normal"&gt;57&lt;/span&gt;
&lt;span class="normal"&gt;58&lt;/span&gt;
&lt;span class="normal"&gt;59&lt;/span&gt;
&lt;span class="normal"&gt;60&lt;/span&gt;
&lt;span class="normal"&gt;61&lt;/span&gt;
&lt;span class="normal"&gt;62&lt;/span&gt;
&lt;span class="normal"&gt;63&lt;/span&gt;
&lt;span class="normal"&gt;64&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;characters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Turn a 1-hot encoding or a probability distribution over the possible&lt;/span&gt;
&lt;span class="sd"&gt;    characters back into its (most likely) character representation.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;id2char&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;batches2string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Convert a sequence of batches back into their (most likely) string&lt;/span&gt;
&lt;span class="sd"&gt;    representation.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;characters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rnn_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;text_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;### initialization&lt;/span&gt;
    &lt;span class="n"&gt;segment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;
    &lt;span class="n"&gt;cursors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;offset&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;segment&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;offset&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

    &lt;span class="n"&gt;batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;batch_initial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;cursor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;id_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;char2id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cursor&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;batch_initial&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;id_&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;

        &lt;span class="c1"&gt;# move cursor&lt;/span&gt;
        &lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt;

    &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_initial&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;### generate loop&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;cursor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="n"&gt;id_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;char2id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cursor&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;id_&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;

                &lt;span class="c1"&gt;# move cursor&lt;/span&gt;
                &lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt;
            &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;batches&lt;/span&gt;  &lt;span class="c1"&gt;# [last batch of previous batches] + [unrollings]&lt;/span&gt;


&lt;span class="c1"&gt;# demonstrate generator&lt;/span&gt;
&lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;
&lt;span class="n"&gt;num_unrollings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

&lt;span class="n"&gt;train_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rnn_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;valid_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rnn_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;*** train_batches:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches2string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_batches&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches2string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_batches&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;*** valid_batches:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches2string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_batches&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches2string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_batches&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;*** train_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ons&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;anarchi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;when&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;milita&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lleria&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;arch&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;abbeys&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;and&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;married&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;urr&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;hel&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;and&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ric&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;and&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;litur&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ay&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;opened&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;f&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tion&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;from&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;migration&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;new&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;york&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ot&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;he&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;boeing&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;listed&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;wi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;eber&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;has&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;pr&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;o&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;be&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;made&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;yer&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;who&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;rec&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ore&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;signifi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;fierce&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;cr&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;two&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;six&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ei&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;aristotle&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ity&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;can&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;be&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;and&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;intrac&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tion&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dy&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;to&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;pass&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;f&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;certain&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;d&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;at&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;it&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;will&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;convince&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ent&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;told&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;hi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ampaign&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;and&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rver&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;side&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ious&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;texts&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;o&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;capitaliz&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;duplicate&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gh&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ann&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;es&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;d&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ine&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;january&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ross&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;zero&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;cal&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;theorie&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ast&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;instanc&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;dimensiona&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;most&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;holy&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;t&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;support&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;u&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;is&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;still&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;oscillati&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;o&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;eight&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;sub&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;italy&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;la&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;s&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;tower&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;klahoma&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;pre&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;erprise&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;lin&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ws&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;becomes&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;et&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;in&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;a&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;naz&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;fabian&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;etchy&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;to&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;re&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;sharman&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ne&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ised&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;empero&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ting&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;in&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;pol&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;d&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;neo&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;latin&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;th&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;risky&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ri&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;encyclopedi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;fense&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;duating&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;fro&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;treet&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;grid&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ations&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;more&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;appeal&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;d&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;si&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;have&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;mad&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ists&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;advoca&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ary&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;governm&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;hes&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;nationa&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;d&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;monasteri&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;raca&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;prince&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;chard&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;baer&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rgical&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;lang&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;for&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;passeng&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;nationa&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;took&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;place&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ther&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;well&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;k&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seven&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;six&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ith&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;a&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;gloss&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;robably&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;bee&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;to&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;recogniz&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ceived&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;icant&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;than&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ritic&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;th&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ight&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;in&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;sig&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;s&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;uncaused&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;lost&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;as&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;in&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;cellular&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ic&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;size&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;him&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;a&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;stic&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;drugs&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;confu&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;take&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;to&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;co&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;priest&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;im&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;to&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;name&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;d&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;barred&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;at&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;standard&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;fo&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;such&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;as&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;es&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ze&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;on&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;g&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;or&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;d&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;hiver&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;one&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;eight&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;mar&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;lead&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ch&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;es&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;classica&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ce&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;non&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;al&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;analysis&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mormons&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;bel&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;t&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;or&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;at&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;lea&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;disagreed&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ing&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;system&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;btypes&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;base&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anguages&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;th&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;r&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;commissio&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ess&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;one&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;nin&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;nux&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;suse&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;li&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;first&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;zi&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;concentr&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;society&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ne&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;elatively&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;etworks&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;sha&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;or&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;hirohito&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;litical&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ini&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;n&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;most&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;iskerdoo&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ri&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ic&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;overview&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;air&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;compone&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;om&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;acnm&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;acc&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;centerline&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;than&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;any&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;devotional&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;de&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;such&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;dev&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;*** valid_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;an&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;定義一下待會會用到的函數。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample_distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Sample one element from a distribution assumed to be an array of normalized&lt;/span&gt;
&lt;span class="sd"&gt;    probabilities.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Turn a (column) prediction into 1-hot encoded samples.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;logprob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Log-probability of the true labels in a predicted batch.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;1e-10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-10&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;開始建制LSTM Model。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;  1&lt;/span&gt;
&lt;span class="normal"&gt;  2&lt;/span&gt;
&lt;span class="normal"&gt;  3&lt;/span&gt;
&lt;span class="normal"&gt;  4&lt;/span&gt;
&lt;span class="normal"&gt;  5&lt;/span&gt;
&lt;span class="normal"&gt;  6&lt;/span&gt;
&lt;span class="normal"&gt;  7&lt;/span&gt;
&lt;span class="normal"&gt;  8&lt;/span&gt;
&lt;span class="normal"&gt;  9&lt;/span&gt;
&lt;span class="normal"&gt; 10&lt;/span&gt;
&lt;span class="normal"&gt; 11&lt;/span&gt;
&lt;span class="normal"&gt; 12&lt;/span&gt;
&lt;span class="normal"&gt; 13&lt;/span&gt;
&lt;span class="normal"&gt; 14&lt;/span&gt;
&lt;span class="normal"&gt; 15&lt;/span&gt;
&lt;span class="normal"&gt; 16&lt;/span&gt;
&lt;span class="normal"&gt; 17&lt;/span&gt;
&lt;span class="normal"&gt; 18&lt;/span&gt;
&lt;span class="normal"&gt; 19&lt;/span&gt;
&lt;span class="normal"&gt; 20&lt;/span&gt;
&lt;span class="normal"&gt; 21&lt;/span&gt;
&lt;span class="normal"&gt; 22&lt;/span&gt;
&lt;span class="normal"&gt; 23&lt;/span&gt;
&lt;span class="normal"&gt; 24&lt;/span&gt;
&lt;span class="normal"&gt; 25&lt;/span&gt;
&lt;span class="normal"&gt; 26&lt;/span&gt;
&lt;span class="normal"&gt; 27&lt;/span&gt;
&lt;span class="normal"&gt; 28&lt;/span&gt;
&lt;span class="normal"&gt; 29&lt;/span&gt;
&lt;span class="normal"&gt; 30&lt;/span&gt;
&lt;span class="normal"&gt; 31&lt;/span&gt;
&lt;span class="normal"&gt; 32&lt;/span&gt;
&lt;span class="normal"&gt; 33&lt;/span&gt;
&lt;span class="normal"&gt; 34&lt;/span&gt;
&lt;span class="normal"&gt; 35&lt;/span&gt;
&lt;span class="normal"&gt; 36&lt;/span&gt;
&lt;span class="normal"&gt; 37&lt;/span&gt;
&lt;span class="normal"&gt; 38&lt;/span&gt;
&lt;span class="normal"&gt; 39&lt;/span&gt;
&lt;span class="normal"&gt; 40&lt;/span&gt;
&lt;span class="normal"&gt; 41&lt;/span&gt;
&lt;span class="normal"&gt; 42&lt;/span&gt;
&lt;span class="normal"&gt; 43&lt;/span&gt;
&lt;span class="normal"&gt; 44&lt;/span&gt;
&lt;span class="normal"&gt; 45&lt;/span&gt;
&lt;span class="normal"&gt; 46&lt;/span&gt;
&lt;span class="normal"&gt; 47&lt;/span&gt;
&lt;span class="normal"&gt; 48&lt;/span&gt;
&lt;span class="normal"&gt; 49&lt;/span&gt;
&lt;span class="normal"&gt; 50&lt;/span&gt;
&lt;span class="normal"&gt; 51&lt;/span&gt;
&lt;span class="normal"&gt; 52&lt;/span&gt;
&lt;span class="normal"&gt; 53&lt;/span&gt;
&lt;span class="normal"&gt; 54&lt;/span&gt;
&lt;span class="normal"&gt; 55&lt;/span&gt;
&lt;span class="normal"&gt; 56&lt;/span&gt;
&lt;span class="normal"&gt; 57&lt;/span&gt;
&lt;span class="normal"&gt; 58&lt;/span&gt;
&lt;span class="normal"&gt; 59&lt;/span&gt;
&lt;span class="normal"&gt; 60&lt;/span&gt;
&lt;span class="normal"&gt; 61&lt;/span&gt;
&lt;span class="normal"&gt; 62&lt;/span&gt;
&lt;span class="normal"&gt; 63&lt;/span&gt;
&lt;span class="normal"&gt; 64&lt;/span&gt;
&lt;span class="normal"&gt; 65&lt;/span&gt;
&lt;span class="normal"&gt; 66&lt;/span&gt;
&lt;span class="normal"&gt; 67&lt;/span&gt;
&lt;span class="normal"&gt; 68&lt;/span&gt;
&lt;span class="normal"&gt; 69&lt;/span&gt;
&lt;span class="normal"&gt; 70&lt;/span&gt;
&lt;span class="normal"&gt; 71&lt;/span&gt;
&lt;span class="normal"&gt; 72&lt;/span&gt;
&lt;span class="normal"&gt; 73&lt;/span&gt;
&lt;span class="normal"&gt; 74&lt;/span&gt;
&lt;span class="normal"&gt; 75&lt;/span&gt;
&lt;span class="normal"&gt; 76&lt;/span&gt;
&lt;span class="normal"&gt; 77&lt;/span&gt;
&lt;span class="normal"&gt; 78&lt;/span&gt;
&lt;span class="normal"&gt; 79&lt;/span&gt;
&lt;span class="normal"&gt; 80&lt;/span&gt;
&lt;span class="normal"&gt; 81&lt;/span&gt;
&lt;span class="normal"&gt; 82&lt;/span&gt;
&lt;span class="normal"&gt; 83&lt;/span&gt;
&lt;span class="normal"&gt; 84&lt;/span&gt;
&lt;span class="normal"&gt; 85&lt;/span&gt;
&lt;span class="normal"&gt; 86&lt;/span&gt;
&lt;span class="normal"&gt; 87&lt;/span&gt;
&lt;span class="normal"&gt; 88&lt;/span&gt;
&lt;span class="normal"&gt; 89&lt;/span&gt;
&lt;span class="normal"&gt; 90&lt;/span&gt;
&lt;span class="normal"&gt; 91&lt;/span&gt;
&lt;span class="normal"&gt; 92&lt;/span&gt;
&lt;span class="normal"&gt; 93&lt;/span&gt;
&lt;span class="normal"&gt; 94&lt;/span&gt;
&lt;span class="normal"&gt; 95&lt;/span&gt;
&lt;span class="normal"&gt; 96&lt;/span&gt;
&lt;span class="normal"&gt; 97&lt;/span&gt;
&lt;span class="normal"&gt; 98&lt;/span&gt;
&lt;span class="normal"&gt; 99&lt;/span&gt;
&lt;span class="normal"&gt;100&lt;/span&gt;
&lt;span class="normal"&gt;101&lt;/span&gt;
&lt;span class="normal"&gt;102&lt;/span&gt;
&lt;span class="normal"&gt;103&lt;/span&gt;
&lt;span class="normal"&gt;104&lt;/span&gt;
&lt;span class="normal"&gt;105&lt;/span&gt;
&lt;span class="normal"&gt;106&lt;/span&gt;
&lt;span class="normal"&gt;107&lt;/span&gt;
&lt;span class="normal"&gt;108&lt;/span&gt;
&lt;span class="normal"&gt;109&lt;/span&gt;
&lt;span class="normal"&gt;110&lt;/span&gt;
&lt;span class="normal"&gt;111&lt;/span&gt;
&lt;span class="normal"&gt;112&lt;/span&gt;
&lt;span class="normal"&gt;113&lt;/span&gt;
&lt;span class="normal"&gt;114&lt;/span&gt;
&lt;span class="normal"&gt;115&lt;/span&gt;
&lt;span class="normal"&gt;116&lt;/span&gt;
&lt;span class="normal"&gt;117&lt;/span&gt;
&lt;span class="normal"&gt;118&lt;/span&gt;
&lt;span class="normal"&gt;119&lt;/span&gt;
&lt;span class="normal"&gt;120&lt;/span&gt;
&lt;span class="normal"&gt;121&lt;/span&gt;
&lt;span class="normal"&gt;122&lt;/span&gt;
&lt;span class="normal"&gt;123&lt;/span&gt;
&lt;span class="normal"&gt;124&lt;/span&gt;
&lt;span class="normal"&gt;125&lt;/span&gt;
&lt;span class="normal"&gt;126&lt;/span&gt;
&lt;span class="normal"&gt;127&lt;/span&gt;
&lt;span class="normal"&gt;128&lt;/span&gt;
&lt;span class="normal"&gt;129&lt;/span&gt;
&lt;span class="normal"&gt;130&lt;/span&gt;
&lt;span class="normal"&gt;131&lt;/span&gt;
&lt;span class="normal"&gt;132&lt;/span&gt;
&lt;span class="normal"&gt;133&lt;/span&gt;
&lt;span class="normal"&gt;134&lt;/span&gt;
&lt;span class="normal"&gt;135&lt;/span&gt;
&lt;span class="normal"&gt;136&lt;/span&gt;
&lt;span class="normal"&gt;137&lt;/span&gt;
&lt;span class="normal"&gt;138&lt;/span&gt;
&lt;span class="normal"&gt;139&lt;/span&gt;
&lt;span class="normal"&gt;140&lt;/span&gt;
&lt;span class="normal"&gt;141&lt;/span&gt;
&lt;span class="normal"&gt;142&lt;/span&gt;
&lt;span class="normal"&gt;143&lt;/span&gt;
&lt;span class="normal"&gt;144&lt;/span&gt;
&lt;span class="normal"&gt;145&lt;/span&gt;
&lt;span class="normal"&gt;146&lt;/span&gt;
&lt;span class="normal"&gt;147&lt;/span&gt;
&lt;span class="normal"&gt;148&lt;/span&gt;
&lt;span class="normal"&gt;149&lt;/span&gt;
&lt;span class="normal"&gt;150&lt;/span&gt;
&lt;span class="normal"&gt;151&lt;/span&gt;
&lt;span class="normal"&gt;152&lt;/span&gt;
&lt;span class="normal"&gt;153&lt;/span&gt;
&lt;span class="normal"&gt;154&lt;/span&gt;
&lt;span class="normal"&gt;155&lt;/span&gt;
&lt;span class="normal"&gt;156&lt;/span&gt;
&lt;span class="normal"&gt;157&lt;/span&gt;
&lt;span class="normal"&gt;158&lt;/span&gt;
&lt;span class="normal"&gt;159&lt;/span&gt;
&lt;span class="normal"&gt;160&lt;/span&gt;
&lt;span class="normal"&gt;161&lt;/span&gt;
&lt;span class="normal"&gt;162&lt;/span&gt;
&lt;span class="normal"&gt;163&lt;/span&gt;
&lt;span class="normal"&gt;164&lt;/span&gt;
&lt;span class="normal"&gt;165&lt;/span&gt;
&lt;span class="normal"&gt;166&lt;/span&gt;
&lt;span class="normal"&gt;167&lt;/span&gt;
&lt;span class="normal"&gt;168&lt;/span&gt;
&lt;span class="normal"&gt;169&lt;/span&gt;
&lt;span class="normal"&gt;170&lt;/span&gt;
&lt;span class="normal"&gt;171&lt;/span&gt;
&lt;span class="normal"&gt;172&lt;/span&gt;
&lt;span class="normal"&gt;173&lt;/span&gt;
&lt;span class="normal"&gt;174&lt;/span&gt;
&lt;span class="normal"&gt;175&lt;/span&gt;
&lt;span class="normal"&gt;176&lt;/span&gt;
&lt;span class="normal"&gt;177&lt;/span&gt;
&lt;span class="normal"&gt;178&lt;/span&gt;
&lt;span class="normal"&gt;179&lt;/span&gt;
&lt;span class="normal"&gt;180&lt;/span&gt;
&lt;span class="normal"&gt;181&lt;/span&gt;
&lt;span class="normal"&gt;182&lt;/span&gt;
&lt;span class="normal"&gt;183&lt;/span&gt;
&lt;span class="normal"&gt;184&lt;/span&gt;
&lt;span class="normal"&gt;185&lt;/span&gt;
&lt;span class="normal"&gt;186&lt;/span&gt;
&lt;span class="normal"&gt;187&lt;/span&gt;
&lt;span class="normal"&gt;188&lt;/span&gt;
&lt;span class="normal"&gt;189&lt;/span&gt;
&lt;span class="normal"&gt;190&lt;/span&gt;
&lt;span class="normal"&gt;191&lt;/span&gt;
&lt;span class="normal"&gt;192&lt;/span&gt;
&lt;span class="normal"&gt;193&lt;/span&gt;
&lt;span class="normal"&gt;194&lt;/span&gt;
&lt;span class="normal"&gt;195&lt;/span&gt;
&lt;span class="normal"&gt;196&lt;/span&gt;
&lt;span class="normal"&gt;197&lt;/span&gt;
&lt;span class="normal"&gt;198&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;LSTM&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_unrollings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unrollings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_unrollings&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_memory&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saved&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new grap&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unrollings&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unrollings&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;  &lt;span class="c1"&gt;# labels are inputs shifted by one time step.&lt;/span&gt;


            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;n_batch&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;

            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdagradOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# gradient clipping&lt;/span&gt;

            &lt;span class="c1"&gt;# output gradients one by one&lt;/span&gt;
            &lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compute_gradients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip_by_global_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# clip gradient&lt;/span&gt;
            &lt;span class="c1"&gt;# apply clipped gradients&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_gradients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Sampling and validation eval: batch 1, no unrolling.&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

            &lt;span class="n"&gt;saved_sample_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="n"&gt;saved_sample_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_sample_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;     &lt;span class="c1"&gt;# reset sample state operator&lt;/span&gt;
                &lt;span class="n"&gt;saved_sample_output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
                &lt;span class="n"&gt;saved_sample_state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;

            &lt;span class="n"&gt;sample_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lstm_cell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;saved_sample_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;saved_sample_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;control_dependencies&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;saved_sample_output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_output&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                          &lt;span class="n"&gt;saved_sample_state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_state&lt;/span&gt;&lt;span class="p"&gt;)]):&lt;/span&gt;
                &lt;span class="c1"&gt;# use tf.control_dependencies to make sure &amp;#39;saving&amp;#39; before &amp;#39;prediction&amp;#39;&lt;/span&gt;

                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xw_plus_b&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;lstm_cell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&amp;quot;Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf&lt;/span&gt;
&lt;span class="sd"&gt;        Note that in this formulation, we omit the various connections between the&lt;/span&gt;
&lt;span class="sd"&gt;        previous state and the gates.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="c1"&gt;## Build Input Gate&lt;/span&gt;
        &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;ib&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;input_gate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;im&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ib&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;## Build Forget Gate&lt;/span&gt;
        &lt;span class="n"&gt;fx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;forget_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;fm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;forget_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;fb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;forget_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;forget_gate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;## Memory&lt;/span&gt;
        &lt;span class="n"&gt;cx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;memory_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;cm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;memory_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;cb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;memory&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;update&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cb&lt;/span&gt;
        &lt;span class="c1"&gt;## Update State&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;forget_gate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;input_gate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;## Build Output Gate&lt;/span&gt;
        &lt;span class="n"&gt;ox&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;output_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;om&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;output_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;ob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;output_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;output_gate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ox&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;om&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ob&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;## Ouput&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_gate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saved&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;input_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;input_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;forget_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;forget_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;output_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;output_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;memory_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;memory_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;

            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;input_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;forget_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;output_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;memory&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="c1"&gt;# Variables saving state across unrollings.&lt;/span&gt;
        &lt;span class="n"&gt;saved_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;saved_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="c1"&gt;# Unrolled LSTM loop.&lt;/span&gt;
        &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;saved_output&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;saved_state&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;input_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lstm_cell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# State saving across unrollings.&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;control_dependencies&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;saved_output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                      &lt;span class="n"&gt;saved_state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)]):&lt;/span&gt;
            &lt;span class="c1"&gt;# use tf.control_dependencies to make sure &amp;#39;saving&amp;#39; before &amp;#39;calculating loss&amp;#39;&lt;/span&gt;

            &lt;span class="c1"&gt;# Classifier&lt;/span&gt;
            &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xw_plus_b&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unrollings&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;perplexity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;sum_logprob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;sample_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_sample_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sample_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;newshape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                &lt;span class="n"&gt;sample_label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;newshape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sample_input&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
                &lt;span class="n"&gt;sum_logprob&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;logprob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;perplexity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sum_logprob&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sample_size&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;perplexity&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;feed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;id2char&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;)]])&lt;/span&gt;
        &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;characters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_sample_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;feed&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
            &lt;span class="n"&gt;feed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;characters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sentence&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;
&lt;span class="normal"&gt;53&lt;/span&gt;
&lt;span class="normal"&gt;54&lt;/span&gt;
&lt;span class="normal"&gt;55&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# build training batch generator&lt;/span&gt;
&lt;span class="n"&gt;batch_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rnn_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# build validation data&lt;/span&gt;
&lt;span class="n"&gt;valid_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rnn_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;valid_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_batches&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_size&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# build LSTM model&lt;/span&gt;
&lt;span class="n"&gt;model_LSTM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LSTM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_unrollings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# initial model&lt;/span&gt;
&lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# online training&lt;/span&gt;
&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;
&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;
&lt;span class="n"&gt;valid_freq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_generator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;

    &lt;span class="n"&gt;train_perplexity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;perplexity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;s loss = &lt;/span&gt;&lt;span class="si"&gt;%6.4f&lt;/span&gt;&lt;span class="s1"&gt;, perplexity = &lt;/span&gt;&lt;span class="si"&gt;%6.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;
           &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_perplexity&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;valid_freq&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;=============== Validation ===============&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;validation perplexity = &lt;/span&gt;&lt;span class="si"&gt;%6.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;perplexity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Generate From &lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;a&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;:  &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Generate From &lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;h&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;:  &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Generate From &lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;m&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;:  &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;==========================================&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;
&lt;span class="normal"&gt;53&lt;/span&gt;
&lt;span class="normal"&gt;54&lt;/span&gt;
&lt;span class="normal"&gt;55&lt;/span&gt;
&lt;span class="normal"&gt;56&lt;/span&gt;
&lt;span class="normal"&gt;57&lt;/span&gt;
&lt;span class="normal"&gt;58&lt;/span&gt;
&lt;span class="normal"&gt;59&lt;/span&gt;
&lt;span class="normal"&gt;60&lt;/span&gt;
&lt;span class="normal"&gt;61&lt;/span&gt;
&lt;span class="normal"&gt;62&lt;/span&gt;
&lt;span class="normal"&gt;63&lt;/span&gt;
&lt;span class="normal"&gt;64&lt;/span&gt;
&lt;span class="normal"&gt;65&lt;/span&gt;
&lt;span class="normal"&gt;66&lt;/span&gt;
&lt;span class="normal"&gt;67&lt;/span&gt;
&lt;span class="normal"&gt;68&lt;/span&gt;
&lt;span class="normal"&gt;69&lt;/span&gt;
&lt;span class="normal"&gt;70&lt;/span&gt;
&lt;span class="normal"&gt;71&lt;/span&gt;
&lt;span class="normal"&gt;72&lt;/span&gt;
&lt;span class="normal"&gt;73&lt;/span&gt;
&lt;span class="normal"&gt;74&lt;/span&gt;
&lt;span class="normal"&gt;75&lt;/span&gt;
&lt;span class="normal"&gt;76&lt;/span&gt;
&lt;span class="normal"&gt;77&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 1/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;66s loss = 1.8249, perplexity = 5.6840&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 2/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.5348, perplexity = 5.7269&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 3/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.4754, perplexity = 5.7866&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 4/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.4412, perplexity = 5.3462&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 5/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.4246, perplexity = 5.8845&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.7260&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ah plays agrestiom scattery at an experiments the a&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ht number om one nine six three kg aid rosta franci&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;m within v like opens and solepolity ledania as was&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 6/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.4094, perplexity = 6.0429&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 7/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.3954, perplexity = 5.6133&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 8/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3905, perplexity = 5.4791&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 9/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3675, perplexity = 5.7168&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 10/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3861, perplexity = 5.3937&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.5992&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ands their hypenman sam diversion passes to rouke t&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;hash pryess the setuluply see include the grophistr&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;merhouses tourism in vertic or influence carbon min&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 11/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.3782, perplexity = 5.5835&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 12/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3802, perplexity = 6.0567&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 13/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3723, perplexity = 6.0672&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 14/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3729, perplexity = 6.4365&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 15/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3682, perplexity = 6.2878&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.7153&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ate at decade a july uses mobe on the john press to&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;htell yullandi is u one five it naval railandly eng&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ment theory president and much three sinit in harde&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 16/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;65s loss = 1.3647, perplexity = 5.5579&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 17/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3691, perplexity = 5.3885&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 18/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.3535, perplexity = 6.4797&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 19/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3637, perplexity = 5.8126&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 20/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3567, perplexity = 5.9839&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.6210&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ate treaty jack a golderazogon develoged civilized&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;hyene is ricpstowed dark preferent crurts annivaril&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;mer centine all level end of a character of tracks&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 21/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;65s loss = 1.3584, perplexity = 6.0557&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 22/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3535, perplexity = 7.0777&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 23/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3700, perplexity = 5.7674&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 24/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3609, perplexity = 6.1226&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 25/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.3663, perplexity = 6.2711&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.6048&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;an vary palest in some live halleten converting to&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;heper could use that the l bidging the five zero th&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;mer yort can the real forexanded or rather then for&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 26/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;66s loss = 1.3551, perplexity = 6.1640&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 27/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;65s loss = 1.3586, perplexity = 6.3620&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 28/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;65s loss = 1.3744, perplexity = 5.5748&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 29/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.3634, perplexity = 6.0498&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 30/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3671, perplexity = 6.2313&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.4751&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;an one brivistrial empir thorodox to an of one city&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ho wing two he wonders marding where never boat lit&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;mptemeignt linerical premore logical boldving on ch&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;最後來產生一篇以"t"為開頭的1000字文章吧！&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;th the oppose asia college on all of indirect i suicide upse angence and including khazool cashle with jeremp of the case hasway was catiline tribui s law can be wounds to free from an eventually locations university colid for admirum syn semition goths display the might the official up it alder stowinity name like or day elenth names and lesk external links a loons for have the genione e elevang cress leven isbn effects on cultural leave to oldincil he hokerzon blacklomen with the known resolvement of literated by college founded to families in ak urke player jain of highling fake state a first o al reason into the son then mmpt one nine three three npunt university unexal and currently amnyanipation behavion from ber and ii variety of the gupife number topan has one three zero z capital prime genary brown one nine five nine so universities country recipient the vegetables bether form the distinct de plus out as a first a johnson quicky s remain which an death to anti in panibus series&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;看得出來LSTM想表達什麼嗎，哈哈！&lt;/p&gt;
&lt;h3 id="reference"&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb"&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>實作Tensorflow (5)：Word2Vec</title><link href="https://ycc.idv.tw/tensorflow-tutorial_5.html" rel="alternate"></link><published>2017-11-19T12:00:00+08:00</published><updated>2017-11-19T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-19:/tensorflow-tutorial_5.html</id><summary type="html">&lt;p&gt;Word2Vec觀念解析 / Word2Vec的架構 / Word2Vec的兩種常用方法：Skip-Gram和CBOW / 準備文本語料庫 / 實作Skip-Gram / 實作CBOW (Continuous Bag of Words)&lt;/p&gt;</summary><content type="html">&lt;p&gt;機器有辦法自行從文本中觀察出詞彙間的相似度嗎？是可以的，word2vec是"word to vector"的縮寫，代表的正是將每個字轉換成向量，而一旦兩個字的向量越是靠近，就代表它的相似度越高，我們究竟要如何得到這些向量呢？方法簡單但出奇有效，文章的最後會向大家呈現它的精彩的結果。&lt;/p&gt;
&lt;p&gt;本單元程式碼Skip-Gram Word2Vec部分可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/05_1_word2vec_SkipGram.py"&gt;Github&lt;/a&gt;下載，CBOW Word2Vec部分可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/05_2_word2vec_CBOW.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;h3 id="word2vec"&gt;Word2Vec觀念解析&lt;/h3&gt;
&lt;p&gt;Word2Vec的形式和Autoencoder有點像，一樣是從高維度的空間轉換到低維度的空間，再轉換回去原本的維度，只是這一次轉回去的東西不再是原本一模一樣的東西了。&lt;/p&gt;
&lt;p&gt;Word2Vec的Input和Output這次變成是上下文的文字組合，舉個例子，"by the way"這個用法如果多次被機器看過的話，機器是有辦法去學習到這樣的規律的，此時"by"與"the"和"way"便會產生一個上下文的關聯性，為了將這樣的關聯性建立起來，我們希望當我輸入"by"時，機器有辦法預測並輸出"the"或"way"，這代表在機器內部它已經學習到了上下文的關聯性。&lt;/p&gt;
&lt;p&gt;那如果今天這個機器也同時看到很多次的"on the way"這種用法，所以當我輸入"on"時，機器要有辦法預測並輸出"the"或"way"，但是我們不希望"on"和"by"兩個詞在學習時是分開學習的，我們希望機器可以因為"by the way"和"on the way"的結構很相似，所以有辦法抓出"on"和"by"是彼此相似的結論。&lt;/p&gt;
&lt;p&gt;如何做到呢？答案就是限縮這個上下文的關聯性的儲存維度，如果我的字彙量有1000個，這1000個字彙彼此有上下文的關聯性，最完整表示上下文關聯性的方法就是設置一個1000x1000或者更大的表格，把所有字彙間的上下文關聯性全部存起來，但我們不想要這麼做，我要求機器用更小的表格來儲存上下文的關聯性，此時機器被迫將一些詞彙使用同樣的表格位置，同樣的轉換。一旦限縮了上下文關聯性的儲存維度，"on the way"和"by the way"中的"on"和"by"就會被迫分為同一類，因此我們成功的建立了字詞間的相似性關係。&lt;/p&gt;
&lt;h3 id="word2vec_1"&gt;Word2Vec的架構&lt;/h3&gt;
&lt;p&gt;&lt;img alt="word2vec" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.008.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;實作上如上圖所示，我們輸入一個字詞，譬如"cat"，通常會將他轉成One-hot encoding表示，但要注意喔！文本的字彙量是非常龐大的，所以當我們使用One-hot encoding表示時，將會出現一個非常長但Sparse的向量，相同的輸出層也同樣是一個很長的One-hot encoding，它的維度會和輸入層一樣大，因為我們要分析的字彙在輸入和輸出是一樣多的。&lt;/p&gt;
&lt;p&gt;然後，和Autoencoder使用一樣的手法，中間的Hidden Layer放置低維度、少神經元的一層，但不同於Autoencoder，Word2Vec所有的轉換都是線性的，沒有非線性的Activation Function夾在其中，為什麼呢？因為我們的輸入是Sparse的而且只有0和1的差別，所以每一條通路就變成只有導通或不導通的差別，Activation Function有加等於沒加，使用線性就足夠了。&lt;/p&gt;
&lt;p&gt;這個中間的Hidden Layer被稱為Embedding Matrix，它做了一個線性的Dimension Reduction，將原本高維度的One-hot encoding降低成低維度，然後再透過一個線性模型轉換回去原本的維度。假設字彙的數量有N個，所以輸入矩陣X是一個1xN的矩陣，輸出的矩陣同樣也是1xN的矩陣，當我先做一個線性的Dimension Reduction，將維度降到d維，此時Embedding Matrix會是一個Nxd的矩陣V，然後再由線性模型轉換回去原本的維度，這個轉換矩陣W是一個Nxd矩陣，因此綜合上述，可用一個簡潔的表示式表示：&lt;span class="math"&gt;\(Y=W^T VX\)&lt;/span&gt;，我們的目標就是找出這個W和V矩陣的每個元素。&lt;/p&gt;
&lt;p&gt;你會想說線性模型很簡單啊！就是仿照Autoencoder的作法，然後把Activation Function拿掉不就了事了，並且因為輸出是One-hot Encoding所以最後套用Softmax，那不就輕鬆完成！但是真正的大魔王就出在字彙量，字彙量一旦很大，事情就變得不可收拾了，而且字彙量是一定小不得的，那怎麼辦？&lt;/p&gt;
&lt;p&gt;在Dimension Reduction我們可以採取一個快速的方法，因為除了我要表示的字的位置是1以外其他都是0，所以其他都可以不看，我們就直接看是在第幾個位置上是1，然後再到Embedding Matrix上找到相應的行直接取出就是答案了，這樣查詢的動作，在Tensorflow中可以使用&lt;code&gt;tf.nn.embedding_lookup&lt;/code&gt;來辦到。&lt;/p&gt;
&lt;p&gt;再接下來最後的Cross-Entropy Loss計算也非常龐大，因為有幾個字彙就需要累加幾組數字，我們有一招偷吃步的方法叫做「Sampled Softmax」，作法是這樣的，我們不去計算全部詞彙的Cross-Entropy，而是選擇幾組詞彙來評估Cross-Entropy，在選擇上我們會隨機挑選一些Labels和預測結果差異度很大的詞彙(稱為Negative Examples)來算Cross-Entropy，我們在Tensorflow可以使用&lt;code&gt;tf.nn.sampled_softmax_loss&lt;/code&gt;來辦到「Sampled Softmax」。&lt;/p&gt;
&lt;p&gt;我們先不管輸入和輸出究竟怎麼取得，如果我們成功的建立了輸入和輸出的上下文關係，此時中間的Embedding空間正是精華的所在，經過剛剛推論，我們預期在這個空間當中，相似的詞彙會彼此靠近，我們評估兩個向量的相似性可以使用Cosine來評估，當兩向量的夾角越小代表它們越是相似，待會的實作當中我們將會利用Cosine來建立Similarity的大小，藉此來找到前幾個和它很靠近的詞彙。&lt;/p&gt;
&lt;p&gt;另外，經研究指出這個Embedding空間的效果不只是可以算出詞彙間的相似性，還可以顯示詞彙間的比較關係，例如：北京之於中國，等同於台北之於台灣，這樣的比較關係也顯示在這個Embedding空間裡頭，所以在這空間裡會有以下的向量關係式：&lt;span class="math"&gt;\(V_{北京} - V_{中國}+V_{台灣}=V_{台北}\)&lt;/span&gt;，是不是很神奇啊！&lt;/p&gt;
&lt;h3 id="word2vecskip-gramcbow"&gt;Word2Vec的兩種常用方法：Skip-Gram和CBOW&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Skip-Gram和CBOW" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.009.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;剛剛一直在講的是中間的結構應該怎麼建立，現在來看看我們可以輸入和輸出哪些詞彙來建立起上下文的關係，有兩種常用的類別：Skip-Gram和CBOW。&lt;/p&gt;
&lt;p&gt;Skip-Gram如上圖所示，當我輸入一個&lt;span class="math"&gt;\(word(t)\)&lt;/span&gt;時，我希望它能輸出它的前文和後文，這是相當直覺的建立上下文的方法，所以如果我希望用前一個字和後一個字來訓練我的Word2Vec，我就會有兩組數據：&lt;span class="math"&gt;\((w(t),w(t-1))\)&lt;/span&gt;和&lt;span class="math"&gt;\((w(t),w(t+1))\)&lt;/span&gt;，相當好理解。&lt;/p&gt;
&lt;p&gt;而CBOW(Continuous Bag of Words)使用另外一種方法來建立上下文關係，它將一排字挖掉中間一個字，然後希望由上下文的關係有辦法猜出中間那個字，就像是填空題，此時輸入層就變成會有多於1個字，那該怎麼處理，答案是轉換到Embedding空間後再相加平均，因為是線性轉換，所以直接線性累加就可以了。&lt;/p&gt;
&lt;h3 id="_1"&gt;準備文本語料庫&lt;/h3&gt;
&lt;p&gt;先帶入一些待會會用到的函式庫，並且決定我們要取用多少&lt;code&gt;VOCABULARY_SIZE&lt;/code&gt;個詞彙量來做訓練。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;zipfile&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;urllib.request&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;urlretrieve&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;

&lt;span class="n"&gt;VOCABULARY_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;接下來下載Dataset，並做一些前處理。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;
&lt;span class="normal"&gt;53&lt;/span&gt;
&lt;span class="normal"&gt;54&lt;/span&gt;
&lt;span class="normal"&gt;55&lt;/span&gt;
&lt;span class="normal"&gt;56&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maybe_download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_bytes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Download a file if not present, and make sure it&amp;#39;s the right size.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urlretrieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;statinfo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;statinfo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;st_size&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;expected_bytes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Found and verified &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;statinfo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;st_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;Failed to verify &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;. Can you get to it with a browser?&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Extract the first file enclosed in a zip file as a list of words&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;zipfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ZipFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;namelist&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocabulary_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VOCABULARY_SIZE&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;collections&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocabulary_size&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;unk_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="c1"&gt;# dictionary[&amp;#39;UNK&amp;#39;]&lt;/span&gt;
            &lt;span class="n"&gt;unk_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unk_count&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unk_count&lt;/span&gt;
    &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;


&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Downloading text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;maybe_download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;http://mattmahoney.net/dc/text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;./text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;31344016&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;=====&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Data size &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;First 10 words: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;=====&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;build_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                &lt;span class="n"&gt;vocabulary_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VOCABULARY_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;  &lt;span class="c1"&gt;# Hint to reduce memory.&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Most common words (+UNK)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Sample data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Downloading text8.zip&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Found and verified ./text8.zip&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=====&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Data size 17005207&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;First 10 words&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=====&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Most common words (+UNK) [[&amp;#39;UNK&amp;#39;, 189230], (&amp;#39;the&amp;#39;, 1061396), (&amp;#39;of&amp;#39;, 593677), (&amp;#39;and&amp;#39;, 416629), (&amp;#39;one&amp;#39;, 411764)]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;我們取用&lt;code&gt;VOCABULARY_SIZE = 100000&lt;/code&gt;，也是說我們將文本中的詞彙按出現次數的多寡來排列，取前面&lt;code&gt;VOCABULARY_SIZE&lt;/code&gt;個保留，其餘詞彙皆歸類到「UNK Token」裡頭，UNK代表UNKnown的縮寫。&lt;/p&gt;
&lt;p&gt;我們文本的字詞數量總共有17005207個字，開頭前十個字的句子是'anarchism originated as a term of abuse first used against'。所有的這17005207個字會依照&lt;code&gt;dictionary&lt;/code&gt;給予每個字Index，而文本會被表示為一個由整數所構成的List，這會放在&lt;code&gt;data&lt;/code&gt;裡頭，而這個Index也就直接當作One-hot Encoding中代表這個詞彙的維度位置。當我想要把Index轉換回去我們看得懂的字的時候，就需要&lt;code&gt;reverse_dictionary&lt;/code&gt;的幫忙，有了這些，我們的語料庫就已經建立完成了。&lt;/p&gt;
&lt;h3 id="skip-gram"&gt;實作Skip-Gram&lt;/h3&gt;
&lt;p&gt;有了語料庫，我們就可以產生出我想要的輸入和輸出，在Skip-Gram方法，如果我的輸入是&lt;code&gt;target word&lt;/code&gt;，我會先從&lt;code&gt;target word&lt;/code&gt;向前、向後看出去&lt;code&gt;skip_window&lt;/code&gt;的大小，所以可以選擇當作輸出的字有&lt;code&gt;skip_window*2&lt;/code&gt;個，接下來我從這&lt;code&gt;skip_window*2&lt;/code&gt;個中選擇&lt;code&gt;num_skips&lt;/code&gt;個當作輸出，所以一個&lt;code&gt;target word&lt;/code&gt;會產生&lt;code&gt;num_skips&lt;/code&gt;筆數據，如果我一個batch需要&lt;code&gt;batch_size&lt;/code&gt;筆數據，我就必須有&lt;code&gt;batch_size//num_skips&lt;/code&gt;個&lt;code&gt;target word&lt;/code&gt;，依照這樣的規則下面建立一個Generator來掃描文本，並輸出要訓練使用的Batch Data。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;skip_gram_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;num_skips&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;num_skips&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt;

    &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;span&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# [ skip_window target skip_window ]&lt;/span&gt;
    &lt;span class="n"&gt;buffer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collections&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;span&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# initialization&lt;/span&gt;
    &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;span&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# generate&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt;  &lt;span class="c1"&gt;# target label at the center of the buffer&lt;/span&gt;
        &lt;span class="n"&gt;targets_to_avoid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;targets_to_avoid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;span&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;targets_to_avoid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

        &lt;span class="c1"&gt;# Recycle&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

        &lt;span class="c1"&gt;# scan data&lt;/span&gt;
        &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Enough num to output&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;


&lt;span class="c1"&gt;# demonstrate generator&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;di&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;di&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)]:&lt;/span&gt;
    &lt;span class="n"&gt;batch_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;skip_gram_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_generator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;with num_skips = &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; and skip_window = &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;:&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;    batch:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bi&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;bi&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;    labels:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;li&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;li&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;
&lt;span class="normal"&gt;9&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;with num_skips = 2 and skip_window = 1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;with num_skips = 4 and skip_window = 2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;  1&lt;/span&gt;
&lt;span class="normal"&gt;  2&lt;/span&gt;
&lt;span class="normal"&gt;  3&lt;/span&gt;
&lt;span class="normal"&gt;  4&lt;/span&gt;
&lt;span class="normal"&gt;  5&lt;/span&gt;
&lt;span class="normal"&gt;  6&lt;/span&gt;
&lt;span class="normal"&gt;  7&lt;/span&gt;
&lt;span class="normal"&gt;  8&lt;/span&gt;
&lt;span class="normal"&gt;  9&lt;/span&gt;
&lt;span class="normal"&gt; 10&lt;/span&gt;
&lt;span class="normal"&gt; 11&lt;/span&gt;
&lt;span class="normal"&gt; 12&lt;/span&gt;
&lt;span class="normal"&gt; 13&lt;/span&gt;
&lt;span class="normal"&gt; 14&lt;/span&gt;
&lt;span class="normal"&gt; 15&lt;/span&gt;
&lt;span class="normal"&gt; 16&lt;/span&gt;
&lt;span class="normal"&gt; 17&lt;/span&gt;
&lt;span class="normal"&gt; 18&lt;/span&gt;
&lt;span class="normal"&gt; 19&lt;/span&gt;
&lt;span class="normal"&gt; 20&lt;/span&gt;
&lt;span class="normal"&gt; 21&lt;/span&gt;
&lt;span class="normal"&gt; 22&lt;/span&gt;
&lt;span class="normal"&gt; 23&lt;/span&gt;
&lt;span class="normal"&gt; 24&lt;/span&gt;
&lt;span class="normal"&gt; 25&lt;/span&gt;
&lt;span class="normal"&gt; 26&lt;/span&gt;
&lt;span class="normal"&gt; 27&lt;/span&gt;
&lt;span class="normal"&gt; 28&lt;/span&gt;
&lt;span class="normal"&gt; 29&lt;/span&gt;
&lt;span class="normal"&gt; 30&lt;/span&gt;
&lt;span class="normal"&gt; 31&lt;/span&gt;
&lt;span class="normal"&gt; 32&lt;/span&gt;
&lt;span class="normal"&gt; 33&lt;/span&gt;
&lt;span class="normal"&gt; 34&lt;/span&gt;
&lt;span class="normal"&gt; 35&lt;/span&gt;
&lt;span class="normal"&gt; 36&lt;/span&gt;
&lt;span class="normal"&gt; 37&lt;/span&gt;
&lt;span class="normal"&gt; 38&lt;/span&gt;
&lt;span class="normal"&gt; 39&lt;/span&gt;
&lt;span class="normal"&gt; 40&lt;/span&gt;
&lt;span class="normal"&gt; 41&lt;/span&gt;
&lt;span class="normal"&gt; 42&lt;/span&gt;
&lt;span class="normal"&gt; 43&lt;/span&gt;
&lt;span class="normal"&gt; 44&lt;/span&gt;
&lt;span class="normal"&gt; 45&lt;/span&gt;
&lt;span class="normal"&gt; 46&lt;/span&gt;
&lt;span class="normal"&gt; 47&lt;/span&gt;
&lt;span class="normal"&gt; 48&lt;/span&gt;
&lt;span class="normal"&gt; 49&lt;/span&gt;
&lt;span class="normal"&gt; 50&lt;/span&gt;
&lt;span class="normal"&gt; 51&lt;/span&gt;
&lt;span class="normal"&gt; 52&lt;/span&gt;
&lt;span class="normal"&gt; 53&lt;/span&gt;
&lt;span class="normal"&gt; 54&lt;/span&gt;
&lt;span class="normal"&gt; 55&lt;/span&gt;
&lt;span class="normal"&gt; 56&lt;/span&gt;
&lt;span class="normal"&gt; 57&lt;/span&gt;
&lt;span class="normal"&gt; 58&lt;/span&gt;
&lt;span class="normal"&gt; 59&lt;/span&gt;
&lt;span class="normal"&gt; 60&lt;/span&gt;
&lt;span class="normal"&gt; 61&lt;/span&gt;
&lt;span class="normal"&gt; 62&lt;/span&gt;
&lt;span class="normal"&gt; 63&lt;/span&gt;
&lt;span class="normal"&gt; 64&lt;/span&gt;
&lt;span class="normal"&gt; 65&lt;/span&gt;
&lt;span class="normal"&gt; 66&lt;/span&gt;
&lt;span class="normal"&gt; 67&lt;/span&gt;
&lt;span class="normal"&gt; 68&lt;/span&gt;
&lt;span class="normal"&gt; 69&lt;/span&gt;
&lt;span class="normal"&gt; 70&lt;/span&gt;
&lt;span class="normal"&gt; 71&lt;/span&gt;
&lt;span class="normal"&gt; 72&lt;/span&gt;
&lt;span class="normal"&gt; 73&lt;/span&gt;
&lt;span class="normal"&gt; 74&lt;/span&gt;
&lt;span class="normal"&gt; 75&lt;/span&gt;
&lt;span class="normal"&gt; 76&lt;/span&gt;
&lt;span class="normal"&gt; 77&lt;/span&gt;
&lt;span class="normal"&gt; 78&lt;/span&gt;
&lt;span class="normal"&gt; 79&lt;/span&gt;
&lt;span class="normal"&gt; 80&lt;/span&gt;
&lt;span class="normal"&gt; 81&lt;/span&gt;
&lt;span class="normal"&gt; 82&lt;/span&gt;
&lt;span class="normal"&gt; 83&lt;/span&gt;
&lt;span class="normal"&gt; 84&lt;/span&gt;
&lt;span class="normal"&gt; 85&lt;/span&gt;
&lt;span class="normal"&gt; 86&lt;/span&gt;
&lt;span class="normal"&gt; 87&lt;/span&gt;
&lt;span class="normal"&gt; 88&lt;/span&gt;
&lt;span class="normal"&gt; 89&lt;/span&gt;
&lt;span class="normal"&gt; 90&lt;/span&gt;
&lt;span class="normal"&gt; 91&lt;/span&gt;
&lt;span class="normal"&gt; 92&lt;/span&gt;
&lt;span class="normal"&gt; 93&lt;/span&gt;
&lt;span class="normal"&gt; 94&lt;/span&gt;
&lt;span class="normal"&gt; 95&lt;/span&gt;
&lt;span class="normal"&gt; 96&lt;/span&gt;
&lt;span class="normal"&gt; 97&lt;/span&gt;
&lt;span class="normal"&gt; 98&lt;/span&gt;
&lt;span class="normal"&gt; 99&lt;/span&gt;
&lt;span class="normal"&gt;100&lt;/span&gt;
&lt;span class="normal"&gt;101&lt;/span&gt;
&lt;span class="normal"&gt;102&lt;/span&gt;
&lt;span class="normal"&gt;103&lt;/span&gt;
&lt;span class="normal"&gt;104&lt;/span&gt;
&lt;span class="normal"&gt;105&lt;/span&gt;
&lt;span class="normal"&gt;106&lt;/span&gt;
&lt;span class="normal"&gt;107&lt;/span&gt;
&lt;span class="normal"&gt;108&lt;/span&gt;
&lt;span class="normal"&gt;109&lt;/span&gt;
&lt;span class="normal"&gt;110&lt;/span&gt;
&lt;span class="normal"&gt;111&lt;/span&gt;
&lt;span class="normal"&gt;112&lt;/span&gt;
&lt;span class="normal"&gt;113&lt;/span&gt;
&lt;span class="normal"&gt;114&lt;/span&gt;
&lt;span class="normal"&gt;115&lt;/span&gt;
&lt;span class="normal"&gt;116&lt;/span&gt;
&lt;span class="normal"&gt;117&lt;/span&gt;
&lt;span class="normal"&gt;118&lt;/span&gt;
&lt;span class="normal"&gt;119&lt;/span&gt;
&lt;span class="normal"&gt;120&lt;/span&gt;
&lt;span class="normal"&gt;121&lt;/span&gt;
&lt;span class="normal"&gt;122&lt;/span&gt;
&lt;span class="normal"&gt;123&lt;/span&gt;
&lt;span class="normal"&gt;124&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SkipGram&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_embedding&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new grap&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# normalize embeddings&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                          &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdagradOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# similarity&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                               &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_similarity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_uniform&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                                  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                             &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                                 &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="c1"&gt;# Look up embeddings for inputs.&lt;/span&gt;
        &lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Compute the softmax loss, using a sample of the negative labels each time.&lt;/span&gt;
        &lt;span class="n"&gt;num_softmax_sampled&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;

        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                 &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sampled_softmax_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                            &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                            &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;num_sampled&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_softmax_sampled&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;nearest_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;similarity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_similarity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
        &lt;span class="n"&gt;X_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;valid_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;nearests&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;valid_word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# select highest similarity word&lt;/span&gt;
            &lt;span class="n"&gt;nearest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;similarity&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;nearest&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;以上就是我建立的Model，這裡我採取&lt;code&gt;online_fit&lt;/code&gt;的方法，不同於之前的&lt;code&gt;fit&lt;/code&gt;，&lt;code&gt;online_fit&lt;/code&gt;可以不用事先將所有Data一次餵進去，而是可以陸續的餵入Data，所以我會從上面的Generator陸續產生Batch Data並餵入Model裡來做訓練。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# build skip-gram batch generator&lt;/span&gt;
&lt;span class="n"&gt;batch_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;skip_gram_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# build skip-gram model&lt;/span&gt;
&lt;span class="n"&gt;model_SkipGram&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SkipGram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VOCABULARY_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# initial model&lt;/span&gt;
&lt;span class="n"&gt;model_SkipGram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# online training&lt;/span&gt;
&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;
&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_generator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_SkipGram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;s loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch 1/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    4.2150&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 2/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.7561&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 3/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.6276&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 4/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.5098&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 5/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.5123&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 6/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.5000&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 7/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.5155&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 8/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.3983&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 9/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.4418&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.4118&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 11/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3993&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 12/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.4074&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 13/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.3243&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 14/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.3448&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 15/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3607&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 16/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3408&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 17/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3705&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 18/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3894&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 19/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3536&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 20/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3123&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 21/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3046&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 22/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3117&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 23/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3023&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 24/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2623&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 25/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3197&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 26/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2833&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 27/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2456&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 28/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2272&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 29/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2663&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 30/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2274&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 31/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2335&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 32/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.3003&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 33/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.2507&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 34/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2486&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 35/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2382&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 36/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2687&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 37/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2145&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 38/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2437&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 39/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2171&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 40/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.0492&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 41/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.9380&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 42/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1556&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 43/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1804&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 44/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.2800&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 45/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1366&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 46/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2190&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 47/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2381&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 48/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2419&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 49/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.0127&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 50/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1232&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;我們來看看效果如何，我們使用Embedding Vectors彼此間的Cosine來定義出字詞間的相關性，並且列出8個最為靠近的字詞。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;valid_words_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;210&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;239&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;392&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;396&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nearests&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_SkipGram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nearest_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_words_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Nearest to &lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;two&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;three&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;four&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;five&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;eight&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;six&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;one&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seven&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;zero&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;that&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;which&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;however&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;thus&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;what&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sepulchres&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dancewriting&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tatars&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;resent&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;his&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;her&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;their&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;your&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;my&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;its&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;our&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;othniel&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;personal&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;were&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;are&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;was&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;have&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;remain&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;junkanoo&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;those&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;include&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;had&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;all&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;both&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;various&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;many&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;several&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;every&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;these&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;some&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;obtaining&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;area&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;areas&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;region&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;territory&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;location&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;xylophone&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;stadium&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;city&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;island&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;east&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;west&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;south&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;southeast&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;north&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;eastern&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;southwest&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;central&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mainland&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;himself&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;him&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;themselves&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;herself&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;them&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;itself&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;wignacourt&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;majored&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mankiewicz&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;white&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;green&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;yellow&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dark&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;papyri&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;kemal&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;結果相當驚人，與'two'靠近的真的都是數字類型的文字，與'that'靠近的都是文法功能性的詞彙，與'his'靠近的都是所有格代名詞，與'were'靠近的是be動詞，與'all'最靠近的是'both'，與'east'靠近的都是一些代表方向的詞彙，與'white'靠近的都是一些顏色的詞彙，真的是太神奇了！&lt;/p&gt;
&lt;p&gt;接下來直接來觀察Embedding空間，以下使用t-SNE來圖像化Embedding空間。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pylab&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.manifold&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;More labels than embeddings&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# in inches&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
        &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;xytext&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;textcoords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;offset points&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="n"&gt;ha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;right&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;va&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bottom&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;visualization_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;
&lt;span class="c1"&gt;# transform embeddings to 2D by t-SNE&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_SkipGram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;visualization_words&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;tsne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;perplexity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pca&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;exact&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;two_d_embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tsne&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# list labels&lt;/span&gt;
&lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;model_SkipGram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;visualization_words&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="c1"&gt;# plot&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;two_d_embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/05_output_13_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;如此一來你將可以簡單的看出，哪些詞彙彼此相似而靠近。&lt;/p&gt;
&lt;h3 id="cbow-continuous-bag-of-words"&gt;實作CBOW (Continuous Bag of Words)&lt;/h3&gt;
&lt;p&gt;接著看CBOW的方法，如果我預期輸出的字是&lt;code&gt;target word&lt;/code&gt;，從&lt;code&gt;target word&lt;/code&gt;向前向後看出去&lt;code&gt;context_window&lt;/code&gt;的大小，看到的字都當作我的輸入，所以我輸入的字總共需要&lt;code&gt;context_window*2&lt;/code&gt;個，一個&lt;code&gt;target word&lt;/code&gt;只會產生一筆數據，如果我一個batch需要&lt;code&gt;batch_size&lt;/code&gt;筆數據，我就必須有&lt;code&gt;batch_size&lt;/code&gt;個&lt;code&gt;target word&lt;/code&gt;，依照這樣的規則下面建立一個Generator來掃描文本，並輸出要訓練使用的Batch Data。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;
&lt;span class="normal"&gt;53&lt;/span&gt;
&lt;span class="normal"&gt;54&lt;/span&gt;
&lt;span class="normal"&gt;55&lt;/span&gt;
&lt;span class="normal"&gt;56&lt;/span&gt;
&lt;span class="normal"&gt;57&lt;/span&gt;
&lt;span class="normal"&gt;58&lt;/span&gt;
&lt;span class="normal"&gt;59&lt;/span&gt;
&lt;span class="normal"&gt;60&lt;/span&gt;
&lt;span class="normal"&gt;61&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cbow_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;span&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;context_window&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# [ context_window target context_window ]&lt;/span&gt;
    &lt;span class="n"&gt;num_bow&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;span&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_bow&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;buffer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collections&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;span&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# initialization&lt;/span&gt;
    &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;span&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# generate&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;context_window&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;bow&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;bow&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bow&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;
        &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

        &lt;span class="c1"&gt;# Recycle&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

        &lt;span class="c1"&gt;# scan data&lt;/span&gt;
        &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Enough num to output&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# demonstrate generator&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;di&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;di&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;context_window&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;batch_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cbow_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_generator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;with context_window = &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;:&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;batch:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;show_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
        &lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
            &lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
        &lt;span class="n"&gt;show_batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;show_batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;labels:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;li&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;li&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;with context_window = 1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;with context_window = 2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;early&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;early&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;working&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;  1&lt;/span&gt;
&lt;span class="normal"&gt;  2&lt;/span&gt;
&lt;span class="normal"&gt;  3&lt;/span&gt;
&lt;span class="normal"&gt;  4&lt;/span&gt;
&lt;span class="normal"&gt;  5&lt;/span&gt;
&lt;span class="normal"&gt;  6&lt;/span&gt;
&lt;span class="normal"&gt;  7&lt;/span&gt;
&lt;span class="normal"&gt;  8&lt;/span&gt;
&lt;span class="normal"&gt;  9&lt;/span&gt;
&lt;span class="normal"&gt; 10&lt;/span&gt;
&lt;span class="normal"&gt; 11&lt;/span&gt;
&lt;span class="normal"&gt; 12&lt;/span&gt;
&lt;span class="normal"&gt; 13&lt;/span&gt;
&lt;span class="normal"&gt; 14&lt;/span&gt;
&lt;span class="normal"&gt; 15&lt;/span&gt;
&lt;span class="normal"&gt; 16&lt;/span&gt;
&lt;span class="normal"&gt; 17&lt;/span&gt;
&lt;span class="normal"&gt; 18&lt;/span&gt;
&lt;span class="normal"&gt; 19&lt;/span&gt;
&lt;span class="normal"&gt; 20&lt;/span&gt;
&lt;span class="normal"&gt; 21&lt;/span&gt;
&lt;span class="normal"&gt; 22&lt;/span&gt;
&lt;span class="normal"&gt; 23&lt;/span&gt;
&lt;span class="normal"&gt; 24&lt;/span&gt;
&lt;span class="normal"&gt; 25&lt;/span&gt;
&lt;span class="normal"&gt; 26&lt;/span&gt;
&lt;span class="normal"&gt; 27&lt;/span&gt;
&lt;span class="normal"&gt; 28&lt;/span&gt;
&lt;span class="normal"&gt; 29&lt;/span&gt;
&lt;span class="normal"&gt; 30&lt;/span&gt;
&lt;span class="normal"&gt; 31&lt;/span&gt;
&lt;span class="normal"&gt; 32&lt;/span&gt;
&lt;span class="normal"&gt; 33&lt;/span&gt;
&lt;span class="normal"&gt; 34&lt;/span&gt;
&lt;span class="normal"&gt; 35&lt;/span&gt;
&lt;span class="normal"&gt; 36&lt;/span&gt;
&lt;span class="normal"&gt; 37&lt;/span&gt;
&lt;span class="normal"&gt; 38&lt;/span&gt;
&lt;span class="normal"&gt; 39&lt;/span&gt;
&lt;span class="normal"&gt; 40&lt;/span&gt;
&lt;span class="normal"&gt; 41&lt;/span&gt;
&lt;span class="normal"&gt; 42&lt;/span&gt;
&lt;span class="normal"&gt; 43&lt;/span&gt;
&lt;span class="normal"&gt; 44&lt;/span&gt;
&lt;span class="normal"&gt; 45&lt;/span&gt;
&lt;span class="normal"&gt; 46&lt;/span&gt;
&lt;span class="normal"&gt; 47&lt;/span&gt;
&lt;span class="normal"&gt; 48&lt;/span&gt;
&lt;span class="normal"&gt; 49&lt;/span&gt;
&lt;span class="normal"&gt; 50&lt;/span&gt;
&lt;span class="normal"&gt; 51&lt;/span&gt;
&lt;span class="normal"&gt; 52&lt;/span&gt;
&lt;span class="normal"&gt; 53&lt;/span&gt;
&lt;span class="normal"&gt; 54&lt;/span&gt;
&lt;span class="normal"&gt; 55&lt;/span&gt;
&lt;span class="normal"&gt; 56&lt;/span&gt;
&lt;span class="normal"&gt; 57&lt;/span&gt;
&lt;span class="normal"&gt; 58&lt;/span&gt;
&lt;span class="normal"&gt; 59&lt;/span&gt;
&lt;span class="normal"&gt; 60&lt;/span&gt;
&lt;span class="normal"&gt; 61&lt;/span&gt;
&lt;span class="normal"&gt; 62&lt;/span&gt;
&lt;span class="normal"&gt; 63&lt;/span&gt;
&lt;span class="normal"&gt; 64&lt;/span&gt;
&lt;span class="normal"&gt; 65&lt;/span&gt;
&lt;span class="normal"&gt; 66&lt;/span&gt;
&lt;span class="normal"&gt; 67&lt;/span&gt;
&lt;span class="normal"&gt; 68&lt;/span&gt;
&lt;span class="normal"&gt; 69&lt;/span&gt;
&lt;span class="normal"&gt; 70&lt;/span&gt;
&lt;span class="normal"&gt; 71&lt;/span&gt;
&lt;span class="normal"&gt; 72&lt;/span&gt;
&lt;span class="normal"&gt; 73&lt;/span&gt;
&lt;span class="normal"&gt; 74&lt;/span&gt;
&lt;span class="normal"&gt; 75&lt;/span&gt;
&lt;span class="normal"&gt; 76&lt;/span&gt;
&lt;span class="normal"&gt; 77&lt;/span&gt;
&lt;span class="normal"&gt; 78&lt;/span&gt;
&lt;span class="normal"&gt; 79&lt;/span&gt;
&lt;span class="normal"&gt; 80&lt;/span&gt;
&lt;span class="normal"&gt; 81&lt;/span&gt;
&lt;span class="normal"&gt; 82&lt;/span&gt;
&lt;span class="normal"&gt; 83&lt;/span&gt;
&lt;span class="normal"&gt; 84&lt;/span&gt;
&lt;span class="normal"&gt; 85&lt;/span&gt;
&lt;span class="normal"&gt; 86&lt;/span&gt;
&lt;span class="normal"&gt; 87&lt;/span&gt;
&lt;span class="normal"&gt; 88&lt;/span&gt;
&lt;span class="normal"&gt; 89&lt;/span&gt;
&lt;span class="normal"&gt; 90&lt;/span&gt;
&lt;span class="normal"&gt; 91&lt;/span&gt;
&lt;span class="normal"&gt; 92&lt;/span&gt;
&lt;span class="normal"&gt; 93&lt;/span&gt;
&lt;span class="normal"&gt; 94&lt;/span&gt;
&lt;span class="normal"&gt; 95&lt;/span&gt;
&lt;span class="normal"&gt; 96&lt;/span&gt;
&lt;span class="normal"&gt; 97&lt;/span&gt;
&lt;span class="normal"&gt; 98&lt;/span&gt;
&lt;span class="normal"&gt; 99&lt;/span&gt;
&lt;span class="normal"&gt;100&lt;/span&gt;
&lt;span class="normal"&gt;101&lt;/span&gt;
&lt;span class="normal"&gt;102&lt;/span&gt;
&lt;span class="normal"&gt;103&lt;/span&gt;
&lt;span class="normal"&gt;104&lt;/span&gt;
&lt;span class="normal"&gt;105&lt;/span&gt;
&lt;span class="normal"&gt;106&lt;/span&gt;
&lt;span class="normal"&gt;107&lt;/span&gt;
&lt;span class="normal"&gt;108&lt;/span&gt;
&lt;span class="normal"&gt;109&lt;/span&gt;
&lt;span class="normal"&gt;110&lt;/span&gt;
&lt;span class="normal"&gt;111&lt;/span&gt;
&lt;span class="normal"&gt;112&lt;/span&gt;
&lt;span class="normal"&gt;113&lt;/span&gt;
&lt;span class="normal"&gt;114&lt;/span&gt;
&lt;span class="normal"&gt;115&lt;/span&gt;
&lt;span class="normal"&gt;116&lt;/span&gt;
&lt;span class="normal"&gt;117&lt;/span&gt;
&lt;span class="normal"&gt;118&lt;/span&gt;
&lt;span class="normal"&gt;119&lt;/span&gt;
&lt;span class="normal"&gt;120&lt;/span&gt;
&lt;span class="normal"&gt;121&lt;/span&gt;
&lt;span class="normal"&gt;122&lt;/span&gt;
&lt;span class="normal"&gt;123&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_embedding&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;context_window&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new grap&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their predictions and loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# normalize embeddings&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                          &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdagradOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

            &lt;span class="c1"&gt;# similarity&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                               &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_similarity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_uniform&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                                  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                                &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="c1"&gt;# Look up embeddings for inputs.&lt;/span&gt;
        &lt;span class="n"&gt;embed_bow&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embed_bow&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Compute the softmax loss, using a sample of the negative labels each time.&lt;/span&gt;
        &lt;span class="n"&gt;num_softmax_sampled&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;

        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                 &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sampled_softmax_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                            &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                            &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;num_sampled&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_softmax_sampled&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;nearest_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;similarity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_similarity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
        &lt;span class="n"&gt;X_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;valid_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;nearests&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;valid_word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# select highest similarity word&lt;/span&gt;
            &lt;span class="n"&gt;nearest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;similarity&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;nearest&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;context_window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="c1"&gt;# build CBOW batch generator&lt;/span&gt;
&lt;span class="n"&gt;batch_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cbow_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# build CBOW model&lt;/span&gt;
&lt;span class="n"&gt;model_CBOW&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VOCABULARY_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# initialize model&lt;/span&gt;
&lt;span class="n"&gt;model_CBOW&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# online training&lt;/span&gt;
&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;
&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_generator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_CBOW&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;s loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch 1/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.8700&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 2/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2961&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 3/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1988&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 4/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1201&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 5/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.0734&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 6/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.0239&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 7/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.9378&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 8/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.9549&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 9/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.9651&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.9028&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 11/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.8770&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 12/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.8298&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 13/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.8437&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 14/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7681&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 15/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7823&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 16/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7867&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 17/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7540&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 18/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7567&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 19/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7340&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 20/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.6212&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 21/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5187&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 22/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7150&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 23/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.6647&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 24/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7381&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 25/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5337&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 26/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.6587&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 27/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.6648&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 28/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5963&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 29/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5418&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 30/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.6041&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 31/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5535&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 32/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5928&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 33/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5535&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 34/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5233&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 35/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5658&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 36/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5966&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 37/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5422&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 38/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5673&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 39/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5142&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 40/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5175&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 41/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4909&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 42/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4872&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 43/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5513&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 44/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4917&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 45/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5198&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 46/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5007&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 47/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.2530&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 48/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4154&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 49/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4927&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 50/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4948&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;valid_words_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;210&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;239&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;392&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;396&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nearests&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_CBOW&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nearest_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_words_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Nearest to &lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;two&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;three&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;four&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;five&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;six&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seven&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;eight&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;nine&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;zero&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;that&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;which&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;what&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;furthermore&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;however&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;talmudic&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;endress&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tonight&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;how&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;his&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;her&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;their&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;my&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;your&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;its&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;our&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;the&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;photographs&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;were&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;are&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;have&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;include&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;contain&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;was&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;vigorous&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tend&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;substituting&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;all&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;various&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;both&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;many&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;every&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;shamed&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;everyone&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;those&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;wiccan&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;area&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;areas&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;region&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;regions&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;taipan&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;northeast&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;boundaries&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;hattin&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;surface&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;east&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;west&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;southeast&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;south&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;northwest&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;southwest&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;eastern&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;northeast&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;north&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;himself&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;him&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;themselves&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;herself&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;itself&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;them&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;donal&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;activex&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;carnaval&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;white&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;morel&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;green&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;bluish&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dead&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lessig&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pylab&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.manifold&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;More labels than embeddings&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# in inches&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
        &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;xytext&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;textcoords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;offset points&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="n"&gt;ha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;right&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;va&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bottom&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;visualization_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;
&lt;span class="c1"&gt;# transform embeddings to 2D by t-SNE&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_CBOW&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;visualization_words&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;tsne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;perplexity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pca&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;exact&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;two_d_embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tsne&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# list labels&lt;/span&gt;
&lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;model_CBOW&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;visualization_words&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="c1"&gt;# plot&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;two_d_embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/05_output_20_0.png" /&gt;&lt;/p&gt;
&lt;h3 id="reference"&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>實作Tensorflow (4)：Autoencoder</title><link href="https://ycc.idv.tw/tensorflow-tutorial_4.html" rel="alternate"></link><published>2017-11-18T12:00:00+08:00</published><updated>2017-11-18T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-18:/tensorflow-tutorial_4.html</id><summary type="html">&lt;p&gt;Autoencoder觀念解析 / Autoencoder程式碼 / 測試Autoencoder / 壓縮碼Code與視覺化 / 去雜訊(De-noise) Autoencoder&lt;/p&gt;</summary><content type="html">&lt;p&gt;Autoencoder是一個Neurel Network重要的工具，我個人認為它還漂亮的呈現Neurel Network的強大。&lt;/p&gt;
&lt;p&gt;本單元程式碼Autoencoder部分可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/04_1_Autoencoder_on_MNIST.py"&gt;Github&lt;/a&gt;下載，De-noise Autoencoder部分可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/04_2_DenoiseAutoencoder_on_MNIST.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;h3 id="autoencoder"&gt;Autoencoder觀念解析&lt;/h3&gt;
&lt;p&gt;在「機器學習技法」的系列文章，我也&lt;a href="/ml-course-techniques_6.html"&gt;曾經介紹過Autoencoder&lt;/a&gt;，可以搭配這篇服用。&lt;/p&gt;
&lt;p&gt;Autoencoder概念很簡單，就是做資訊的壓縮，概念是這樣的，當我在一層當中使用神經元愈多，可以儲存的資訊量也就愈多，相反的神經元越少，可以儲存的資訊量越少，如果我要使用Neurel Network作資料壓縮的話，我希望的是可以使用比原本更少的資訊量來儲存，如果原本是一張MNIST的圖，有28x28=784個Pixels，所以可以想知，如果我要作壓縮就要使得壓縮後的神經元可以比784個更少。&lt;/p&gt;
&lt;p&gt;但是什麼都不做我們就可以平白無故的做到壓縮？當然不行，我們還得從資料中找到一些規律，套用這些規律把多餘的東西去除，留下精髓，我們才可以把資料作壓縮，所以在實作上我們會建立一個神經元由大到小的Neurel Network，逐步的轉換，逐步的壓縮資訊。&lt;/p&gt;
&lt;p&gt;那麼壓縮的目的是為了什麼？當然是有辦法還原回去原本狀態，這樣的壓縮才是有意義的，例如：將文檔打包成RAR，檔案大小會變小，但如果實際要再使用這個檔案，那就必須先做解壓縮，然後還原回去原本的檔案，這裡的還原率必須是百分之一百的，Autoencoder一樣的有一個機制可以還原，在實作上我們會建立一個神經元由小到大的Neurel Network，逐步的還原回去原本的狀態。&lt;/p&gt;
&lt;p&gt;因此一個Autoencoder的圖像就出現了，我們需要有一組「Encoder」來逐步的壓縮，最後留下非常精簡的「Embedding Code」，而這組「Embedding Code」可以再經由「Decoder」還原回去原本的樣子，那我們怎麼讓他自己產生「Encoder」和「Decoder」呢？把原本的Input當作Output的目標答案去訓練Neurel Network就可以了，這就是Autoencoder巧妙的地方。&lt;/p&gt;
&lt;p&gt;不管是「Encoder」還是「Decoder」他們的權重是可以調整的，所以如果你將Encoder+Decoder的結構建立好並搭配Input當作Output的目標答案，它在Training的過程，Autoencoder會試著找出最好的權重來使得資訊可以盡量完整還原回去，所以Autoencoder可以自行找出了Encoder和Decoder。&lt;/p&gt;
&lt;p&gt;Encoder的效果等同於做Dimension Reduction，Encoder轉換原本數據到一個新的空間，這個空間可以比原本Features描述的空間更能精簡的描述這群數據，而中間這層Layer的數值Embedding Code就是新空間裡頭的座標，有些時候我們會用這個新空間來判斷每筆Data之間彼此的接近程度。&lt;/p&gt;
&lt;p&gt;&lt;img alt="autoencoder" src="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/img/TensorflowTutorial.007.jpeg?raw=true" /&gt;&lt;/p&gt;
&lt;h3 id="autoencoder_1"&gt;Autoencoder程式碼&lt;/h3&gt;
&lt;p&gt;實現Autoencoder和之前DNN並沒有太大的差異，只有兩點要特別提醒一下。&lt;/p&gt;
&lt;p&gt;第一點，以下我會特別把&lt;code&gt;encoder&lt;/code&gt;額外的在&lt;code&gt;structure&lt;/code&gt;裡頭輸出出來，並且增加新的函數&lt;code&gt;encode&lt;/code&gt;，讓使用者可以使用Train好的Encoder來做Encode。&lt;/p&gt;
&lt;p&gt;第二點，以下的Regularizer不是採用單純的L2 Regularizer，我將會使用Weight-Elimination L2 Regularizer，這個Regularizer的好處是會使得權重接近Sparse，也就是說權重會留下比較多的0，這有一個好處，就是每個神經元彼此之間的依賴減少了，因為內積(評估相依性)時有0的那個維度將不會有所貢獻。&lt;/p&gt;
&lt;p&gt;Weight-Elimination L2 Regularizer有這樣的效果原因是這樣的，L2 Regularizer在抑制W的方法是，如果&lt;span class="math"&gt;\(W\)&lt;/span&gt;的分量大的話就抑制多一點，如果分量小就抑制少一點（因為&lt;span class="math"&gt;\(W^2\)&lt;/span&gt;微分為一次），所以最後會留下很多不為0的微小分量，不夠Sparse，這樣的Regularization顯然不夠好，L1 Regularizer可以解決這個問題（因為在大部分位置微分為常數），但不幸的是它無法微分，沒辦法作Backpropagation，所以就有了L2 Regularizer的衍生版本，&lt;/p&gt;
&lt;p&gt;Weight-elimination L2 regularizer: 
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{jk} (W_{jk} (ℓ))^2 / [1+ (W_{jk} (ℓ) )^2]
$$&lt;/div&gt;
&lt;p&gt;這麼一來不管W大或小，它受到抑制的值大小接近的 (Weight-elimination L2 regularizer微分為 &lt;span class="math"&gt;\(-1\)&lt;/span&gt;次方)，因此就可以使得部分&lt;span class="math"&gt;\(W\)&lt;/span&gt;可以為&lt;span class="math"&gt;\(0\)&lt;/span&gt;，達成Sparse的目的。&lt;/p&gt;
&lt;p&gt;那為什麼我要特別在Autoencoder講究Sparse特性呢？原因是我們現在正在做的事是Dimension Reduction，做這件事就好像是替原本空間找出新的軸，而這個軸的數量比原本空間軸的數量來得小，達到Dimension Reduction的效果，所以我們會希望這個新的軸彼此間可以不要太多的依賴，什麼是不依賴呢？直角座標就是最不依賴的座標系，X軸和Y軸內積為0，這樣的軸展開的效率是最好的，所以我們希望在做Regularization的同時可以減少新軸的彼此間的依賴性。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Config the matplotlib backend as plotting inline in IPython&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;  1&lt;/span&gt;
&lt;span class="normal"&gt;  2&lt;/span&gt;
&lt;span class="normal"&gt;  3&lt;/span&gt;
&lt;span class="normal"&gt;  4&lt;/span&gt;
&lt;span class="normal"&gt;  5&lt;/span&gt;
&lt;span class="normal"&gt;  6&lt;/span&gt;
&lt;span class="normal"&gt;  7&lt;/span&gt;
&lt;span class="normal"&gt;  8&lt;/span&gt;
&lt;span class="normal"&gt;  9&lt;/span&gt;
&lt;span class="normal"&gt; 10&lt;/span&gt;
&lt;span class="normal"&gt; 11&lt;/span&gt;
&lt;span class="normal"&gt; 12&lt;/span&gt;
&lt;span class="normal"&gt; 13&lt;/span&gt;
&lt;span class="normal"&gt; 14&lt;/span&gt;
&lt;span class="normal"&gt; 15&lt;/span&gt;
&lt;span class="normal"&gt; 16&lt;/span&gt;
&lt;span class="normal"&gt; 17&lt;/span&gt;
&lt;span class="normal"&gt; 18&lt;/span&gt;
&lt;span class="normal"&gt; 19&lt;/span&gt;
&lt;span class="normal"&gt; 20&lt;/span&gt;
&lt;span class="normal"&gt; 21&lt;/span&gt;
&lt;span class="normal"&gt; 22&lt;/span&gt;
&lt;span class="normal"&gt; 23&lt;/span&gt;
&lt;span class="normal"&gt; 24&lt;/span&gt;
&lt;span class="normal"&gt; 25&lt;/span&gt;
&lt;span class="normal"&gt; 26&lt;/span&gt;
&lt;span class="normal"&gt; 27&lt;/span&gt;
&lt;span class="normal"&gt; 28&lt;/span&gt;
&lt;span class="normal"&gt; 29&lt;/span&gt;
&lt;span class="normal"&gt; 30&lt;/span&gt;
&lt;span class="normal"&gt; 31&lt;/span&gt;
&lt;span class="normal"&gt; 32&lt;/span&gt;
&lt;span class="normal"&gt; 33&lt;/span&gt;
&lt;span class="normal"&gt; 34&lt;/span&gt;
&lt;span class="normal"&gt; 35&lt;/span&gt;
&lt;span class="normal"&gt; 36&lt;/span&gt;
&lt;span class="normal"&gt; 37&lt;/span&gt;
&lt;span class="normal"&gt; 38&lt;/span&gt;
&lt;span class="normal"&gt; 39&lt;/span&gt;
&lt;span class="normal"&gt; 40&lt;/span&gt;
&lt;span class="normal"&gt; 41&lt;/span&gt;
&lt;span class="normal"&gt; 42&lt;/span&gt;
&lt;span class="normal"&gt; 43&lt;/span&gt;
&lt;span class="normal"&gt; 44&lt;/span&gt;
&lt;span class="normal"&gt; 45&lt;/span&gt;
&lt;span class="normal"&gt; 46&lt;/span&gt;
&lt;span class="normal"&gt; 47&lt;/span&gt;
&lt;span class="normal"&gt; 48&lt;/span&gt;
&lt;span class="normal"&gt; 49&lt;/span&gt;
&lt;span class="normal"&gt; 50&lt;/span&gt;
&lt;span class="normal"&gt; 51&lt;/span&gt;
&lt;span class="normal"&gt; 52&lt;/span&gt;
&lt;span class="normal"&gt; 53&lt;/span&gt;
&lt;span class="normal"&gt; 54&lt;/span&gt;
&lt;span class="normal"&gt; 55&lt;/span&gt;
&lt;span class="normal"&gt; 56&lt;/span&gt;
&lt;span class="normal"&gt; 57&lt;/span&gt;
&lt;span class="normal"&gt; 58&lt;/span&gt;
&lt;span class="normal"&gt; 59&lt;/span&gt;
&lt;span class="normal"&gt; 60&lt;/span&gt;
&lt;span class="normal"&gt; 61&lt;/span&gt;
&lt;span class="normal"&gt; 62&lt;/span&gt;
&lt;span class="normal"&gt; 63&lt;/span&gt;
&lt;span class="normal"&gt; 64&lt;/span&gt;
&lt;span class="normal"&gt; 65&lt;/span&gt;
&lt;span class="normal"&gt; 66&lt;/span&gt;
&lt;span class="normal"&gt; 67&lt;/span&gt;
&lt;span class="normal"&gt; 68&lt;/span&gt;
&lt;span class="normal"&gt; 69&lt;/span&gt;
&lt;span class="normal"&gt; 70&lt;/span&gt;
&lt;span class="normal"&gt; 71&lt;/span&gt;
&lt;span class="normal"&gt; 72&lt;/span&gt;
&lt;span class="normal"&gt; 73&lt;/span&gt;
&lt;span class="normal"&gt; 74&lt;/span&gt;
&lt;span class="normal"&gt; 75&lt;/span&gt;
&lt;span class="normal"&gt; 76&lt;/span&gt;
&lt;span class="normal"&gt; 77&lt;/span&gt;
&lt;span class="normal"&gt; 78&lt;/span&gt;
&lt;span class="normal"&gt; 79&lt;/span&gt;
&lt;span class="normal"&gt; 80&lt;/span&gt;
&lt;span class="normal"&gt; 81&lt;/span&gt;
&lt;span class="normal"&gt; 82&lt;/span&gt;
&lt;span class="normal"&gt; 83&lt;/span&gt;
&lt;span class="normal"&gt; 84&lt;/span&gt;
&lt;span class="normal"&gt; 85&lt;/span&gt;
&lt;span class="normal"&gt; 86&lt;/span&gt;
&lt;span class="normal"&gt; 87&lt;/span&gt;
&lt;span class="normal"&gt; 88&lt;/span&gt;
&lt;span class="normal"&gt; 89&lt;/span&gt;
&lt;span class="normal"&gt; 90&lt;/span&gt;
&lt;span class="normal"&gt; 91&lt;/span&gt;
&lt;span class="normal"&gt; 92&lt;/span&gt;
&lt;span class="normal"&gt; 93&lt;/span&gt;
&lt;span class="normal"&gt; 94&lt;/span&gt;
&lt;span class="normal"&gt; 95&lt;/span&gt;
&lt;span class="normal"&gt; 96&lt;/span&gt;
&lt;span class="normal"&gt; 97&lt;/span&gt;
&lt;span class="normal"&gt; 98&lt;/span&gt;
&lt;span class="normal"&gt; 99&lt;/span&gt;
&lt;span class="normal"&gt;100&lt;/span&gt;
&lt;span class="normal"&gt;101&lt;/span&gt;
&lt;span class="normal"&gt;102&lt;/span&gt;
&lt;span class="normal"&gt;103&lt;/span&gt;
&lt;span class="normal"&gt;104&lt;/span&gt;
&lt;span class="normal"&gt;105&lt;/span&gt;
&lt;span class="normal"&gt;106&lt;/span&gt;
&lt;span class="normal"&gt;107&lt;/span&gt;
&lt;span class="normal"&gt;108&lt;/span&gt;
&lt;span class="normal"&gt;109&lt;/span&gt;
&lt;span class="normal"&gt;110&lt;/span&gt;
&lt;span class="normal"&gt;111&lt;/span&gt;
&lt;span class="normal"&gt;112&lt;/span&gt;
&lt;span class="normal"&gt;113&lt;/span&gt;
&lt;span class="normal"&gt;114&lt;/span&gt;
&lt;span class="normal"&gt;115&lt;/span&gt;
&lt;span class="normal"&gt;116&lt;/span&gt;
&lt;span class="normal"&gt;117&lt;/span&gt;
&lt;span class="normal"&gt;118&lt;/span&gt;
&lt;span class="normal"&gt;119&lt;/span&gt;
&lt;span class="normal"&gt;120&lt;/span&gt;
&lt;span class="normal"&gt;121&lt;/span&gt;
&lt;span class="normal"&gt;122&lt;/span&gt;
&lt;span class="normal"&gt;123&lt;/span&gt;
&lt;span class="normal"&gt;124&lt;/span&gt;
&lt;span class="normal"&gt;125&lt;/span&gt;
&lt;span class="normal"&gt;126&lt;/span&gt;
&lt;span class="normal"&gt;127&lt;/span&gt;
&lt;span class="normal"&gt;128&lt;/span&gt;
&lt;span class="normal"&gt;129&lt;/span&gt;
&lt;span class="normal"&gt;130&lt;/span&gt;
&lt;span class="normal"&gt;131&lt;/span&gt;
&lt;span class="normal"&gt;132&lt;/span&gt;
&lt;span class="normal"&gt;133&lt;/span&gt;
&lt;span class="normal"&gt;134&lt;/span&gt;
&lt;span class="normal"&gt;135&lt;/span&gt;
&lt;span class="normal"&gt;136&lt;/span&gt;
&lt;span class="normal"&gt;137&lt;/span&gt;
&lt;span class="normal"&gt;138&lt;/span&gt;
&lt;span class="normal"&gt;139&lt;/span&gt;
&lt;span class="normal"&gt;140&lt;/span&gt;
&lt;span class="normal"&gt;141&lt;/span&gt;
&lt;span class="normal"&gt;142&lt;/span&gt;
&lt;span class="normal"&gt;143&lt;/span&gt;
&lt;span class="normal"&gt;144&lt;/span&gt;
&lt;span class="normal"&gt;145&lt;/span&gt;
&lt;span class="normal"&gt;146&lt;/span&gt;
&lt;span class="normal"&gt;147&lt;/span&gt;
&lt;span class="normal"&gt;148&lt;/span&gt;
&lt;span class="normal"&gt;149&lt;/span&gt;
&lt;span class="normal"&gt;150&lt;/span&gt;
&lt;span class="normal"&gt;151&lt;/span&gt;
&lt;span class="normal"&gt;152&lt;/span&gt;
&lt;span class="normal"&gt;153&lt;/span&gt;
&lt;span class="normal"&gt;154&lt;/span&gt;
&lt;span class="normal"&gt;155&lt;/span&gt;
&lt;span class="normal"&gt;156&lt;/span&gt;
&lt;span class="normal"&gt;157&lt;/span&gt;
&lt;span class="normal"&gt;158&lt;/span&gt;
&lt;span class="normal"&gt;159&lt;/span&gt;
&lt;span class="normal"&gt;160&lt;/span&gt;
&lt;span class="normal"&gt;161&lt;/span&gt;
&lt;span class="normal"&gt;162&lt;/span&gt;
&lt;span class="normal"&gt;163&lt;/span&gt;
&lt;span class="normal"&gt;164&lt;/span&gt;
&lt;span class="normal"&gt;165&lt;/span&gt;
&lt;span class="normal"&gt;166&lt;/span&gt;
&lt;span class="normal"&gt;167&lt;/span&gt;
&lt;span class="normal"&gt;168&lt;/span&gt;
&lt;span class="normal"&gt;169&lt;/span&gt;
&lt;span class="normal"&gt;170&lt;/span&gt;
&lt;span class="normal"&gt;171&lt;/span&gt;
&lt;span class="normal"&gt;172&lt;/span&gt;
&lt;span class="normal"&gt;173&lt;/span&gt;
&lt;span class="normal"&gt;174&lt;/span&gt;
&lt;span class="normal"&gt;175&lt;/span&gt;
&lt;span class="normal"&gt;176&lt;/span&gt;
&lt;span class="normal"&gt;177&lt;/span&gt;
&lt;span class="normal"&gt;178&lt;/span&gt;
&lt;span class="normal"&gt;179&lt;/span&gt;
&lt;span class="normal"&gt;180&lt;/span&gt;
&lt;span class="normal"&gt;181&lt;/span&gt;
&lt;span class="normal"&gt;182&lt;/span&gt;
&lt;span class="normal"&gt;183&lt;/span&gt;
&lt;span class="normal"&gt;184&lt;/span&gt;
&lt;span class="normal"&gt;185&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Autoencoder&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new grap&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_targets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their predictions and loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                                               &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                               &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_targets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                               &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# regularization loss&lt;/span&gt;
            &lt;span class="c1"&gt;# weight elimination L2 regularizer&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt; \
                &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;

            &lt;span class="c1"&gt;# total loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularizer&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_targets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                                                          &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                          &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_targets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                          &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularizer&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

            &lt;span class="n"&gt;n_encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_encoder&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_encoder&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_encoder&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;n_decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_decoder&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_decoder&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_decoder&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="n"&gt;activation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;

        &lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
                &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))],&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))],&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
                &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))],&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))],&lt;/span&gt;
            &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

            &lt;span class="c1"&gt;# mini-batch gradient descent&lt;/span&gt;
            &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
            &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;index_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_size&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

                &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt;
                             &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_targets&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]}&lt;/span&gt;
                &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

                &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;     &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# evaluate at the end of this epoch&lt;/span&gt;
            &lt;span class="n"&gt;msg_valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;val_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;msg_valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, val_loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;s loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt; &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                   &lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;msg_valid&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_encoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_targets&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="autoencoder_2"&gt;測試Autoencoder&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MNIST_data/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;
&lt;span class="n"&gt;valid_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Autoencoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model_1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img_original&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_original&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    0.0332 , val_loss =    0.0327&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0303 , val_loss =    0.0299&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0288 , val_loss =    0.0285&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  4/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0278 , val_loss =    0.0275&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  5/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0272 , val_loss =    0.0271&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  6/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0267 , val_loss =    0.0268&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  7/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0263 , val_loss =    0.0264&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  8/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0259 , val_loss =    0.0262&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  9/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0256 , val_loss =    0.0259&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0254 , val_loss =    0.0257&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 11/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0252 , val_loss =    0.0257&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 12/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0250 , val_loss =    0.0256&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 13/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0248 , val_loss =    0.0255&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 14/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0247 , val_loss =    0.0254&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 15/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0245 , val_loss =    0.0253&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 16/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0244 , val_loss =    0.0253&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 17/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0242 , val_loss =    0.0251&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 18/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0243 , val_loss =    0.0252&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 19/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0240 , val_loss =    0.0251&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 20/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0239 , val_loss =    0.0250&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_loss =    0.0256&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,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" /&gt;&lt;/p&gt;
&lt;p&gt;上面圖中上排是進去Autoencoder之前的圖片，下排是經過Autoencoder後的圖片，效果是不是很驚人！大致都有辦法還原回去原圖。但是仍有幾張圖還原的不是很好，做個Regularization看看能不能解決這個問題。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Autoencoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model_2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img_original&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_original&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;18s loss =    0.0320 , val_loss =    0.0316&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0296 , val_loss =    0.0294&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0285 , val_loss =    0.0283&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  4/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0277 , val_loss =    0.0277&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  5/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0271 , val_loss =    0.0272&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  6/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0266 , val_loss =    0.0268&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  7/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0262 , val_loss =    0.0265&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  8/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0260 , val_loss =    0.0264&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  9/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0257 , val_loss =    0.0261&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0256 , val_loss =    0.0260&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 11/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0255 , val_loss =    0.0260&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 12/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0251 , val_loss =    0.0257&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 13/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0249 , val_loss =    0.0256&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 14/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0248 , val_loss =    0.0256&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 15/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;18s loss =    0.0247 , val_loss =    0.0255&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 16/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0245 , val_loss =    0.0253&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 17/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0243 , val_loss =    0.0252&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 18/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0242 , val_loss =    0.0252&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 19/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0242 , val_loss =    0.0252&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 20/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0240 , val_loss =    0.0251&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_loss =    0.0256&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,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" /&gt;&lt;/p&gt;
&lt;p&gt;似乎看起來是有效果的，數字還原的比較清晰。&lt;/p&gt;
&lt;h3 id="code"&gt;壓縮碼Code與視覺化&lt;/h3&gt;
&lt;p&gt;剛剛提到在Autoencoder前半段是一個Encoder，所以我們可以利用這個Encoder來做壓縮，會得到一個Code，在上面的這個例子，這個Code總共有4個值，因為中間層有4個神經元，可以把這個Code看成Dimension Reduction的結果，原本一張圖代表的是28x28=784個維度下的一個點，現在經過轉換後變成是4個維度下的一個點，而我們會直覺的認為同樣一群的數字圖形應該會有較高的相似度，所以在4個維度之下，同樣的數字圖片應該會彼此靠近的比較近，甚至聚成一團。&lt;/p&gt;
&lt;p&gt;我想要驗證一下這件事，我們需要先圖像化，不過卻卡在維度太高的問題，人類無法想像高於3個維度以上的空間，也沒辦法將它視覺化，這個時候我們需要再做一次的Dimension Reduction，將維度降到低於3才可以視覺化，那一般手法是使用PCA來做這件事，有關於PCA我之前已經介紹過，請參考&lt;a href="/ml-course-techniques_6.html"&gt;這篇&lt;/a&gt;，如此一來就可以在4個維度中切一個重要的截面來視覺化這些數據。不過記得喔！4個維度才是真正可以表示這群資料，做PCA只是為了畫圖而做的粗略轉換而已。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# get code&lt;/span&gt;
&lt;span class="n"&gt;encode&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# PCA 2D visualization&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt;
&lt;span class="n"&gt;pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# plot&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;colorbar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/04_output_9_0.png" /&gt;&lt;/p&gt;
&lt;p&gt;上面我以不同顏色當作不同的數字圖形，我們可以看到同樣的數字圖形會彼此聚成一團，所以的確同樣的數字的族群會被歸類到具有相似的特性，因此在code裏頭距離是彼此靠近的，還記得一開始我們沒加Regularization時。Model會把5看成是6，在這張圖你就會到原因，因為5號藍綠色和6號黃色靠的很近，很容易誤判。&lt;/p&gt;
&lt;p&gt;這張圖同時揭露了Autoencoder的一個強大特性，注意喔!我們一開始Train這個Autoencoder的時候是沒有給它看任何Labels的，但他卻可以在壓縮資訊的同時找出規律，這個規律可以想成是我們人類在辨認每個不同數字的方法，所以Autoencoder可以在沒有Labels的情況下做歸納和學習，因此Autoencoder常常會被用在Unsupervised Learning (非監督式學習)。&lt;/p&gt;
&lt;p&gt;另外介紹一種也是很流行的方法叫做t-SNE (讀作"tee-snee") ，這裡不多著墨這個方法的原理，但是它卻是目前2D Visualization最流行的作法，PCA只用線性的方式去做座標轉換，也就是從一個橫切面去看數據，這樣粗略的轉換並不能讓我們在視覺化時看出資料和資料間彼此的距離，尤其是從高維度轉換過來，經常會失真，而t-SNE是針對數據和數據間的距離去做轉換，最後被攤成2維時正是顯示數據點的距離關係，更能描述群聚的現象。&lt;/p&gt;
&lt;p&gt;來看看t-SNE做起來效果如何。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# get code&lt;/span&gt;
&lt;span class="n"&gt;encode&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# TSNE 2D visualization&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.manifold&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;
&lt;span class="n"&gt;tsne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_embedded&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tsne&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# plot&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_embedded&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X_embedded&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;colorbar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/04_output_11_0.png" /&gt;&lt;/p&gt;
&lt;h3 id="de-noise-autoencoder"&gt;去雜訊(De-noise) Autoencoder&lt;/h3&gt;
&lt;p&gt;我們巧妙的利用一下Autoencoder，我們將原本Autoencoder的前面加了一道人工雜訊的流程，但是最終又要讓Autoencoder試著去還原出原來沒有加入雜訊的資訊，這麼一來我們將可以找到一個Autoencoder是可以自行消除雜訊的，把這個Denoising Autoencoder加到正常Neural Network的前面，那這個Neural Network就擁有了抑制雜訊的功用，所以可以當作一種Regularization的方法。&lt;/p&gt;
&lt;p&gt;先將圖片加上雜訊。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add_noise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;noise_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;
    &lt;span class="n"&gt;noisy_ndarr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ndarr&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;noise_factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;noisy_ndarr&lt;/span&gt;

&lt;span class="n"&gt;noisy_train_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add_noise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;noisy_valid_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add_noise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;noisy_test_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add_noise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noisy_train_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,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" /&gt;&lt;/p&gt;
&lt;p&gt;圖片現在看起來非常的髒。&lt;/p&gt;
&lt;p&gt;用這些髒圖片當作Input，正常圖當作Output的目標，我們就可以自然而然的Train出可以消除雜訊的Autoencoder。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;denoise_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Autoencoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0003&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;denoise_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;noisy_train_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noisy_valid_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noisy_test_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img_original&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_original&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img_noisy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noisy_test_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_noisy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;denoise_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noisy_test_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;18s loss =    0.0393 , val_loss =    0.0389&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0356 , val_loss =    0.0355&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0341 , val_loss =    0.0341&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  4/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0327 , val_loss =    0.0329&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  5/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0320 , val_loss =    0.0324&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  6/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0314 , val_loss =    0.0320&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  7/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0309 , val_loss =    0.0316&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  8/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;18s loss =    0.0307 , val_loss =    0.0315&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  9/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0302 , val_loss =    0.0312&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0299 , val_loss =    0.0309&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 11/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0298 , val_loss =    0.0310&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 12/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0296 , val_loss =    0.0308&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 13/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0293 , val_loss =    0.0307&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 14/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0291 , val_loss =    0.0306&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 15/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0290 , val_loss =    0.0305&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 16/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0287 , val_loss =    0.0304&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 17/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0287 , val_loss =    0.0304&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 18/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0287 , val_loss =    0.0304&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 19/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0286 , val_loss =    0.0304&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 20/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0285 , val_loss =    0.0303&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_loss =    0.0309&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,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" /&gt;&lt;/p&gt;
&lt;p&gt;上面圖片第一排為原圖，第二排是加完雜訊後的結果，第三排是經過Autoencoder後的圖，傑克真的是太神奇啦！所有的雜訊都被消除掉了，特別注意，這裡我的Regularization下的特別重，原因是雜訊增多了，也更容易Overfitting，所以要下更多的Regularization才能抑制它。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>實作Tensorflow (3)：Build First Convolutional Neurel Network (CNN)</title><link href="https://ycc.idv.tw/tensorflow-tutorial_3.html" rel="alternate"></link><published>2017-11-12T12:00:00+08:00</published><updated>2017-11-12T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-12:/tensorflow-tutorial_3.html</id><summary type="html">&lt;p&gt;影像有什麼特性 / DNN用在影像上的侷限 / Convolutional Neurel Network (CNN) / Convolution Layer / Pooling Layer / 最簡單的CNN架構：LeNet5 / 圖像化&lt;/p&gt;</summary><content type="html">&lt;p&gt;這一章我們終於要討論到影像辨識的重頭戲啦！通常，處理影像類別我們會用Convolutional Neurel Network，聽起來很難很厲害，不過只要了解背後概念你就知道為什麼要這麼做了，讓我們看下去。&lt;/p&gt;
&lt;p&gt;本單元程式碼可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/03_CNN_classification_on_MNIST.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;h3 id="_1"&gt;影像有什麼特性&lt;/h3&gt;
&lt;p&gt;來想一下，影像具備了什麼特性？&lt;/p&gt;
&lt;p&gt;(1) 局域性：通常物件只在一個局域的範圍裡有效，而與太遠的距離無關，譬如我要找一張圖的鳥嘴，鳥嘴的呈現在一張圖當中只會出現在一個小範圍內，所以其實只需要評估這小範圍就可以判斷這是不是鳥嘴了。&lt;/p&gt;
&lt;p&gt;(2) 平移性：通常一張圖任意平移並不影響它的意義，一隻鳥不管是放在圖片的左上角還是右下角，牠都是一隻鳥。&lt;/p&gt;
&lt;p&gt;(3) 縮放性：通常一張圖我把它等比例的放大縮小是不影響它的意義的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="影像特性" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.004.jpeg" /&gt;&lt;/p&gt;
&lt;h3 id="dnn"&gt;DNN用在影像上的侷限&lt;/h3&gt;
&lt;p&gt;我們剛剛看過了影像具有的三種特性：「局域性」、「平移性」和「縮放性」，那我們就拿這三種特性來檢驗上一回的DNN Classification。&lt;/p&gt;
&lt;p&gt;DNN有「局域性」嗎？沒有，因為我們把圖片攤平處理，原本應該是相鄰的關係就被打壞了，事實上DNN的結構會造成每個Input都會同時影響下一層的「每個」神經元，所以相不相鄰根本沒關係，因為每個Pixels的影響是全域的。&lt;/p&gt;
&lt;p&gt;DNN有「平移性」嗎？沒有，沒有局域性就沒有平移性。&lt;/p&gt;
&lt;p&gt;DNN有「縮放性」嗎？我們沒有一層試著去縮放，而且圖片已經被攤平了，難以做到縮放的效果。&lt;/p&gt;
&lt;p&gt;所以其實使用DNN並不能好好的詮釋影像。&lt;/p&gt;
&lt;h3 id="convolutional-neurel-network-cnn"&gt;Convolutional Neurel Network (CNN)&lt;/h3&gt;
&lt;p&gt;我們需要引入新的架構來處理影像，讓它可以擁有以上三種特性，Convolutional Neurel Network (CNN)此時就登場了，CNN有兩大新要素：Convolution Layer和Pooling Layer，Convolution Layer為我們的Model添加了局域性和平移性，而Pooling Layer則讓Model擁有縮放的特性。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Convolution Layer和Pooling Layer" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.005.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;Convolution Layer是由Filters所構成的，Filters可以想像是一張小圖，以上面的例子，Filter是一個鳥嘴的小圖，這張小圖要怎麼去過濾原圖呢？答案是使用Convolution(卷積)，把小圖疊到大圖的任意位置，接下來將大圖小圖對到的相應元素相乘起來再加總一起，然後在另外一張表格中填入這個加總值，接下來移動Filter，重複的動作再做一次，如此循環並將值陸續填入表格中，這表格最後就會像是另外一張圖一樣，而這張圖可以繼續串另外的Neurel Network，這就是Convolution Layer的計算方法。&lt;/p&gt;
&lt;p&gt;舉個例子，假設我今天有矩陣A：&lt;/p&gt;
&lt;p&gt;[[1, 2, 3, 4],&lt;br/&gt;
 [4, 5, 6, 7],&lt;br/&gt;
 [7, 8, 9,10],&lt;br/&gt;
 [1, 3, 5, 7]]&lt;/p&gt;
&lt;p&gt;然後再有一個Filter：&lt;/p&gt;
&lt;p&gt;[[1, 0, 0],&lt;br/&gt;
 [0, 1, 0],&lt;br/&gt;
 [0, 0, 0]]&lt;/p&gt;
&lt;p&gt;使用Filter對A做Convolution得到B為：&lt;/p&gt;
&lt;p&gt;[[6, 8 ],&lt;br/&gt;
 [12,14]]&lt;/p&gt;
&lt;p&gt;原本4x4的矩陣做了Convolution後變成了2x2的矩陣，原因在於邊界限制了Filter的移動，那如果我想要讓Convolution玩的矩陣維持在4x4，怎麼做？我們可以在邊緣的地方鋪上0就可以達到這樣的效果，來試試看，先將矩陣A擴張成矩陣C：&lt;/p&gt;
&lt;p&gt;[[0, 0, 0, 0, 0, 0],&lt;br/&gt;
 [0, 1, 2, 3, 4, 0],&lt;br/&gt;
 [0, 4, 5, 6, 7, 0],&lt;br/&gt;
 [0, 7, 8, 9,10, 0],&lt;br/&gt;
 [0, 1, 3, 5, 7, 0],&lt;br/&gt;
 [0, 0, 0, 0, 0, 0]]&lt;/p&gt;
&lt;p&gt;再使用Filter對C做Convolution會得到D為：&lt;/p&gt;
&lt;p&gt;[[1, 2, 3, 4],&lt;br/&gt;
 [4, 6, 8,10],&lt;br/&gt;
 [7,12,14,16],&lt;br/&gt;
 [1,10,13,16]]&lt;/p&gt;
&lt;p&gt;而此時D就是一個4x4的矩陣。&lt;/p&gt;
&lt;p&gt;Convolution的過程造成怎麼樣的效果呢？當Filter是一個鳥嘴的小圖，一旦遇到與鳥嘴相似的局部，此時加總的值會是一個大的值，如果是一個和鳥嘴無關的局部此時的值會是一個小的值，所以這個Filter具有將特定特徵篩選出來的能力，符合特徵分數高不符合則分數低，因此局部的特徵變得是有意義的，此時我的Model就具有局域性，而且藉由Filter的平移掃視，這個特徵就具有可平移的特性。&lt;/p&gt;
&lt;p&gt;Convolution Layer不同於Fully-connected Layer有兩點，第一，每個Pixels間有相對的距離關係，擁有上下左右的關係才有辦法構成一張圖，第二，除了有距離上的關係以外，它還能在有限範圍內抓出一種特徵模式，所以我們將可以使用影像的語言來做特徵轉換和抓取特徵。&lt;/p&gt;
&lt;p&gt;實際情況下，Filter上的Weights是會自行調整的，Model Fitting的時候，Model會根據數據自行訓練出Filter，也就是說機器可以自行學習出圖片的特徵。通常這樣的Filters會有好幾個，讓機器可以有更多維度可以學習。&lt;/p&gt;
&lt;p&gt;接下來來看Pooling Layer如何讓Model擁有檢視縮放的特性？&lt;/p&gt;
&lt;p&gt;先來看在影像上如何做到放大縮小，以Pixels的觀點來看，最簡單的放大方法是，在每個既有的Pixels附近增加一些與它們相似的新Pixel，這樣做就像是將原本的小圖直接拉成大圖，畫質雖然很差，不過這是最簡單的放大方法。那麼縮小就相反操作，把一群附近的Pixels濃縮成一個Pixel當作代表，就可以達到縮小圖片的效果。&lt;/p&gt;
&lt;p&gt;所以回到Pooling Layer的討論，Pooling做的事情就是在縮小圖片，例如我使用2x2來做Pooling，它會在原圖上以2x2來掃描，所以會有四個元素被檢視，然後從這四個值當中產生一個代表值，把原本2x2的Pixels減少成這個1x1的代表值，如果是Max Pooling就是從四個中選最大的那個，如果是Average Pooling則是平均四個值得到平均值，通常如果是2x2的Pooling我們會以每2格一跳的方式掃視，如果是3x3則會以每3格一跳，以此類推。&lt;/p&gt;
&lt;h3 id="convolution-layer"&gt;Convolution Layer&lt;/h3&gt;
&lt;p&gt;來看一下Tensorflow如何實作Convolution Layer。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Config the matplotlib backend as plotting inline in IPython&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;([[[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]]],&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# shape of img: [batch, in_height, in_width, in_channels]&lt;/span&gt;

    &lt;span class="n"&gt;filter_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;([[[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]],&lt;/span&gt;
                                 &lt;span class="p"&gt;[[[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]]],&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# shape of filter: [filter_height, filter_width, in_channels, out_channels]&lt;/span&gt;

    &lt;span class="n"&gt;conv_strides&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;padding_method&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;

    &lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;filter_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conv_strides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;conv_strides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
        &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;padding_method&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Shape of conv:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Conv:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Shape of conv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;(1, 4, 4, 1)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Conv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[[[[&lt;/span&gt;&lt;span class="nv"&gt;14.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;2.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;3.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;3.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;2.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;14.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;3.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;6.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;6.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;1.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;2.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;1.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;首先一開始是圖片&lt;code&gt;img&lt;/code&gt;的部分，Rank為4，每個維度分別為&lt;code&gt;[batch, in_height, in_width, in_channels]&lt;/code&gt;，in_channels的部分一般是RGB，這邊我採用和MNIST相同的灰階表示，所以in_channels只有1個維度。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;filter_&lt;/code&gt;的部分Rank為4，每個維度分別為&lt;code&gt;[filter_height, filter_width, in_channels, out_channels]&lt;/code&gt;，當如果我想要使用多個filters的時候，我的out_channels就不只1而已，如果有RGB的話，in_channels則會是3。&lt;/p&gt;
&lt;p&gt;接下來來看一下&lt;code&gt;tf.nn.conv2d&lt;/code&gt;裡頭的參數&lt;code&gt;strides&lt;/code&gt;，這可能會讓人感到困惑，它的設定值是&lt;code&gt;[1, conv_strides[0], conv_strides[1], 1]&lt;/code&gt;，我特別把第二、三項額外用&lt;code&gt;conv_strides&lt;/code&gt;來表示，因為這兩個值才是真正代表在圖片上平移每步的距離，那第一項和最後一項的1代表什麼意義呢？是這樣的，Tensorflow是站在維度的角度看平移這件事情，這四個維度分別表示&lt;code&gt;[batch, in_height, in_width, in_channels]&lt;/code&gt;的移動量，所以一般情況下只有&lt;code&gt;in_height&lt;/code&gt;和&lt;code&gt;in_width&lt;/code&gt;需要指定平移的距離。&lt;/p&gt;
&lt;p&gt;最後一個參數就是&lt;code&gt;padding&lt;/code&gt;，有兩種可以選擇，分別為&lt;code&gt;VALID&lt;/code&gt;和&lt;code&gt;SAME&lt;/code&gt;，&lt;code&gt;VALID&lt;/code&gt;指的就是沒有額外鋪上0的邊界的情形，&lt;code&gt;SAME&lt;/code&gt;則是額外鋪上0的邊界，並且使得輸出的維度和輸入一樣。&lt;/p&gt;
&lt;h3 id="pooling-layer"&gt;Pooling Layer&lt;/h3&gt;
&lt;p&gt;接下來來看Tensorflow如何實作Pooling Layer。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;([[[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]]],&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# shape of img: [batch, in_height, in_width, in_channels]&lt;/span&gt;

    &lt;span class="n"&gt;pooling&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Shape of pooling:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pooling&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Pooling:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pooling&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Shape of pooling&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;(1, 2, 2, 1)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Pooling&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[[[[&lt;/span&gt;&lt;span class="nv"&gt;3.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;1.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="nv"&gt;1.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;3.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;tf.nn.max_pool&lt;/code&gt;的參數中&lt;code&gt;ksize&lt;/code&gt;代表kernel size，也就是要Pooling的大小，一樣依照Input layer的Rank去配置，還有&lt;code&gt;strides&lt;/code&gt;決定平移的方法。&lt;/p&gt;
&lt;h3 id="cnnlenet5"&gt;最簡單的CNN架構：LeNet5&lt;/h3&gt;
&lt;p&gt;&lt;img alt="LeNet5" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.006.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;接下來我們就真正的來實作一下CNN網路，和之前兩個單元一樣，我們拿MNIST的分類問題來當作題目。&lt;/p&gt;
&lt;p&gt;本單元介紹的是最簡單的CNN Classification的方法—LeNet5，流程如上圖所示。&lt;/p&gt;
&lt;p&gt;(1) conv1+relu+pooling2：一開始使用Convolution抓取圖片中的特徵，並且加入Activation Function使得Model具有非線性因子，通常在這種非常深的網路，我們會採用Relu，它的好處是不會出現「梯度消失」(Vanishing Gradient)的問題，如果你使用像是tanh或sigmoid這類在飽和區梯度接近0的函數，則就很有可能在深網路的情形下，造成一開始的幾個Layers梯度太小的問題，也就是前面幾層我們無法訓練到，而Relu在Turn-on的情況下是線性的，不會有飽和的問題，也就不會出現「梯度消失」。做完Convolution後，我們已經對於這個圖片有一點認識了，所以減少一些Pixels來減少一些計算量，所以加入Pooling Layer，注意喔！Pooling Layer結束之後，不需要再做一次Activation，因為Pooling只是用來平均前面的結果而已。&lt;/p&gt;
&lt;p&gt;(2) conv3+relu+pooling4：做第二次的圖片特徵抽取。&lt;/p&gt;
&lt;p&gt;(3) fatten5：將抽取完的特徵完全打平，為了接下來的Fully-connected Network做準備。&lt;/p&gt;
&lt;p&gt;(4) fc6+fc7+fc8+softmax：這個部分就和之前DNN Classification做的事情一樣，全盤考慮每一個擷取來的特徵，並且非線性的轉換成最後可以分為相應的10種類別。&lt;/p&gt;
&lt;p&gt;接下來我們看一下程式碼要怎麼寫？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;  1&lt;/span&gt;
&lt;span class="normal"&gt;  2&lt;/span&gt;
&lt;span class="normal"&gt;  3&lt;/span&gt;
&lt;span class="normal"&gt;  4&lt;/span&gt;
&lt;span class="normal"&gt;  5&lt;/span&gt;
&lt;span class="normal"&gt;  6&lt;/span&gt;
&lt;span class="normal"&gt;  7&lt;/span&gt;
&lt;span class="normal"&gt;  8&lt;/span&gt;
&lt;span class="normal"&gt;  9&lt;/span&gt;
&lt;span class="normal"&gt; 10&lt;/span&gt;
&lt;span class="normal"&gt; 11&lt;/span&gt;
&lt;span class="normal"&gt; 12&lt;/span&gt;
&lt;span class="normal"&gt; 13&lt;/span&gt;
&lt;span class="normal"&gt; 14&lt;/span&gt;
&lt;span class="normal"&gt; 15&lt;/span&gt;
&lt;span class="normal"&gt; 16&lt;/span&gt;
&lt;span class="normal"&gt; 17&lt;/span&gt;
&lt;span class="normal"&gt; 18&lt;/span&gt;
&lt;span class="normal"&gt; 19&lt;/span&gt;
&lt;span class="normal"&gt; 20&lt;/span&gt;
&lt;span class="normal"&gt; 21&lt;/span&gt;
&lt;span class="normal"&gt; 22&lt;/span&gt;
&lt;span class="normal"&gt; 23&lt;/span&gt;
&lt;span class="normal"&gt; 24&lt;/span&gt;
&lt;span class="normal"&gt; 25&lt;/span&gt;
&lt;span class="normal"&gt; 26&lt;/span&gt;
&lt;span class="normal"&gt; 27&lt;/span&gt;
&lt;span class="normal"&gt; 28&lt;/span&gt;
&lt;span class="normal"&gt; 29&lt;/span&gt;
&lt;span class="normal"&gt; 30&lt;/span&gt;
&lt;span class="normal"&gt; 31&lt;/span&gt;
&lt;span class="normal"&gt; 32&lt;/span&gt;
&lt;span class="normal"&gt; 33&lt;/span&gt;
&lt;span class="normal"&gt; 34&lt;/span&gt;
&lt;span class="normal"&gt; 35&lt;/span&gt;
&lt;span class="normal"&gt; 36&lt;/span&gt;
&lt;span class="normal"&gt; 37&lt;/span&gt;
&lt;span class="normal"&gt; 38&lt;/span&gt;
&lt;span class="normal"&gt; 39&lt;/span&gt;
&lt;span class="normal"&gt; 40&lt;/span&gt;
&lt;span class="normal"&gt; 41&lt;/span&gt;
&lt;span class="normal"&gt; 42&lt;/span&gt;
&lt;span class="normal"&gt; 43&lt;/span&gt;
&lt;span class="normal"&gt; 44&lt;/span&gt;
&lt;span class="normal"&gt; 45&lt;/span&gt;
&lt;span class="normal"&gt; 46&lt;/span&gt;
&lt;span class="normal"&gt; 47&lt;/span&gt;
&lt;span class="normal"&gt; 48&lt;/span&gt;
&lt;span class="normal"&gt; 49&lt;/span&gt;
&lt;span class="normal"&gt; 50&lt;/span&gt;
&lt;span class="normal"&gt; 51&lt;/span&gt;
&lt;span class="normal"&gt; 52&lt;/span&gt;
&lt;span class="normal"&gt; 53&lt;/span&gt;
&lt;span class="normal"&gt; 54&lt;/span&gt;
&lt;span class="normal"&gt; 55&lt;/span&gt;
&lt;span class="normal"&gt; 56&lt;/span&gt;
&lt;span class="normal"&gt; 57&lt;/span&gt;
&lt;span class="normal"&gt; 58&lt;/span&gt;
&lt;span class="normal"&gt; 59&lt;/span&gt;
&lt;span class="normal"&gt; 60&lt;/span&gt;
&lt;span class="normal"&gt; 61&lt;/span&gt;
&lt;span class="normal"&gt; 62&lt;/span&gt;
&lt;span class="normal"&gt; 63&lt;/span&gt;
&lt;span class="normal"&gt; 64&lt;/span&gt;
&lt;span class="normal"&gt; 65&lt;/span&gt;
&lt;span class="normal"&gt; 66&lt;/span&gt;
&lt;span class="normal"&gt; 67&lt;/span&gt;
&lt;span class="normal"&gt; 68&lt;/span&gt;
&lt;span class="normal"&gt; 69&lt;/span&gt;
&lt;span class="normal"&gt; 70&lt;/span&gt;
&lt;span class="normal"&gt; 71&lt;/span&gt;
&lt;span class="normal"&gt; 72&lt;/span&gt;
&lt;span class="normal"&gt; 73&lt;/span&gt;
&lt;span class="normal"&gt; 74&lt;/span&gt;
&lt;span class="normal"&gt; 75&lt;/span&gt;
&lt;span class="normal"&gt; 76&lt;/span&gt;
&lt;span class="normal"&gt; 77&lt;/span&gt;
&lt;span class="normal"&gt; 78&lt;/span&gt;
&lt;span class="normal"&gt; 79&lt;/span&gt;
&lt;span class="normal"&gt; 80&lt;/span&gt;
&lt;span class="normal"&gt; 81&lt;/span&gt;
&lt;span class="normal"&gt; 82&lt;/span&gt;
&lt;span class="normal"&gt; 83&lt;/span&gt;
&lt;span class="normal"&gt; 84&lt;/span&gt;
&lt;span class="normal"&gt; 85&lt;/span&gt;
&lt;span class="normal"&gt; 86&lt;/span&gt;
&lt;span class="normal"&gt; 87&lt;/span&gt;
&lt;span class="normal"&gt; 88&lt;/span&gt;
&lt;span class="normal"&gt; 89&lt;/span&gt;
&lt;span class="normal"&gt; 90&lt;/span&gt;
&lt;span class="normal"&gt; 91&lt;/span&gt;
&lt;span class="normal"&gt; 92&lt;/span&gt;
&lt;span class="normal"&gt; 93&lt;/span&gt;
&lt;span class="normal"&gt; 94&lt;/span&gt;
&lt;span class="normal"&gt; 95&lt;/span&gt;
&lt;span class="normal"&gt; 96&lt;/span&gt;
&lt;span class="normal"&gt; 97&lt;/span&gt;
&lt;span class="normal"&gt; 98&lt;/span&gt;
&lt;span class="normal"&gt; 99&lt;/span&gt;
&lt;span class="normal"&gt;100&lt;/span&gt;
&lt;span class="normal"&gt;101&lt;/span&gt;
&lt;span class="normal"&gt;102&lt;/span&gt;
&lt;span class="normal"&gt;103&lt;/span&gt;
&lt;span class="normal"&gt;104&lt;/span&gt;
&lt;span class="normal"&gt;105&lt;/span&gt;
&lt;span class="normal"&gt;106&lt;/span&gt;
&lt;span class="normal"&gt;107&lt;/span&gt;
&lt;span class="normal"&gt;108&lt;/span&gt;
&lt;span class="normal"&gt;109&lt;/span&gt;
&lt;span class="normal"&gt;110&lt;/span&gt;
&lt;span class="normal"&gt;111&lt;/span&gt;
&lt;span class="normal"&gt;112&lt;/span&gt;
&lt;span class="normal"&gt;113&lt;/span&gt;
&lt;span class="normal"&gt;114&lt;/span&gt;
&lt;span class="normal"&gt;115&lt;/span&gt;
&lt;span class="normal"&gt;116&lt;/span&gt;
&lt;span class="normal"&gt;117&lt;/span&gt;
&lt;span class="normal"&gt;118&lt;/span&gt;
&lt;span class="normal"&gt;119&lt;/span&gt;
&lt;span class="normal"&gt;120&lt;/span&gt;
&lt;span class="normal"&gt;121&lt;/span&gt;
&lt;span class="normal"&gt;122&lt;/span&gt;
&lt;span class="normal"&gt;123&lt;/span&gt;
&lt;span class="normal"&gt;124&lt;/span&gt;
&lt;span class="normal"&gt;125&lt;/span&gt;
&lt;span class="normal"&gt;126&lt;/span&gt;
&lt;span class="normal"&gt;127&lt;/span&gt;
&lt;span class="normal"&gt;128&lt;/span&gt;
&lt;span class="normal"&gt;129&lt;/span&gt;
&lt;span class="normal"&gt;130&lt;/span&gt;
&lt;span class="normal"&gt;131&lt;/span&gt;
&lt;span class="normal"&gt;132&lt;/span&gt;
&lt;span class="normal"&gt;133&lt;/span&gt;
&lt;span class="normal"&gt;134&lt;/span&gt;
&lt;span class="normal"&gt;135&lt;/span&gt;
&lt;span class="normal"&gt;136&lt;/span&gt;
&lt;span class="normal"&gt;137&lt;/span&gt;
&lt;span class="normal"&gt;138&lt;/span&gt;
&lt;span class="normal"&gt;139&lt;/span&gt;
&lt;span class="normal"&gt;140&lt;/span&gt;
&lt;span class="normal"&gt;141&lt;/span&gt;
&lt;span class="normal"&gt;142&lt;/span&gt;
&lt;span class="normal"&gt;143&lt;/span&gt;
&lt;span class="normal"&gt;144&lt;/span&gt;
&lt;span class="normal"&gt;145&lt;/span&gt;
&lt;span class="normal"&gt;146&lt;/span&gt;
&lt;span class="normal"&gt;147&lt;/span&gt;
&lt;span class="normal"&gt;148&lt;/span&gt;
&lt;span class="normal"&gt;149&lt;/span&gt;
&lt;span class="normal"&gt;150&lt;/span&gt;
&lt;span class="normal"&gt;151&lt;/span&gt;
&lt;span class="normal"&gt;152&lt;/span&gt;
&lt;span class="normal"&gt;153&lt;/span&gt;
&lt;span class="normal"&gt;154&lt;/span&gt;
&lt;span class="normal"&gt;155&lt;/span&gt;
&lt;span class="normal"&gt;156&lt;/span&gt;
&lt;span class="normal"&gt;157&lt;/span&gt;
&lt;span class="normal"&gt;158&lt;/span&gt;
&lt;span class="normal"&gt;159&lt;/span&gt;
&lt;span class="normal"&gt;160&lt;/span&gt;
&lt;span class="normal"&gt;161&lt;/span&gt;
&lt;span class="normal"&gt;162&lt;/span&gt;
&lt;span class="normal"&gt;163&lt;/span&gt;
&lt;span class="normal"&gt;164&lt;/span&gt;
&lt;span class="normal"&gt;165&lt;/span&gt;
&lt;span class="normal"&gt;166&lt;/span&gt;
&lt;span class="normal"&gt;167&lt;/span&gt;
&lt;span class="normal"&gt;168&lt;/span&gt;
&lt;span class="normal"&gt;169&lt;/span&gt;
&lt;span class="normal"&gt;170&lt;/span&gt;
&lt;span class="normal"&gt;171&lt;/span&gt;
&lt;span class="normal"&gt;172&lt;/span&gt;
&lt;span class="normal"&gt;173&lt;/span&gt;
&lt;span class="normal"&gt;174&lt;/span&gt;
&lt;span class="normal"&gt;175&lt;/span&gt;
&lt;span class="normal"&gt;176&lt;/span&gt;
&lt;span class="normal"&gt;177&lt;/span&gt;
&lt;span class="normal"&gt;178&lt;/span&gt;
&lt;span class="normal"&gt;179&lt;/span&gt;
&lt;span class="normal"&gt;180&lt;/span&gt;
&lt;span class="normal"&gt;181&lt;/span&gt;
&lt;span class="normal"&gt;182&lt;/span&gt;
&lt;span class="normal"&gt;183&lt;/span&gt;
&lt;span class="normal"&gt;184&lt;/span&gt;
&lt;span class="normal"&gt;185&lt;/span&gt;
&lt;span class="normal"&gt;186&lt;/span&gt;
&lt;span class="normal"&gt;187&lt;/span&gt;
&lt;span class="normal"&gt;188&lt;/span&gt;
&lt;span class="normal"&gt;189&lt;/span&gt;
&lt;span class="normal"&gt;190&lt;/span&gt;
&lt;span class="normal"&gt;191&lt;/span&gt;
&lt;span class="normal"&gt;192&lt;/span&gt;
&lt;span class="normal"&gt;193&lt;/span&gt;
&lt;span class="normal"&gt;194&lt;/span&gt;
&lt;span class="normal"&gt;195&lt;/span&gt;
&lt;span class="normal"&gt;196&lt;/span&gt;
&lt;span class="normal"&gt;197&lt;/span&gt;
&lt;span class="normal"&gt;198&lt;/span&gt;
&lt;span class="normal"&gt;199&lt;/span&gt;
&lt;span class="normal"&gt;200&lt;/span&gt;
&lt;span class="normal"&gt;201&lt;/span&gt;
&lt;span class="normal"&gt;202&lt;/span&gt;
&lt;span class="normal"&gt;203&lt;/span&gt;
&lt;span class="normal"&gt;204&lt;/span&gt;
&lt;span class="normal"&gt;205&lt;/span&gt;
&lt;span class="normal"&gt;206&lt;/span&gt;
&lt;span class="normal"&gt;207&lt;/span&gt;
&lt;span class="normal"&gt;208&lt;/span&gt;
&lt;span class="normal"&gt;209&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;CNNLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape_picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape_picture&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shape_picture&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new grap&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_pictures&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                 &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape_picture&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                               &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their predictions and loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pictures&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_pictures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# regularization loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l2_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt; \
                &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;

            &lt;span class="c1"&gt;# total loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_pictures&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                               &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape_picture&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                             &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pictures&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_pictures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                 &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pictures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="c1"&gt;## LeNet5 Architecture(http://yann.lecun.com/exdb/lenet/)&lt;/span&gt;
        &lt;span class="c1"&gt;# input:(batch,28,28,1) =&amp;gt; conv1[5x5,6] =&amp;gt; (batch,24,24,6)&lt;/span&gt;
        &lt;span class="c1"&gt;# pool2 =&amp;gt; (batch,12,12,6) =&amp;gt; conv2[5x5,16] =&amp;gt; (batch,8,8,16)&lt;/span&gt;
        &lt;span class="c1"&gt;# pool4 =&amp;gt; fatten5 =&amp;gt; (batch,4x4x16) =&amp;gt; fc6 =&amp;gt; (batch,120)&lt;/span&gt;
        &lt;span class="c1"&gt;# (batch,120) =&amp;gt; fc7 =&amp;gt; (batch,84)&lt;/span&gt;
        &lt;span class="c1"&gt;# (batch,84) =&amp;gt; fc8 =&amp;gt; (batch,10) =&amp;gt; softmax&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                         &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                         &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc6&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                       &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc7&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;84&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                       &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;84&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                       &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc6&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc7&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;84&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_conv_2d_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pictures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                               &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_conv_2d_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;pool4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                               &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;fatten5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_flatten_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;fatten5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fatten5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="n"&gt;fc6&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fatten5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc6&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc6&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                   &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;fc6&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="n"&gt;fc7&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc7&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc7&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                   &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                 &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_conv_2d_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                          &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                          &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
              &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                           &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_flatten_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_shape&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
            &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;# mini-batch gradient descent&lt;/span&gt;
            &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
            &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;index_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_size&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

                &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_pictures&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                &lt;span class="p"&gt;}&lt;/span&gt;
                &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                        &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

                &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%.4f&lt;/span&gt;&lt;span class="s1"&gt;     &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# evaluate at the end of this epoch&lt;/span&gt;
            &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;val_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;val_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, val_loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, val_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;test_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_pictures&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_pictures&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MNIST_data/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;
&lt;span class="n"&gt;valid_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CNNLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;shape_picture&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.07&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;valid_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;test_img&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0894, acc = 97.21%, val_loss =   0.0872, val_acc = 97.34%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0532, acc = 98.41%, val_loss =   0.0589, val_acc = 98.30%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0567, acc = 98.27%, val_loss =   0.0578, val_acc = 98.18%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  4/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0384, acc = 98.85%, val_loss =   0.0475, val_acc = 98.56%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  5/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0307, acc = 99.10%, val_loss =   0.0431, val_acc = 98.82%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  6/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0299, acc = 99.09%, val_loss =   0.0388, val_acc = 98.88%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  7/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0280, acc = 99.17%, val_loss =   0.0403, val_acc = 98.86%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  8/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0233, acc = 99.31%, val_loss =   0.0372, val_acc = 99.02%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  9/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0225, acc = 99.32%, val_loss =   0.0356, val_acc = 99.02%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0234, acc = 99.27%, val_loss =   0.0411, val_acc = 98.84%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_acc = 98.89%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;太棒了！我們的預測效果如果跟之前的結果比較，在Epoch=3已經達到98.5%了，在Epoch=10更是高達99%！&lt;/p&gt;
&lt;h3 id="_2"&gt;圖像化&lt;/h3&gt;
&lt;p&gt;有這麼好的成果，我們不妨就拉進去看，裡面每個Filters究竟是長什麼樣子的？&lt;/p&gt;
&lt;p&gt;先來看第一層Convolution的Filters。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAagAAABTCAYAAADKkJOuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0ABxQREfkSBxQREfkSBxQREfnS/wHRnw01r1VfLAAAAABJRU5ErkJggg==%0A" /&gt;&lt;/p&gt;
&lt;p&gt;再來看第二層Convolution的Filters。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAkkAAAKvCAYAAAB+nVurAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A0N/LBB+YAAAAAElFTkSuQmCC%0A" /&gt;&lt;/p&gt;
&lt;p&gt;單看這些Filters似乎很難看出什麼？&lt;/p&gt;
&lt;p&gt;我們試著丟幾張圖進去做Convolution，看一下在Filters的拆解之下圖片會變怎樣？&lt;/p&gt;
&lt;p&gt;以下圖片的第一行表示原圖片，第二行表示做完第一次Convolution後的結果，第三行表示做完第二次Convolution後的結果。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;picture&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:],(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv1&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv3&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6IAAADFCAYAAABO4U/4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;picture&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:],(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv1&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv3&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6IAAADFCAYAAABO4U/4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0AAAAAAElFTkSuQmCC%0A" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;picture&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv1&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv3&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6IAAADFCAYAAABO4U/4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz%0AAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYXFWB/vHvSS9kD1maJHR3Njokk0AIWVgcYFSUTQww%0AgAFHRAUyIjKI4+8RnBGVcWFURkGc0QxK1GFTIYSBsMtiHCAbQUjCEiAJCYGks5AG0lk65/dH1anq%0A5VZ3rbfuPfV+nqefTp2+Vfe8dU7dzu1zz7nGWouIiIiIiIhIWHqVuwIiIiIiIiJSWXQiKiIiIiIi%0AIqHSiaiIiIiIiIiESieiIiIiIiIiEiqdiIqIiIiIiEiodCIqIiIiIiIioarIE1FjzK+NMZuNMS9m%0A+LkxxtxojFljjPmrMWZa2HUslO8Zfc8H/mdUvnjnA/8z+p4P/M+ofPHOB/5nVL5454PKyFgqFXki%0ACswDTunm56cC45Nfc4D/CqFOxTYPvzPOw+984H/GeShfnPOB/xnn4Xc+8D/jPJQvzvnA/4zzUL44%0A54PKyFgSBZ2IGmNOMca8nDzDv6pYlSo1a+1TwLZuNjkD+K1NeAY40BgzMpzaFYfvGX3PB/5nVL54%0A5wP/M/qeD/zPqHzxzgf+Z1S+eOeDyshYKsZam98TjakCXgE+DmwAlgDnW2tXFa96pWOMGQPcZ609%0ALOBn9wHXWWsXJR8/BnzdWrs0YNs5JP66Qb9+/aZPnDixlNXOye7du1mzZg2TJ0/u8rM1a9YwYsQI%0A+vfvD8CyZcv2Ah+KU8Zc8r3yyiu0tLTssNYO7rxtVPNBcdrQ93wQ3Yzqo2pDJ6r5QMcZ9dGEqOYD%0A9VH10YSo5gP/2zBXy5Yta7bW1vW4obU2ry/gWOChdo+vBq7u4Tk2xl/be3pPpk+fbqPkjTfesJMn%0ATw782Sc+8Qn75z//OfUY2AnMsDHKmEu+j370oxZYZWOUz9rit6Hv+WzEMqqPqg2DvqKUz1odZ9RH%0Ao53PWvVR9dFo57PW/zbMFbDU9pDPWlvQpbn1wJvtHm9IlnVgjJljjFlqjOly1h8z68tdgWKqr6/n%0AzTfbNx+1wMYyVafoOufbsGEDwN6yVagEKq0N8Tyf+mj8qA3jr9LyqY/GT6XlUx+tLCVfrMhaO9da%0AO8NaO6PU+yoxrz4Us2bN4re//S3WWp555hmANmvtpnLXq1g65xs0aBCoDWOl0vKpj8aP2jD+Ki2f%0A+mj8VFo+9dHKUl3AczcCje0eN6Cz+8g4//zzeeKJJ2hubqahoYHvfOc77N2b+Fx/8Ytf5LTTTmPh%0AwoU0NTXRt29fgHVlrXCOcs13yy23MHPmzDLXOjdqw8rKpz4aPWrDymtDPM+nPho9yqc+WtGyuX43%0A6IvESezrwFgSQ8zPA5N7eE6553kW8tXjtc6VcC237xmVL7rURysjn62AjMoXXeqjyhd16qOVkc9W%0ASMa8R0SttfuMMV8GHgKqgF9ba1fm+3oiIiIiIiJSGQq5NBdr7UJgYZHqIiIiIiIiIhWgoBPRcps+%0AfToAZ555JmeffTYAEyZMAMAYAyQuPV6+fDkAq1evBuD73/8+L730UtjVFREREREREWJwIjpnzhwA%0A3A1djz/++NTPpk2bBiRONtufeALMnTsXgPnz5/Pwww+HVl8RERERERHpXslv3yIiIiIiIiLSXuRH%0ARH/xi18A6ZHODz74IHVZ7Q033ADASy+9xJYtW4DECKiIiIiIiIhEl0ZERUREREREJFSRHxG9++67%0AgcSCRJAY/YzbjW5FREREREQkLfInopdeeimQXiF39OjRjBo1CoD169eXrV4iIiIiIiKSH12aKyIi%0AIiIiIqGK/IioW4TI3Y7lu9/9LsOGDQM0IioiIiIiIhJHGhEVERERERGRUEV+RNTp1StxzmyM4W/+%0A5m9S/+5s9erVQOI2LyIiIiIiIhI9kT8RraurA+Diiy8GEvcT/c1vfgOkT0Sttal/u/uI3nrrrR0e%0Ai4iIiIiISDTo0lwREREREREJVaRPROvq6njyySd58sknGTVqFKNGjWL58uXcdttt3HbbbVx66aVc%0AeumlfOlLX2L58uUsX76c6dOnM336dO666y7uuusu2traUmV9+/alb9+++VZnWDGzldqDDz7IhAkT%0AaGpq4rrrruvy83nz5lFXV8fUqVOZOnUqxCwf+J/R93yQW0ZgkjHm4tArWQDf29D3fOB/RuWLdz7w%0AP6Pv+UC/C+Pehr7nK6VIn4hGTHO5K5CttrY2LrvsMh544AFWrVrF7bffzqpVq7psN3v2bFasWMGK%0AFSsgRvnA/4y+54PcMwKrrLU3h17RPPnehr7nA/8zKl9CXPOB/xl9zwf6XejEtQ19z1dqkZ4jOmHC%0ABCZMmADA3XffDcC5554buK27vYu7tctnPvMZAM4880wWL14MkOoYp59+OuvWrStdxcts8eLFNDU1%0AMW7cOADOO+88FixYwKRJk8pcs+LxPaPv+cD/jMoXf75nVL748z2j7/nA/4zKJ92J9IjookWLqKqq%0AoqqqinPPPTfjSWh7zc3NNDc389Of/pSf/vSnfPjDH05dwtva2kprayvXXHMNQ4cOZfr06blUpyao%0A0Bgzxxiz1Biz1N3ztNw2btxIY2Nj6nFDQwMbN27sst1dd93FlClTOOeccyBDPvA/o/KVR64ZgXHG%0AmMYuGxDNjL63oY4zaWpD//OB/xmVrzz0uzAhrm1YCceZUor0iWjEjA0qtNbOtdbOsNbOcCv8xsEn%0AP/lJ1q5dy1//+lc+/vGPQ4Z84H9G5Yuu9hmBncBvgraLa0bf21DHmTTliyb10TTliy79LkzwPR/E%0AN2O+KuJEdO7cucydO5dTTz2VU089ld69e3P44Ydz//33c9ZZZ2U7fJ73Kkdhq6+v580330w93rBh%0AA/X19R22GTp0KAcccACQujVObPKB/xl9zwe5ZyQxpyKnyxjKyfc29D0f+J9R+eKdD/zP6Hs+0O9C%0AiHcb+p6v1CriRLSzww47jNdff51169axf/9+NmzYkM3TWktdr2KZOXMmr776Km+88QZ79uzhjjvu%0AYNasWR222bRpU+rf9957L8QoH/if0fd8kHtG4EBgdZh1LITvbeh7PvA/o/LFOx/4n9H3fKDfhRDv%0ANvQ9X6lFerGiYmtuTixSdfnll/PVr36Vc889F2MMl112Gbt27eKnP/1pd09fG0Ydi6G6upqbbrqJ%0Ak08+mba2Nr7whS8wefJkrrnmGmbMmMGsWbO48cYbuffee6murmbIkCEQo3zgf0bf80HuGYGDgNPL%0AXO2s+d6GvucD/zMqX7zzgf8Zfc8H+l0Y9zb0PV/JWWtD+wJsFL5OOOEEu2TJErtkyRK7efNmu3nz%0AZvuVr3ylp+ct7Snf9OnTbVxlk89WQEbliy710crIZysgo/JFl/qo8kWd+mhl5LMVkrEiL8196qmn%0AUvNFt2zZwpYtW/jxj39c7mqJiIiIiIhUhIq6NLc9d5nuokWLAJg4cWI5qyMiIiIiIlIxehwRNcY0%0AGmMeN8asMsasNMZckSwfYox5xBjzavL74NJXV0REREREROIumxHRfcA/W2uXG2MGAMuMMY8AnwMe%0As9ZeZ4y5CrgK+HrpqlpcbgT0zDPPBGDVqlXlrI6IiIiIiEjF6HFE1Fq7yVq7PPnvFhJLRtcDZ5C+%0Aoe5vgDNLVUkRERERERHxR05zRI0xY4AjgWeB4dZad2Oct4HhGZ4zB5iTbwWvvPJKALZs2QLA//zP%0A/+T7UimjR4/me9/7HgB9+ybuKXvuuecW/LoiIiIiIiLSs6xPRI0x/YG7gK9Ya3caY1I/s9ZaY4wN%0Aep61di4wN/kagdtkctZZZ6VWs507dy6Q/YloXV1d6jXavx7AtGnT2Lx5MwCf/exnAXjppZdyqZqI%0AiIiIiIjkKavbtxhjakichN5qrb07WfyOMWZk8ucjgc2lqaKIiIiIiIj4pMcRUZMY+vwVsNpa+x/t%0AfnQvcCFwXfL7glJUsFevxLnynDmJq3vPPvts7r77blc3ILHwkLsdi1t8yP3MWpv69+rVqwG49dZb%0A+f73vw+kb+MiIiIiIiIi4cjm0ty/BS4AXjDGrEiWfYPECejvjTEXAeuAT5WmiiIiIiIiIuKTHk9E%0ArbWLAJPhxycWtzodzZ8/n1NOOQVIj3RCeq6nmwe6atUqrE1MP3VzSd1I5/z581PPc/NAP/jgg1JW%0AW0RERERERLqR06q55fDQQw91+A5w6aWXlqs6IiIiIiIiUqCsFisSERERERERKRadiIqIiIiIiEio%0AdCIqIiIiIiIiodKJaPZqy12BXDz44INMmDCBpqYmrrvuui4/3717N7Nnz6apqYmjjz4aYpYP/M/o%0Aez7ILSMw0RgzJuQqFsT3NvQ9H/if0fd8oONM3NvQ93ygPqo2rGDW2tC+ABvjr2095Zs+fbqNgn37%0A9tlx48bZ1157ze7evdtOmTLFrly5ssM2P//5z+0//uM/Wmutvf3227PKZysgo/KFJ9eMwGvAnTYm%0AGX1vQx1nEtSG0c1nrY4z1sa7DX3PZ636qLVqw0xfUcqYK2CpzeL3vUZEszfAGJPpNjaRsnjxYpqa%0Amhg3bhy1tbWcd955LFiwoMM2CxYs4MILLwTgnHPOgRjlA/8z+p4Pcs8IbAdOjEtG39vQ93zgf0bf%0A84GOMxDvNvQ9H6iPgtqwkpnESWtIOzNmC/A+0BzaToMNBgYC65KPhwD9gfXttpkCrAb2Jh8fCQy3%0A1naouzFmDjAn+fAw4MUS1TkX2eSbDLxCOt804KDO+cD/jMpXNrlmnABsBo6OSUbf21DHmQS1IZHN%0ABzrOQLzb0Pd8oD4KasOUCGfM1QRr7YAet8pm2LSYX2Q5VFviOpwD3Nzu8QXATZ222QU0tHv8GjAs%0A6tlyyPdip3ytPeWrhIzKF92MwFJ9Dv3PVwkZlS+6GXWcUb6oZ1QfjVa+SmjDPN8TXZrbjY1AY7vH%0ADcmy9va4bYwx1cAgYGsotStcNvlS2yTzVRGffOB/Rt/zQY4Zk/Q5jA7f84H/GX3PBzrOdNgmhm3o%0Aez5QH+2wjdqwslTqiegSYLwxZqwxphY4D7i30zY7AHcx9znAn2zyFD8Gssl3Lx3ztcQoH/if0fd8%0AkHvGwehzGCW+5wP/M/qeD3ScgXi3oe/5QH0U1IaVqwxDtXPKPVycrMdpJK7Vfg34l2TZtcCs5L+/%0ABPwBWAMsBsbFJVuW+Xp3yveNOLVfqTIqX6QzvqHPof/5KiGj8kU6o44zyhf1jOqjEctXCW2Yx/uR%0AVd1DXaxIREREREREpFIvzRUREREREZEy0YmoiEgJGGN+bYzZbIwJXHrdJNxojFljjPmrMWZa2HUs%0AlO8Zfc8H/mdUvnjnA/8zKl+880FlZCyVgk5EjTGnGGNeTr6xVxVr2zAZYxqNMY8bY1YZY1YaY65I%0Aln/bGLPRGPOaMabVGPNWd/WOar6eGGMeMsbsM8bs9jTfKcaYd5MZ3+5hu9jlg4pow7jmmwec0s3P%0ATwXGJ7/WAc9m+iXmxDjjUyRWEVzU3YspX1nMw++M81A+HWfaUb7QzUN9NO5tmLWeTsq7KGASahWJ%0ACbnjgFrgeWBSoduWYTLtSGBa8t8DSEw0ngR8G/h/2dQ7yvmyaMONwCeBlZ7mew2YDRxF4t6w3uSr%0AoDaMbT5gDPBihp/9Ejg/+e8TgLXASz28F7HMmMw3DdgNjFS+6OSrhIzKp+OM8kU/n/potPPl8D64%0AjIHvReevvBcrMsYcC3zbWnty8vHVANbaH2TaFjgpr51FQzPwH9A1Y5j5evVKDGJXVVW5fZOsEwD7%0A9+/v8D2H9m0Dvpl8TmA+a+3Jxpg4r261H/hXj/P12IZVVVUn1dbWFr6jtjYA9uzZU/BruT7t6uX6%0AtNvPvn37OOCAA9i1a1cl9NE2a21150JjzBzgSuDgfv36DZw4cWL4Nctg9+7drFmzhsmTJ3f52Zo1%0AaxgxYgT9+/cHYNmyZRY4ylq7tPO2xpjrgEuBV/v16zc9KhlzyffKK6/Q0tKyw1o7uPO2Uc0HxWlD%0A9dHyyaOPVuRxxvd8oD5aTr63Ya6WLVsW2IZdFHDGew5wc7vHFwA3BWw3h8SZ/RbAxvjruaCMYecb%0AOHCgHThwoG1sbLSNjY12zJgxdsyYMXbkyJF25MiRdsiQIXbIkCG2T58+tk+fPrm89q5u2vAXyXxL%0AI9AOhXzt8TxfYBvSro/W1NTYqVOn5v01ZcoUO2XKFDt69Gg7evTootS7X79+tl+/fnbixIl24sSJ%0AHfY3ZswYO2TIEDt16tRK6aOtPR1zp0+fbqPkjTfesJMnTw782Sc+8Qn75z//OfWYxB9LZtgefqdE%0AKWMu+T760Y9aYJWNUT5ri9+GvuezEWvDPPpoxR9nfM9n1UdD53sb5grYZbM4n+z5TLVA1tq5xpht%0AJK6dvqjU+yuhxqDCYuYbOHAgAMOGDevweM+ePVRXJ5rKjR51/r5u3ToAWlpaANi7d28hVensUaDa%0AWntxzEebMvE6X/s+Wl1d3W0fdf3mnXfeAaC5ublk9Ro5ciQABx10ENBxJDQPvrRhnOveRX19PW++%0A+Wb7IkPiUmsvdM63YcMGgKIefMut0toQz/Ml+6iOMzFSafnURytLIYsVbaTjyVkDmd/UztvGUT8y%0AZ/QhH/ifL9MH35d84Fkb1tTUdP6jilf5chS7jLNmzeK3v/0t1lqeeeYZAKy1mzJsHvt8gwYNgswn%0AorHLB5XXhuB3vmQf7U7sM4LfbQh+51MfjV++QhQyIroEGG+MGUviTTsP+HR322b7wn379gXggw8+%0AKKB6RbeLzBlzyudGN/ft29ehfOrUqQD07t0bSM8DbW1tTc3JGzp0KADDhw8H0nNBR48eDXQdyUr+%0AZYn33nsvm6p1my/Z1iUzYsQIAE444QQgPVLmRn3feustIDF/oP3jHEbtqoB7A8oLzte5Td3o3iGH%0AHALAhAkTOoxwA2zalDgGrV+/Hki3VYEK6qNbt24FSjMSOmbMGACOPfZYAA488EAAtm3bBqRH9Vtb%0AW1PP6du3L7t372b37t2uqKx9NFs1NTWpvmCtxRhDnz59Utnc5zZHOR1nwnD++efzxBNP0NzcTEND%0AA9/5zndSfzj44he/yGmnncbChQtpampyx/XuJhWn2nD69Okh1L5nuea75ZZbmDlzZqaXi1w+KE0b%0AhlDtrKmP5tRHoULaMIRqZ019VH20k8i1YSnlfSJqrd1njPky8BCJ/+D/2lq7sodt7893fxHQD/h9%0AUEZP8vUmseLXMSRWJ03p1NZx95Ax5lvW2l+5Ao/yZdOGseqjxhgaGhp4/fXXIWZ9tLq6mn79+gHp%0AP0y4P0J0cyJ6gDFmA9Chj0I02/D222/v9ufGGH7+85+3f1ybRb7ItGGu+ZKmGGMuikM+KFkbqo+G%0AJM8+quOM//nUR0PkexvmKWMbtlfQHFFr7UJgYbbbZjsHzI0MutG/hx9+GKD9qEgHY8aMYezYxECI%0AG0V1I4ObN28G0v8B7DwKmYMXrLXfy/TDbPK5ET6bXMl2y5YtHX7uRswGD04suOhGiJ566qnUiFtd%0AXR2Qnl/n9ulGTN2Ii3vtLEdCAZZZa2dk+qFr62LNvxs3bhwAn/vc5wC48MILU/ndSOHSpYnFxFau%0ATJxz7Ny5M/B7DpZnyphrPjcifdJJiYWSXV99+eWXARgyZAgATU1NAEyePJkdO3YA8OyzzwKwbNky%0AoGgjoZBFG7qrDTJ5++2Mt1rNmVvpzf1ls76+HkiP+CdPLlOf0UzzmgcOHMjAgQNZsWJFqH20s5Ej%0AR6YyuXZ1mVx7uznark3d6P2rr74KZPV5zNhHIZFxxoyMP46DHvMBC2fMmBHn+UF/zfRL15N86qNq%0Aw6ir+Hyoj0ZdxbehU8gcUREREREREZGclXzV3Hy4+wn+wz/8A5Aefbr55psDt1+7di1r164NpW6F%0AcCM/mbiR38MOOwyA5cuXp37mRnLdaKH7HjXuenY3Qu1Gf90liqtWrQJg48bEejO///3vAXj88cdT%0AI4URmxvcQU1NDQCzZ88G4JJLLgHSI9W33XYbAI899hgADzzwAAALFixgxYoVoda1XNzn1o2EulH7%0AAQMGAOmrE9zooJsj6kYTS62nOeiHHnpoh+9nnHEGACeeeGJq5NvNW3ar4P3lL38B4MknnwTg6aef%0ABtLzmEVERESkI42IioiIiIiISKgiOSL6xBNPAHDWWWcBMG3atJxfw43CuBG5OHAjRW4V1TjatWsX%0AkJ6r6uZxutHezqNeUR3ZzeSII44A0nME3Qjp888/D8BXv/rV8lSsTNz7MGbMmNRqx+4KhgMOOACA%0ANWvWAOnPtWvz9qvjhmnUqFFAesEgN7/6mGOOAeD0008H0qv6uqsRli1bxmuvvQaQGt12y7DH4YoM%0AERERkSjRiKiIiIiIiIiEKpIjos4f/vAHAH75y18C6dEXN8LSnTiNhHb2zW9+E0jPYfvmN79ZyGq/%0AoXJzQN1337iVXZ977jkgvZJz+/m8leBjH/sYkJ77u3PnTh599FEgPdLZecTTzassNzdH1M0BdVdP%0AuNV8XY53330XgMWLFwPxvlJBREREJGqi8T9DERERERERqRiRHhFdtGgRkL6/5mc+8xkAvv3tb5er%0ASiXlRnwvv/xyID0P7f7770+Nxrh5bW7E190vNS4jpnHn5oK6uaFuHqGb31sp3KhhLg455BAgvXpu%0AubjVq91nya3S7T5vldaWIiIiIuWgEVEREREREREJVaRHRJ158+YBsGPHjvJWpMTcHDU3Z82tOtrQ%0A0JC656Ir69+/P5AekXv//feB9L0Zm5ubQ6p1ZXKrA7v7pbq2iys3wu5WvHUj7cXkVpx13BUAbs5m%0AWHNIN2zY0OH7QQcdBGgkVERERCRMGhEVERERERGRUMViRPS+++4DYPDgwQCMGzcOSK9YOmXKlNQI%0A4ZIlS4Bo3tdv0KBBQHo1zs7c6OZbb70FwKuvvgpAfX19al6dW6X07bffBtKruLqfuxFRN0LnRpFd%0Aebl9+MMfBtL3iK2vrwcSI3DuPpNuteQoGzp0KAAnnngiAH/961+B9D1vO6+ie9hhh6VWkc1m1eew%0AuTnGpRgJzcS9D1VVVQCMHz8eCH902d1HtLOJEycC6Tml27ZtC61OIiIiIr7TiKiIiIiIiIiEKtIj%0AoiNHjgTSI4hu5Vg3KtinTx8gMW9ywoQJAEydOhVIzzd78MEHAXjyySdDqnVmPd3b1K0S7FYHdvfi%0A3L17d+qeh25E0c2vc6OobsTTjTK5Ea4hQ4YA6RFUN7pTKq6ep512GpAeOXQjtjt37gTSo16NjY1A%0AYlR79uzZQPq+sb/5zW8AuPLKK0ta53x88pOfBGDSpEkAvPTSS0DmeYa9e/dOZR4zZgwAmzZtAtIj%0A++XU+Z6fPXGjhS53IdyIpHst99qlHhl1V1G448yMGTMAOO6444B0O7k5rHv27OH111/vUNd7770X%0AyHyVg4iIiIgE04ioiEh0DCt3BXLx4IMPMmHCBJqamrjuuuu6/HzevHnU1dUxdepU90fCWOUD/zMq%0AX7zzgf8Zfc8HuWUEJhljLg69kgXwvQ19z1dKkR4RdfMHly1bBmQeUVyzZg3/93//B8AxxxwDwMc/%0A/nEALrroIgBOP/10AG699VYgfc/AMLnRwJ48/vjjXcoWL17c4fuoUaMAOPnkkwGoq6sD0qvnuvm0%0AH3zwAZAekXQr8hZ7LmB1daIrTZ8+HUiPWrtR3RdeeAGAF198MeNruFHeL37xiwAcf/zxQHqu7PXX%0AXw/AL37xi6LWPRdutGzmzJkA3HPPPQB84xvfAODll18OfN7SpUszvqZbCbmcI6PZziH+/Oc/D6Tn%0AJr/yyitA+p6cxeBGG4t931H3PrtRePf94IMPBuDTn/40kB7Nf/PNN4F0H96xY0dqFHXKlCkAjBgx%0AAkj30YULFwIFrcAbm+Wu29rauOyyy3jkkUdoaGhg5syZzJo1K3WVgDN79mxuuukmAIwxsckH/mdU%0AvoS45gP/M/qeD3LPaIxZZa29uUzVzZnvbeh7vlKL9ImoO3np6ZJWSF9y6i6Vc9/dJZQXXnghALfc%0AcgsAP/jBDwD4/e9/X8Qah8ddpvzf//3fABx11FFA+nJCx/3n2y0I49TW1gLp/2wXyl0KfPvtt+f9%0AGu6y4q997Wsdyq+44gogfdLt6n7jjTfmva981dTUAPCTn/wEgAULFhT8mlG4NLenE6drrrkGSF/G%0A6k5A3R9ESrE4mLvdi9tHodzJsvsjiTsRdQtMuffgu9/9LgB/+tOfgPQfc4YMGZL645j7Q49rO3cb%0AH/cHsMcee4z9+/d7fUuYxYsX09TUlFo87rzzzmPBggVdfvnGme8ZlS/+fM/oez7wP6PySXd0aa6I%0ASAlk8we0ADVBhcaYOcaYpcaYpVu2bCmsYkWycePG1Mk8JO53vHHjxi7b3XXXXUyZMoVzzjkHMuQD%0A/zMqX/jUR9PUhtHMB7lnBMYZYxq7bEA0M/rehpVwnCmlSI+IutEGd9mnG3XLxf/+7/92+O4WwPnV%0Ar34FpEcQf/jDHxZU12Jwi6K4EZhcdL50140eussa3X+K3aWFhx12GJAeMY3iLUWcG264AYC7774b%0AgDlz5gDpRYzc6GQY3IJS7tJUX7hR5s7OPfdcID0C6BbrcbercYtlTZw4MXVZcp4nYBlt3bq1KK/j%0ARrPdd3eAv+OOOwBYt24dkF5EqrOqqqrU4ltulNZdPjxsWGK6hzuefOQjH2HTpk35TAEYG1RorZ0L%0AzAWYMWNGcd/gEvrkJz/J+eefzwEHHMAvf/lL7rrrrsB84H9G5Ysm9dE05Yuu9hmNMTuB3wAf7bxd%0AXDP63oaVcJzJl0ZERURKIM9Vf/sWux6lUl9f3+HS/g0bNqQuXXaGDh2a+mPXxRdfDDHKB/5nVL54%0A5wP/M/qeD3LPSGItgemhVbBAvreh7/lKLdIjok4+I6GZuLmibtTm3//93wHYvn07kJ5zWQ75jIRm%0A8tBDDwFw5plnAunbt7gRHbeo06BBg4Boj4g67oPuFis69dRTATjrrLMAmD9/fsnrUMqRUHdphxux%0A27FjBwCEz3AqAAAgAElEQVQtLS0l37eb++m+O24RHvc9k969e3PEEUd0KFu9ejWQ/xxYt7CWO6C7%0Aean5cp+v7haO6k5bW1tqLrr77uaxujmi7kqDUaNG0dDQwPLlyznkkENS70V1dTXGmNTtpQLem9zu%0Ao1NGM2fO5NVXX+WNN96gvr6eO+64g9tuu63DNps2bUr1qeS8/djkA/8zKl+884H/GX3PB7lnBA4E%0AVoddz3z53oa+5yu1WJyIiojETa9evWhoaEhdzgyJk+u2tjastV0WEEtaG1b9ClVdXc1NN93EySef%0ATFtbG1/4wheYPHky11xzDTNmzGDWrFnceOON3HvvvVRXV7t7Gq8tc7Vz4ntG5Yt3PvA/o+/5IPeM%0AwEHA6WWudtZ8b0Pf85WaKfZ8rm53ZkzkrnW+8847ATj22GOBblfoXGatndHda0UxX2duVU83Euzm%0Azn7rW9/qMR9EM6MbQXT/sW9tzfiHpki34cCBA4H0yKgbAXXzD90oXDej1z3m69u3r50wYUIRaps9%0A1x7utizZcrcCcp/J+fPnR7aPur43fvz4Dt9d9s7zT91tltzcXLei7549e3rMOGPGDJvvqG65GWOy%0AakPfMypfdKmPJihfdKmPJvieDyojo+aIioiIiIiISKgq/tLc2bNnA/Doo48CpO4D1P5yOp+4Ubcj%0AjzwSgHnz5pWxNsXhRg7jvpLtzp07AVi5ciWAW6Y9NVrvVmAt5Xxe9x6+8847ADQ3d7znsht9dvM2%0A3aq53XGL9tTV1QHp1WozcSPAbnVkNxIcxhzgfLW1tQHwxhtvAOk6u8+by+Tu/+tGQIcPHw6kV68u%0AdB6siIiISFxoRFRERERERERCVfEjos71118P0GEJ5rBNnDixQx3cPLJcuFW5Mt0L0c23e++994Ce%0AV0OV7rnVT90IVzG5lVg/8YlPAOl7d5aCW5na9Yc9e/YEbudGTIN+7uabu3mRblTVrf6brUsvvRRI%0Ar3D985//PKfnl5NbBdd937ZtG5B+T9z3ESNGANDU1AQQePNrEREREZ9pRFRERERERERCFckR0Q99%0A6ENAer5VptG9YvrLX/4ClHaeYUNDA5C42W17bh6Zux+mG5VasmQJkJg76OaQuTl57p6IyWWgUz93%0AIzGZ3jO3/cKFCwF45plnCsoURW4l0kyjesUwefJkID2y9dhjjxXttfv16wek7/W6a9cuAB555JGi%0A7aMz15+yfc/eeuutDt+L6dprrwXSI6q5rrYbRe4+vq6/nH322UD6fX/uuefKUzERERGRMtGIqIiI%0AiIiIiISqxxFRY0wj8FtgOGCBudbaG4wxQ4A7gTEkbsz6KWvt9mJU6uijjwbgy1/+MgCXXHIJkN+c%0AyWy5FUu7EXj3+VxkGqV0q6NOmjQJSM/XW7t2LQBjxoxxNzHucv9Bx43kupVVO6utraWtrY0bbriB%0AI444gmeeeYY9e/awfXuqycYbYwbn24aufm4Uz62u+vjjj+fzct069dRTgfRo0uLFi9m9e3f7ezFm%0AemrBbTh69GgAZs2aBaRHK5ctWwbkNh9y8ODBAAwaNKjDa7tVct0I+r/+67/y2muvZfOSeefbunVr%0Avk8tmiOPPJLt27czefJkLrnkEkaMGMF7773H/PnzXVsX1EfLwa0Y7PrsUUcdBaQ/4+4+xm4uqYiI%0AiEilyGZEdB/wz9baScAxwGXGmEnAVcBj1trxwGPJx7FVU1OTOpnKYERYdSmFAQMGUFdXxwUXXMDy%0A5ctpaWnpfHLTQozbsLa2FmNMT5vFsg2NMamT1B7EMp9z+eWXc+utt/L000/zn//5n2zYsIEFCxYw%0AdOhQPvKRj0DM+6iIiIiIpPU4Imqt3QRsSv67xRizGqgHzgA+nNzsN8ATwNeLUSk3X9Pd4/PHP/4x%0AkF49091rz426tR8pdScjbp5d//79gfSqpm7EqvOoWRZzQwfnHKQTd6/BztzKma6uVVWJgS1378G2%0AtrbUHFA3qurmgrq8bvQ0k8MOOwxIjB4aY3jqqac6b7IVOJM829DV3Y3i/e3f/i2Qnu/nVlR9+OGH%0AgfS9MN0qorW1tal7U7p7uboVfg8++GAg/f49++yzANx6661AenTJ7aMbBbfhRz/6USA9er1q1Sog%0APXe384jotGnTgEQ+NwLqcvXt2xdIz+91/dm9xr/8y78A6XtPZiHvfO+++26+Ty3Y7373OwCOO+44%0AAO677z4GDBjAAw88wJIlSxg7dqybL1pQH3XHhPHjx3cod/3HcXO4cxnddp9V17au3V1/cXOzX3zx%0ARQD++Mc/dngsIiIiUmlymiNqjBkDHAk8CwxPnqQCvE3i0t2g58wxxiw1xiwtoJ5REHjSHrd8ra2t%0AqVu3dLIXtaH3+TqfdEXRli1bWLduHcOGDaO1tbX9ZeiV0EdFREREKkLWq+YaY/oDdwFfsdbubH8Z%0ApLXWGmMCh6OstXOBucnX6HHIChKjdgA/+9nPgPRo3vHHHw+k76/oRjG3bduWmuPpRjrdqIZbrbJU%0A8snX2bx584DEf8CB1CWz7nFVVVVqDl+mUdVMhg9P/L99zZo17N+/v6cViPNuQzd6dMcddwDw9NNP%0AA3DuuecC8OlPfxqAf/u3f+vwPHdS3NbWlsq7cuVKAF5++WUg/f641yx2m+bShm4u36JFi4Cu8377%0A9OkDpEeE3SW11dXVqRFbdzLoRsPcKsk9jWrnq32+vn375tVHiyl5mS1z5swB0ve+feihh2htbeXa%0Aa6/l8MMPZ/369ezfv7/ziGXefdSNOLuVaw899FAgPVLqVq92+3N92n2vrq5Ozed1I5xuBNQdk1y5%0AOw65tnWfC3dFQClWGxYRERGJk6xGRI0xNSROQm+11t6dLH7HGDMy+fORwObSVDEyoj+U1A1rbU8L%0AotSgNoy7WOdra2tj7ty5jB07NnWCV1NT0/6y+UrooyIiIiIVIZtVcw3wK2C1tfY/2v3oXuBC4Lrk%0A9wXFrpybA9iZW4nS/Qc1l1HCXr0S595udCqLeYVO9hPG8nT//fcX7bVczs2bN2ebcShwW7H2v27d%0AOiA9v9d9d9z8SDcftqWlpVi77k7BbfjnP/8ZSK8SvHlz8HnRo48+Wuiu8pF3PjfSl+tcUfc+7Nu3%0AL9WWhxxyCACNjY1Aev6km/s7ceJEIL1S9dNPP421lj/84Q/U1NTQu3dvVq5cyf79+xkwYADbtm1z%0AI/sF9dHO88RdVpfdcaPabm7y0KFDgcTop7sSxI3ku/npzz//PJAexXf3AHb3QG1ubs632iIiIiJe%0AyubS3L8FLgBeMMa4e4N8g8QJ6O+NMRcB64BPlaaKkdHtNa0eGEiiTX3mexvGNt+mTZt4++236d+/%0Af+qkbeTIkQwfPpy1a9e6S9MroY+KiIiIVIRsVs1dBGS6L8aJxa1OdtxKq/lwoyJ5yG1yZpnlkfMV%0Aa21oNzN09wANWcFtGPH7Peadz81xfPPNN4Ge7yvqRgvdKGd9fX1qFN6NILqVnd18STc66OaAu9FE%0A93meOnVq4L7cqtIrVqwoqI+6EVg3N9ed8LpRTDdXtfMIqZszunXr1tRruOe6x2501d1XNocrLTqr%0A7XmT6HjwwQe54ooraGtr4+KLL+aqqzreXWf37t189rOfZdmyZa5fxCof+J/R93yQW0ZgojFmjLV2%0AbTnqmg/f29D3fKA+qjasXDmtmisiIiXVUO4KZKutrY3LLruMBx54gFWrVnH77benbmfk/OpXv2Lw%0A4MGsWbOGK6+8EmKUD/zP6Hs+yD0j8A7w7+Woaz58b0Pf84H6KKgNK1nWq+YWSTPwfvJ7FA0jc91G%0AZ/F83/OB/xm9zrdr167mFStWFJzPrfpaotVfM2UsSh918zvdvWzd9xLoBxwMvJp8PCL5fV+7uo0H%0A3iJRX4BpxhhjCxhSDcvixYtpampKjYqfd955LFiwIHWPXYAFCxbw7W9/G4BzzjmH888/f0Bc8oH/%0AGX3PB7lnBLYDJ8Ylo+9t6Hs+UB8FtWHY9Y0SE3Z+Y8xSa+2MUHeapWLUzfd8xXydUlAbhvMapeRL%0ARmPMOcAp1tqLk48vAI4GjnF1M8a8mNxmQ/Lxa8DR1trmTq81B5iTfHgY8GI4Kbo1mMS83XXJx0OA%0A/sD6dttMBl4hcQ9YgGnAQZ3zgf8Zla9scs04gcTq3BX3OVS+slEfVRumRDhjriZYawf0uJW1NtQv%0AYGnY+wyzbr7nq4SMyqeMRcpxDnBzu8cXADe1rxuJXzAN7R6/BgyLerbu8nXapnO+1p7yVUJG5Ytu%0ARmCpPofKF+WM6qPRylcJbZjne5JV3TVHVESkNDYCje0eNyTLArcxxlQDg4DuV4qKjnzyVRGffOB/%0ARt/zQY4Zk/Q5jA7f84H6aIdt1IaVpRwnonPLsM9sFaNuvucr5uuUgtownNcoJV8yLgHGG2PGGmNq%0AgfNI3H+5fd3c/Zgh8RfVP9nknxJjIFO+9jrna4lRPvA/o+/5IPeMg9HnMEp8zwfqo6A2rFzlHrrV%0Al770pS9fv4DTSMwJeQ34l2TZtcCs5L97A38A1gCLgXFZvOaccucqIN83snxdrzMqX6QzvqHPofJF%0APKP6aMTyVUIb5vF+ZFX30BcrEhERERERkcqmOaIiIiIiIiISqtBORI0xpxhjXjbGrDHGXBXWfjPU%0ApdEY87gxZpUxZqUx5opk+RBjzCPGmFeT3wfn+LpeZ1S+cPmeUfnyO86IiIiI+CCUE1FjTBXwc+BU%0AYBJwvjFmUvfPKql9wD9baycBxwCXJetzFfCYtXY88FjycVZ8z6h8ZeF7RuXL/TgTmRPtXBhjfm2M%0A2WwS903tbjvliyjfM/qeD/zPqHyp7WKZD/zP6Hs+yD5jSkgTVo8FHmr3+Grg6nJPpG1XnwXAx4GX%0AgZHJspHAy8qofFH58j2j8vX4/CoSiyCMA2qB54FJ5c6VZd1PIHGD8heVL375KiGj7/kqIaPyxTtf%0AJWT0PV+2Gdt/VVMAY8wpwA3JN+1ma+11GTatB940xnRYGckY8/1C9l9ks5Lf3zLGpArb1Xk/iVWw%0Aust4UlQy9urVK+jxrJqaGoC3evfuzQEHHMD+/fvp1auXtdZmk69obdj+PXaSHbgQPbXh+9ba/hme%0AW7Y+2rmtnM7vRzKTW30tKGOs+mgG3bVhQX002fe7qKqqCixvbW3Npd7ZKuQ4cxSJ1fZea1e2Muiz%0AlK9MfTHTe5TJ3r17M/7MGHNVd/msta+3ez+Kmi8Mxpj3MhxnYpVvxIgRHR4PHjyYnTt30tbW1mMb%0AUsI+GgZf+uikSekLQt577z1qa2ux1rJ379793TytItqQHPMdfPDBgeUjR47sUvbBBx8Ebrt69epu%0A95ELX/pod/Jtw87HLifT/wHefPPNguqZrwpow7ZsNsr7RLTdZXAfBzYAS4wx91prV+X7mhG3gsSl%0AfrHI2Ldv38Dy4cOHp/5trWX9+vU0Njayfv36UPP17t27S9muXbtKvdtexphJUWu/fv36BZbv2bMn%0AsHz37t2ZXipWfTQPBeXL9MtpwIABgeWrVpXlLewuYz1Q0t+Y/fsH/51m0KBBgeVtbcG/Z956661M%0Au2iljPlCkuk4E6t8n/vc51L/3r9/P3PnzuWSSy7hF7/4he9t6E2+O++8E0h8Tk8//XTuueceRowY%0AwbRp00w3vwtjlTGDorfhZZddFlj+jW98o0vZc889F7jttGnTct1tJt700W7knfHzn/98YHmm/wNc%0AccUVeVaxIJXQhpn/It1OIXNEU2fs1to9wB3AGRm23Qg0FrCvKLD0nDFWdu/eTU1NjfsrUTb54t6G%0A2/A7n3d9tJNK6KM9ZfSB7/m6O87E0qZNmxg8eDAHHnigK/K9Db3K98ILLzBq1CgaGxvd7/s2PMqX%0AgVdtGMD3fOB/Rt/zZaWQS3M7n7FvAI7uvJExZg4wBzi8gH1FRZeM7fLFzr59+6iu7tAFfG/DPST6%0AbQce5QPP+mgA3/soZG7DK4Hg68PiJbANgRnAucaYqSHXp9gCjzPEOF9LS0vnKwfUR2Nk8+bNnUeD%0ALJl/F3rbhr7nS4plH81AbVgBSr5qrrV2rrV2BnBWqfdVDi5fMqOXKqUN8Tyf+mh8WWvnkjjJ3lru%0AupTQv5LId265K1IiXudTH40/39vQ93xJ6qPx53UbdlbIiWjny+Aa6ObSP2vtwgL2FRXdZoyb6upq%0A9u3b177I9zasxe984FkfDeB7H4UMGa21+4Avh1+dojoAuInEasEdtMv3UNiVKrJReJZvwIABtLS0%0AuIfZtGGceddHDzroIN5+++32RdXABcaYizpvW0FtGGfe9dEAakMP2tAYsyHoONOeyXelUmNMNfAK%0AcCKJ/zQtAT5trV3ZzXMCd5ZpRcZME45XrFjRpay5uTlw27Vr12aqTq6Wkzh4Z8yYKV9M9JgPoKGh%0AwV5++eVdyleuDH7K7373u2LVrxh2ATPz6aOZzJ49O7D8Rz/6UZeyxsbg6YvPPvtsYPn1118fWP7I%0AI4+k/m2tpaWlhX79+tHS0tJjG9bW1tqgCfvlWjUuR1n1Ud8/h5nyjR49OvAF283r62D79u1dyt59%0A993AbYNWhgR46aWXAsu7UQltWPTjTMTodyGxz9hjH62trbUHHXRQl/KNG2Pxd84e27Bv37720EMP%0A7VL+wgsvBL7g/v3dLTQcukroo3kfZzKd13zlK18JLL/hhhvyq2FhKqENl2VzJV7eI6KdzthXA7/v%0A7s30wGT8zuh7PoBtvuUzxtCnTx/ef/998L8Nfc8H/mf0PR94eJzpxPc29D0fqI/Gne/5wP+MvufL%0AWkH3EU1eBufDpXDZeNFa+71yV6KEfM8H8HbPm8SPW/l4x44dvreh7/nA/4y+5wNPjzPt+N6GvucD%0A9dG48z0f+J/R93xZK/liRSIiIiIiIiLt6URUREREREREQqUTUREREREREQlVQXNEi6WtrS2w/Oab%0Aby74tf/rv/4rsLzdUvQdPPbYY4HlDz0U91WUi2Pjxo1cddVV5a5GZNx5551Zlw8aNChw26uvvjqw%0AvLW1NbB8x44dWdauq71798ZlhdzQnHzyyYHlUf3MDxw4kGOOOaZL+cMPPxy4/bp16wreZ6bVdDOt%0AeD506NDA8s2bNxdcF/HPj3/848DyoP5yyy23BG47cODAwPJRo0YFlj/xxBOB5fneSSCKMmUxxvT4%0A3L1798Zlhdy87Nq1i+eff77c1QBgyJAhgeWZ+m7QnSMkLZv+LZkdffTRgeXXXnttYPlJJ50UWJ5t%0AO2hEVEREREREREKlE1EREREREREJlU5ERUREREREJFQ6ERUREREREZFQ6URUREREREREQmXCXCHO%0AGBP55egGDx4cWL59+/Zl1toZ3T135MiR9qKLLsr6Nb/2ta/lUcOS6TEfxKMNu9Fjxijl+9znPhdY%0APnbs2MDyb33rW7HKl4eS9NGPfOQjgeWHH354l7L6+vrAbevq6gLL+/TpE1g+Z86cwPKWlhYv2jDT%0ACqY7d+7UcQY48sgjbdCqrQceeGCp6lRMPebr06ePHTNmTJfy5ubmwO33798fWL5t27bca5elmpqa%0AwPK9e/eqj5J7vl69gsc1pkyZ0qUs0/+JMq1im0c/8OI42g31UTLny7Ta8Pr164tQraLxpg3PPPPM%0AwPJ77rknq4waERUREREREZFQ6URUREREREREQqUTUREREREREQmVTkRFREREREQkVNXlrkDUbN++%0APe/n1tfX893vfjfr7SO2WFFWpk+fztKlS7uUb9myJXD7448/PrD85ZdfLmq9iqVXr17069evS/nH%0APvaxwO3Hjx8fWL548eIuZUGLk3Rn3rx5geVNTU05vU42/u7v/i6wPFO7zpo1q0vZokWLArfNVB4V%0Ajz/+eE7lQWbMCJ6P/6Mf/SiwfOfOnYHlxpis9xllmfJJQlVVFYMGDSp3NUqmtbWVl156qdzV6Nbe%0AvXvLXQWvZFpwasWKFV3KDj300MBtP/vZzwaWZ1rEKJdjdGcDBgzgmGOO6VL+yCOP5P2aEq6BAwdy%0A7LHHdik//fTTA7e//PLLS12linTPPfcU9HyNiIqIiIiIiEiodCIqIiIiIiIiodKJqIiIiIiIiIRK%0AJ6IiIiIiIiISKp2IioiIiIiISKiMtTb/JxuzFmgB2oB91trgpSPT2+e/s/LbBazqLmNDQ4P9p3/6%0Apy7lX//610tZr2LpMR/AIYccYn/wgx90Kc+0atbChQsDy99///0uZTU1NYHbDhkyJLB848aNmaqZ%0AyQfW2q5L4rbT1NRkf/jDH3Ypz7TK39atWwPL//jHP3Yp+9Of/hS47apVq7qrUi56bENjjO3Vq+vf%0AnwYPHhy4faZ8Qc4444zA8vnz5weWDxgwILA8qG8kZdVH43CcGTlyZIfHmzdvxhjDvn37smrDUtev%0As871dQ444IDA8rVr12Z6KW/asBs9HmcmTpxob7755i7lN9xwQ+D2QceTMopkHy2iSuijkWzDTMeZ%0AL33pS4Hlu3fv7vD4Zz/7GbW1tWzZsiWS+YpIfRT/80HmjCeddFLg9pn+X7xy5cocq1cUy3rKB8UZ%0AEf2ItXZqNjuLuR47TMz5ng9gdbkrUGK+t6HX+YYOHQqeZ8T/fKDjTNz5ng88znjBBReAx/mSfM8H%0A/mf0PV/WdGmuiIiIiIiIhKrQE1ELPGyMWWaMmRO0gTFmjjFmqTFmaYH7Kre/CcrYPl83lxTGQWA+%0A6Jgx5jeqHxZU2D7fu+++G3adiqnHPlqOShVRVn007EoVS/IyaLVh/DP2eJzZsWNH2HUqJvVRTzP6%0AkO+2224Dj/MlqY96mg+8ypiVQk9Ej7PWTgNOBS4zxpzQeQNr7Vxr7QwPhqBfJSBj+3z9+nU7LSjq%0AAvNBx4wDBw4sQ9WK5qCe8g0aNKgc9SqWHvtomepVLFn10TLUq2BDhw6lrq4O1IY+ZOzxOHPggQeW%0Ao17Foj7qaca457vwwgu5+OKLwdN87aiPepoPvMqYlYJORK21G5PfNwPzgaOKUamI2offGX3PB7AD%0Av/P53obe5quqqnL/9DZjku/5QMeZuPM9H3iasd0fyr3M147v+cD/jL7ny1req+YaY/oBvay1Lcl/%0APwJca619MNNzxo8fb2+88cYu5cOGBV7JxMyZMwPL33jjjS5lS5YsCdz2d7/7XWD5o48+Glje2toa%0AWA48B7TSTcaYr+DVYz7IPeNxxx0XWD5x4sQuZYcffnjgtplW5rz66qsDy7dv356pOu8D53SXr66u%0Azp599tldyseOHRu4/R133BFYvmLFiky7KKUe23DIkCH25JNP7lKe6ZLkTJ+HL3/5y13K/v7v/z5w%0AW2NMhurmrCR9NGKKfpzJ9BkM6gfjx48P3HbMmDGB5Y2NjYHl7S89/eCDD7DW0q9fPyZPnlwJbdjj%0AcSbm+fS7EP8zRilf0O9k6Lia9Pvvv8/+/fsZMGAAxphY5cuDN320/TlIsdow05V7EZta5k0bdqPk%0Aq+YOBxYZY54HFgP3d/dmeuBv8Duj7/kAdniez/c29D0feJhx69atfOYzn+Gss84CD/MF0HEm3nzP%0ABx5mfOeddzjuuOM44ogjwMN8nXiZT21YmarzfaK19nXgiCLWJepWWmu/V+5KlJDv+QDeLncFSsz3%0ANvQ9H3iYsbGxMXUv2cmTJ3uXL4COM/Hmez7wMOO4ceN4/vnnATDGeJevEy/zqQ0rk27fIiIiIiIi%0AIqHSiaiIiIiIiIiESieiIiIiIiIiEqq854jmY82aNZx22mldyk866aTA7b/5zW9m/dpBK+kC3Hff%0AfVm/RiW48sorA8t/8pOflGR/ixYtyqm83Jqbm/nlL39Z7mqUzPbt2zOu9JuLxx9/vAi1kWL60Ic+%0AFFh+/fXXB5YfdVTXVePvueeewG2POeaY/CsmFaeuro5PfepTXcozrax9wgldbqUHQHV11/+iPPXU%0AU4HbPvDAA4Hlb78dPGV33rx5geWSUFtby8EHH9ylfOvWrYHbt7S0lKwud911V2D5JZdcUvR9NTU1%0ABZbv378/sPz1118veh0qyRe+8IWiv2Yp+6IUn0ZERUREREREJFQ6ERUREREREZFQ6URURERERERE%0AQqUTUREREREREQmVTkRFREREREQkVMZaG97OjNkCrEs+HAY0h7bzwvc52lpb190GEchXyH57zAeR%0AyOh7G/qer5D9qo/ifz7okDFufRTUhlHIV8h+dZxB+UKiPpqZ2pBIZCz97/swT0Q77NiYpdbaGb7u%0Asxz5wt6v2jDe+1Qfjf8+lS/++1Ubxn+/asN471N9NP77VBvGd5+6NFdERERERERCpRNRERERERER%0ACVU5T0Tner7PcuQLe79qw3jvU300/vtUvvjvV20Y//2qDeO9T/XR+O9TbRjTfZZtjqiIiIiIiIhU%0AJl2aKyIiIiIiIqHSiaiIiIiIiIiEKvQTUWPMKcaYl40xa4wxV4W437XGmBeMMSuMMUtLvK/QM/qe%0AL7nfUDL6ni+5L/XR0uzX64zKV9R9qY+WZp9e50vu1+uMylfUfamPlmafXudL7jecjNba0L6AKuA1%0AYBxQCzwPTApp32uBYb5m9D1fWBl9z1fOjL7nq4SMyhfvfJWQ0fd8lZBR+eKdrxIy+p4vzIxhj4ge%0ABayx1r5urd0D3AGcEXIdSs33jMoXf75n9D0f+J9R+eLP94y+5wP/Mypf/Pme0fd8oZ+I1gNvtnu8%0AIVkWBgs8bIxZZoyZU8L9lCuj7/kgnIy+5wP10VLyPaPyFYf6aOn4ng/8z6h8xaE+Wjq+54OQMlaX%0A6oUj6Dhr7UZjzEHAI8aYl6y1T5W7UkXkez7wP6PyxZ/vGZUv/nzP6Hs+8D+j8sWf7xl9zwchZQx7%0ARHQj0NjucUOyrOSstRuT3zcD80kMd5dCWTL6ng9Cy+h7PlAfLRnfMypf0aiPlojv+cD/jMpXNOqj%0AJeJ7PggvY9gnokuA8caYscaYWuA84N5S79QY088YM8D9GzgJeLFEuws9o+/5INSMvucD9dGS8D2j%0A8hWfQGAAAAC/SURBVBWV+mgJ+J4P/M+ofEWlPloCvueDcDOGemmutXafMebLwEMkVoL6tbV2ZQi7%0AHg7MN8ZAIvNt1toHS7GjMmX0PR+ElNH3fKA+WkK+Z1S+IlEfLRnf84H/GZWvSNRHS8b3fBBiRmMT%0AS/SKiIiIiIiIhCLsS3NFRERERESkwulEVEREREREREKlE1EREREREREJlU5ERUREREREJFQ6ERUR%0AEREREZFQ6URUREREREREQqUTUREREREREQnV/wdMIV737w01twAAAABJRU5ErkJggg==%0A" /&gt;&lt;/p&gt;
&lt;p&gt;有看出什麼規律嗎？哈哈。&lt;/p&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>[吉他] 方大同-三人遊</title><link href="https://ycc.idv.tw/uwarn-performance_1.html" rel="alternate"></link><published>2017-11-09T12:00:00+08:00</published><updated>2017-11-09T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-09:/uwarn-performance_1.html</id><summary type="html">&lt;p&gt;有窩表演&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;div class="video-container"&gt;
    &lt;iframe src="https://www.youtube.com/embed/8lwc81a9mAo" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</content><category term="Life"></category><category term="吉他"></category></entry><entry><title>[吉他] 光良-傷心地鐵</title><link href="https://ycc.idv.tw/uwarn-performance_2.html" rel="alternate"></link><published>2017-11-08T12:00:00+08:00</published><updated>2017-11-08T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-08:/uwarn-performance_2.html</id><summary type="html">&lt;p&gt;有窩表演&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;div class="video-container"&gt;
    &lt;iframe src="https://www.youtube.com/embed/7KpOh8N-9Z0" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;</content><category term="Life"></category><category term="吉他"></category></entry><entry><title>實作Tensorflow (2)：Build First Deep Neurel Network (DNN)</title><link href="https://ycc.idv.tw/tensorflow-tutorial_2.html" rel="alternate"></link><published>2017-11-07T12:00:00+08:00</published><updated>2017-11-07T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-07:/tensorflow-tutorial_2.html</id><summary type="html">&lt;p&gt;增加Hidden Layer / Activation Function的選擇 / Mini-Batch Gradient Descent / Regularization / Weight Regularization / Dropout / Optimizer的選擇 / 來看看程式怎麼寫&lt;/p&gt;</summary><content type="html">&lt;p&gt;接續著&lt;a href="/tensorflow-tutorial_1.html"&gt;上一回&lt;/a&gt;，我們已經有一個單層的Neurel Network，緊接著我們來試著一步一步改造它，讓它成為我們常使用的Deep Neurel Network的形式。&lt;/p&gt;
&lt;p&gt;本單元程式碼可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/02_DNN_classification_on_MNIST.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Config the matplotlib backend as plotting inline in IPython&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="hidden-layer"&gt;增加Hidden Layer&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們只有一層Neurel Network，也就是做完一個線性轉換後，就直接使用Softmax Layer來轉換成機率表示方式，這樣的結構並不夠powerful，我們需要把它的結構弄的又窄又深，這樣效果才會好，詳細原因請參考&lt;a href="/ml-course-techniques_6.html"&gt;這一篇的介紹&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;因此，我們來試著加入一層Hidden Layer，來打造成兩層的Neurel Network，並在兩層之間加入Activation Function，為我的Model增加非線性因子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# build neurel network structure and return their predictions and loss&lt;/span&gt;
    &lt;span class="c1"&gt;### Variable&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="c1"&gt;### Structure&lt;/span&gt;
    &lt;span class="c1"&gt;# layer 1&lt;/span&gt;
    &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# layer 2&lt;/span&gt;
    &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;首先在變數需要有二層的fully-connect參數，注意這些參數的大小，受到Hidden Layer的神經元數目&lt;code&gt;n_hidden&lt;/code&gt;決定。接下來開始建構整個Neurel Network的結構，&lt;code&gt;fc1&lt;/code&gt;產生一個fully-connect的結果，並且通過Activation Function再輸出，然後進到下一層，第二層直接使用&lt;code&gt;fc1&lt;/code&gt;的結果當作新的輸入，再做一次fully-connect，並且讓它通過Softmax Layer來完成最後的Logistic轉換，它的loss一樣的是使用cross-entropy來評估。&lt;/p&gt;
&lt;h3 id="activation-function"&gt;Activation Function的選擇&lt;/h3&gt;
&lt;p&gt;剛剛提到的Hidden Layer可以採用不同的Activation Function，我列幾個常使用的Activation Function給大家看看。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;
&lt;span class="normal"&gt;9&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;tanh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;sigmoid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;relu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tanh&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="s1"&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A/j+3uXalz1k0gQAAAABJRU5ErkJggg==%0A" /&gt;&lt;/p&gt;
&lt;p&gt;這些Activation Function的使用時機可以簡單這樣說，當我們想要輸出值在+1和-1之間時使用tanh，而當我們想要一個輸出值衡為正時使用sigmoid，它可以將輸出值壓在+1和0之間。tanh比sigmoid多了兩個好處，第一，tanh的梯度變化大於sigmoid，有利於訓練效率，第二，tanh的輸出均值為0，可以避免將前層的梯度偏差帶到下一層。&lt;/p&gt;
&lt;p&gt;不過，以上的這兩種Activation Function在非常深的網路都會有一個共通問題—梯度消失，仔細看上圖，tanh和sigmoid在極大和極小的地方都會彎成平的，所以每過一次這種Activation Function，訊號就會減小一點，當我們在深網路做Backpropagation時，訊號在過程中不斷的被磨損，到了前面的幾層就已經耗損完畢，此時更新的梯度近乎0，也就是梯度消失，那麼前面的這些層就再也訓練不到了。&lt;/p&gt;
&lt;p&gt;Relu正可以解決梯度消失的問題，如上圖，在正的部分Relu是線性的，所以多少訊號進來就多少訊號出去，如此一來就不會有耗損的問題，但特別注意，因為tanh和sigmoid會將輸出值限制在一個範圍內，所以有Normalization的味道，但是Relu沒有限制，Normalization可以使我們訓練的效率提升（因為梯度的方向可以直指低點），所以Relu常常會搭配Normalization Layer一起使用，來額外做Normalization，或者是最近一篇Paper提到的一種新的Activation Function：SELU，類似於Relu但是輸出值會是Normalize過的，非常神奇，在這邊我不多論述。&lt;/p&gt;
&lt;h3 id="mini-batch-gradient-descent"&gt;Mini-Batch Gradient Descent&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們的Gradient Descent採用的是將所有的Data一次全考慮進去，評估完所有的Data在一次性的更新權重參數，這樣的作法好處是比較穩定，因為我考慮的是真正的Training Set的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;，但缺點就是計算時間長，因為要考慮所有Training Set的每筆數據，需要做大矩陣的計算，而且通常Training Set的數據量也不可以太小，這麼一來計算時間就會拉的很長。&lt;/p&gt;
&lt;p&gt;因此，有另外一種作法一次只考慮「一筆」數據，使用一筆數據來評估並更新權重參數，每一筆雖然更新結果都不怎麼準確，但是當我隨著時間看過整個Training Set後，就會有平均的效果，所以最後只要Learning Rate不要太大，最後的結果還是可以朝向最佳解的，這個手法會使得Gradient Descent具有隨機性，因此又被稱為Stochastic Gradient Descent，它所帶來的優點是計算時間變短了，我們將可以避免去涉及大矩陣的運算，但缺點是一次只評估一筆數據，將會非常的不穩定。&lt;/p&gt;
&lt;p&gt;另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Mini-Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，Mini-Batch Gradient Descent一次評估k組數據，並更新參數W，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的架構設計是非常有利於矩陣計算的，因為GPU會利用它強大的平行化將矩陣運算中每個元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Mini-Batch Gradient Descent，這個k值不要超過GPU平行計算所能容納的最大上限，這是個既有效率又更為穩定的作法，順道一提Tensorflow是可以支援GPU的計算的。&lt;/p&gt;
&lt;p&gt;實務經驗告訴我們Mini-Batch Gradient Descent雖然穩定性比Gradient Descent差，但是收斂的速度卻一點都不輸給Gradient Descent，原因就出在更新的次數，Mini-Batch Gradient Descent一次看的數據筆數比較少，所以Mini-Batch一個Epoch可以更新參數好幾次，而Gradient Descent卻只能更新一次，Mini-Batch的靈敏性，使得它的收斂速度更為快速。打個比方，就好像是兩艘船在搜尋小島，Gradient Descent像是巡洋艦，它有更好設備可以有更好的觀測能力，但是因為它的笨重造成它反應不夠靈敏，Mini-Batch Gradient Descent就像是小船一樣，雖然觀測設備沒這麼好，但是反應靈敏，卻是可以更容易率先找到小島。&lt;/p&gt;
&lt;p&gt;那我們來看看要怎麼做到Mini-Batch Gradient Descent。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="n"&gt;略&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;

    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="c1"&gt;# mini-batch gradient descent&lt;/span&gt;
        &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;index_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_size&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

            &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;     &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="n"&gt;略&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;在每一個&lt;code&gt;epoch&lt;/code&gt;都完整的看過數據一遍，而mini-batch gradient descent是隨機取&lt;code&gt;batch_size&lt;/code&gt;筆數據來更新權重，所以我採用這樣的作法，先依照數據的筆數&lt;code&gt;N&lt;/code&gt;列出可能的Index有哪些，然後再做一個&lt;code&gt;random.shuffle&lt;/code&gt;來做到隨機採樣，然後接下來只要簡單的從前面取&lt;code&gt;batch_size&lt;/code&gt;筆數據進行更新，直到用盡所有的index為止，就可以做到mini-batch的效果。&lt;/p&gt;
&lt;h3 id="regularization"&gt;Regularization&lt;/h3&gt;
&lt;p&gt;當你開始加深你的DNN時，就已經在增加Model的複雜度，增加複雜度想當然爾的可以增加對於數據的描述能力，在分類問題中代表可以增加精確度，不過要特別注意Overfitting的出現，當Model越複雜越容易產生Overfitting，Overfitting的結果是有看過的數據描述的很好，但沒看過的數據預測就很差，所以在Training的過程要特別注意Validation Set的表現，如果發現Training Set的表現越來越好，但是Validation Set的表現裹足不前甚至變的更差，那就很有可能已經Overfitting了。&lt;/p&gt;
&lt;p&gt;如果你已經看到Overfitting出現了，有什麼方法可以抑制他呢？這個時候就需要Regularization的幫忙，在Neurel Network常見的Regularization有兩種：Weight Regularization和Dropout，待會會一一介紹。&lt;/p&gt;
&lt;p&gt;這邊特別注意，不要每次精確度沒有提升就怪Overfitting！如果連你的Training Set都沒辦法有好的表現，這就可能不是Overfitting，反而可能是Underfitting，這個時候不要再增加Regularization，而是試著去調整Learning Rate，或者增加模型的複雜度，加深DNN或增加神經元的數目。&lt;/p&gt;
&lt;h3 id="weight-regularization"&gt;Weight Regularization&lt;/h3&gt;
&lt;p&gt;我們可以藉著在Loss Function裡頭加入Weight的貢獻，來達到限制W的大小的目標，這樣做可以降低Overfitting，詳細原理請&lt;a href="/ml-course-foundations_4.html"&gt;參考這篇的Regularization的部分&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;我們來看看應該怎麼做到L2 Regularization。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# regularization loss&lt;/span&gt;
&lt;span class="n"&gt;regularization&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l2_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;
&lt;span class="c1"&gt;# total loss&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;regularization&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;在這裡我習慣將Loss對Weights的個數做平均，這有一個好處當我在調整神經元數目時，&lt;code&gt;alpha&lt;/code&gt;可以不需要大動作調整。&lt;/p&gt;
&lt;h3 id="dropout"&gt;Dropout&lt;/h3&gt;
&lt;p&gt;Dropout是Deep Learning常用的Regularization技巧，它的作法是在訓練的時候我先隨機把部份的神經元關閉，使用較少的神經元訓練，來達到Regularization的效果，最後在「推論」的時候再使用全部的神經元，我個人覺得這有Aggregation Model的味道，分別訓練出許多的sub-model再做Aggregation以達到截長補短的效果。&lt;/p&gt;
&lt;p&gt;實作上有一些細節必須要注意，當我們關閉一些神經元時，也就是等於減少部份的貢獻量，所以我們需要依照相應比例來給予權重，以抵銷減少的部分。舉個例子，假設今天原本應該要輸出的值有十個，這十個值都是1，然後因為Dropout，變成五個1五個0的輸出，我們看到原本貢獻量因為Dropout一半而少一半，這樣並不合理，會導致我後面的Weights在更新的時候低估更新量，所以我們必須要將「沒被Dropout的部分」權重乘上一倍，才可以解決問題。因此，如果Dropout &lt;span class="math"&gt;\(r\)&lt;/span&gt;倍的神經元，權重就要乘以&lt;span class="math"&gt;\((1/r)\)&lt;/span&gt;倍，我們來看看Tensorflow怎麼做的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;
&lt;span class="normal"&gt;9&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                           &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                           &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# 3 Data, 8 dim. Score&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Original S =&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;S_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# dropout ratio = 1 - keep_prob = 0.5&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Dropout S =&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S_drop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Original S =&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[[1. 1. 1. 1. 1. 1. 1. 1.]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[3. 3. 3. 3. 3. 3. 3. 3.]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[5. 5. 5. 5. 5. 5. 5. 5.]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Dropout S =&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[[ 2.  0.  0.  2.  0.  0.  2.  0.]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[ 0.  0.  6.  0.  0.  6.  0.  6.]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[10. 10. 10.  0. 10. 10. 10. 10.]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;我們看到因為dropout 0.5倍，所以輸出值權重乘上2倍，另外一提，Tensorflow的Dropout機制是隨機的，所以Drop out的比例會接近我們想要的比例，但不是絕對剛好。&lt;/p&gt;
&lt;p&gt;那我要怎麼把Dropout放進去我的Model呢？特別注意，我們並不希望已經Training完的Model還有Dropout這一層，所以我在&lt;code&gt;structure&lt;/code&gt;裡頭設計一個&lt;code&gt;train&lt;/code&gt;的開關，當我在Training過程就把它打開，Dropout這一層就會被加進去，「推論」的時候就關閉，保持原有的神經元數量。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

        &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="n"&gt;略&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

        &lt;span class="c1"&gt;# layer 1&lt;/span&gt;
        &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# layer 2&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="n"&gt;略&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="optimizer"&gt;Optimizer的選擇&lt;/h3&gt;
&lt;p&gt;我們可以自由的替換我們想要使用的Optimizer。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# define training operation&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;在Tensorflow中目前有以下十種Optimizer供我們使用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tf.train.GradientDescentOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.AdadeltaOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.AdagradOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.AdagradDAOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.MomentumOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.AdamOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.FtrlOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.ProximalGradientDescentOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.ProximalAdagradOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.RMSPropOptimizer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果想要了解每個Optimizer的演算法可以參考&lt;a href="http://ruder.io/optimizing-gradient-descent/"&gt;這篇有詳細的說明&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="_1"&gt;來看看程式怎麼寫&lt;/h3&gt;
&lt;p&gt;講了那麼多，來看看完整的程式怎麼寫？照慣例，先畫個流程圖。&lt;/p&gt;
&lt;p&gt;&lt;img alt="DNNLogisticClassification" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.003.jpeg" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;  1&lt;/span&gt;
&lt;span class="normal"&gt;  2&lt;/span&gt;
&lt;span class="normal"&gt;  3&lt;/span&gt;
&lt;span class="normal"&gt;  4&lt;/span&gt;
&lt;span class="normal"&gt;  5&lt;/span&gt;
&lt;span class="normal"&gt;  6&lt;/span&gt;
&lt;span class="normal"&gt;  7&lt;/span&gt;
&lt;span class="normal"&gt;  8&lt;/span&gt;
&lt;span class="normal"&gt;  9&lt;/span&gt;
&lt;span class="normal"&gt; 10&lt;/span&gt;
&lt;span class="normal"&gt; 11&lt;/span&gt;
&lt;span class="normal"&gt; 12&lt;/span&gt;
&lt;span class="normal"&gt; 13&lt;/span&gt;
&lt;span class="normal"&gt; 14&lt;/span&gt;
&lt;span class="normal"&gt; 15&lt;/span&gt;
&lt;span class="normal"&gt; 16&lt;/span&gt;
&lt;span class="normal"&gt; 17&lt;/span&gt;
&lt;span class="normal"&gt; 18&lt;/span&gt;
&lt;span class="normal"&gt; 19&lt;/span&gt;
&lt;span class="normal"&gt; 20&lt;/span&gt;
&lt;span class="normal"&gt; 21&lt;/span&gt;
&lt;span class="normal"&gt; 22&lt;/span&gt;
&lt;span class="normal"&gt; 23&lt;/span&gt;
&lt;span class="normal"&gt; 24&lt;/span&gt;
&lt;span class="normal"&gt; 25&lt;/span&gt;
&lt;span class="normal"&gt; 26&lt;/span&gt;
&lt;span class="normal"&gt; 27&lt;/span&gt;
&lt;span class="normal"&gt; 28&lt;/span&gt;
&lt;span class="normal"&gt; 29&lt;/span&gt;
&lt;span class="normal"&gt; 30&lt;/span&gt;
&lt;span class="normal"&gt; 31&lt;/span&gt;
&lt;span class="normal"&gt; 32&lt;/span&gt;
&lt;span class="normal"&gt; 33&lt;/span&gt;
&lt;span class="normal"&gt; 34&lt;/span&gt;
&lt;span class="normal"&gt; 35&lt;/span&gt;
&lt;span class="normal"&gt; 36&lt;/span&gt;
&lt;span class="normal"&gt; 37&lt;/span&gt;
&lt;span class="normal"&gt; 38&lt;/span&gt;
&lt;span class="normal"&gt; 39&lt;/span&gt;
&lt;span class="normal"&gt; 40&lt;/span&gt;
&lt;span class="normal"&gt; 41&lt;/span&gt;
&lt;span class="normal"&gt; 42&lt;/span&gt;
&lt;span class="normal"&gt; 43&lt;/span&gt;
&lt;span class="normal"&gt; 44&lt;/span&gt;
&lt;span class="normal"&gt; 45&lt;/span&gt;
&lt;span class="normal"&gt; 46&lt;/span&gt;
&lt;span class="normal"&gt; 47&lt;/span&gt;
&lt;span class="normal"&gt; 48&lt;/span&gt;
&lt;span class="normal"&gt; 49&lt;/span&gt;
&lt;span class="normal"&gt; 50&lt;/span&gt;
&lt;span class="normal"&gt; 51&lt;/span&gt;
&lt;span class="normal"&gt; 52&lt;/span&gt;
&lt;span class="normal"&gt; 53&lt;/span&gt;
&lt;span class="normal"&gt; 54&lt;/span&gt;
&lt;span class="normal"&gt; 55&lt;/span&gt;
&lt;span class="normal"&gt; 56&lt;/span&gt;
&lt;span class="normal"&gt; 57&lt;/span&gt;
&lt;span class="normal"&gt; 58&lt;/span&gt;
&lt;span class="normal"&gt; 59&lt;/span&gt;
&lt;span class="normal"&gt; 60&lt;/span&gt;
&lt;span class="normal"&gt; 61&lt;/span&gt;
&lt;span class="normal"&gt; 62&lt;/span&gt;
&lt;span class="normal"&gt; 63&lt;/span&gt;
&lt;span class="normal"&gt; 64&lt;/span&gt;
&lt;span class="normal"&gt; 65&lt;/span&gt;
&lt;span class="normal"&gt; 66&lt;/span&gt;
&lt;span class="normal"&gt; 67&lt;/span&gt;
&lt;span class="normal"&gt; 68&lt;/span&gt;
&lt;span class="normal"&gt; 69&lt;/span&gt;
&lt;span class="normal"&gt; 70&lt;/span&gt;
&lt;span class="normal"&gt; 71&lt;/span&gt;
&lt;span class="normal"&gt; 72&lt;/span&gt;
&lt;span class="normal"&gt; 73&lt;/span&gt;
&lt;span class="normal"&gt; 74&lt;/span&gt;
&lt;span class="normal"&gt; 75&lt;/span&gt;
&lt;span class="normal"&gt; 76&lt;/span&gt;
&lt;span class="normal"&gt; 77&lt;/span&gt;
&lt;span class="normal"&gt; 78&lt;/span&gt;
&lt;span class="normal"&gt; 79&lt;/span&gt;
&lt;span class="normal"&gt; 80&lt;/span&gt;
&lt;span class="normal"&gt; 81&lt;/span&gt;
&lt;span class="normal"&gt; 82&lt;/span&gt;
&lt;span class="normal"&gt; 83&lt;/span&gt;
&lt;span class="normal"&gt; 84&lt;/span&gt;
&lt;span class="normal"&gt; 85&lt;/span&gt;
&lt;span class="normal"&gt; 86&lt;/span&gt;
&lt;span class="normal"&gt; 87&lt;/span&gt;
&lt;span class="normal"&gt; 88&lt;/span&gt;
&lt;span class="normal"&gt; 89&lt;/span&gt;
&lt;span class="normal"&gt; 90&lt;/span&gt;
&lt;span class="normal"&gt; 91&lt;/span&gt;
&lt;span class="normal"&gt; 92&lt;/span&gt;
&lt;span class="normal"&gt; 93&lt;/span&gt;
&lt;span class="normal"&gt; 94&lt;/span&gt;
&lt;span class="normal"&gt; 95&lt;/span&gt;
&lt;span class="normal"&gt; 96&lt;/span&gt;
&lt;span class="normal"&gt; 97&lt;/span&gt;
&lt;span class="normal"&gt; 98&lt;/span&gt;
&lt;span class="normal"&gt; 99&lt;/span&gt;
&lt;span class="normal"&gt;100&lt;/span&gt;
&lt;span class="normal"&gt;101&lt;/span&gt;
&lt;span class="normal"&gt;102&lt;/span&gt;
&lt;span class="normal"&gt;103&lt;/span&gt;
&lt;span class="normal"&gt;104&lt;/span&gt;
&lt;span class="normal"&gt;105&lt;/span&gt;
&lt;span class="normal"&gt;106&lt;/span&gt;
&lt;span class="normal"&gt;107&lt;/span&gt;
&lt;span class="normal"&gt;108&lt;/span&gt;
&lt;span class="normal"&gt;109&lt;/span&gt;
&lt;span class="normal"&gt;110&lt;/span&gt;
&lt;span class="normal"&gt;111&lt;/span&gt;
&lt;span class="normal"&gt;112&lt;/span&gt;
&lt;span class="normal"&gt;113&lt;/span&gt;
&lt;span class="normal"&gt;114&lt;/span&gt;
&lt;span class="normal"&gt;115&lt;/span&gt;
&lt;span class="normal"&gt;116&lt;/span&gt;
&lt;span class="normal"&gt;117&lt;/span&gt;
&lt;span class="normal"&gt;118&lt;/span&gt;
&lt;span class="normal"&gt;119&lt;/span&gt;
&lt;span class="normal"&gt;120&lt;/span&gt;
&lt;span class="normal"&gt;121&lt;/span&gt;
&lt;span class="normal"&gt;122&lt;/span&gt;
&lt;span class="normal"&gt;123&lt;/span&gt;
&lt;span class="normal"&gt;124&lt;/span&gt;
&lt;span class="normal"&gt;125&lt;/span&gt;
&lt;span class="normal"&gt;126&lt;/span&gt;
&lt;span class="normal"&gt;127&lt;/span&gt;
&lt;span class="normal"&gt;128&lt;/span&gt;
&lt;span class="normal"&gt;129&lt;/span&gt;
&lt;span class="normal"&gt;130&lt;/span&gt;
&lt;span class="normal"&gt;131&lt;/span&gt;
&lt;span class="normal"&gt;132&lt;/span&gt;
&lt;span class="normal"&gt;133&lt;/span&gt;
&lt;span class="normal"&gt;134&lt;/span&gt;
&lt;span class="normal"&gt;135&lt;/span&gt;
&lt;span class="normal"&gt;136&lt;/span&gt;
&lt;span class="normal"&gt;137&lt;/span&gt;
&lt;span class="normal"&gt;138&lt;/span&gt;
&lt;span class="normal"&gt;139&lt;/span&gt;
&lt;span class="normal"&gt;140&lt;/span&gt;
&lt;span class="normal"&gt;141&lt;/span&gt;
&lt;span class="normal"&gt;142&lt;/span&gt;
&lt;span class="normal"&gt;143&lt;/span&gt;
&lt;span class="normal"&gt;144&lt;/span&gt;
&lt;span class="normal"&gt;145&lt;/span&gt;
&lt;span class="normal"&gt;146&lt;/span&gt;
&lt;span class="normal"&gt;147&lt;/span&gt;
&lt;span class="normal"&gt;148&lt;/span&gt;
&lt;span class="normal"&gt;149&lt;/span&gt;
&lt;span class="normal"&gt;150&lt;/span&gt;
&lt;span class="normal"&gt;151&lt;/span&gt;
&lt;span class="normal"&gt;152&lt;/span&gt;
&lt;span class="normal"&gt;153&lt;/span&gt;
&lt;span class="normal"&gt;154&lt;/span&gt;
&lt;span class="normal"&gt;155&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DNNLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Building Graph&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their predictions and loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# regularization loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l2_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt; \
                &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;

            &lt;span class="c1"&gt;# total loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                 &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                 &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                 &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# build neurel network structure and return their predictions and loss&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="c1"&gt;# layer 1&lt;/span&gt;
        &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                   &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# layer 2&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                 &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# fully connected layer&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;# mini-batch gradient descent&lt;/span&gt;
            &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
            &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;index_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_size&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

                &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                &lt;span class="p"&gt;}&lt;/span&gt;
                &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

                &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;     &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# evaluate at the end of this epoch&lt;/span&gt;
            &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;val_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;val_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, val_loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, val_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;test_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MNIST_data/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;
&lt;span class="n"&gt;valid_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DNNLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/ 3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.4948, acc = 88.12%, val_loss =   0.5729, val_acc = 88.14%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/ 3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.3343, acc = 91.21%, val_loss =   0.3831, val_acc = 91.04%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/ 3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.2890, acc = 92.73%, val_loss =   0.3708, val_acc = 92.06%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_acc = 91.17%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;跟上次的結果比，你會發現有長足的進步，精確率來到90幾，大家可以&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/02_DNN_classification_on_MNIST.py"&gt;下載程式碼&lt;/a&gt;，試著調整參數使得DNN Model的精確率可以更高，參數包含：
* Hidden Layer的神經元數量
* 不同的Activation Function
* 不同的Batch Size
* 調整Weight Regularization的比例
* 調整Dropout Ratio
* 選擇不同Optimizer
* 使得DNN更深&lt;/p&gt;
&lt;p&gt;調整Model是重要的工作，試著自己動手做做看，你可以讓你的Model有多準呢？&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>實作Tensorflow (1)：Simple Logistic Classification on MNIST</title><link href="https://ycc.idv.tw/tensorflow-tutorial_1.html" rel="alternate"></link><published>2017-10-23T12:00:00+08:00</published><updated>2017-10-23T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-10-23:/tensorflow-tutorial_1.html</id><summary type="html">&lt;p&gt;MNIST Dataset / Softmax / Cross-Entropy Loss / 分離數據的重要性 / Tensorflow工作流程 / Tensorflow的基本「張量」元素 / Session的操作 / 第一個Tensorflow Model&lt;/p&gt;</summary><content type="html">&lt;p&gt;初次學習Tensorflow最困難的地方莫過於不知道從何下手，已經學會很多的Deep Learning理論，但是要自己使用Tensorflow將Network建起來卻是非常困難的，這篇文章我會先簡單的介紹幾個Tensorflow的概念，最後利用這些概念建立一個簡單的分類模型。&lt;/p&gt;
&lt;p&gt;本單元程式碼可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/01_simple_logistic_classification_on_MNIST.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;p&gt;首先，先&lt;code&gt;import&lt;/code&gt;一些會用到的function&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Config the matplotlib backend as plotting inline in IPython&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="mnist-dataset"&gt;MNIST Dataset&lt;/h3&gt;
&lt;p&gt;定義&lt;code&gt;summary&lt;/code&gt; function以便於觀察ndarray。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* shape: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* min: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* max: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* avg: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* std: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* unique: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;ndarray是numpy的基本元素，它非常便於我們做矩陣的運算。&lt;/p&gt;
&lt;p&gt;我們使用MNIST Dataset來當作我們練習的標的，MNIST包含一包手寫數字的圖片，每張圖片大小為28x28，每一張圖片都是一個手寫的阿拉伯數字包含0到9，並且標記上它所對應的數字。我們的目標就是要利用MNIST做到手寫數字辨識。&lt;/p&gt;
&lt;p&gt;在Tensorflow你可以很簡單的得到「處理過後的」MNIST，只要利用以下程式碼，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MNIST_data/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;
&lt;span class="n"&gt;valid_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;每個&lt;code&gt;train_data&lt;/code&gt;、&lt;code&gt;valid_data&lt;/code&gt;、&lt;code&gt;test_data&lt;/code&gt;都包含兩部分：圖片和標籤。&lt;/p&gt;
&lt;p&gt;我們來看一下圖片的部分，&lt;code&gt;train_data.images&lt;/code&gt;一共有55000張圖，每一張圖原本大小是28x28，不過特別注意這裡的Data已經先做過預先處理了，因此圖片已經被打平成28x28=784的一維矩陣了，另外每個Pixel的值也先做過「Normalization」了，通常會這樣處理，每個值減去128再除以128，所以你可以從以下的&lt;code&gt;summary&lt;/code&gt;中看到它的最大最小值落在0到1之間，還有這個Dataset也已經做過亂數重排了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;
&lt;span class="normal"&gt;27&lt;/span&gt;
&lt;span class="normal"&gt;28&lt;/span&gt;
&lt;span class="normal"&gt;29&lt;/span&gt;
&lt;span class="normal"&gt;30&lt;/span&gt;
&lt;span class="normal"&gt;31&lt;/span&gt;
&lt;span class="normal"&gt;32&lt;/span&gt;
&lt;span class="normal"&gt;33&lt;/span&gt;
&lt;span class="normal"&gt;34&lt;/span&gt;
&lt;span class="normal"&gt;35&lt;/span&gt;
&lt;span class="normal"&gt;36&lt;/span&gt;
&lt;span class="normal"&gt;37&lt;/span&gt;
&lt;span class="normal"&gt;38&lt;/span&gt;
&lt;span class="normal"&gt;39&lt;/span&gt;
&lt;span class="normal"&gt;40&lt;/span&gt;
&lt;span class="normal"&gt;41&lt;/span&gt;
&lt;span class="normal"&gt;42&lt;/span&gt;
&lt;span class="normal"&gt;43&lt;/span&gt;
&lt;span class="normal"&gt;44&lt;/span&gt;
&lt;span class="normal"&gt;45&lt;/span&gt;
&lt;span class="normal"&gt;46&lt;/span&gt;
&lt;span class="normal"&gt;47&lt;/span&gt;
&lt;span class="normal"&gt;48&lt;/span&gt;
&lt;span class="normal"&gt;49&lt;/span&gt;
&lt;span class="normal"&gt;50&lt;/span&gt;
&lt;span class="normal"&gt;51&lt;/span&gt;
&lt;span class="normal"&gt;52&lt;/span&gt;
&lt;span class="normal"&gt;53&lt;/span&gt;
&lt;span class="normal"&gt;54&lt;/span&gt;
&lt;span class="normal"&gt;55&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;...&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* shape&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;(55000, 784)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* min&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* max&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;1.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* avg&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.13070042431354523&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.30815958976745605&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* unique&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0.         0.00392157 0.00784314 0.01176471 0.01568628 0.01960784&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.02352941 0.02745098 0.03137255 0.03529412 0.03921569 0.04313726&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.04705883 0.0509804  0.05490196 0.05882353 0.0627451  0.06666667&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.07058824 0.07450981 0.07843138 0.08235294 0.08627451 0.09019608&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.09411766 0.09803922 0.10196079 0.10588236 0.10980393 0.1137255&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.11764707 0.12156864 0.1254902  0.12941177 0.13333334 0.13725491&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.14117648 0.14509805 0.14901961 0.15294118 0.15686275 0.16078432&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.16470589 0.16862746 0.17254902 0.1764706  0.18039216 0.18431373&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.18823531 0.19215688 0.19607845 0.20000002 0.20392159 0.20784315&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.21176472 0.21568629 0.21960786 0.22352943 0.227451   0.23137257&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.23529413 0.2392157  0.24313727 0.24705884 0.2509804  0.25490198&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.25882354 0.2627451  0.26666668 0.27058825 0.27450982 0.2784314&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.28235295 0.28627452 0.2901961  0.29411766 0.29803923 0.3019608&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.30588236 0.30980393 0.3137255  0.31764707 0.32156864 0.3254902&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.32941177 0.33333334 0.3372549  0.34117648 0.34509805 0.34901962&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.3529412  0.35686275 0.36078432 0.3647059  0.36862746 0.37254903&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.37647063 0.3803922  0.38431376 0.38823533 0.3921569  0.39607847&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.40000004 0.4039216  0.40784317 0.41176474 0.4156863  0.41960788&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.42352945 0.427451   0.43137258 0.43529415 0.43921572 0.4431373&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.44705886 0.45098042 0.454902   0.45882356 0.46274513 0.4666667&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.47058827 0.47450984 0.4784314  0.48235297 0.48627454 0.4901961&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.49411768 0.49803925 0.5019608  0.5058824  0.50980395 0.5137255&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.5176471  0.52156866 0.5254902  0.5294118  0.53333336 0.5372549&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.5411765  0.54509807 0.54901963 0.5529412  0.5568628  0.56078434&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.5647059  0.5686275  0.57254905 0.5764706  0.5803922  0.58431375&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.5882353  0.5921569  0.59607846 0.6        0.6039216  0.60784316&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.6117647  0.6156863  0.61960787 0.62352943 0.627451   0.6313726&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.63529414 0.6392157  0.6431373  0.64705884 0.6509804  0.654902&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.65882355 0.6627451  0.6666667  0.67058825 0.6745098  0.6784314&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.68235296 0.6862745  0.6901961  0.69411767 0.69803923 0.7019608&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.7058824  0.70980394 0.7137255  0.7176471  0.72156864 0.7254902&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.7294118  0.73333335 0.7372549  0.7411765  0.74509805 0.7490196&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.75294125 0.7568628  0.7607844  0.76470596 0.7686275  0.7725491&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.77647066 0.7803922  0.7843138  0.78823537 0.79215693 0.7960785&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.8000001  0.80392164 0.8078432  0.8117648  0.81568635 0.8196079&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.8235295  0.82745105 0.8313726  0.8352942  0.83921576 0.8431373&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.8470589  0.85098046 0.854902   0.8588236  0.86274517 0.86666673&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.8705883  0.8745099  0.87843144 0.882353   0.8862746  0.89019614&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.8941177  0.8980393  0.90196085 0.9058824  0.909804   0.91372555&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.9176471  0.9215687  0.92549026 0.9294118  0.9333334  0.93725497&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.94117653 0.9450981  0.9490197  0.95294124 0.9568628  0.9607844&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.96470594 0.9686275  0.9725491  0.97647065 0.9803922  0.9843138&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.98823535 0.9921569  0.9960785  1.&lt;/span&gt;&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;來試著畫圖來看看，我們使用ndarray的index功能來選出第10張圖片，&lt;code&gt;train_data.images[10,:]&lt;/code&gt;表示的是選第一軸的第10個和第二軸的全部。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_fatten_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plot_fatten_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,:])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0AB4Ii/EBQhB8I6v8A+Md7QMI5IyUAAAAASUVORK5CYII=%0A" /&gt;&lt;/p&gt;
&lt;p&gt;很顯而易見的，這是一個0。&lt;/p&gt;
&lt;p&gt;接下來來看標籤的部分，&lt;code&gt;train_data.labels&lt;/code&gt;不意外的一樣的也是有相應的55000筆資料，所對應的就是前面的每一張圖片，總共有10種類型:0到9，所以大小為(55000, 10)。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 1. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;...&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 1. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* shape&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;(55000, 10)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* min&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* max&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;1.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* avg&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.30000000000000004&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* unique&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 1.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;所以我們來看看上面那張圖片的標籤，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;1. 0. 0. 0. 0. 0. 0. 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;看起來的確沒錯，在0的位置標示1.，而其他地方標示為0.，因此這是一個標示為0的label沒有錯，這種表示方法稱為One-Hot Encoding，它具有機率的涵義，所代表的是有100%的機會落在0的類別上。&lt;/p&gt;
&lt;h3 id="softmax"&gt;Softmax&lt;/h3&gt;
&lt;p&gt;通常One-Hot Encoding會搭配Softmax一同服用，最後的Output結果如果是機率分布，那我也需要讓我的Neurel Network可以輸出機率分布。&lt;/p&gt;
&lt;p&gt;&lt;img alt="softmax" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.001.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;通過Softmax這一層，我們就可以將輸出轉變為以「機率」表示。&lt;/p&gt;
&lt;p&gt;我們可以來手刻一個Softmax Function，不過直接套用Tensorflow中函數的也是可以的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# avoid exp function go to too large,&lt;/span&gt;
    &lt;span class="c1"&gt;# pre-reduce before applying exp function&lt;/span&gt;
    &lt;span class="n"&gt;max_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;max_score&lt;/span&gt;

    &lt;span class="n"&gt;exp_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sum_exp_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;exp_s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;softmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp_s&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sum_exp_s&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;softmax&lt;/span&gt;

&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0.8360188  0.11314284 0.05083836&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="cross-entropy-loss"&gt;Cross-Entropy Loss&lt;/h3&gt;
&lt;p&gt;一旦我們要處理機率預測的問題，就不可以使用單純的「平方誤差」，而必須使用Cross-Entropy Loss，是這樣計算的：&lt;/p&gt;
&lt;div class="math"&gt;$$
Loss_{cross-entropy} = - \sum_i y_i ln(s_i)
$$&lt;/div&gt;
&lt;p&gt;
其中，&lt;span class="math"&gt;\(y_i\)&lt;/span&gt;為目標Label，&lt;span class="math"&gt;\(s_i\)&lt;/span&gt;為經過Softmax產生的預測值。&lt;/p&gt;
&lt;p&gt;至於如果你想要了解為何需要使用Cross-Entropy Loss？這我在機器學習基石的筆記中已經有提及過，請看&lt;a href="/ml-course-foundations_3.html"&gt;介紹Logistic Regression的部分&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="_1"&gt;分離數據的重要性&lt;/h3&gt;
&lt;p&gt;在MNIST Dataset中，你會發現分為Training Dataset、Validation Dataset和Testing Dataset，這樣的作法在Machine Learning中是常見且必要的。&lt;/p&gt;
&lt;p&gt;流程是這樣的，我們會先使用Training Dataset來訓練Model，並且使用Validation Dataset來檢驗Model的好壞，我們會依據Validation Dataset的檢驗調整Model上的參數，試著盡可能的壓低Validation Dataset的Error，記住！在過程中所產生的所有Models都要保留下來，因為最後選擇的Model並不是Validation Dataset的Error最小的，而是要再由Testing Dataset來做最後的挑選，挑選出能使Testing Dataset的Error最小的Model。&lt;/p&gt;
&lt;p&gt;這所有的作法都是為了避免Overfitting的情況發生，也就是機器可能因為看過一筆Data，結果就把這筆Data給完整記了起來，而Data本身含有雜訊，雜訊就這樣滲透到Model裡，確實做到分離是很重要的，讓Model在測試階段時可以使用沒有看過的Data。&lt;/p&gt;
&lt;p&gt;因此，Validation Dataset的分離是為了避免讓Model在Training階段看到要驗證的資料，所以更能正確的評估Model的好壞。但這樣是不夠的，人為會根據Validation Dataset來調整Model，這樣無形之中已經將Validation Dataset的資訊間接的經由人傳給了Model，所以還是沒有徹底分離，因此在最後挑選Models時，我們會使用另外一筆從沒看過的資料Testing Dataset來做挑選，一旦挑選完就不能再去調整任何參數了。&lt;/p&gt;
&lt;h3 id="tensorflow"&gt;Tensorflow工作流程&lt;/h3&gt;
&lt;p&gt;我們這一篇將會使用Tensorflow實作最簡單的單層Neurel Network，在這之前我們來看看Tensorflow是如何運作的？&lt;/p&gt;
&lt;p&gt;深度學習是由一層一層可以微分的神經元所連接而成，數學上可以表示為張量(Tensor)的表示式，我們一般講的矩陣運算是指2x2的矩陣運算，而張量(Tensor)則是拓寬到n維陣列做計算，在Machine Learning當中我們常常需要處理到相當高維度的計算，例如：有五張28x28的彩色圖的表示就必須使用到四維張量，第一維表示第幾張、第二、三維表示圖片的大小、第四維則表示RGB，如果你是物理系的學生應該也對張量不陌生，廣義相對論裡頭大量的使用四維張量運算，三維空間加一維時間。&lt;/p&gt;
&lt;p&gt;而在做Neurel Network時，我們會根據需求不同設計不同形式但合理的流程(Flow)，再使用數據來訓練我的Model。所以，這就是Tensorflow命名由來：Tensor+Flow。&lt;/p&gt;
&lt;p&gt;因此，一開始要先設計Model的結構，這在Tensorflow裡頭稱為Graph，Graph的作用是事先決定Neurel Network的結構，決定Neuron要怎麼連接？決定哪一些窗口是可以由外部置放數據的？決定哪一些變數是可以被訓練的？哪一些變數是不可以被訓練的？定義將要怎麼樣優化這個系統？...等等。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;my_graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# Initialize a new graph&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;my_graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt; &lt;span class="c1"&gt;# Create a scope to build graph&lt;/span&gt;
    &lt;span class="c1"&gt;# ...&lt;/span&gt;
    &lt;span class="c1"&gt;# detail of building graph&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;Graph只是一個結構，它不具有有效的資訊，而當我們定義完成Graph之後，接下來我們需要創造一個環境叫做Session，Session會將Graph的結構複製一份，然後再放入資訊進行Training或是預測等等，因此Session是具有有效資訊的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;my_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# Copy graph into session&lt;/span&gt;
    &lt;span class="c1"&gt;# ...&lt;/span&gt;
    &lt;span class="c1"&gt;# detail of doing machine learning  &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;還有另外一種寫法也是相同作用的，我個人比較喜歡下面這種寫法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;my_session&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;my_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;my_session&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="tensorflow_1"&gt;Tensorflow的基本「張量」元素&lt;/h3&gt;
&lt;p&gt;接下來我們就來看看有哪些構成Graph的基本元素可以使用。&lt;/p&gt;
&lt;p&gt;(1) 常數張量：&lt;/p&gt;
&lt;p&gt;一開始來看看「常數張量」，常數指的是在Model中不會改變的數值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;(2) 變數張量：&lt;/p&gt;
&lt;p&gt;與常數截然不同的就是變數，「變數張量」是指在訓練當中可以改變的值，一般「變數張量」會用作於Machine Learning需要被訓練的參數，如果你沒有特別設定，在最佳化的過程中，Tensorflow會自動調整「變數張量」的數值來最佳化。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;因為變數通常是未知且待優化的參數，所以我們一般會使用Initalizer來設定它的初始值，&lt;code&gt;tf.truncated_normal(shape=(3,5))&lt;/code&gt;會隨機產生大小3x5的矩陣，它的值呈常態分佈但只取兩個標準差以內的數值。&lt;/p&gt;
&lt;p&gt;如果今天你想要有一個「變數張量」但是又不希望它因為最佳化而改變，這時你要特別指定&lt;code&gt;trainable&lt;/code&gt;為&lt;code&gt;False&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;(3) 置放張量：&lt;/p&gt;
&lt;p&gt;另外有一些張量負責擔任輸入窗口的角色，稱為Placeholder。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;因為我們在訓練之前還尚未知道Data的數量，所以這裡使用None來表示未知。&lt;code&gt;tf.placeholder&lt;/code&gt;在Graph階段是沒有數值的，必須等到Session階段才將數值給輸入進去。&lt;/p&gt;
&lt;p&gt;(4) 操作型張量：&lt;/p&gt;
&lt;p&gt;這類張量並不含有實際數值，而是一種操作，常用的「操作型張量」有兩種，第一種是作為最佳化使用，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;選擇Optimizer和最佳化的方式來定義最佳化的操作方法，上述的例子是使用learning_rate為0.5的Gradient Descent來降低loss。&lt;/p&gt;
&lt;p&gt;另外一種是初始化的操作，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;這一個步驟是必要的但常常被忽略，還記得剛剛我們定義「變數張量」時有用到Initalizer，這些Initalizer在Graph完成時還不具有數值，必須使用&lt;code&gt;init_op&lt;/code&gt;來給予數值，所以記住一定要放&lt;code&gt;init_op&lt;/code&gt;進去Graph裡頭，而且必須先定義完成所有會用到的Initalizer再來設定這個&lt;code&gt;init_op&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id="session"&gt;Session的操作&lt;/h3&gt;
&lt;p&gt;「張量」元素具有兩個面向：功能和數值，在Graph階段「張量」只具有功能但不具有數值，只有到了Session階段才開始有數值，那如何將這些數值取出來呢？有兩種方法，以1+1當作範例來看看，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;g1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;g1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# add x and y&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;g1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sol&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# print tensor, not their value&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Tensor(&amp;quot;Add:0&amp;quot;, shape=(), dtype=int32)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;g1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="c1"&gt;# evaluate their value&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;2&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;g1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sol&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# another way of evaluating value&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;2&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;那如果我想使用placeholder來做到x+y呢？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;g2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;g2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# add x and y&lt;/span&gt;

&lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;g2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# if x = 2 and y = 3&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sol&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;5&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# if x = 5 and y = 7&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sol&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;12&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;因為x和y是placeholder，所以必須使用&lt;code&gt;feed_dict&lt;/code&gt;來餵入相關資訊，否則會報錯。&lt;/p&gt;
&lt;h3 id="tensorflow-model"&gt;第一個Tensorflow Model&lt;/h3&gt;
&lt;p&gt;有了以上的認識我們就可以來建立我們第一個Model。&lt;/p&gt;
&lt;p&gt;以下我會使用物件導向的寫法，讓程式碼更有條理。&lt;/p&gt;
&lt;p&gt;Machine Learning在操作上可以整理成三個大步驟：建構(Building)、訓練(Fitting)和推論(Inference)，所以我們將會使用這三大步驟來建製我們的Model。&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;SimpleLogisticClassification&lt;/code&gt;裡頭，「建構」的動作在&lt;code&gt;__init__&lt;/code&gt;中會進行，由&lt;code&gt;build&lt;/code&gt;函式來建立Graph，其中我將Neurel Network的結構分離存於&lt;code&gt;structure&lt;/code&gt;裡。「訓練」的動作在&lt;code&gt;fit&lt;/code&gt;中進行，這裡採用傳統的Gradient Descent的方法，將所有Data全部考慮進去最佳化，未來會再介紹Batch Gradient Descent。最後，「推論」的部分在&lt;code&gt;predict&lt;/code&gt;和&lt;code&gt;evaluate&lt;/code&gt;中進行。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SimpleLogisticClassification&lt;/code&gt;將會建構一個只有一層的Neurel Network，也就是說沒有Hidden Layer，畫個圖。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Simple Logistic Classification" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.002.jpeg" /&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;  1&lt;/span&gt;
&lt;span class="normal"&gt;  2&lt;/span&gt;
&lt;span class="normal"&gt;  3&lt;/span&gt;
&lt;span class="normal"&gt;  4&lt;/span&gt;
&lt;span class="normal"&gt;  5&lt;/span&gt;
&lt;span class="normal"&gt;  6&lt;/span&gt;
&lt;span class="normal"&gt;  7&lt;/span&gt;
&lt;span class="normal"&gt;  8&lt;/span&gt;
&lt;span class="normal"&gt;  9&lt;/span&gt;
&lt;span class="normal"&gt; 10&lt;/span&gt;
&lt;span class="normal"&gt; 11&lt;/span&gt;
&lt;span class="normal"&gt; 12&lt;/span&gt;
&lt;span class="normal"&gt; 13&lt;/span&gt;
&lt;span class="normal"&gt; 14&lt;/span&gt;
&lt;span class="normal"&gt; 15&lt;/span&gt;
&lt;span class="normal"&gt; 16&lt;/span&gt;
&lt;span class="normal"&gt; 17&lt;/span&gt;
&lt;span class="normal"&gt; 18&lt;/span&gt;
&lt;span class="normal"&gt; 19&lt;/span&gt;
&lt;span class="normal"&gt; 20&lt;/span&gt;
&lt;span class="normal"&gt; 21&lt;/span&gt;
&lt;span class="normal"&gt; 22&lt;/span&gt;
&lt;span class="normal"&gt; 23&lt;/span&gt;
&lt;span class="normal"&gt; 24&lt;/span&gt;
&lt;span class="normal"&gt; 25&lt;/span&gt;
&lt;span class="normal"&gt; 26&lt;/span&gt;
&lt;span class="normal"&gt; 27&lt;/span&gt;
&lt;span class="normal"&gt; 28&lt;/span&gt;
&lt;span class="normal"&gt; 29&lt;/span&gt;
&lt;span class="normal"&gt; 30&lt;/span&gt;
&lt;span class="normal"&gt; 31&lt;/span&gt;
&lt;span class="normal"&gt; 32&lt;/span&gt;
&lt;span class="normal"&gt; 33&lt;/span&gt;
&lt;span class="normal"&gt; 34&lt;/span&gt;
&lt;span class="normal"&gt; 35&lt;/span&gt;
&lt;span class="normal"&gt; 36&lt;/span&gt;
&lt;span class="normal"&gt; 37&lt;/span&gt;
&lt;span class="normal"&gt; 38&lt;/span&gt;
&lt;span class="normal"&gt; 39&lt;/span&gt;
&lt;span class="normal"&gt; 40&lt;/span&gt;
&lt;span class="normal"&gt; 41&lt;/span&gt;
&lt;span class="normal"&gt; 42&lt;/span&gt;
&lt;span class="normal"&gt; 43&lt;/span&gt;
&lt;span class="normal"&gt; 44&lt;/span&gt;
&lt;span class="normal"&gt; 45&lt;/span&gt;
&lt;span class="normal"&gt; 46&lt;/span&gt;
&lt;span class="normal"&gt; 47&lt;/span&gt;
&lt;span class="normal"&gt; 48&lt;/span&gt;
&lt;span class="normal"&gt; 49&lt;/span&gt;
&lt;span class="normal"&gt; 50&lt;/span&gt;
&lt;span class="normal"&gt; 51&lt;/span&gt;
&lt;span class="normal"&gt; 52&lt;/span&gt;
&lt;span class="normal"&gt; 53&lt;/span&gt;
&lt;span class="normal"&gt; 54&lt;/span&gt;
&lt;span class="normal"&gt; 55&lt;/span&gt;
&lt;span class="normal"&gt; 56&lt;/span&gt;
&lt;span class="normal"&gt; 57&lt;/span&gt;
&lt;span class="normal"&gt; 58&lt;/span&gt;
&lt;span class="normal"&gt; 59&lt;/span&gt;
&lt;span class="normal"&gt; 60&lt;/span&gt;
&lt;span class="normal"&gt; 61&lt;/span&gt;
&lt;span class="normal"&gt; 62&lt;/span&gt;
&lt;span class="normal"&gt; 63&lt;/span&gt;
&lt;span class="normal"&gt; 64&lt;/span&gt;
&lt;span class="normal"&gt; 65&lt;/span&gt;
&lt;span class="normal"&gt; 66&lt;/span&gt;
&lt;span class="normal"&gt; 67&lt;/span&gt;
&lt;span class="normal"&gt; 68&lt;/span&gt;
&lt;span class="normal"&gt; 69&lt;/span&gt;
&lt;span class="normal"&gt; 70&lt;/span&gt;
&lt;span class="normal"&gt; 71&lt;/span&gt;
&lt;span class="normal"&gt; 72&lt;/span&gt;
&lt;span class="normal"&gt; 73&lt;/span&gt;
&lt;span class="normal"&gt; 74&lt;/span&gt;
&lt;span class="normal"&gt; 75&lt;/span&gt;
&lt;span class="normal"&gt; 76&lt;/span&gt;
&lt;span class="normal"&gt; 77&lt;/span&gt;
&lt;span class="normal"&gt; 78&lt;/span&gt;
&lt;span class="normal"&gt; 79&lt;/span&gt;
&lt;span class="normal"&gt; 80&lt;/span&gt;
&lt;span class="normal"&gt; 81&lt;/span&gt;
&lt;span class="normal"&gt; 82&lt;/span&gt;
&lt;span class="normal"&gt; 83&lt;/span&gt;
&lt;span class="normal"&gt; 84&lt;/span&gt;
&lt;span class="normal"&gt; 85&lt;/span&gt;
&lt;span class="normal"&gt; 86&lt;/span&gt;
&lt;span class="normal"&gt; 87&lt;/span&gt;
&lt;span class="normal"&gt; 88&lt;/span&gt;
&lt;span class="normal"&gt; 89&lt;/span&gt;
&lt;span class="normal"&gt; 90&lt;/span&gt;
&lt;span class="normal"&gt; 91&lt;/span&gt;
&lt;span class="normal"&gt; 92&lt;/span&gt;
&lt;span class="normal"&gt; 93&lt;/span&gt;
&lt;span class="normal"&gt; 94&lt;/span&gt;
&lt;span class="normal"&gt; 95&lt;/span&gt;
&lt;span class="normal"&gt; 96&lt;/span&gt;
&lt;span class="normal"&gt; 97&lt;/span&gt;
&lt;span class="normal"&gt; 98&lt;/span&gt;
&lt;span class="normal"&gt; 99&lt;/span&gt;
&lt;span class="normal"&gt;100&lt;/span&gt;
&lt;span class="normal"&gt;101&lt;/span&gt;
&lt;span class="normal"&gt;102&lt;/span&gt;
&lt;span class="normal"&gt;103&lt;/span&gt;
&lt;span class="normal"&gt;104&lt;/span&gt;
&lt;span class="normal"&gt;105&lt;/span&gt;
&lt;span class="normal"&gt;106&lt;/span&gt;
&lt;span class="normal"&gt;107&lt;/span&gt;
&lt;span class="normal"&gt;108&lt;/span&gt;
&lt;span class="normal"&gt;109&lt;/span&gt;
&lt;span class="normal"&gt;110&lt;/span&gt;
&lt;span class="normal"&gt;111&lt;/span&gt;
&lt;span class="normal"&gt;112&lt;/span&gt;
&lt;span class="normal"&gt;113&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SimpleLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Building Graph&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their predictions and loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                        &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# build neurel network structure and return their predictions and loss&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="c1"&gt;# one fully connected layer&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# predictions&lt;/span&gt;
        &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# loss: softmax cross entropy&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                 &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# fully connected layer&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;# fully gradient descent&lt;/span&gt;
            &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# evaluate at the end of this epoch&lt;/span&gt;
            &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;val_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;val_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, val_loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, val_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;test_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   9.2515, acc = 12.81%, val_loss =   9.4888, val_acc = 11.92%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   8.2946, acc = 13.89%, val_loss =   8.5156, val_acc = 13.10%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   7.5609, acc = 15.92%, val_loss =   7.7680, val_acc = 15.02%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  4/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   6.9563, acc = 18.31%, val_loss =   7.1521, val_acc = 17.44%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  5/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   6.4402, acc = 20.94%, val_loss =   6.6249, val_acc = 19.80%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  6/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   5.9915, acc = 23.35%, val_loss =   6.1650, val_acc = 22.38%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  7/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   5.5971, acc = 25.79%, val_loss =   5.7596, val_acc = 24.98%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  8/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   5.2479, acc = 28.18%, val_loss =   5.4001, val_acc = 27.30%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  9/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   4.9376, acc = 30.46%, val_loss =   5.0803, val_acc = 29.86%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   4.6608, acc = 32.71%, val_loss =   4.7947, val_acc = 32.20%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_acc = 33.58%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>股票策略：移動停損法</title><link href="https://ycc.idv.tw/stock-sell-point.html" rel="alternate"></link><published>2017-09-05T12:00:00+08:00</published><updated>2017-09-05T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-09-05:/stock-sell-point.html</id><summary type="html">&lt;p&gt;簡言之就是「從買入當天開始算起，以過程的每天當中最高股價當作基準點，向下去設停損點，或是停利點，低於這點就當天賣，或隔天賣」&lt;/p&gt;</summary><content type="html">&lt;p&gt;最近在研究股票，在網路上看到這篇文章&lt;/p&gt;
&lt;p&gt;&lt;a href="https://m.mobile01.com/topicdetail.php?f=291&amp;amp;t=3065829&amp;amp;p=1"&gt;https://m.mobile01.com/topicdetail.php?f=291&amp;amp;t=3065829&amp;amp;p=1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;覺得相當實用，在這邊跟大家分享，並且當作給自己參考的筆記。&lt;/p&gt;
&lt;p&gt;一般我們常常說玩股票要做好風險管理，最廣為人知的就是設停損點，假設今天我以20元買入一張股票，假如停損比率設10%好了，那我的停損點就是18元，股票一到這個價位就忍痛賣出，以做到風險管理，避免自己大賠。&lt;/p&gt;
&lt;p&gt;停損點的另外一個相反就是停利點，也就是當賺到某一個比例的金額時就獲利了結，假設一樣20元買入一張股票，設20%停利，也就是24元的時候賣出，設停利原本的目的是為了讓自己賣在高點，不過卻是有可能造成反效果。&lt;/p&gt;
&lt;p&gt;如果今天你幸運的買到一張飆股，一連兩天漲了兩根來到了24元，到了你設下的停利點，你不會真的就把它給賣了吧！正常人應該會等它漲到夠了，等它開始反轉時再考慮賣出。那究竟應該在反轉後跌多少才應該賣？這就變成了新的問題，總不能還是以停損點當作賣點吧！那你應該永遠賺不到錢～&lt;/p&gt;
&lt;p&gt;所以這篇的作者提供了一個方法，把停損點和停利點結合成為一點，而這一點會隨著股價上漲而移動上漲，所以下面的網友就稱這個叫做移動停損法。聽起來很神奇，其實概念很簡單，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;從買入當天開始算起，以過程的每天當中最高股價當作基準點，向下去設停損點，或是停利點，低於這點就當天賣，或隔天賣。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這就是我們剛剛說的，等到股價反轉後再來賣，而這個方法也可以順便做到停損，所以這點既是停損點也是停利點，但行為上都一樣是賣出，差別只在於你有沒有賺到錢囉！所以接下來我會以「賣出點」來稱這一點，避免大家被混淆。&lt;/p&gt;
&lt;p&gt;舉個例子好了，如果第一天我以50元買入一張股票，當天最高價位來到了51元，所以過程中最高價位目前是51元，假設我停損停利設10%，所以如果第二天股價低於51*90%=45.9元就賣出，45.9元就是目前的賣出點，假設未來都沒有價位超出51元，那賣出點永遠都在45.9元不會變。好，如果到了第二天，它飆上去了，收盤54元，最高價位曾經來到了55元，此時賣出點也跟著增加，以買股以來的最高價55元當作新的基準向下10％算出新的賣出點，也就是55*90%=49.5元，第三天氣勢依舊，開盤直接跳空漲停，最高價位來到了59.4元，所以賣出點目前是59.4*90%=53.45元，第四天開始反轉，微微下跌，最低價來到55元，最高價為57元，沒有低於賣出點所以不賣，此時賣出點也沒有改變，因為最高價沒有超過第三天，所以一樣是用第三天當作基準點向下算出賣出點53.45元，第五天終於稱不住了，股價跌到了53.45元，然後就當天賣出或隔天賣出，假如當天賣出的話，那就是53.45元，所以最後我的投報率是(53.45 - 50) / 50 = 6.9 %。&lt;/p&gt;
&lt;p&gt;那這個停損停利的比例該怎麼訂呢？在這個作者寫這篇文章的時候漲跌停百分比為7%，所以他設10%很合理，不會只有一天跌停就賣出了，而目前的漲跌停百分比為10%，所以我認為應該設把停損停利百分比設更高一點，建議在10~15%之間。&lt;/p&gt;
&lt;p&gt;那如果遇到除息的話，當然要將你口袋裡的股息從最高價上扣掉，在重新計算賣出點，這樣的賣出點才合理。&lt;/p&gt;
&lt;p&gt;以上的策略在一種情形下會賺錢，那就是你投資的股票漲的比跌的還多，當然這是一句廢話啦！這讓我想起之前在跟我朋友聊股票，我朋友說他的股票都賠錢，我就跟他開玩笑，股票不就是買在低點，然後賣在高點，你怎麼不照做，難怪會賠錢。移動停損法提供一種紀律讓你不會賠太多，但如果真的要獲利的話，還是得從基本面或技術面去分析股票，並在低價買進好股票，然後等它上漲。&lt;/p&gt;</content><category term="Reading"></category></entry><entry><title>如何辨別機器學習模型的好壞？秒懂Confusion Matrix</title><link href="https://ycc.idv.tw/confusion-matrix.html" rel="alternate"></link><published>2017-08-04T12:00:00+08:00</published><updated>2017-08-04T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-08-04:/confusion-matrix.html</id><summary type="html">&lt;p&gt;本篇介紹包含Confusion Matrix, True Positive, False Negative, False Positive, True Negative, Type I Error, Type II Error, Prevalence, Accuracy, Precision, Recall, F1 Measure, F Measure, Sensitivity, Specificity, ROC Curve, AUC, TPR, FNR, FPR, TNR, FDR, FOR, PPV, NPV, 算數平均, 幾何平均, 調和平均&lt;/p&gt;</summary><content type="html">&lt;p&gt;有時要鑑別一個模型的好或壞，並不能簡單的看出來，所以我們需要用一些指標去判定它的好壞，也作為我們挑選模型的依據。如果你稍微查一下有哪些指標，你就會發現指標多到讓人家眼花撩亂，一堆名詞就攤在那邊，讓人無從下手。&lt;/p&gt;
&lt;p&gt;有一種分類問題常用的指標稱之為Confusion Matrix，這個命名很有趣，這個表格的確是很讓人感到很困惑啊！至少在看完這篇之前。Confusion Matrix是用於分類問題的一種常用的指標，它衍生很多不同的指標，下面這張圖我將Confusion Matrix畫出來，並把一些比較重要的衍生指標給標出來。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.001" src="/media/mechine_learning_measure/mechine_learning_measure.001.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;我猜想，你一定看得很模糊吧！沒關係我在這篇文章中會帶大家認識這個圖裡的各個名詞。&lt;/p&gt;
&lt;p&gt;一開始我們從下面這個表格開始講起，這個表格就是所謂的Confusion Matrix，前面的True和False代表預測本身的結果是正確還是不正確的，而後面的Positive和Negative則是代表預測的方向是正向還是負向的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.002" src="/media/mechine_learning_measure/mechine_learning_measure.002.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;舉iphone當例子，iphone具有指紋識別解鎖系統，假如iphone判定這個指紋是屬於使用者的，它就會解鎖，所以今天如果你按壓了，而iphone也順利解鎖了，那這種情形就屬於左上角的情況，稱為True Positive，也就是「正確的正向預測」，如果不幸的你按壓iphone，結果iphone認不得你的指紋，這就是左下角的情況，稱為False Negative，也就是「錯誤的負向預測」，接下來找你朋友一起來測試，正常情形下你朋友的指紋應該沒辦法讓iphone解鎖，這是右下角的情況，稱為True Negative，也就是「正確的負向預測」，如果令人意外的是你的朋友把你的手機解鎖了，那你最好改成用密碼鎖...，這種情況就是右上角的狀況，稱為False Positive，也就是「錯誤的正向預測」。&lt;/p&gt;
&lt;p&gt;從上面的描述，我們當然希望我們的模型True Positive和True Negative都可以多多出現，而False Positive和False Negative可以盡量不要出現，因此這兩種狀況就稱之為Error，又各自又命名為Type I Error和Type II Error，這兩種錯誤，錯的很不一樣，如果今天指紋辨識不是放在iphone，而是放在你家大門鎖上，那你最不希望發生哪類錯誤？當然是Type I Error，也就是False Positive，此時機器會把陌生人當成主人的開門，這是我們不想看到的，我們寧可被關在門外（Type II Error）！但如果今天這個辨別系統是用在Google廣告，Google Ad會預測一個產品的潛在客戶，並做廣告投放，這個時候反而是較不希望Type II Error發生，也就是False Negative，這叫做寧可錯殺一百個也不要放過一個潛在客戶。所以下次在訓練你的模型時想清楚你不想要Type I Error還是Type II Error （鰲拜：我全都要...），並且用一些方法來放掉另一種錯誤，來降低這個我們不希望發生的錯誤。&lt;/p&gt;
&lt;p&gt;Confusion Matrix還有衍生很多形形色色的指標，我接下來就一一的介紹。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我們把所有正確的情況，也就是True Positive和True Negative，把它加總起來除上所有情形個數，那就是Accuracy，這也是最常用的指標，但是在某些情形下這個指標會失效&lt;/strong&gt;，如果今天實際正向的例子很少，譬如有一個信用卡盜刷偵測機器人，看了一個月的信用卡紀錄，其中真正是盜刷的資料筆數是相當少的，那我只要簡單一步來設計我的模型就可以使它Accuracy達到99%以上，你猜到了嗎？那就是通通預測沒有盜刷的情況發生，所以顯然我們需要別種指標來應對這種情況。&lt;/p&gt;
&lt;p&gt;Precision（準確率）和Recall（召回率）這個時候就派上用場了，Precision和Recall同時關注的都是True Positive（都在分子），但是角度不一樣，&lt;strong&gt;Precision看的是在預測正向的情形下，實際的「精準度」是多少，而Recall則是看在實際情形為正向的狀況下，預測「能召回多少」實際正向的答案&lt;/strong&gt;。一樣的，如果是門禁系統，我們希望Precision可以很高，Recall就相較比較不重要，我們比較在意的是預測正向（開門）的答對多少，比較不在意實際正向（是主人）的答對多少。如果是廣告投放，則Recall很重要，Precision就顯得沒這麼重要了，因為此時我們比較在意的是實際正向（是潛在客戶）的答對多少，而相對比較不在意預測正向（廣告投出）答對多少。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Precision和Recall都不去考慮True Negative&lt;/strong&gt;，因為通常True Negative會是答對的Null Hypothesis，簡單講就是最無聊的正確結果。在門禁的解鎖問題就是陌生人按壓且門不開；在廣告投放的例子中就是廣告不投，結果那個人也不是潛在客戶：在信用卡盜刷的例子，機器人認為正常的刷卡紀錄，其實也正是正常的。在通常的命題之下，實際是正向的結果是比負向少的，理所當然預測正向的結果也要比負向少，所以True Negative通常是量最多的，也是最無聊的。&lt;/p&gt;
&lt;p&gt;補充：Null Hypothesis通常代表比較常見的情況，在統計上我們要驗證某種概念成立，我們通常會假設一個最普通的Null Hypothesis當作正常情況，然後嘗試著利用實驗數據去否定這個Null Hypothesis，舉例：你要證明一種藥物是有效的，那你要先假設一個Null Hypothesis，譬如說給患者吃個安慰劑（可能是一顆糖果），你的藥要有辦法和Null Hypothesis產生顯著的差異，你才能證明你的藥是有效的。所以用在機器學習的例子當中，通常會把最普通的情況當作Negative，也就是當作Null Hypothesis來看待。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.003" src="/media/mechine_learning_measure/mechine_learning_measure.003.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;如果今天我覺得Precision和Recall都同等重要，我想要用一個指標來統合標誌它，這就是F1 Score或稱F1 Measure，它是F Measure的一個特例，當belta=1時就是F1 Measure，代表Precision和Recall都同等重要，那如果我希望多看中一點Precision，那&lt;span class="math"&gt;\(belta\)&lt;/span&gt;就可以選擇小一點，當&lt;span class="math"&gt;\(belta=0\)&lt;/span&gt;時，F Measure就是Precision；如果我希望多看中一點Recall，那belta就可以選擇大一點，當belta無限大時，F Measure就是Recall。&lt;/p&gt;
&lt;p&gt;如果你仔細看F1 Measure，你會發現它的平均方法是「調和平均」，帶大家go-through三種平均方法，你就能明白為什麼要使用調和平均了。下圖列出了三種平均方法的使用時機，我們要去了解資料或數列的特性，我們才能知道要採取哪種平均方法較為恰當，大多情況算數平均都可以使用，因為我們都假設有線性關係存在，譬如說平均距離；幾何平均常用於人口計算，因為人口增加是成比例增加的；調和平均常用於計算平均速率，在固定距離下，所花時間就是平均速率，這數據成倒數關係，&lt;strong&gt;而F1 Measure也同樣是這樣的數據特性，在固定TP的情況下，有不同的分母，所以這裡使用調和平均較為適當&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.004" src="/media/mechine_learning_measure/mechine_learning_measure.004.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;下圖的名詞看一下有印象就好。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.005" src="/media/mechine_learning_measure/mechine_learning_measure.005.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;最後這頁來講一下醫學上常用的指標，首先是Prevalence（盛行率），如果以人口當作所有的樣本，實際得病的患者所佔的比例就代表這個病的盛行情況。&lt;/p&gt;
&lt;p&gt;如果今天有一個診斷方法可以判定病人是否有得此病，有兩個指標可以看，那就是Sensitivity和Specificity，Sensitivity就是Recall，它代表的是診斷方法是否夠靈敏可以將真正得病的人診斷出來，其實就是真正有病症的患者有多少可以被偵測出來，而Specificity則代表實際沒病症的人有多少被檢驗正確的。兩種指標都是越高越好。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.006" src="/media/mechine_learning_measure/mechine_learning_measure.006.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;通常在醫學上，會通過一些閥值來斷定病人是否有得此病，而這個閥值就會影響Sensitivity和Specificity，這個不同閥值Sensitivity和Specificity的分布情況可以畫成ROC Curve，而ROC Curve底下的面積稱為AUC，AUC越大越好。&lt;/p&gt;
&lt;p&gt;想必這個時候你再回去看第一張圖就更加了解了，有了這些指標，我們就多一把尺來評斷我們的分類模型究竟是做的好還是不好。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category></entry><entry><title>Python玩數據 (3)：Numpy [2/2]</title><link href="https://ycc.idv.tw/python-play-with-data_3.html" rel="alternate"></link><published>2017-05-06T12:00:00+08:00</published><updated>2017-05-06T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-05-06:/python-play-with-data_3.html</id><summary type="html">&lt;p&gt;產生ndarray的其他方法 / Broadcasting / Slice and Fancy Indexing /&lt;/p&gt;</summary><content type="html">&lt;p&gt;在上一章節的討論，我們已經有了Numpy的基礎概念，在這一篇當中，我們會更深入的了解Numpy還有什麼進階的功能，包括：產生ndarray的多種方法、broadcast的概念以及ndarray的進階操作手法。&lt;/p&gt;
&lt;h3 id="ndarray"&gt;產生ndarray的其他方法&lt;/h3&gt;
&lt;p&gt;在上一章，ndarray的產生方法是由list產生的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;Numpy還提供產生ndarray的其他方式，幫助我們更容易的產生ndarray，譬如，產生一個數列。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;5.&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;5.5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;stop指的是停止的那點，那點是不包含在產生的數列的。&lt;/p&gt;
&lt;p&gt;1D的數列也可轉換成多維度的數列。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;K&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;另外還有一種可以產生連續數列的方法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;  &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.75&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;  &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.5&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.75&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;  &lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;這個函數是這樣的，0是起始值，2是最終值，這個最終值是包含在數列裡的，9是代表數列會有九個數字，所以它會自動從這區間找九個數字均勻分配。&lt;/p&gt;
&lt;p&gt;另外，也可以產生一個全部都零或一的數列，或是矩陣中的「單位矩陣」。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;     &lt;span class="c1"&gt;# &amp;quot;eye&amp;quot; means &amp;quot;I&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;或者，你想要亂數產生也可以。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.14405468&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.2312139&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.79134702&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.18676625&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.95305253&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.44833768&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.87919535&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.69051727&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;如果你想要數列依照你給定的規則產生，就先定義好函數，然後再利用&lt;code&gt;fromfunction&lt;/code&gt;製造數列。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;func1&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromfunction&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;這麼一來，每個位置的值都是由我們所定義的函數所決定。如果你覺得那個&lt;code&gt;func1&lt;/code&gt;名稱很多餘，還有下面這個方法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromfunction&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;上面我使用了&lt;code&gt;lambda&lt;/code&gt;，這個東西稱之為『匿名函數』。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;和&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;something&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;上面這兩個函式是等價的，差異只在於，第一個函式是沒有名稱的，稱為匿名函數，第二種就是一般的函式，具有名稱。&lt;/p&gt;
&lt;h3 id="broadcasting"&gt;Broadcasting&lt;/h3&gt;
&lt;p&gt;在上一章，我有提到一般的矩陣運算，在Numpy中是採用element-wise operation，也就是每個相應元素做運算，然後產生新的ndarray，這個前題是兩組要運算的ndarray他們的shape是相同的，那如果遇到shape不一致，Numpy會怎麼處理呢？實際上，Numpy會幫你把陣列給延伸展開，就像廣播(broadcasting)一樣的傳遞出去，這遵照所謂的broadcasting rules。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;
&lt;span class="normal"&gt;9&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;      &lt;span class="c1"&gt;# element-wise plus&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;6.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;上面就是最普遍的兩個相同shape的矩陣作運算。&lt;/p&gt;
&lt;p&gt;那如果是這個情況呢？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;6.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;8.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;你會發現如果矩陣對一個單一元素作運算，其實就等同於這個單一元素對矩陣內的元素分別作運算，這個方式相當好理解，那如果是這樣呢？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;10.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;14.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;22.&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;    &lt;span class="c1"&gt;# [[ 2*5, 2*7, 2*11 ],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;15.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;21.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;33.&lt;/span&gt; &lt;span class="p"&gt;]])&lt;/span&gt;   &lt;span class="c1"&gt;#  [ 3*5, 3*7, 3*11 ]]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;讓我來分解解說一下broadcasting究竟做了什麼，broadcasting能自動填滿矩陣有一個大前提，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;參與運算的所有矩陣必須符合以下規則才能做broadcasting，所有矩陣的shape由axis＝-1開始對齊去比較彼此間的rank，所有矩陣的在每個axis下的rank必須符合以下兩種規則其中之一：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;所有rank為同一個值&lt;/li&gt;
&lt;li&gt;只能有一個矩陣rank為非0或1，其餘矩陣的rank都要為0或1&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面這個例子，在axis= -2之下，只有C矩陣rank具有非0或1的2，而D的rank則為1；在axis= -1之下，只有D矩陣rank具有非0或1的3，而C的rank則為1，因此這兩個陣列可以使用broadcasting rule來延伸。&lt;/p&gt;
&lt;p&gt;為什麼我們需要這樣的前提假設，原因是符合這樣的情況下，我們可以藉由重複的複製貼上來使得兩個或多個矩陣有一樣的shape，C矩陣shape為(2,1)，所以在axis= -2的方向上，重複貼3次就會產生出shape為(2,3)的矩陣；D矩陣的shape為(1,3)，所以在axis= -1的方向上，重複貼2次就會產生出(2,3)的矩陣，如此一來兩個矩陣都是(2,3)就可以作element-wise operation。&lt;/p&gt;
&lt;p&gt;逐步示範一下，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;C  = [[2],[3]] # shape = (2,1)
C&amp;#39; = [[2,2,2],[3,3,3]] # shape = (2,3)
D  = [[5,7,11]] # shape = (1,3)
D&amp;#39; = [[5,7,11],[5,7,11]] # shape = (2,3)
E = C&amp;#39; * D&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;以下這些都是同樣道理&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;A      (2d array):  5 x 4
B      (1d array):      1
Result (2d array):  5 x 4

A      (2d array):  5 x 4
B      (1d array):      4
Result (2d array):  5 x 4

A      (3d array):  15 x 3 x 5
B      (3d array):  15 x 1 x 5
Result (3d array):  15 x 3 x 5

A      (3d array):  15 x 3 x 5
B      (2d array):       3 x 5
Result (3d array):  15 x 3 x 5

A      (3d array):  2 x 3 x 4
B      (2d array):      3 x 1
Result (3d array):  2 x 3 x 4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;我們再來看一下，如果維度不一樣是怎麼運作，譬如說2D碰上3D的，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;分解一下&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;
&lt;span class="normal"&gt;17&lt;/span&gt;
&lt;span class="normal"&gt;18&lt;/span&gt;
&lt;span class="normal"&gt;19&lt;/span&gt;
&lt;span class="normal"&gt;20&lt;/span&gt;
&lt;span class="normal"&gt;21&lt;/span&gt;
&lt;span class="normal"&gt;22&lt;/span&gt;
&lt;span class="normal"&gt;23&lt;/span&gt;
&lt;span class="normal"&gt;24&lt;/span&gt;
&lt;span class="normal"&gt;25&lt;/span&gt;
&lt;span class="normal"&gt;26&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;F  = [[[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11]],
      [[12, 13, 14, 15],
       [16, 17, 18, 19],
       [20, 21, 22, 23]]]  # shape = (2,3,4)
G = [[1],
     [2],
     [3]] # shape = (3,1)
G&amp;#39;= [[1,1,1,1],
     [2,2,2,2],
     [3,3,3,3]] # shape = (3,4)
G&amp;quot;= [[[1,1,1,1],
      [2,2,2,2],
      [3,3,3,3]],
     [[1,1,1,1],
      [2,2,2,2],
      [3,3,3,3]]] # shape = (2,3,4)

H = F + G&amp;quot;
   = [[[ 0+1,  1+1,  2+1,  3+1],
       [ 4+2,  5+2,  6+2,  7+2],
       [ 8+3,  9+3, 10+3, 11+3]],
      [[12+1, 13+1, 14+1, 15+1],
       [16+2, 17+2, 18+2, 19+2],
       [20+3, 21+3, 22+3, 23+3]]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;那這樣的性質可以怎麼運用呢？舉個例子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;Example&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="err"&gt;請問平面上這些點&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;102.0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;203.0&lt;/span&gt;&lt;span class="o"&gt;),(&lt;/span&gt;&lt;span class="mf"&gt;132.0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;193.0&lt;/span&gt;&lt;span class="o"&gt;),(&lt;/span&gt;&lt;span class="mf"&gt;45.0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;155.0&lt;/span&gt;&lt;span class="o"&gt;),(&lt;/span&gt;&lt;span class="mf"&gt;57.0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;173.0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="err"&gt;，哪一點最接近&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;111.0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;188.0&lt;/span&gt;&lt;span class="o"&gt;)?&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;observation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;111.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;188.0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;codes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mf"&gt;102.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;203.0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;132.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;193.0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;45.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;155.0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mf"&gt;57.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;173.0&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;codes&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;observation&lt;/span&gt;  &lt;span class="c1"&gt;# broadcasting&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;diff&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;9.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;15.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;21.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;66.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;33.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;54.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;15.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;diff&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# distance&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;17.49285568&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;21.58703314&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;73.79024326&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;56.04462508&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;nearest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;nearest&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt;   &lt;span class="c1"&gt;# ANS is (102.0, 203.0)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="slice-and-fancy-indexing"&gt;Slice and Fancy Indexing&lt;/h3&gt;
&lt;p&gt;最後，來看一下我們可以怎麼去切ndarray。在python內建語言中，常見的slice是這個樣子&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;那如果是維度再加一級呢？則是這個樣子&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;如果是ndarray，我們常常處理維度大於1的陣列，如果用這個方法來slice，就顯得非常麻煩，Numpy提供了一種比較直覺的方式來做slice。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;在中括號裡頭用逗點隔開來表示在各個axis上要取的位置，還可以填入一個陣列來取出一個範圍。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;
&lt;span class="normal"&gt;9&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt;   &lt;span class="c1"&gt;# &amp;quot;:&amp;quot;代表全取，效果和 M[1,0:2]一樣&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="c1"&gt;# 寫成陣列也可以，效果和 M[1,0:2]一樣&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="c1"&gt;# 還可以做到在axis=0的方向上取範圍，這是list做不到的&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;M&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;我們也可以引入一個ndarray來做篩選，常見使用的是布林陣列。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;b是由一個邏輯運算產生，這個邏輯運算會對矩陣作element-wise operation，所以會得出一個大小相同的布林陣列。而我們可以將b引入N當作篩選器，把符合的給取出來。事實上還可以更強大的去將取出來的值改值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;上面我將取出來的值加倍，這樣的手法來取值改值會直接影響到原陣列，這是一個很重要的手法。&lt;/p&gt;
&lt;h3 id="_1"&gt;子彈總結&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;產生ndarray的其他方法：np.arange, np.linspace, np.zeros, np.ones, np.eye, np.random.random 和 np.fromfunction&lt;/li&gt;
&lt;li&gt;Broadcasting的前題：所有矩陣的shape由axis＝-1開始對齊去比較彼此間的rank，所有矩陣的在每個axis下的rank必須符合以下兩種規則其中之一：&lt;/li&gt;
&lt;li&gt;所有rank為同一個值&lt;/li&gt;
&lt;li&gt;只能有一個矩陣rank為非0或1，其餘矩陣的rank都要為0或1&lt;/li&gt;
&lt;li&gt;Slicing Method ( Ex: M[1,0:2] )&lt;/li&gt;
&lt;li&gt;布林陣列的取值賦值方法&lt;/li&gt;
&lt;/ul&gt;</content><category term="CS"></category><category term="Python"></category></entry><entry><title>機器學習技法 學習筆記 (7)：Radial Basis Function Network與Matrix Factorization</title><link href="https://ycc.idv.tw/ml-course-techniques_7.html" rel="alternate"></link><published>2017-04-22T12:00:00+08:00</published><updated>2017-04-22T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-04-22:/ml-course-techniques_7.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋Radial Basis Function (RBF) Network、K-Means、One-Hot Encoding和Matrix Factorization&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="radial-basis-function-rbf-network"&gt;Radial Basis Function (RBF) Network&lt;/h3&gt;
&lt;p&gt;回顧一下Gaussian Kernel SVM，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W = 𝚺_{n=sv}  α_n y_n Z_n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(G_{SVM}\)&lt;/span&gt;   &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= sign[WZ+b]\)&lt;/span&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= sign{[𝚺_{n=sv} α_n y_n K(X_n,X)]+b}\)&lt;/span&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(⇒ G_{SVM} = sign{[𝚺_{n=sv} α_n y_n exp(-γ |X-X_n|^2)]+b}\)&lt;/span&gt; &lt;br/&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;看到這個式子你想到了什麼？有沒有融會貫通的感覺，你同樣的可以把上面的式子看成是Aggregation，又或者是Network。&lt;/p&gt;
&lt;p&gt;先來定義一下RBF Function， 其實就是Gaussian Function，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RBF Function: &lt;span class="math"&gt;\(RBF(X,X_n)=exp(-γ|X-X_n|^2)\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以我們可以仿造SVM的形式來造一個Network，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(G=Output{[𝚺_m β_m RBF(X,μ_m)]+b}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;當&lt;span class="math"&gt;\(Output\)&lt;/span&gt;為&lt;span class="math"&gt;\(sign\)&lt;/span&gt; Function、&lt;span class="math"&gt;\(β_m\)&lt;/span&gt;為&lt;span class="math"&gt;\(α_n y_n\)&lt;/span&gt;就回到特例SVM了。&lt;/p&gt;
&lt;p&gt;我們來細看這個式子傳遞的概念，RBF Network的第一層是先產生&lt;span class="math"&gt;\(M\)&lt;/span&gt;組&lt;span class="math"&gt;\(RBF(X,μ_m)\)&lt;/span&gt;，意味著以這&lt;span class="math"&gt;\(M\)&lt;/span&gt;個位置&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;當作中心點來評估各個&lt;span class="math"&gt;\(X\)&lt;/span&gt;與它的相似程度，RBF是有評估相似度的味道，越接近&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;的點，RBF越大，並隨著與&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;距離變大，RBF的值也快速遞減，所以這&lt;span class="math"&gt;\(M\)&lt;/span&gt;個&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;是有象徵性的，越接近它你越受它的影響。&lt;/p&gt;
&lt;p&gt;決定了每一筆數據各是受哪些&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;影響，接下來第二層是由這M個代表性的位置來進行投票決定最後的結果，這意味的不同的地方對&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;最後結果也有不同的影響力，舉個例子，假設在SVM裡頭，某個&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;如果它的&lt;span class="math"&gt;\(y_m =+1\)&lt;/span&gt;，那它對最後的影響就會是正的；那如果某個&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;的&lt;span class="math"&gt;\(y_m=-1\)&lt;/span&gt;，那它對最後的影響就會是負的，所以一個點進來，先評估一下它和象徵性的幾個點&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;的距離，如果相鄰幾點都是正的，這個點最後的結果就會是正的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="RBF Network" src="/media/MachineLearningTechniques/MachineLearningTechniques.000_05.png" /&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RBF Network在歷史上是Neural Network的一個分支，不過從上面的介紹你就會發現，它們的結構是有差異的，演算法也就不一樣。&lt;/p&gt;
&lt;p&gt;通常最佳化RBF Network做法是這樣的，我們會先用一些方法將&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;決定，如果&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;很懶惰的就直接使用所有的Training Data，總共有&lt;span class="math"&gt;\(N\)&lt;/span&gt;個，這&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;就叫做&lt;strong&gt;「Full RBF Network」&lt;/strong&gt;。&lt;strong&gt;我們也可以使用一些歸納的演算法找出代表資料群體的幾個象徵性的中心點，例如待會會介紹的K-Means的方法&lt;/strong&gt;，找出k個&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;再做計算，這樣的RBF Network稱為&lt;strong&gt;「k Nearest Neighbor RBF Network」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;找到了&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;就已經決定了所有的RBF Function，接下來就可以線性組合這些RBF Function，我們可以使用Regression的方法來求取&lt;span class="math"&gt;\(β_m\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;而如果你使用「Full RBF Network」，你會發現做完Regression後&lt;span class="math"&gt;\(E_{in}=0\)&lt;/span&gt;，這是典型的Overfitting，那這時你可能就要採用有Regularization的Regression啦！譬如說Ridge Regression之類的。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="k-means"&gt;K-Means&lt;/h3&gt;
&lt;p&gt;&lt;img alt="K-Means" src="/media/MachineLearningTechniques/MachineLearningTechniques.000_06.png" /&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;接下來來看怎麼用K-Means找到代表資料群體的幾個象徵性的中心點。&lt;/p&gt;
&lt;p&gt;首先，先決定要有幾個「中心點」，這裡假設我要有&lt;span class="math"&gt;\(k\)&lt;/span&gt;個好了，接下來先隨機給這些「中心點」一個初始的位置，接下來根據數據的靠近程度開始歸類，如果一筆數據比較所有的「中心點」後發現離「中心點」A是最近的話，那這筆數據就歸「中心點」A了，就用這樣的規則把所有數據都做分類。&lt;/p&gt;
&lt;p&gt;分完類後，接下來平均每一個資料群體裡的數據座標找出新的代表這個群體的「中心點」，然後又拿這個新的「中心點」根據數據的靠近程度再歸類一次，如此循環多次，直到收斂為止。這樣的話，這&lt;span class="math"&gt;\(k\)&lt;/span&gt;個「中心點」收斂後會各自佔據四方，並且代表某個群體的中心點。我們就可以找到代表性的&lt;span class="math"&gt;\(k\)&lt;/span&gt;個點，並拿這些點做「k Nearest Neighbor RBF Network」。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="one-hot-encoding"&gt;One-Hot Encoding&lt;/h3&gt;
&lt;p&gt;討論這麼久的ML，我們還沒有討論過假設遇到「類別」要怎麼處理！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;通常遇到類別的狀況，我們還是需要把它轉換成數值或向量來處理，常見的方法叫做One-Hot Encoding。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;舉個例子，如果要描述血型應該要怎麼做？我們可是無法拿字串下去Regression的啊～此時就需要One-Hot Encoding，假設血型有A, B, AB, O四種，我們可以這樣設定，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(A = [1, 0, 0, 0]^T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(B = [0, 1, 0, 0]^T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(AB = [0, 0, 1, 0]^T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(O = [0, 0, 0, 1]^T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;就是這麼簡單，這個動作就叫做One-Hot Encoding。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="matrix-factorization"&gt;Matrix Factorization&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;那如果今天我的Input和Output都是類別，而我們想要讓機器自己去找到匹配Input和Output的機制，解決這個問題的方法稱之為Matrix Factorization。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Matrix Factorization精神上有點像是Autoencoder，Autoencoder找出隱含在Data裡的特性，而Matrix Factorization則是找出隱含的匹配關係。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;舉個例子，如果Netflix有了一堆用戶和他們曾看過的電影的資料，我們想要從中抽取出用戶與他愛看的電影之間的關係，所以這不單單只是匹配而已，單純匹配就只需要硬碟就做的到了，我們要做的是找出匹配的規律，並且用更少、更精簡的方式表示這個匹配關係，舉個例子，有可能有部分用戶會被歸納到愛看恐怖片的，並且同時這些客戶會被連結到具有恐怖元素的電影，我們預期Matrix Factorization會有自行歸納整理的能力。&lt;/p&gt;
&lt;p&gt;可以仿造Autoencoder來設計Matrix Factorization，而你會發現Activation Function只要使用線性就已經足夠了，因為對於One-Hot Encoding的類別來說，只有一條通道是有效的，這已經具有開關的味道了，所以我們不用在Activation Function上面再弄一道開關，所以採用Linear就足夠了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Matrix Factorization" src="/media/MachineLearningTechniques/MachineLearningTechniques.000_07.png" /&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;因為是線性模型的緣故，我們可以很簡單的使用矩陣來描述，&lt;/p&gt;
&lt;p&gt;Hypothesis: &lt;span class="math"&gt;\(h(X) = W^TVX\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;而如果是某一用戶，則&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(h(X_n) = W^TV_n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;對某個用戶而言與他匹配的電影是一個向量，上面紀錄了他看過的電影，假設我再指定一部電影&lt;span class="math"&gt;\(m\)&lt;/span&gt;，此時&lt;span class="math"&gt;\(W_m^T V_n\)&lt;/span&gt;就代表這個用戶有沒有看過這部電影。&lt;/p&gt;
&lt;p&gt;用這個方法來想問題，假設今天你把用戶和電影填成一個大的表格，或是矩陣，有交集的部分就打個勾，這個矩陣的每個元素表示成&lt;span class="math"&gt;\(r_{nm}\)&lt;/span&gt;，有打勾的部分&lt;span class="math"&gt;\(r_{nm}=1\)&lt;/span&gt;，沒打勾的部分&lt;span class="math"&gt;\(r_{nm}=0\)&lt;/span&gt;，那我們做的轉換W和V最終就是為了讓&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W_m^T V_n ≈r_{nm}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;為了評估匹配的好壞，我們定義Error Function為&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(E_{in}(\{W_m\},\{V_n\}) = (1/𝚺_m |D_m|)×𝚺_{n,m} (r_{nm}-W_{m}^TV_n)^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;最佳化Matrix Factorization有兩個演算法，一個是Alternating Least Squares，另外一個是SGD。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="alternating-least-squares-for-matrix-factorization"&gt;Alternating Least Squares for Matrix Factorization&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Alternating Least Squares for Matrix Factorization" src="/media/MachineLearningTechniques/MachineLearningTechniques.000_08.png" /&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第一個方法是利用Linear Regression交互的優化&lt;span class="math"&gt;\(W_m\)&lt;/span&gt;和&lt;span class="math"&gt;\(V_n\)&lt;/span&gt;，我們的目標是使得&lt;span class="math"&gt;\(W_m^T V_n =r_{nm}\)&lt;/span&gt;，這式子可以用兩個角度看，如果固定&lt;span class="math"&gt;\(W_m\)&lt;/span&gt;，優化&lt;span class="math"&gt;\(V_n\)&lt;/span&gt;，那就是線性擬合&lt;span class="math"&gt;\(\{V_n, r_{nm}\}\)&lt;/span&gt;的問題；那如果固定&lt;span class="math"&gt;\(V_n\)&lt;/span&gt;，優化&lt;span class="math"&gt;\(W_m\)&lt;/span&gt;，這就是線性擬合&lt;span class="math"&gt;\(\{W_{m}, r_{nm}\}\)&lt;/span&gt;的問題。&lt;strong&gt;因此，交替優化&lt;span class="math"&gt;\(W_m\)&lt;/span&gt;和&lt;span class="math"&gt;\(V_n\)&lt;/span&gt;就可以使得&lt;span class="math"&gt;\(W_m^T V_n\)&lt;/span&gt;越來越接近&lt;span class="math"&gt;\(r_{nm}\)&lt;/span&gt;了&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="sgd-for-matrix-factorization"&gt;SGD for Matrix Factorization&lt;/h3&gt;
&lt;p&gt;&lt;img alt="SGD for Matrix Factorization" src="/media/MachineLearningTechniques/MachineLearningTechniques.000_09.png" /&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第二個方法則是老招—Gradient Descent，這裡採用隨機的版本SGD，所以過程中我們會隨意的從&lt;span class="math"&gt;\((n,m)\)&lt;/span&gt;中挑點，然後根據Error Measure&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{in}(\{W_m\},\{V_n\}) = (1/𝚺_m |D_m|) \times 𝚺_{n,m} (r_{nm}-W_{m}^T V_n )^2
$$&lt;/div&gt;
&lt;p&gt;
我們就可以得到更新&lt;span class="math"&gt;\(W_m\)&lt;/span&gt;和&lt;span class="math"&gt;\(V_n\)&lt;/span&gt;的方法，詳細的方法見上圖所示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目前，SGD方法是處理大型Matrix Factorization最流行的作法。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="_1"&gt;結語&lt;/h3&gt;
&lt;p&gt;本篇介紹類似Neural Network的兩種Network結構，分別為Radial Basis Function (RBF) Network和Matrix Factorization。&lt;/p&gt;
&lt;p&gt;在做RBF Network時，我們先找出幾個代表的中心，並評估一筆資料與這些中心的距離，再來再考慮不同中心對於答案的貢獻，加總起來可以預測這筆資料的答案，我們可以使用K-Means的方法來找出k點代表性的中心點來做RBF Network。&lt;/p&gt;
&lt;p&gt;Matrix Factorization和Autoencoder有點類似，Autoencoder目標在於找出隱含在Data裡的特性，而Matrix Factorization則是找出隱含的匹配關係，並且介紹了兩種Matrix Factorization的演算法：Alternating Least Squares和SGD方法。&lt;/p&gt;
&lt;p&gt;這系列的介紹文章，到這裡算是走到尾聲了，最後跟大家推薦一下老師的最後一堂課的投影片：&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/216_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/216_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;這個投影片裡頭林軒田教授用心的彙整了一整個學期的內容，很值得一看。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)</title><link href="https://ycc.idv.tw/ml-course-techniques_6.html" rel="alternate"></link><published>2017-04-17T12:00:00+08:00</published><updated>2017-04-17T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-04-17:/ml-course-techniques_6.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋神經網路(Neural Network, NN)、深度學習(Deep Learning, DL)、反向傳播算法(Backpropagation, BP)、Weight-elimination Regularizer、Early Stop、Autoencoder、Principal Component Analysis (PCA)&lt;/p&gt;</summary><content type="html">&lt;h3 id="neural-network"&gt;神經網路(Neural Network)&lt;/h3&gt;
&lt;p&gt;最後一個主題，我們要來講第三種「特徵轉換」— Extraction Models，其實就是現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，包括下圍棋的AlphaGo、Tesla的自動駕駛都是採用這一類的Machine Learning。&lt;/p&gt;
&lt;p&gt;Extraction Models的基本款就是廣為人知的「神經網路」(Neural Network)，它的特色是使用神經元來做非線性的特徵轉換，那如果具有多層神經元，就是做了多次的非線性特徵轉換，這就是所謂的「深度學習」(Deep Learning)。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neural Network" src="/media/MachineLearningTechniques/MachineLearningTechniques.016.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;上圖左側就是具有一層神經元的Neural Network，首先我們有一組特徵&lt;span class="math"&gt;\(X\)&lt;/span&gt;，通常我們會加入一個維度&lt;span class="math"&gt;\(X_{0}=1\)&lt;/span&gt;，這是為了可以讓結構變得更好看，未來可以與&lt;span class="math"&gt;\(W_{0}\)&lt;/span&gt;相乘產生常數項。使用&lt;span class="math"&gt;\(W\)&lt;/span&gt;來給予特徵&lt;span class="math"&gt;\(X\)&lt;/span&gt;權重，最後總和的結果稱之為Score，&lt;span class="math"&gt;\(s = W_{0}X_{0}+𝚺_{i=1}W_{i}X_{i} = 𝚺_{i=0}W_{i}X_{i}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;這個Score會被輸入到一個Activation Function裡頭，&lt;strong&gt;Activation Function的用意就是開關&lt;/strong&gt;，當Score大於某個閥值，就打通線路讓這條路的貢獻可以繼續向後傳遞；當Score小於某個閥值，就關閉線路，所以Activation Function可以是Binary Function，但在實際操作之下不會使用像Binary Function這類不可以微分的Activation Function，所以我們會找具有相似特性但又可以微分的函數，例如&lt;span class="math"&gt;\(tanh\)&lt;/span&gt;或者是&lt;span class="math"&gt;\(ReLU\)&lt;/span&gt;這類比較接近開關效果的函數，經過Activation Function轉換後的輸出表示成&lt;span class="math"&gt;\(g_{t} = σ(𝚺_{i}W_{i}X_{i})\)&lt;/span&gt;，這個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;就稱為神經元、&lt;span class="math"&gt;\(σ\)&lt;/span&gt;為Activation Function、&lt;span class="math"&gt;\(𝚺_{i} W_{i}X_{i}\)&lt;/span&gt;是Score。&lt;/p&gt;
&lt;p&gt;如果我們有多組權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;就能產生多組神經元&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，然後最後把&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;做線性組合並使用Output Function &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt;來衡量出最後的答案，Output Function可以是Linear Classification的Binary Function &lt;span class="math"&gt;\(h(x)=sign(x)\)&lt;/span&gt;，不過一樣的問題，它不可以微分，通常不會被使用，常見的是使用Linear Regression &lt;span class="math"&gt;\(h(x)=x\)&lt;/span&gt;，或者Logistic Regression &lt;span class="math"&gt;\(h(x)=Θ(x)\)&lt;/span&gt;來當作Output Function，最後的結果可以表示成 &lt;span class="math"&gt;\(y=h(𝚺_{t}α_{t}g_{t})\)&lt;/span&gt;，看到這個式子有沒有覺得很熟悉，它就像我們上一回講的Aggregation，將特徵X使用特徵轉換轉成使用&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;表示，再來組合這些&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;成為最後的Model，所以單層的Neural Network就使用到了Aggregation，它繼承了Aggregation的優點。&lt;/p&gt;
&lt;p&gt;有了這個Model的形式了，我們可以使用Gradient Descent的手法來做最佳化，這也就是為什麼要讓操作過程當中所使用的函數都可以微分的原因。Gradient Descent在Neural Network的領域裡面發展出一套方法稱為Backpropagation，我們待會會介紹。&lt;strong&gt;因此實現Backpropagation，我只要餵Data進去，Model就會去尋找可以描述這組Data的特徵轉換&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，這就好像是可以從Data中萃取出隱含的Feature一樣，所以這類的Models才會被統稱為Extraction Models&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="deep-learning"&gt;深度學習(Deep Learning)&lt;/h3&gt;
&lt;p&gt;剛剛我們介紹了最基本款的Neural Network，那如果這個Neural Network有好幾層，我還會稱它為Deep Learning，所以基本上Deep Neural Network和Deep Learning是指同一件事，那為什麼會有兩個名字呢？其實是有歷史典故的。&lt;/p&gt;
&lt;p&gt;Neural Network的歷史相當悠久，早在1958年就有人提出以Perceptron當作Activation Function的單層Neural Network，大家也知道一層的Neural Network是不Powerful的，所以在1969年，就有人寫了論文叫做「perceptron has limitation」，從那時起Neural Network的方法就很少人研究了。&lt;/p&gt;
&lt;p&gt;直到1980年代，有人開始使用多層的Neural Network，並在1989年，Yann LeCun博士等人就已經將反向傳播演算法(Backpropagation, BP)應用於Neural Network，當時Neural Network的架構已經和現在的Deep Learning很接近了，不過礙於當時的硬體設備計算力不足，Neural Network無法發揮功效，並且緊接的&lt;strong&gt;有人在1989年證明了只要使用一層Neural Network就可以代表任意函數，那為何還要Deep呢？&lt;/strong&gt;所以Deep Neural Network這方法就徹底黑掉了。&lt;/p&gt;
&lt;p&gt;一直到了最近，&lt;strong&gt;G. E. Hinton博士為了讓Deep Neural Network起死回生，重新給了它一個新名字「Deep Learning」&lt;/strong&gt;，再加上他在2006年提出的RBM初始化方法，這是一個非常複雜的方法，所以在學術界就造成了一股流行，雖然後來被證明RBM是沒有用的，不過卻因為很多人參與研究Deep Learning的關係，也找出了解決Deep Learning痛處的方法，&lt;strong&gt;2009年開始有人發現使用GPU可以大大的加速Deep Learning&lt;/strong&gt;，從這一刻起，Deep Learning就開始流行起來，直到去年的2016年3月，圍棋程式Alpha GO運用Deep Learning技術以4:1擊敗世界頂尖棋手李世乭，Deep Learning正式掀起了AI的狂潮。&lt;/p&gt;
&lt;p&gt;聽完這個故事我們知道改名字的重要性XDD，不過大家是否還有看到什麼關鍵，「使用一層Neural Network就可以代表任意函數，那為何還要Deep呢？」這句話，這不就否定了我們今天做的事情了嗎？的確，使用一層的Neural Network就可以形成任意函數，而且完全可以用一層的神經元來表示任何多層的神經元，數學上是行得通的，但重點是參數量。Deep Learning的學習方法和人有點類似，我們在學習一個艱深的理論時，會先單元式的針對幾個簡單的概念學習，然後在整合這些概念去理解更高層次的問題，Deep Learning透過多層結構學習，雖然第一層的神經元沒有很多，能學到的也只是簡單的概念而已，不過第二層再重組這些簡單概念，第三層再用更高層次的方法看問題，所以同樣的問題使用一層Neural Network可能需要很多神經元才有辦法描述，但是Deep Learning卻可以使用更少的神經元做到一樣的效果，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;同樣表示的數學轉換過程，雖然單層和多層都是做得到相同轉換的，但是多層所用的參數量是比單層來得少的，依照VC Generalization Bound理論 (請參考：&lt;a href="/ml-course-foundations_2.html"&gt;機器學習基石 學習筆記 (2)：為什麼機器可以學習?&lt;/a&gt;) 告訴我們可調控的參數量代表模型的複雜度，所以多層的NN比單層的有個優勢是在做到同樣的數學轉換的情況下更不容易Overfitting。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;因此，Deep Learning中每一層當中做了Aggregation，在增加模型複雜度的同時，也因為平均的效果而做到截長補短，這具有Regularization的效果，並且在採用多層且瘦的結構也同時因為「模組化」而做到降低參數使用量，來減少模型複雜度，這就不難想像Deep Learning為何如此強大。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="backpropagation-bp"&gt;反向傳播算法(Backpropagation, BP)&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Neural Network" src="/media/MachineLearningTechniques/MachineLearningTechniques.017.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;我們接下來就來看一下Deep Learning的演算法—反向傳播法，我們來看要怎麼從Gradient Descent來推出這個算法。&lt;/p&gt;
&lt;p&gt;看一下上面的圖，我畫出了具有&lt;span class="math"&gt;\(L\)&lt;/span&gt;層深的Deep Learning，每一層都有一個權重&lt;span class="math"&gt;\(W_{ij}^{(ℓ)}\)&lt;/span&gt;，因此我們可以估計出每一層的Score &lt;span class="math"&gt;\(s_{j}^{(ℓ)}= 𝚺_{i} W_{ij}^{(ℓ)}X_{i}^{(ℓ-1)}\)&lt;/span&gt;，把Score &lt;span class="math"&gt;\(s_{j}^{(ℓ)}\)&lt;/span&gt;通過Activation Function，就可以得到下一層的Input，如此不斷的疊上去，直到最後一層L為Output Layer，Output最後的結果&lt;span class="math"&gt;\(y\)&lt;/span&gt;，這裡我使用Linear Function來當作Output Function，這就是Deep Learning最簡單的架構。&lt;/p&gt;
&lt;p&gt;而我們需要Training的就是這些權重&lt;span class="math"&gt;\(W_{ij}^{(ℓ)}\)&lt;/span&gt;，我們如何一步一步的更新&lt;span class="math"&gt;\(W_{ij}^{(ℓ)}\)&lt;/span&gt;，使得它可以Fit數據呢？回想一下Gradient Descent的流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定義出Error函數&lt;/li&gt;
&lt;li&gt;Error函數讓我們可以去評估&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;算出它的梯度&lt;span class="math"&gt;\(∇E_{in}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;朝著&lt;span class="math"&gt;\(∇E_{in}\)&lt;/span&gt;的反方向更新參數W，而每次只跨出&lt;span class="math"&gt;\(η\)&lt;/span&gt;大小的一步&lt;/li&gt;
&lt;li&gt;反覆的計算新參數&lt;span class="math"&gt;\(W\)&lt;/span&gt;的梯度，並一再的更新參數&lt;span class="math"&gt;\(W\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;假設使用平方誤差的話，Error函數在這邊就是&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(L = (1/2) (y-\overline{y})^{2}\)&lt;/span&gt;，&lt;/p&gt;
&lt;p&gt;因此我們的更新公式可以表示成&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W_{ij}^{(ℓ)} ←  W_{ij}^{(ℓ)}-η×∂L/∂W_{ij}^{(ℓ)}\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;那我們要怎麼解這個式子呢？關鍵就在&lt;span class="math"&gt;\(∂L/∂W_{ij}^{(ℓ)}\)&lt;/span&gt;這項要怎麼計算，這一項在Output Layer (&lt;span class="math"&gt;\(ℓ=L\)&lt;/span&gt;)是很好計算的，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(∂L/∂W_{ij}^{(L)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= \frac{∂L}{∂s_{j}^{(L)}} \frac{∂s_{j}^{(L)}}{{∂W_{ij}^{(L)}}}\)&lt;/span&gt;  (連鎖率)&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= {δ_{j}^{(L)}}×{X_{i}^{(L-1)}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上式當中我們使用了微分的連鎖率，並且令&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(δ_{j}^{(L)} = ∂L/∂s_{j}^{(L)}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(δ_{j}^{(L)}\)&lt;/span&gt;這一項被稱為Backward Pass Term，而&lt;span class="math"&gt;\(X_{i}^{(L-1)}\)&lt;/span&gt;這項被稱為Forward Pass Term，所以&lt;span class="math"&gt;\(L\)&lt;/span&gt;層權重的更新取決於Forward Pass Term和Backward Pass Term相乘&lt;span class="math"&gt;\(δ_{j}^{(L)}×X_{i}^{(L-1)}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;我們先來看一下&lt;span class="math"&gt;\(L\)&lt;/span&gt;層的Forward Pass Term要怎麼計算，&lt;span class="math"&gt;\(X_{i}^{(L-1)}\)&lt;/span&gt;這項是很容易求的，我們只要讓數據一路從&lt;span class="math"&gt;\(0\)&lt;/span&gt;層傳遞上來就可以自然而然的得到&lt;span class="math"&gt;\(X_{i}^{(L-1)}\)&lt;/span&gt;的值，所以我們會稱&lt;span class="math"&gt;\(X_{i}^{(L-1)}\)&lt;/span&gt;這一項為Forward Pass Term，因為我們必須要往前傳遞才可以得到這個值。&lt;/p&gt;
&lt;p&gt;再來看一下&lt;span class="math"&gt;\(L\)&lt;/span&gt;層的Backward Pass Term要怎麼計算，&lt;span class="math"&gt;\(δ_{j}^{(L)}\)&lt;/span&gt;一樣是很容易求得的，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(δ_{j}^{(L)} = ∂L/∂s_{j}^{(L)} = ∂[(1/2) (y-\overline{y})^{2}]/∂y = (y-\overline{y})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;你會發現這一項的計算需要得到誤差的資訊，而誤差資訊要等到Forward的動作做完才有辦法得到，所以資訊的傳遞方向是從尾巴一路回到頭，是一個Backword的動作。&lt;/p&gt;
&lt;p&gt;因此，最後一層也是Output Layer的更新公式如下：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W_{ij}^{(L)} ←  W_{ij}^{(L)}-η×δ_{j}^{(L)}×X_{i}^{(L-1)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;權重的更新取決於Input和Error的影響，需要考慮Forward Pass Term和Backward Pass Term。&lt;/p&gt;
&lt;p&gt;那除了Output這一層以外的權重應該怎麼更新？來看一下&lt;span class="math"&gt;\((ℓ)\)&lt;/span&gt;層，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(∂L/∂W_{ij}^{(ℓ)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= \frac{∂L}{∂s_{j}^{(ℓ)}}\frac{∂s_{j}^{(ℓ)}}{∂W_{ij}^{(ℓ)}}\)&lt;/span&gt; (連鎖率)&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= δ_{j}^{(ℓ)}×X_{i}^{(ℓ-1)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;一樣是Forward Pass Term和Backword Pass Term相乘，不過&lt;span class="math"&gt;\(δ_{j}^{(ℓ)}\)&lt;/span&gt;這一項的計算有點技巧性，來看一下，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(δ_{j}^{(ℓ)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= ∂L/∂s_{j}^{(ℓ)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= 𝚺_{k} \frac{∂L}{∂s_{k}^{(ℓ+1)}}\frac{∂s_{k}^{(ℓ+1)}}{∂X_{jk}^{(ℓ)}}\frac{∂X_{jk}^{(ℓ)}}{∂s_{j}^{(ℓ)}}\)&lt;/span&gt; (連鎖率)&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= 𝚺_{k} {δ_{k}^{(ℓ+1)}}×{W_{jk}^{(ℓ)}}×{σ'(s_{j}^{(ℓ)})}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W_{jk}^{(ℓ)}\)&lt;/span&gt;和&lt;span class="math"&gt;\(σ'(s_{j}^{(ℓ)})\)&lt;/span&gt;都是Forward之後就會得到的資訊，而&lt;span class="math"&gt;\(δ_{k}^{(ℓ+1)}\)&lt;/span&gt; 而是需要Backward才可以得到，我們已經知道&lt;span class="math"&gt;\(δ_{j}^{(ℓ=L)}\)&lt;/span&gt;的值，就可以從&lt;span class="math"&gt;\(δ_{j}^{(ℓ=L)}\)&lt;/span&gt;開始利用上面的公式，一路Backward把所有的&lt;span class="math"&gt;\(δ_{j}\)&lt;/span&gt;都找齊。好！那現在我們已經找到了更新所有Weights的方法了。&lt;/p&gt;
&lt;p&gt;看一下上圖中的最下面的Flow，一開始我們Forward，把所有&lt;span class="math"&gt;\(X\)&lt;/span&gt;和&lt;span class="math"&gt;\(s\)&lt;/span&gt;都得到，到了Output Layer，我們得到了&lt;span class="math"&gt;\(δ_{j}^{(ℓ=L)}\)&lt;/span&gt;，再Backward回去找出所有的&lt;span class="math"&gt;\(δ\)&lt;/span&gt;，接下來就可以用Forward Pass Term和Backword Pass Term來Update所有的&lt;span class="math"&gt;\(W\)&lt;/span&gt;了。&lt;/p&gt;
&lt;p&gt;總結一下，反向傳播算法(Backpropagation, BP)更新權重的方法為&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(W_{ij}^{(ℓ)} ←  W_{ij}^{(ℓ)}-η×δ_{j}^{(ℓ)}×X_{i}^{(ℓ-1)}\)&lt;/span&gt;  &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If output layer (&lt;span class="math"&gt;\(ℓ=L\)&lt;/span&gt;), &lt;span class="math"&gt;\(δ_{j}^{(ℓ=L)}=(y-ŷ)\)&lt;/span&gt;  &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If other layer, &lt;span class="math"&gt;\(δ_{j}^{(ℓ)}= σ'(s_{j}^{(ℓ)}) × 𝚺_{k} δ_{k}^{(ℓ+1)}×W_{jk}^{(ℓ)}\)&lt;/span&gt;  &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(δ_{j}^{(ℓ)}\)&lt;/span&gt;為Backword Pass Term；&lt;span class="math"&gt;\(X_{i}^{(ℓ-1)}\)&lt;/span&gt;為Forward Pass Term。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="regularization-in-deep-learning"&gt;Regularization in Deep Learning&lt;/h3&gt;
&lt;p&gt;那麼使用Deep Learning的時候，我們要怎麼避免Overfitting呢？有五個方法。&lt;/p&gt;
&lt;p&gt;第一個方法，就是我們剛剛提過的&lt;strong&gt;「設計Deep Neural Network的結構」&lt;/strong&gt;，藉由限縮一層當中的神經元來達到一種限制，做到Regularization。&lt;/p&gt;
&lt;p&gt;第二個方法是&lt;strong&gt;「限制W的大小」&lt;/strong&gt;，和標準Regularization作一樣的事情，我們將&lt;span class="math"&gt;\(W\)&lt;/span&gt;的大小加進去Cost裡頭做Fitting，例如使用L2 Regularizer &lt;span class="math"&gt;\(Ω(W)=𝚺(W_{jk}^{(ℓ)})^{2}\)&lt;/span&gt;，但這樣使用有一個問題就是&lt;span class="math"&gt;\(W\)&lt;/span&gt;並不是Sparse的，L2 Regularizer在抑制&lt;span class="math"&gt;\(W\)&lt;/span&gt;的方法是，如果W的分量大的話就抑制多一點，如果分量小就抑制少一點（因為&lt;span class="math"&gt;\(W^{2}\)&lt;/span&gt;微分為1次），所以最後會留下很多很小的分量，造成計算量大大增加，尤其像是Deep Learing這麼龐大的Model，這樣的Regularization顯然不夠好，L1 Regularizer顯然可以解決這個問題（因為在大部分位置微分為常數），但不幸的是它無法微分，所以就有了L2 Regularizer的衍生版本，&lt;/p&gt;
&lt;p&gt;Weight-elimination L2 regularizer: &lt;span class="math"&gt;\(𝚺\frac{(W_{jk}^{(ℓ)})^{2}}{1+(W_{jk}^{(ℓ)})^{2}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這麼一來不管&lt;span class="math"&gt;\(W\)&lt;/span&gt;大或小，它受到抑制的值大小是接近的 (因為Weight-elimination L2 regularizer微分為 &lt;span class="math"&gt;\(-1\)&lt;/span&gt;次方)，因此就可以使得部分&lt;span class="math"&gt;\(W\)&lt;/span&gt;可以為&lt;span class="math"&gt;\(0\)&lt;/span&gt;，大大便利於我們做計算。&lt;/p&gt;
&lt;p&gt;第三種方法是最常使用的&lt;strong&gt;「Early Stopping」&lt;/strong&gt;，所謂的Early Stopping就是，在做Backpropagation的過程去觀察Validation Data的Error有沒有脫離Training Data的Error太多，如果開始出現差異，我們就立刻停止計算，這樣就可以確保Model裡的參數沒有使得Model產生Overfitting，是一個很直接的作法。&lt;/p&gt;
&lt;p&gt;第四種方法是&lt;strong&gt;「Drop-out」&lt;/strong&gt;，在Deep Learing Fitting的過程中，隨機的關閉部分神經元，藉由這樣的作法使得Fitting的過程使用較少的神經元，並且使得結構是瘦長狀的，來達到Regularization。&lt;/p&gt;
&lt;p&gt;第五種方法是接下來會用更大篇幅介紹的&lt;strong&gt;「Denoising Autoencoder」&lt;/strong&gt;，在Deep Neural Network前面加入這樣的結構有助於抑制雜訊。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="autoencoder"&gt;Autoencoder&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Regularization in Deep Learning" src="/media/MachineLearningTechniques/MachineLearningTechniques.018.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;Neural Network針對不同需要發展出很多不同的型態，包括CNN, RNN，還有接下來要介紹的Autoencoder，&lt;strong&gt;Autoencoder是一種可以將資料重要資訊保留下來的Neural Network&lt;/strong&gt;，效果有點像是資料壓縮，在做資料壓縮時，會有一個稱為Encoder的方法可以將資料壓縮，那當然還要有另外一個方法將它還原回去，這方法稱為Decoder，壓縮的過程就是用更精簡的方式保存了資料。&lt;strong&gt;Autoencoder同樣的有Encoder和Decoder，不過它不像資料壓縮一樣可以百分之一百還原，不過特別之處是Autoencoder會試著從Data中自己學習出Encoder和Decoder，並盡量讓資料在壓縮完了可以還原回去原始數據&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;見上圖中Basic Autoencoder的部分，透過兩層的轉換，我們試著讓Input &lt;span class="math"&gt;\(X\)&lt;/span&gt;可以完整還原回去，通常中間這一層會使用比較少的神經元，因為我們想要將資訊做壓縮，所以第一層的部分就是一個Encoder，而第二層則是Decoder，他們由權重&lt;span class="math"&gt;\(W_{jk}^{(ℓ)}\)&lt;/span&gt;決定，而在Training的過程，Autoencoder會試著找出最好的&lt;span class="math"&gt;\(W_{jk}^{(ℓ)}\)&lt;/span&gt;來使得資訊可以盡量完整還原回去，這也代表Autoencoder可以自行找出了Encoder和Decoder。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Encoder這一段就是在做一個Demension Reduction&lt;/strong&gt;，Encoder轉換原本數據到一個新的空間，這個空間可以比原本Features描述的空間更能精準的描述這群數據，而中間這層Layer的數值就是新空間裡頭的座標，有些時候我們會用這個新空間來判斷每筆Data之間彼此的接近程度。&lt;/p&gt;
&lt;p&gt;我們也可以讓Encoder和Decoder可以設計的更複雜一點，所以你同樣的可以使用多層結構，稱之為Deep Autoencoder。另外，也有人使用Autoencoder的方法來Pre-train Deep Neural Network的各個權重。&lt;/p&gt;
&lt;p&gt;緊接著介紹兩種特殊的例子，第一個是Linear Autoencoder，我們把所有的Activation Function改成線性的，這個方法可以等效於待會要講的Principal Component Analysis (PCA)的方法，PCA是一個全然線性的方法，所以它的效力會比Autoencoder差一點。&lt;/p&gt;
&lt;p&gt;第二個是剛剛提到的Denoising Autoencoder，我們在原本Autoencoder的前面加了一道增加人工雜訊的流程，但是又要讓Autoencoder試著去還原出原來沒有加入雜訊的資訊，這麼一來&lt;strong&gt;我們將可以找到一個Autoencoder是可以消除雜訊的&lt;/strong&gt;，把這個Denoising Autoencoder加到正常Neural Network的前面，那這個Neural Network就擁有了抑制雜訊的功用，所以可以當作一種Regularization的方法。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="principal-component-analysis-pca"&gt;Principal Component Analysis (PCA)&lt;/h3&gt;
&lt;p&gt;最後來講一下Principal Component Analysis (PCA)，它不太算是Deep Learning的範疇，不過它是一個傳統且重要的Dimension Reduction的方法，我們就來看一下。&lt;/p&gt;
&lt;p&gt;&lt;img alt="PCA" src="/media/MachineLearningTechniques/MachineLearningTechniques.019.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;PCA的演算法是這樣的，第一步先求出資料Features的平均值，並且將各個Features減掉平均值，令為&lt;span class="math"&gt;\(ζ\)&lt;/span&gt;，第二步求出由&lt;span class="math"&gt;\(ζ^{T}ζ\)&lt;/span&gt;產生的矩陣的Eigenvalue和Eigenvector，第三步，從這些Eigenvalue和Eigenvector中挑選前面&lt;span class="math"&gt;\(k\)&lt;/span&gt;個，並組成轉換矩陣&lt;span class="math"&gt;\(W\)&lt;/span&gt;，而最終PCA的轉換就是&lt;span class="math"&gt;\(Φ(x)=W^{T}(X-mean(X))\)&lt;/span&gt;，這個轉換做的就是Dimension Reduction，將數據降維到&lt;span class="math"&gt;\(k\)&lt;/span&gt;維。&lt;/p&gt;
&lt;p&gt;PCA做的事是這樣的，每一個Eigenvector代表新空間裡頭的一個軸，而Eigenvalue代表站在這個軸上看資料的離散程度，當然我們如果可以描述每筆資料越分離，就代表這樣的描述方法越好，所以Eigenvalue越大的Eigenvector越是重要，&lt;strong&gt;所以取前面&lt;span class="math"&gt;\(k\)&lt;/span&gt;個Eigenvector的用意是在降低維度的過程，還可以盡量的保持對數據的描述力，而且Eigenvector彼此是正交的，也就是說在新空間裡頭的每個軸是彼此垂直，彼此沒有Dependent的軸是最精簡的，所以PCA所做的Dimension Reduction一定是線性模型中最好、最有效率的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;另外，剛剛有提到的Linear Autoencode幾乎是等效於PCA，大家可以看上圖中的描述，這裡不多贅述，不過不同的是，Linear Autoencoder並沒有限制新空間軸必須是正交的特性，所以它的效率一定會比PCA來的差。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="_1"&gt;結語&lt;/h3&gt;
&lt;p&gt;這一篇當中，我們介紹了Neural Network，並且探討多層Neural Network—Deep Neural Network，也等同於Deep Learning，並且說明為什麼需要「Deep」，然後介紹Deep Learning最重要的演算法—反向傳播算法，接著介紹五種常用的Regularization的方法：設計Deep Neural Network的結構、限制W的大小、Early Stopping、Drop-out和Denoising Autoencoder。&lt;/p&gt;
&lt;p&gt;介紹完以上內容，我們就已經對於Deep Learning的全貌有了一些認識了，緊接著來看Deep Learning的特殊例子—Autoencoder，Autoencoder可以用來做Dimension Reduction，那既然提到了Dimension Reduction，那就不得不在講一下重要的線性方法PCA。&lt;/p&gt;
&lt;p&gt;那在下一回，我們會繼續探討Neural Network還有哪些特殊的分支。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>Python玩數據 (2)：Numpy [1/2]</title><link href="https://ycc.idv.tw/python-play-with-data_2.html" rel="alternate"></link><published>2017-04-17T12:00:00+08:00</published><updated>2017-04-17T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-04-17:/python-play-with-data_2.html</id><summary type="html">&lt;p&gt;Python常見的資料型別 / Numpy的數學運算 / Numpy基礎元素：ndarray / Numpy的矩陣運算&lt;/p&gt;</summary><content type="html">&lt;p&gt;在上一次我們已經成功了安裝了IPython，這將會是我們這系列教學的主要舞台，而今天我要教大家在這個舞台上利用Numpy來做一些簡單的科學計算。&lt;/p&gt;
&lt;h3 id="ipython"&gt;IPython&lt;/h3&gt;
&lt;p&gt;像上次一樣，打開IPython，緊接著把numpy和pandas載入，載入numpy之後我們習慣用&lt;code&gt;as&lt;/code&gt;將它縮寫為&lt;code&gt;np&lt;/code&gt;，pandas則縮寫為&lt;code&gt;pd&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="ipython" src="/media/PlayDataWithPython/ipython.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;IPython是一個具有互動式介面的python執行介面，你可以一邊寫一邊理解目前的狀況，舉個例子&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt; &lt;span class="c1"&gt;# integer(整數)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;     &lt;span class="c1"&gt;# check variable a&lt;/span&gt;
&lt;span class="mi"&gt;12&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;在第一行中，我令變數a為12，而第二行只要把變數a直接key出來，我們就可以立刻查看變數裡頭有什麼內容，注意喔！在一般的python語言中，直接把變數key出來這件事是沒有意義的，這只有在IPython上才有的方便功能，&lt;strong&gt;有了這樣一個互動式的介面，讓我們在處理數據的時候可以隨時查看，目前數據的狀況&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="python"&gt;Python常見的資料型別&lt;/h3&gt;
&lt;p&gt;Python常見的資料型別有整數(integer)、浮點數(floating-point number)、字串(string)、串列(list)、序對(tuple)、字典(dictionary)，可以使用&lt;code&gt;type()&lt;/code&gt;來查詢資料型別。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="c1"&gt;# integer&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;40.0&lt;/span&gt; &lt;span class="c1"&gt;# float, 必須有&amp;#39;.&amp;#39;&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;word&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;# string&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# list&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;6&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# tuple&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="c1"&gt;# dictionary&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;str&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;list&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;dict&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;list和tuple裡面可以塞入任意的資料型別，甚至可以塞入另外一個list，或是自己定義的物件，list和tuple其實非常的相似，差異只在於tuple一旦決定了就不能在變更，但是list卻可以。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;
&lt;span class="normal"&gt;9&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;new&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;new&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# 取出第一項(index=0)加一再設定回去第一項&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;new&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# 刪除index=1的那項&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;new&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;new&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# fail&lt;/span&gt;
&lt;span class="ne"&gt;AttributeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tuple&amp;#39;&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt; &lt;span class="n"&gt;has&lt;/span&gt; &lt;span class="n"&gt;no&lt;/span&gt; &lt;span class="n"&gt;attribute&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;append&amp;#39;&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="c1"&gt;# fail&lt;/span&gt;
&lt;span class="ne"&gt;TypeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tuple&amp;#39;&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt; &lt;span class="n"&gt;does&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;support&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="n"&gt;assignment&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c1"&gt;# fail&lt;/span&gt;
&lt;span class="ne"&gt;TypeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;tuple&amp;#39;&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt; &lt;span class="n"&gt;doesn&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;t support item deletion&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;在python中，整數和浮點數可以作簡單的四則運算&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;6.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;
&lt;span class="mf"&gt;2.0&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;  &lt;span class="c1"&gt;# 指數&lt;/span&gt;
&lt;span class="mi"&gt;9&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;一群整數做完運算輸出是整數，一群浮點數做完運算輸出是浮點數，那假如整數和浮點數混雜的情形呢？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="c1"&gt;# 整數除整數，結果必定是整是，是整數0，而不是想像中的0.5，這種運算效果有點像求商&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;
&lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="c1"&gt;# 只要有任意浮點數出現，整數強迫轉為浮點數，然後再做運算，這才是我們要的結果&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;
&lt;span class="mf"&gt;9.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;所以在運算之前，你要想清楚你想要的目標是什麼？如果你有一個整數變數&lt;code&gt;someInt&lt;/code&gt;接下來要作浮點數運算，可以使用&lt;code&gt;float(someInt)&lt;/code&gt;強制先轉成浮點數再做接下來的運算，這樣比較不會犯錯。&lt;/p&gt;
&lt;p&gt;事實上，轉成浮點數這樣的自動轉換在python中是很少見的，python是屬於&lt;strong&gt;強型別語言，所以型別和型別之間有很強的區份性，常常不會自動轉換&lt;/strong&gt;，如果需要轉換必須要作額外的操作。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;# fail&lt;/span&gt;
&lt;span class="ne"&gt;TypeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;unsupported&lt;/span&gt; &lt;span class="n"&gt;operand&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;int&amp;#39;&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;str&amp;#39;&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 使用int()將字串轉成整數&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;常見的型別轉換函數有&lt;code&gt;int()&lt;/code&gt;, &lt;code&gt;float()&lt;/code&gt;, &lt;code&gt;str()&lt;/code&gt;, &lt;code&gt;list()&lt;/code&gt;, &lt;code&gt;tuple()&lt;/code&gt;。所以如果要對一個tuple做更改，可以先轉成list再做運算。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 在index為0的地方插入整數4&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="numpy"&gt;Numpy的數學運算&lt;/h3&gt;
&lt;p&gt;在上一段我簡單介紹了python內建的運算，在大多數情況，內建的運算就已經足夠應付了，不過如果遇到複雜的運算，例如：三角函數、取最大最小值、exp、log、開根號、矩陣運算，我們就需要用到 Numpy    。&lt;/p&gt;
&lt;p&gt;首先先介紹Numpy的一些數學運算，Numpy的數學運算詳細&lt;a href="https://docs.scipy.org/doc/numpy/reference/routines.math.html"&gt;參考這&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;我這邊舉幾個比較常見的例子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c1"&gt;# 加總&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c1"&gt;# 最大值&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="c1"&gt;# 最小值&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 求餘數&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 求sin&lt;/span&gt;
&lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# ln 和 e&lt;/span&gt;
&lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="numpyndarray"&gt;Numpy基礎元素：ndarray&lt;/h3&gt;
&lt;p&gt;Numpy最重要的元素就是ndarray，它是N-Dimensional Array的縮寫，在Numpy裡，dimesions被稱為axes，而axes的數量被稱為rank，axes是一個重要的概念，了解這個概念基本上就把Numpy搞懂一半以上了。&lt;/p&gt;
&lt;p&gt;先來建立一個簡單1D的ndarray&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;從外到內第一個遇到的中括號就是axis=0，往內就遞增上去，所以從1到2再到3，這個方向就叫做axis=0，Numpy大部分的運算都支援陣列的運算，經常你需要限制要在哪個axis方向上作運算，舉個例子&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# axis為None的時候則加總所有元素&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# fail 因為A只有一維&lt;/span&gt;
&lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;axis&amp;#39;&lt;/span&gt; &lt;span class="n"&gt;entry&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;bounds&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;另外，也可以由內往外數，最內部的第一個中括後就是axis=-1，越外面就越負。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;剛來上面的例子可能看不出效果，再來就稍微有趣一點，我們來看看2D的ndarray&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;   &lt;span class="c1"&gt;# [1+4, 2+5, 3+6]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;    &lt;span class="c1"&gt;# [1+2+3, 4+5+6]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;有看懂axis怎麼運作嗎？最外面的中括號是axis=0，它包含[1,2,3]和[4,5,6]兩個元素，方向就是從[1,2,3]到[4,5,6]的方向，在這個方向上做sum，所以結果就會得到[1+4, 2+5, 3+6]。若是axis=1則是第二層中括號，也就是1到3和4到5的方向，所以結果會是[1+2+3, 4+5+6]。&lt;/p&gt;
&lt;p&gt;一樣從內而外也可以，如果axis=None或defalut情形下，則是對矩陣內所有元素作運算。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="mi"&gt;21&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# same as above&lt;/span&gt;
&lt;span class="mi"&gt;21&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 和axis=1等價&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;相信大家已經有感覺了，那3D也是一樣道理的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;
&lt;span class="normal"&gt;12&lt;/span&gt;
&lt;span class="normal"&gt;13&lt;/span&gt;
&lt;span class="normal"&gt;14&lt;/span&gt;
&lt;span class="normal"&gt;15&lt;/span&gt;
&lt;span class="normal"&gt;16&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]],[[&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;    &lt;span class="c1"&gt;# [1+7,  2+8,  3+9 ]&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;   &lt;span class="c1"&gt;# [4+10, 5+11, 6+12]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;    &lt;span class="c1"&gt;# [1+4, 2+5, 3+6]&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;   &lt;span class="c1"&gt;# [7+10, 8+11, 9+12]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;        &lt;span class="c1"&gt;# [1+2+3, 4+5+6]&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;       &lt;span class="c1"&gt;# [7+8+9, 10+11+12]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;畫張圖可能比較好理解一點，在各個方向上加總的結果都不一樣。&lt;/p&gt;
&lt;p&gt;&lt;img alt="ndarray axis" src="/media/PlayDataWithPython/ndarray_axis.png" /&gt;&lt;/p&gt;
&lt;p&gt;同樣，axis的概念也可以用在矩陣的shape&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;(3, 2)&lt;/code&gt;這樣的shape我們就一點都不意外了，axis=0有三個元素，而axis=1有兩個元素。shape可以直接改，如果數量恰當的話就會自動重組。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;

       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;]]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;axis=0有兩個元素，axis=1有一個元素，axis=2有三個元素。&lt;/p&gt;
&lt;p&gt;同樣的概念也可以用在取出單一元素上。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;在axis=0上選第二個元素(1)，在axis=1上選第一個元素(0)，在axis=2上選第二個元素(1)，所以選出來的元素就是5啦！&lt;/p&gt;
&lt;p&gt;有了axis的概念，我們來看另外一個重要的概念—dtype。&lt;/p&gt;
&lt;p&gt;ndarray有其資料型別，這個資料型別就稱為dtype，有哪些內建的資料型別呢？我們可以透過numpy的內建資料來查看。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sctypes&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;complex&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;complex64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;complex128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;complex256&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;float&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float128&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;int&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int64&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;others&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;unicode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;void&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
 &lt;span class="s1"&gt;&amp;#39;uint&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint64&lt;/span&gt;&lt;span class="p"&gt;]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;有複數、浮點數、整數，另外每個資料型別還可以由資料的儲存容量大小來區分，例如：numpy.int32就代表是容量為32bits的整數。我們可以在設置ndarray的時候事先強迫設成某資料型別。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;int32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;
&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;int32&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t2&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;t2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;
&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="numpy_1"&gt;Numpy的矩陣運算&lt;/h3&gt;
&lt;p&gt;有了ndarray就可以作矩陣的運算了，矩陣運算有兩種系統，一種是element-wise(元素方面) operation，一種是matrix operation。&lt;/p&gt;
&lt;p&gt;這樣講好像很抽象，我來解釋一下，element-wise operation就是每個元素獨立運算，例如，以下例子就是element-wise的相加。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;      &lt;span class="c1"&gt;# element-wise plus&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;6.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;A和B矩陣中同樣位置的元素相加，再放到新的矩陣中，這一種操作就叫做element-wise operation。&lt;/p&gt;
&lt;p&gt;在numpy中如果沒有特別指定，所有的運算都是這類的運算，我們來看一下減、乘和除。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;
&lt;span class="normal"&gt;9&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;      &lt;span class="c1"&gt;# element-wise minus&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;      &lt;span class="c1"&gt;# element-wise multiply&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;      &lt;span class="c1"&gt;# element-wise divide&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;那我如果想要作矩陣操作(matrix operation)呢？譬如說矩陣內積，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 矩陣內積&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;  &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;15.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;還有更多的矩陣操作，&lt;/p&gt;
&lt;p&gt;矩陣轉置&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="c1"&gt;# 矩陣轉置&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;反矩陣&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A_rev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# 反矩陣&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A_rev&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2.&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;A_rev&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;  &lt;span class="mf"&gt;1.00000000e+00&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;0.00000000e+00&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt;  &lt;span class="mf"&gt;8.88178420e-16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mf"&gt;1.00000000e+00&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;A和A的反矩陣內積為單位矩陣，你有注意到&lt;code&gt;8.88178420e-16&lt;/code&gt;這個奇怪的數字嗎？這是因為python在計算的過程有一些誤差的緣故，所以才會產生一個這麼小的數字，但基本上可以看作是0。&lt;/p&gt;
&lt;p&gt;另外矩陣跟矩陣間也可以合併。&lt;/p&gt;
&lt;p&gt;垂直方向合併&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;V&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;V&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;水平方向合併&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;當然也可以分割矩陣，&lt;/p&gt;
&lt;p&gt;垂直方向分割&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vsplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;V&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# 2代表切兩份&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;水平方向分割&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hsplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# 4代表切四份&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;1.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;3.&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;2.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;4.&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;5.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt; &lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.&lt;/span&gt;&lt;span class="p"&gt;]])]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;h3 id="_1"&gt;子彈總結&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Python常見的資料型別：整數(integer)、浮點數(floating-point number)、字串(string)、串列(list)、序對(tuple)、字典(dictionary)&lt;/li&gt;
&lt;li&gt;ndarray的axes概念很重要，這會決定函數操作的方式，例如：np.sum&lt;/li&gt;
&lt;li&gt;ndarray的資料型別(dtype)，例如：'float64', 'int64', 'string', ...&lt;/li&gt;
&lt;li&gt;numpy的矩陣運算有element-wise operation和matrix operation兩種&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Numpy的基礎概念我們已經有了，在下一篇當中會再更深入的了解Numpy還有什麼進階的功能，包括：產生ndarray的多種方法、broadcast的概念以及ndarray的進階操作手法。&lt;/p&gt;</content><category term="CS"></category><category term="Python"></category></entry><entry><title>大數據 Big Data:A Revolution That Will Transform How We Live, Work, and Think</title><link href="https://ycc.idv.tw/big-data-a-revolution.html" rel="alternate"></link><published>2017-04-07T12:00:00+08:00</published><updated>2017-04-07T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-04-07:/big-data-a-revolution.html</id><summary type="html">&lt;p&gt;樣本=總體 / 允許不精確 / 「是什麼」比「為什麼」還重要 / 大數據時代的商業變革 / 全息社會&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="cover" src="/media/Reading/BigData_pic.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;最近，Big Data這個詞相當的紅，但是對於這個詞我們還是有很多的誤會，一個常見的疑問是，究竟多大才可以稱得上是大數據呢？事實上，我接下來要介紹的這本書告訴你，大數據多「大」不是重點，重點是你怎麼看待和處理數據。&lt;/p&gt;
&lt;p&gt;「&lt;a href="http://www.books.com.tw/products/0010587258"&gt;大數據&lt;/a&gt;」這本書分為三個部分，在第一個部分，作者為讀者介紹大數據的三大思維變革，包括：採用全體數據取代抽樣數據、容忍資料的混雜特性、「是什麼」比「為什麼」還重要，第二部分則在講述大數據如何改變了商業、市場和社會的本質。第三部分在探討大數據會對人類產生什麼不好的影響，而我們如何去避免。本篇我主要著墨於第一部分和第二部分。&lt;/p&gt;
&lt;h3 id="_1"&gt;樣本=總體&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;大數據是指不採用統計「隨機採樣」這樣的捷徑，而直接處理所有的數據。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在資料分析中，如果要研究的對象（母群體）非常的龐大、資料量非常大，我們通常會採取「隨機採樣」來處理，這條捷徑在處理特定問題非常成功，也因此它成為現代社會、現代測量領域的主要路數，但這方式存在著一些缺陷。&lt;/p&gt;
&lt;p&gt;「隨機採樣」的缺陷之一是無法瞭解更深層次的細節。在宏觀領域起作用的方法在微觀領域失去了作用。隨機採樣就像印象派的畫作一樣，遠看很不錯，可以看見整個整體趨勢，但是一旦聚焦於某一點，就會變得模糊不清。&lt;/p&gt;
&lt;p&gt;另外，「隨機採樣」還有一個缺陷是缺乏延展性，人們只能從採樣數據中得出事先設計好的問題的結果——千萬不要奢求採樣的數據還能回答你突然意識到的其他問題，也就是調查得出的數據不能夠重新分析以實現計劃之外的目的。&lt;/p&gt;
&lt;p&gt;不過，在目前這個技術和資訊爆炸的時代，我們訊息量的增長速度比世界經濟的增長速度快4倍，而電腦數據處理能力的增長速度則比世界經濟的增長速度快9倍，也因此我們有更充沛的資料和處理資料的能力，所以是時候應該丟棄以往的「隨機採樣」，而直接採用「樣本=總體」的方式。&lt;/p&gt;
&lt;p&gt;Xoom是一個專門從事跨境匯款業務的公司。2011年，它注意到用「發現卡」從新紐澤西州匯款的交易量比正常情況多一些，系統於是啟動警報。Xoom公司的CEO John Kunze(約翰·孔澤) 解釋說：「這個系統關注的是不應該出現的情況。」單獨來看，每筆交易都是合法的，但是事實證明這是一個犯罪集團在試圖詐騙。而要能發現異常的唯一方法是，需要檢查所有的數據，找出「隨機採樣」分析法所獲取不到的訊息。&lt;/p&gt;
&lt;p&gt;另外一個例子，Lytro相機，它把大數據運用到了基本的攝影中。與傳統相機只可以記錄一束光不同，Lytro相機可以記錄整個光場裡所有的光，可以達到1100萬束之多。具體生成什麼樣的照片則可以在拍攝之後再依照需要決定。用戶沒必要在一開始就聚焦，因為該相機可以捕捉到所有的數據，所以之後可以選擇聚焦圖像中的任一一點。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大數據所謂的「大」，並不是指數據量有多大，而是指如何處理數據的方法，直接處理「樣本=總體」，而非傳統的「隨機採樣」，我們將得到更多的細節，做更多的事。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="_2"&gt;允許不精確&lt;/h3&gt;
&lt;p&gt;對於採取隨機取樣的小數據而言，保證每筆資料的質量是相當重要的，為了使結果更加準確，很多科學家都致力於優化測量工具。不過，面對大數據的時候，我們可能增加不少不正確的資料，正因為我們無法逐一的檢查，甚至在資料的格式上也難以統一，因此大數據本身就具有混雜的特性。&lt;/p&gt;
&lt;p&gt;不過這混雜所造成的不準確也可以因為數據量大而彌補，事實上，&lt;strong&gt;大數據的簡單演算法比小數據的複雜演算法更為有效&lt;/strong&gt;，舉個例子，在冷戰時期，美國掌握了大量關於蘇聯的各種資料，但缺少翻譯這些資料的人手。所以，計算機翻譯也成了急需解決的問題。那個時候的科學家想藉由結合文法規則和字典來創造一個翻譯機器， 最後卻失敗了，他們發現機器翻譯不能只是讓電腦熟悉常用規則，還必須教會電腦處理「特殊的」語言情況。畢竟，翻譯不僅僅只是記憶和複述，也涉及選詞，而明確地教會電腦這些是非常困難的。&lt;/p&gt;
&lt;p&gt;時間拉回到現代，Google翻譯則採取另外一種方式，Google翻譯系統不由程式設計師直接告訴計算機要怎麼做，而是靠著資料來訓練計算機學習怎麼做，計算機會盡量吸收它能找到的所有翻譯文本，從各式各樣語言的公司網站上尋找對譯的文檔，還會去尋找聯合國和歐盟這些國際組織發佈的官方文件和報告的譯本，藉由這大量的數據去預測對譯詞語應該是什麼，&lt;strong&gt;然而儘管其輸入來源很混亂，但相較於其他翻譯系統而言，Google的翻譯質量相對而言還是最好的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;要想獲得大規模數據帶來的好處，混亂應該是一種標準途徑，而不應該去竭力避免，不過數據量一旦大，這些混亂所帶來的不精確將被彌補。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="_3"&gt;「是什麼」比「為什麼」還重要&lt;/h3&gt;
&lt;p&gt;大數據利用數值方法，他可以看到人類不容易看出來的相關性，兩件事雖然擁有相關性，但並不代表他們擁有因果關係，但是在大部分時間裡，相關性比因果關係更為重要。&lt;/p&gt;
&lt;p&gt;美國折扣零售商塔吉特（Target）使用大數據的相關性分析已經有很多年了。《紐約時報》的記者杜西格（Charles Duhigg）就在一份報道中闡述了塔吉特公司怎樣在完全不和准媽媽對話的前提下預測一個女性會在什麼時候懷孕。塔吉特公司注意到，資料上的婦女會在懷孕大概第三個月的時候買很多無香乳液。幾個月之後，她們會買一些營養品，比如鎂、鈣、鋅。公司最終找出了大概20多種關聯項目，這些關聯項目可以給顧客進行「懷孕趨勢」評分。杜西格在《習慣的力量》（The Power of Habit）一書中講到了接下來發生的事情。一天，一個男人衝進了一家位於明尼阿波利斯市郊的塔吉特商店，要求經理出來見他。他氣憤地說：「我女兒還是高中生，你們卻給她郵寄嬰兒服和嬰兒床的優惠券，你們是在鼓勵她懷孕嗎？」而當幾天後，經理打電話向這個男人致歉時，這個男人的語氣變得平和起來。他說：「我跟我的女兒談過了，她的預產期是8月份，是我完全沒有意識到這個事情的發生，應該說抱歉的人是我。」 &lt;/p&gt;
&lt;p&gt;在上述的例子，我們雖然不見得可以找出這20項關聯項和懷孕之間的因果關係，不過他們確實相關，所以我們可以用來預測。&lt;strong&gt;有些時候我們只需要知道「是什麼」就夠了，沒必要知道「為什麼」。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="_4"&gt;大數據時代的商業變革&lt;/h3&gt;
&lt;p&gt;Matthew Fontaine Maury是一位前途看好的美國海軍軍官，1839年，卻意外的出了車禍，使得他無法繼續在海上工作，不過危機就是轉機，在近三年的休養，美國海軍將他安排進辦公室，讓他負責修復陳舊的圖表和儀器，他在其中挖到了寶，那是一批航海日記，日記裡頭詳細的記載了特定時間在特定地點的風、水和天氣狀況，Maury意識到如果把這些資料整理起來，將會呈現一張全新的航海圖，這些數據將會比大家口耳相傳的經驗還有用，後來也證明Maury是對的，這資料幫助船長們省去了三分之一左右的航程，後來全世界第一條跨越大西洋的電報電纜也是建基在這個基礎之上。&lt;/p&gt;
&lt;p&gt;數據就像是一座鑽石礦，透過分析我們可以將其中的鑽石給掏出，事實上這金礦無所不在，數據可能藏於書籍或網路文本、數據可能藏於方位、數據可能藏於溝通網絡、數據可能藏於微型運動感測器，仔細留意，數據幾乎無所不在，什麼都可以量化，有了大數據的思維，我們不會再把世界看成只有單純是自然現象或是社會現象，我們會意識到世界的本質就是由眾多信息所構成的，而這會帶來的是一場商業上的變革。&lt;/p&gt;
&lt;p&gt;作者認為大數據時代，依照提供價值不同，分別會出現三類的大數據公司，第一種是擁有大量數據的公司，第二種是擁有技能挖掘數據的公司，最後一種是提供嶄新大數據思維的公司，能從數據中創造出意想不到的價值，第三種是作者最為推崇的，作者列了幾種數據創新的方法。&lt;/p&gt;
&lt;p&gt;作者提了五種數據創新方法，第一種是&lt;strong&gt;數據再利用&lt;/strong&gt;，有許多數據因為儲存成本低而被保存下來，不過沒有被充分的利用，數據科學家稱之為「數據墳場」，從這墳場中我們可以盜到很多的寶，就像Maury從航海日記撈出了許多有用的資訊一樣。&lt;/p&gt;
&lt;p&gt;第二種是&lt;strong&gt;數據間的整合&lt;/strong&gt;，丹麥同時擁有從1985年起的手機用戶數據庫和該國所有癌症患者的資訊，有人想到如果整合這兩者資訊，研究人員可以研究手機用戶是不是比非手機用戶顯示出更容易得癌症，最後，研究結果沒有發現這兩者存在著相關性。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;隨著大數據的出現，數據的總和比部分更有價值。當我們將多個數據集的總和重組在一起時，重組總和本身的價值也比單個總和更大&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第三種是&lt;strong&gt;具可擴張性的數據&lt;/strong&gt;，要使得數據可以一再的利用，我們必須在一開始就設計好他的可擴張性，也就是要盡可能的一次蒐集所有資料齊全，舉個知名的例子，Google街景拍攝，其備受爭議的街景汽車不僅僅拍攝房屋和道路的照片，他還同時採集了每個位置的GPS數據，甚至還加入了無線網路名稱的蒐集，一輛Google街景車每時每刻都在累積大量的各方面的數據，而這些資訊可能在目前用不到，不過未來的某天可能會用到，花一次的錢可以得到更多的好處。&lt;/p&gt;
&lt;p&gt;第四種是&lt;strong&gt;必須考慮數據的折舊&lt;/strong&gt;，譬如你在亞馬遜十年前買一本書的資訊，一定不會比昨天剛購買的資訊重要，所以資料還必須考慮它隨時間下降的重要程度。&lt;/p&gt;
&lt;p&gt;第五種是&lt;strong&gt;數據廢氣能回收再利用&lt;/strong&gt;，什麼是數據廢氣呢？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一個用來描述人們在網上留下的數字軌跡的藝術詞彙出現了，這就是「數據廢氣」。它是用戶在線交互的副產品，包括瀏覽了哪些頁面、停留了多久、滑鼠光標停留的位置、輸入了什麼信息等。許多公司因此對系統進行了設計，使自己能夠得到數據廢氣並循環利用，以改善現有的服務或開發新服務。 &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Google就是這方面的高手，例如錯誤拼寫校正，Google擁有世界上最完整的拼寫檢查器，基本上涵蓋了各種語言，而且Google幾乎免費的獲得這種能力，它依據每天處理的30億個錯誤拼寫的查詢，一個巧妙的反饋系統可以讓用戶告訴Google他其實是想輸入什麼字，當搜尋頁面頂部顯示「你要找的是不是：流行病學」時，如果是的話，你將會點選並讓Google了解你真正想查的字詞，原本輸入錯誤這樣的數據廢氣卻被巧妙的回收再利用來優化它的系統。&lt;/p&gt;
&lt;h3 id="_5"&gt;全息社會&lt;/h3&gt;
&lt;p&gt;大數據正在慢慢影響這個社會，包括我們的知識取得方式，包括我們的社交活動，甚至在未來會決定人類很多的決策，大至公司策略發展，小至個人理財規劃，確實，大數據和機器學習的引入可能會取代掉許多目前的工作，不過也同時會創造更多新的工作內容，讓人類可以盡情發揮潛能，把更多的精力放在創造之上，如果亨利·福特問大數據他的顧客想要的是什麼，大數據將會回答，「一匹更快的馬。」在全息社會中，包括創意、直覺、冒險精神和知識野心在內的人類特性的培養顯得尤為重要，人類的進步正是源自我們的獨創性。&lt;/p&gt;</content><category term="Reading"></category></entry><entry><title>機器學習技法 學習筆記 (5)：Boost Aggregation Models</title><link href="https://ycc.idv.tw/ml-course-techniques_5.html" rel="alternate"></link><published>2017-04-02T12:00:00+08:00</published><updated>2017-04-02T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-04-02:/ml-course-techniques_5.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋AdaBoost (Adaptive Boost)、Gradient Boost、AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)。&lt;/p&gt;</summary><content type="html">&lt;h3 id="boost"&gt;Boost的精髓&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們介紹的Aggregation Models都屬於沒有Boost的，不管是Bagging或Decision Tree都沒有要試著在Training的過程中改善Model，&lt;strong&gt;而這篇將要提到的Boost方法，則是在產生每個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;時試圖讓Model整體更完善，更能發揮Aggregation Models中截長補短中的「補短」的效果，也就是說&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;可以彼此互補不足之處&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那實際上我們應該怎麼做才能實踐Boost呢？其實方法的道理早就透漏在上一回中的Bagging和Decision Tree裡頭了，不管是Bagging和Decision Tree都是使用變換Data來做到變異度，在這個方法下Model的架構可以本身是不變的，這帶來相當的便利性，而今天我們要講的Boost也同樣的利用「變換Data」來做到變異度，但不同的是Boost的過程中「變換Data」這件事是有目標性的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boost方法在「變換Data」時會試著去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善，這就是Boost的主要精髓。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="adaboost-adaptive-boost-for-classification"&gt;AdaBoost (Adaptive Boost) for Classification&lt;/h3&gt;
&lt;p&gt;剛剛上一段的最後我已經揭露了Boost的真正精髓，拿這樣的概念來做分類問題，就是我們接下來要談的AdaBoost，全名稱為Adaptive Boost。&lt;/p&gt;
&lt;p&gt;在分類問題中我們怎麼做到「凸顯原先做錯的Data」？簡單的想法是這樣的，我們可以減少原本已經是正確分類的Data的數量，然後增加原本錯誤分類的Data的數量，&lt;strong&gt;增減Data的數量其實是等效於改變每筆Data的權重&lt;/strong&gt;，假如我們給每筆資料權重，要做的事是拉低正確分類Data的權重，而且拉高錯誤分類Data的權重。&lt;/p&gt;
&lt;p&gt;那我們應該要提升權重或降低權重到什麼程度才是OK的呢？換個方式思考，我們為什麼要去調整權重？目的其實是要去凸顯原先做錯的部分，降低原本做對的部分，也就是想&lt;strong&gt;藉由調整每筆Data的多寡或權重來做到「弭平原先的預測性」，最好可以讓原本的預測方法看起來是隨機分布&lt;/strong&gt;，也就是「錯誤率＝正確率」，讓它像是擲銅板一樣，沒有什麼預測能力。&lt;/p&gt;
&lt;p&gt;&lt;img alt="AdaBoost" src="/media/MachineLearningTechniques/MachineLearningTechniques.012.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;有了概念之後，我們來看實際應該要怎麼做？見上圖說明，首先我們需要先將Data權重&lt;span class="math"&gt;\(u^{(1)}\)&lt;/span&gt;先初始化，接下來就可以開始找&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;了，我們使用任意一個分類問題的Model搭配上Data的權重，求得一組&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，接下來計算這組&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的&lt;strong&gt;「錯誤率」&lt;span class="math"&gt;\(ε_{t}\)&lt;/span&gt;&lt;/strong&gt;，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(ε_{t}= 𝚺_{n} u_{n}^{(t)} ⟦y_{n}≠g_{t}(x_{n})⟧ / 𝚺_{n} u_{n}^{(t)}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有注意到考慮「錯誤率」&lt;span class="math"&gt;\(ε_{t}\)&lt;/span&gt;的時候必須要評估&lt;span class="math"&gt;\(u_{n}^{(t)}\)&lt;/span&gt;，要記得會有Data權重是為了表示增加或減少原本的Data的數量，所以依照每筆Data的出現機會不同，會有不同的權重，也就會有對「錯誤率」不同的貢獻程度。&lt;/p&gt;
&lt;p&gt;那為了待會要對權重重新分配，我們先定義了&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;，在未來我會將錯誤的Data的權重乘上&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;，即&lt;span class="math"&gt;\(u_{n}^{(t+1)}=u_{n}^{(t)}×β_{t}\)&lt;/span&gt;，並且把正確的Data權重除以&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;，即&lt;span class="math"&gt;\(u_{n}^{(t+1)}=u_{n}^{(t)}/β_{t}\)&lt;/span&gt;，&lt;strong&gt;而期望的結果是重新分配的Dataset在&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的預測下可以表現的像隨機的一樣，於是乎下一次使用這組Dataset訓練出來的&lt;span class="math"&gt;\(g_{t+1}\)&lt;/span&gt;將會彌補&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的不足&lt;/strong&gt;，根據這樣的原則我們來推一下&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(𝚺_{n} u_{n}^{(t+1)} ⟦y_{n}≠g_{t}(x_{n})⟧ / 𝚺_{n} u_{n}^{(t+1)}=1/2\)&lt;/span&gt; (預測能力像隨機分布)&lt;/p&gt;
&lt;p&gt;⇒  &lt;span class="math"&gt;\(𝚺_{n} u_{n}^{(t+1)} ⟦y_{n}≠g_{t}(x_{n})⟧ = 𝚺_{n} u_{n}^{(t+1)} ⟦y_{n}=g_{t}(x_{n})⟧\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;⇒  &lt;span class="math"&gt;\(𝚺_{n} (u_{n}^{(t)}×β_{t})  ⟦y_{n}≠g_{t}(x_{n})⟧ = 𝚺_{n} (u_{n}^{(t)}/β_{t}) ⟦y_{n}=g_{t}(x_{n})⟧\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;⇒  &lt;span class="math"&gt;\(β_{t}^{2} = \frac{𝚺_{n} u_{n}^{(t)} ⟦y_{n}=g_{t}(x_{n})⟧ }{𝚺_{n} u_{n}^{(t)}  ⟦y_{n}≠g_{t}(x_{n})⟧}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;⇒  &lt;span class="math"&gt;\(β_{t}^{2} = \frac{𝚺_{n} u_{n}^{(t)} ⟦y_{n}=g_{t}(x_{n})⟧ /  𝚺_{n} u_{n}^{(t)}}{ 𝚺_{n} u_{n}^{(t)}  ⟦y_{n}≠g_{t}(x_{n})⟧ / 𝚺_{n} u_{n}^{(t)}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;⇒  &lt;span class="math"&gt;\(β_{t}^{2} = \frac{1-ε_{t}}{ε_{t}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;⇒  &lt;strong&gt;&lt;span class="math"&gt;\(β_{t} = \sqrt{\frac{1-ε_{t}}{ε_{t}}}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以我們就可以利用這個&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;來更新我的Data權重，並且在多次迭代後，得到很多個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;。而將來我們會把所有的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;做線性組合，而我們希望&lt;strong&gt;「錯誤率」越低的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;可以有更高的貢獻度&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;&lt;/strong&gt;，所以使用&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;緊接著計算「&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的權重」&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;，定義為&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(α_{t} = ln(β_t)\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以當一個百分之一百可以完全預測的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;出現時，它的&lt;span class="math"&gt;\(ε_{t}=0\)&lt;/span&gt;，此時它的&lt;span class="math"&gt;\(β_{t} →∞\)&lt;/span&gt;，同時&lt;span class="math"&gt;\(α_{t} →∞\)&lt;/span&gt;，所以這樣的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;會有完全的貢獻。&lt;/p&gt;
&lt;p&gt;如果一個預測效果很差的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;出現，它的&lt;span class="math"&gt;\(ε_{t}=1/2\)&lt;/span&gt;，此時它的&lt;span class="math"&gt;\(β_{t}=1\)&lt;/span&gt;，同時&lt;span class="math"&gt;\(α_{t}=0\)&lt;/span&gt;，所以這樣的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;並沒有任何參考價值。&lt;/p&gt;
&lt;p&gt;那如果出現一個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;它的&lt;span class="math"&gt;\(ε_{t} &amp;gt; 1/2\)&lt;/span&gt;，那這樣的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;並不能說它沒有用處，反而是一個很好的反指標，我們只需要反著看就好了，當&lt;span class="math"&gt;\(ε_{t} &amp;gt; 1/2\)&lt;/span&gt;時，&lt;span class="math"&gt;\(β_{t} &amp;lt; 1\)&lt;/span&gt;，所以&lt;span class="math"&gt;\(α_{t} &amp;lt; 0\)&lt;/span&gt;，這樣的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;具有逆向的貢獻。&lt;/p&gt;
&lt;p&gt;最後只要把這些訓練好的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;乘上各自的&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;再加總起來，我們就完成了AdaBoost啦！&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="gradient-boost-for-regression"&gt;Gradient Boost for Regression&lt;/h3&gt;
&lt;p&gt;剛剛我們講了AdaBoost，是個很神奇的方法，當我們做錯了，沒關係！從哪裡跌倒就從哪裡站起來，利用這種精神我們就可以做到Boost的效果，但美中不足的是上面的方法只能用在「分類問題」上，那如果我也想在「Regression問題」也做到Boost呢？這就是接下來要講的GradientBoost的方法。&lt;/p&gt;
&lt;p&gt;在課程中林軒田教授是從AdaBoost出發經過推導後，得到一個很像是Gradient Decent的式子，接下來將式子一般化成為可以使用任意Error Measure的形式，我稍微列一下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;GradientBoost: &lt;span class="math"&gt;\(min_{η}\ min_{h}\ (1/N) 𝚺_{n} Error[𝚺_{τ=1}^{τ=t-1} α_{τ} g_{τ}(x_{n}) + η h(x_{n}), y_{n}]\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們這邊會考慮Error為平方誤差&lt;span class="math"&gt;\((s-y)^{2}\)&lt;/span&gt;的結果，詳細的推導這邊就不多加討論，可以到影片中學習，這裡我想要從我觀察出來的觀點，概念性的來看這個GradientBoost的方法。&lt;/p&gt;
&lt;p&gt;「從哪裡跌倒就從哪裡站起來」就是Boost的精神，所以今天你有一個Regression問題沒做好，&lt;strong&gt;留下了餘數Residual，怎麼辦？那我就把這個餘數當作另外一個Regression問題來做它&lt;/strong&gt;，再把這個結果附到先前的那個就好啦！如果第一次Regression後的Model是&lt;span class="math"&gt;\(g_{1}(x)\)&lt;/span&gt;，那剩下的沒做好的餘數就應該是&lt;span class="math"&gt;\(y(x)-g_{1}(x)\)&lt;/span&gt;，我們拿這個餘數下去在做一次Regression得到另外一個Model &lt;span class="math"&gt;\(g_{2}(x)\)&lt;/span&gt;，此時合併這兩個結果的餘數就變成了&lt;span class="math"&gt;\(y(x)-g_{1}(x)-g_{2}(x)\)&lt;/span&gt;，就可以使用這個餘數繼續做下去，最後組合所有的&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;就會得到一個更好的Model。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gradient Boost" src="/media/MachineLearningTechniques/MachineLearningTechniques.013.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;依循這樣的概念我們來看GradientBoost作法，如上圖，一開始我們先初始化每一筆Data的預測值&lt;span class="math"&gt;\(s_{n}\)&lt;/span&gt;為0，再接下來開始產生&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，我們先把Data的 &lt;span class="math"&gt;\(y_{n}\)&lt;/span&gt; 減去每一筆Data當前的預測值&lt;span class="math"&gt;\(s_{n}\)&lt;/span&gt;，就會產生餘數&lt;span class="math"&gt;\((y_{n}-s_{n})\)&lt;/span&gt;，當然，在一開始&lt;span class="math"&gt;\(s_{n}=0\)&lt;/span&gt;，所以&lt;span class="math"&gt;\(y_{n}-s_{n}=y_{n}\)&lt;/span&gt;，等於是對原問題求解。&lt;/p&gt;
&lt;p&gt;接下來因為最後我們要線性組合&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;，所以需要決定&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;前面的係數&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;，也就是貢獻度，這個&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;的決定方式是去求解一個One-Variable-Linear-Regression (單變數線性迴歸)，目的是&lt;strong&gt;去縮放&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;使得它更接近剛剛的餘數&lt;span class="math"&gt;\((y_{n}-s_{n})\)&lt;/span&gt;，而找到這個縮放值就是&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;&lt;/strong&gt;。所以每一次&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;的產生都是為了可以把G(x)描述的更好，最後&lt;span class="math"&gt;\(G(x)=𝚺_{t} α_{t}g_{t}(x)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;看到這裡有人一定會認為One-Variable-Linear-Regression求&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;這一步是多餘的，因為在一開始做&lt;span class="math"&gt;\(\{x_{n},y_{n}-s_{n}\}\)&lt;/span&gt;的Regression中我們已經最佳化過&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;，那為什麼還要把&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;乘上&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;再做同樣的事呢？&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;一定是1的啊！就像我一開始舉的例子一樣啊！其實問題就出在於你把&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;理所當然的看成是線性模型，你才會覺得這一步是多餘的，如果&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;不是線性的，求&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;就很重要的，因為你要使用線性組合來組出&lt;span class="math"&gt;\(G(x)\)&lt;/span&gt;，但是你的&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;不是線性的，所以你只好在外面再用線性模型來包裝一遍。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="adaboosted-decision-treegradient-boosted-decision-tree-gbdt"&gt;AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)&lt;/h3&gt;
&lt;p&gt;&lt;img alt="AdaBoosted and GrandientBoosted DTree" src="/media/MachineLearningTechniques/MachineLearningTechniques.014.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;和Random Forest一樣，我們也可以將AdaBoost和GradientBoost套用到Decision Tree上面，&lt;strong&gt;如果是處理分類問題就使用AdaBoosted Decision Tree；那如果是處理Regression問題可以使用Gradient Boosted Decision Tree&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;但要特別注意的是，這邊的Decision Tree都必須是弱的，也就是Pruning過後的樹，如果直接使用完全長成的樹，你會發現在AdaBoosted Decision Tree中，因為&lt;span class="math"&gt;\(ε_{t}=0\)&lt;/span&gt;所以&lt;span class="math"&gt;\(α_{t}→∞\)&lt;/span&gt;；在Gradient Boosted Decision Tree中，&lt;span class="math"&gt;\(y_{n}-s_{n}→0\)&lt;/span&gt;，因為錯誤出現的太少了，所以造成我們不能真正使用到Boost的效果，也就失去做Boost的意義了，&lt;strong&gt;因此在做AdaBoosted Decision Tree或Gradient Boosted Decision Tree時要使用「弱」一點的Decision Tree&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="_1"&gt;結語&lt;/h3&gt;
&lt;p&gt;這一篇當中，我們完整提了Boost的方法，Boost的精神就是從哪裡跌倒就從哪裡站起來，使用變換Data權重的手法去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善。&lt;/p&gt;
&lt;p&gt;我們提了兩種Boost的方法，如果是處理分類問題就使用AdaBoost；如果是處理Regression問題可以使用GradientBoost，而且這兩種方法都可以和Decision Tree做結合。&lt;/p&gt;
&lt;p&gt;以上兩回，我們已經完成了Aggregation Models了，接下來的下一回將要探討的就是現今很流行的類神經網路和深度學習等等。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>輕鬆談演算法的複雜度分界：什麼是P, NP, NP-Complete, NP-Hard問題</title><link href="https://ycc.idv.tw/algorithm-complexity-theory.html" rel="alternate"></link><published>2017-03-30T12:00:00+08:00</published><updated>2017-03-30T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-03-30:/algorithm-complexity-theory.html</id><summary type="html">&lt;p&gt;Turing Machine / 時間複雜度 / P＝NP？ / NP-Complete 問題&lt;/p&gt;</summary><content type="html">&lt;p&gt;在寫程式的時候，會聽到有人說這些問題是NP-Complete問題，或說這些是P問題，那這到底是什麼東西？其實這就是一套定義演算法複雜度的方法，今天我就想帶大家來聊聊這個艱澀但有趣的話題。&lt;/p&gt;
&lt;h3 id="turing-machine"&gt;Turing Machine&lt;/h3&gt;
&lt;p&gt;我們先從 &lt;a href="https://en.wikipedia.org/wiki/Turing_machine"&gt;Turing machine&lt;/a&gt;（圖靈機）開始講起，Turing machine是現代電腦的基本雛型，是英國數學家圖靈（Alan Turing）於1936年提出的一種抽象計算模型，這個計算模型在猜想上可以「計算所有在演算法中可計算的問題」，也就是可以解決人類所有可解的問題，這個猜想稱之為 &lt;a href="https://en.wikipedia.org/wiki/Church–Turing_thesis"&gt;Church–Turing thesis&lt;/a&gt;（thesis代表假設或猜想），僅管目前還無法證明這個猜想，但是目前為止它幾乎是完全被接受的。&lt;/p&gt;
&lt;p&gt;簡單的談一下 Turing machine的基本架構，首先我們需要一個磁帶，這一條磁帶上面可以一格一格的填入一些 symbols，這可以是單純的 0/1 symbols 或者更多種類的 symbols，但這些 symbols 的數量必須是有限的，而這個symbols就可以當作我的輸入，接下來我需要一個讀寫頭，這個讀寫頭會在磁帶上讀取或寫入 symbol，或左右移動，另外這個讀寫頭存有一個 state，這個 state 會隨著狀況改變，然後我就利用 symbol 和 state 來建立一個規則表，舉個例子，譬如說：初始的 state 是 &lt;span class="math"&gt;\(q_0\)&lt;/span&gt;，如果讀寫頭在 &lt;span class="math"&gt;\(q_0\)&lt;/span&gt; 的情況下讀到 symbol 0，就寫入 symbol 1，並且向右移動3格，並且改變 state &lt;span class="math"&gt;\(q_0\)&lt;/span&gt; 為 &lt;span class="math"&gt;\(q_1\)&lt;/span&gt;，... 等等，藉由可操作的規則來完成我們想做的運算，最後最重要的是它必須有一個 halt state 讓機器知道已經計算完畢了。Turing machine 不僅僅在理論上可以做任何的計算，而更有價值的是 Turing machine 的架構是有辦法用物理的方式來製造的，所以才會有現代電腦這玩意兒。&lt;/p&gt;
&lt;p&gt;說到電腦，更嚴謹地說，我們當今的電腦架構是比較接近 &lt;a href="https://en.wikipedia.org/wiki/Turing_machine"&gt;deterministic Turing machine&lt;/a&gt; (DTM)，與之對比的是 &lt;a href="https://en.wikipedia.org/wiki/Non-deterministic_Turing_machine"&gt;non-deterministic Turing machine&lt;/a&gt; (NTM)，我來好好的解釋一下，deterministic 的中文稱為決定性，所以 non-deterministic 就是非決定性。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果給予 Turing machine 某個 state 和某個 symbol 下，它的下一步如果只有一種可能，那我們就稱它為 deterministic Turing machine (DTM)，&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以上述的讀取頭每次就依照當下特定的 symbol 和 state 然後「決定」下一步應該要怎麼動作。&lt;/p&gt;
&lt;p&gt;但是 non-deterministic Turing machine (NTM) 就不拘於此，針對某個 state 和某個 symbol 它的下一步可能會有很多種，它會是一個分支，它可能同時要向右移3格，又同時要向左移動2格，所以你可以想像一下你的讀寫頭一分為二，然後再各自進行自己的任務，這個分支可以有無限多個，只要最後有某個分支到達 halt state，我們就解完問題並停止計算，這就是 non-deterministic Turing machine (NTM)。&lt;/p&gt;
&lt;p&gt;顯而易見的，DTM 只是 NTM 的特例，所以 NTM 比 DTM 擁有更快的計算速度，但這裡不要誤會喔！不管是 DTM 和 NTM 能解的問題是一樣多的，而且在數學上可以將 NTM 的計算方式轉換成 DTM 的計算方式，他們差異點只是解決相同問題所用到的時間複雜度不一樣，不過這就很關鍵。&lt;/p&gt;
&lt;h3 id="_1"&gt;時間複雜度&lt;/h3&gt;
&lt;p&gt;接下來，我要開始切入正題，我們來聊聊時間複雜度吧！什麼是時間複雜度呢？時間複雜度用來評估演算法需要花多少時間做計算，我們常用&lt;a href="https://zh.wikipedia.org/wiki/大O符号"&gt;大O符號&lt;/a&gt;來描述，代表的是一個漸進的函數數量級上界，舉個例子，假設我想要在一個有序的數列&lt;span class="math"&gt;\(2,3,5,7,13,27\)&lt;/span&gt;中找到&lt;span class="math"&gt;\(7\)&lt;/span&gt;的位置，最簡單的做法就是從第一個元素開始檢查起，如果不是元素&lt;span class="math"&gt;\(7\)&lt;/span&gt;就再找下一個，直到找到為止，所以最差的情形就是我一路找直到了最後一個元素，如果數列有&lt;span class="math"&gt;\(Ｎ\)&lt;/span&gt;個元素，我們最差的情形就是做了&lt;span class="math"&gt;\(Ｎ\)&lt;/span&gt;次的比較，而每次做比較所花的時間是一個常數時間，因此這個演算法的上界將被 &lt;span class="math"&gt;\(a×N\)&lt;/span&gt; 所界定，&lt;span class="math"&gt;\(a\)&lt;/span&gt;為常數，所以這個演算法的時間複雜度為&lt;span class="math"&gt;\(O(N)\)&lt;/span&gt;，再舉個稍微難一點的例子：&lt;a href="https://en.wikipedia.org/wiki/Subset_sum_problem"&gt;子集合加總問題&lt;/a&gt;，假設給予一組集合&lt;span class="math"&gt;\(\{−7, −3, −2, 5, 8\}\)&lt;/span&gt;，然後問是否有一組子集合相加為&lt;span class="math"&gt;\(0\)&lt;/span&gt;，怎麼做呢？最簡單的做法就是，窮舉出所有可能的子集合然後相加驗證是否剛好為0，假設集合中有Ｎ個元素，我會有&lt;span class="math"&gt;\(2^N\)&lt;/span&gt;種的子集合，而且要加總最多&lt;span class="math"&gt;\(Ｎ\)&lt;/span&gt;個元素，所以這個過程的時間複雜度為 &lt;span class="math"&gt;\(O(N×(2^N))\)&lt;/span&gt;。特別提醒，以上的分析方式大致上是符合DTM和現代電腦的運作方式，一步接著一步做（step-by-step），而NTM就不這麼分析問題，當然兩者看待同一個問題的時間複雜度就會不一樣。&lt;/p&gt;
&lt;p&gt;剛剛有提到 Turing machine 可以解所有演算法問題，那如果我製造一台機器符合 Turing machine或者是我購買一台電腦，是不是就可以躺著解所有的問題了，很可惜的，並不是的！我們剛剛有簡單的帶大家了解時間複雜度，我們知道每種演算法有其計算時間，子集合加總問題的時間複雜度為&lt;span class="math"&gt;\(O(N×(2^N))\)&lt;/span&gt;，如果今天很單純的，我的元素只有&lt;span class="math"&gt;\(1000\)&lt;/span&gt;個，這個數量不過分吧！但大家試著計算一下&lt;span class="math"&gt;\(1000 ×(2^{1000})\)&lt;/span&gt;就會發現這是一個天文數字，它大到縱使每個計算只需要&lt;span class="math"&gt;\(0.00001\)&lt;/span&gt;秒，也需要遠遠超過地球年齡的時間才有辦法算完，因此這類問題就算是可解的，也並不實際代表你可以解完，因為你必須考慮解問題要花多少時間。&lt;/p&gt;
&lt;p&gt;所以現實面是只有在一個數量級時間以下的問題我們才好應付，這個數量級被稱為 polynomial time（多項式時間），用大&lt;span class="math"&gt;\(Ｏ\)&lt;/span&gt;表示為&lt;span class="math"&gt;\(Ｏ(N^k)\)&lt;/span&gt;，剛剛上面提到的數列找元素問題，它得時間複雜度為&lt;span class="math"&gt;\(O(N)\)&lt;/span&gt;，為 &lt;a href="https://zh.wikipedia.org/wiki/多項式時間"&gt;polynomial time&lt;/a&gt;，這是屬於好對付的問題，如果超過 polynomial time 的問題我們稱為 &lt;a href="https://en.wikipedia.org/wiki/Intractability_(complexity)"&gt;intractable&lt;/a&gt; problem (難解的問題)。&lt;/p&gt;
&lt;h3 id="pnp"&gt;P＝NP？&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;如果有一群演算法用DTM來做計算所需時間是 polynomial time，那這類演算法或問題被稱為Ｐ問題，Ｐ就是 polynomial-time 的縮寫。&lt;/p&gt;
&lt;p&gt;另外如果有一群演算法用NTM來做計算所需時間是 polynomial time，那這類問題被稱為NP問題，NP是 non-deterministic polynomial-time 的縮寫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;NP問題還有另一個數學上等價的判斷方法，從驗證解的難度來界定，&lt;strong&gt;如果用DTM來驗證一組解是否正確只需要 polynomial time，那這個問題就是一個NP問題&lt;/strong&gt;，剛剛子集合加總問題，我們要驗證解是否正確很簡單也很快速，我們只要把解的數字加總起來看是不是為&lt;span class="math"&gt;\(0\)&lt;/span&gt;就可以了，所以子集合加總問題是一個NP問題，但因為這個問題的時間複雜度為 &lt;span class="math"&gt;\(O(N×(2^N))\)&lt;/span&gt;，所以它不是一個Ｐ問題。&lt;/p&gt;
&lt;p&gt;如果有一天我們可以找到一種演算法來解所有的NP問題，並且只需要 polynomial time，那這些問題就是既是NP問題也是P問題了，也就是代表今天你用DTM和NTM都可以在polynomial time以內把問題解完，有這麼好的事嗎？這就牽扯出一樁數學懸案。&lt;/p&gt;
&lt;p&gt;在討論這個問題之前，我先補充一件事，剛剛我提到NP問題有兩種定義是等價的，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Definition of NP problem: &lt;/p&gt;
&lt;p&gt;NTM可在 polynomial time 內解決的問題 &lt;span class="math"&gt;\(\equiv\)&lt;/span&gt; 問題的解有辦法在DTM polynomial time下被驗證&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;這兩種定義如何連結起來呢？我來粗略地說明一下，因為NTM有無窮多個分支讓我利用，那我就讓每個分支去窮舉每種可能的解，然後再驗證每個分支的解是否正確，而驗證的過程只需要 polynomial time，所以自然在NTM下我只需要 polynomial time 就可以將這個問題給解完，也因此它們是等價的。&lt;/p&gt;
&lt;p&gt;那也許大家還有一個疑問，有什麼問題是無法在 polynomial time 內驗證解的？我們稍稍的改一下子集合加總問題，改問「這集合之中最多有多少種子集合符合加總為&lt;span class="math"&gt;\(0\)&lt;/span&gt; ?」這時候如果我告訴你解是&lt;span class="math"&gt;\(3\)&lt;/span&gt;個，你要怎麼驗證這個答案是對的，你會發現你幾乎還是需要再重新解同樣的問題才有辦法驗證，這種問題被稱為Co-NP問題（&lt;a href="https://zh.wikipedia.org/wiki/反NP"&gt;反NP問題&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;毋庸置疑的，NP問題必定包含P問題，在DTM之下為 polynomial time 可解決的，在NTM之下也必定是 polynomial time 可解決的，但是P問題會等價於NP問題嗎？（&lt;a href="https://en.wikipedia.org/wiki/P_versus_NP_problem"&gt;P=NP?&lt;/a&gt;）這個問題到目前為止還是數學界無法證明的問題，目前既不能證明&lt;span class="math"&gt;\(P=NP\)&lt;/span&gt;也不能證明&lt;span class="math"&gt;\(P\neq NP\)&lt;/span&gt;，克雷數學研究所曾在2000年公布千禧年大獎七大難題，每解破一題的解答者，會頒發獎金100萬美元，裡面的其中一題就是&lt;span class="math"&gt;\(P=NP\)&lt;/span&gt;?問題，那為什麼這個問題很重要呢？ 舉個例子，有一種我們現今常用的加密方法叫做RSA加密，它的概念非常的簡單，RSA加密利用兩個大質數相乘的合數當作驗證，只有用這兩個大質數的其中一個才有辦法整除它並解鎖，如果你想要暴力破解這個鎖是很困難的，你需要超過 polynomial time 的時間，但是你要驗證解是否正確是很容易的，根據剛剛的論述，&lt;a href="https://zh.wikipedia.org/wiki/RSA加密演算法"&gt;RSA加密&lt;/a&gt;是一個NP問題，如果今天有人找到方式可以把NP問題當作P問題處理，也就是說他可以輕易地用現代的電腦去解開RSA加密，不僅如此，目前的加密方法幾乎都是NP問題，這一定會造成世界不少的動盪，不過往好處想，只要確立了&lt;span class="math"&gt;\(NP=P\)&lt;/span&gt;，我們會大幅提升人類的計算力，可以拿來解很多我們現今無法解的難題，含括各領域：人工智慧、物理、醫學 ...，人類知識科技將大步的躍進。&lt;/p&gt;
&lt;h3 id="np-complete"&gt;NP-Complete 問題&lt;/h3&gt;
&lt;p&gt;當數學家試圖解決 &lt;span class="math"&gt;\(NP=P\)&lt;/span&gt;? 問題時，導出了一個重要的概念— NP-Complete問題。1971年美國 Stephen A. Cook提出了&lt;a href="https://zh.wikipedia.org/wiki/Cook-Levin理論"&gt;Cook-Levin理論&lt;/a&gt;，這個數學理論指出任何一個NP裡面的問題都可以在 polynomial time 內，使用DTM，將之化約成「一個布林方程式是否存在解」的問題，這個被化約的問題又稱為布爾可滿足性問題（SAT），我們稱SAT問題為NP-Complete問題。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;只要滿足以下兩個條件的，我們都稱之為&lt;a href="https://en.wikipedia.org/wiki/NP-completeness"&gt;NP-Complete&lt;/a&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;「問題」本身是一個NP問題 &lt;/li&gt;
&lt;li&gt;所有的NP問題都可以用DTM在 polynomial time 內化約成為這個「問題」。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;這個概念非常強大，假設我證明了SAT是P問題，就等於今天我隨便拿到一個NP問題就可以在 polynomial time 內把問題轉換成SAT，然後再用 polynomial time 把SAT解掉，所以所有的NP問題都只是P問題了，也就是&lt;span class="math"&gt;\(P=NP\)&lt;/span&gt;，因此NP-Complete問題就是解決 &lt;span class="math"&gt;\(P=NP\)&lt;/span&gt; 的關鍵，如果可以證明NP-Complete問題為P問題，就可以間接證明&lt;span class="math"&gt;\(P=NP\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;NP-Complete 問題不只有SAT一種，在Cook提出Cook-Levin理論的隔一年，1972年，Richard Karp將這個想法往前推進了一步，他證明了&lt;a href="https://zh.wikipedia.org/wiki/卡普的二十一個NP-完全問題"&gt;21個不同但都難解的組合數學與圖論問題為NP-Complete問題&lt;/a&gt;，一樣的其中的任何一種只要被證明為P問題，都可以間接證明P=NP，目前已經有更多問題被證明為NP-Complete 問題。&lt;/p&gt;
&lt;p&gt;大家可能還會看到一個名詞叫做&lt;a href="https://en.wikipedia.org/wiki/NP-hardness"&gt;NP-Hard&lt;/a&gt;，它的定義如下，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;所有的NP問題都可以用DTM在 polynomial time 內化約成為一個「問題」，這個「問題」就叫做NP-Hard Problem&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以NP-Complete問題是NP-Hard 問題的一種特例，NP-Hard 問題可以不必是NP問題，譬如停機問題就是一個NP-Hard 問題但不是一個NP問題。&lt;/p&gt;
&lt;h3 id="_2"&gt;後話&lt;/h3&gt;
&lt;p&gt;最後，以下面這張圖作個結尾，左圖是假設&lt;span class="math"&gt;\(P\neq NP\)&lt;/span&gt;被證明的情形，NP-Hard有兩個部分，一個部分它同時是個NP問題，另外一部分則不是，所謂的NP問題就是可以用NTM在 polynomial time內給解掉的問題，另外其解的驗證必定能用DTM在 polynomial time內完成，兩種定義是等價的，有一部分的NP問題是屬於P問題，這些問題大部分都是易解的，有另外一部分的NP問題為NP-Complete問題，這些問題被視為難解的問題，我們只能用逼進的方法盡量接近答案。&lt;/p&gt;
&lt;p&gt;右圖是假設P＝NP被證明的情形，此時NP-Complete問題已經被證明為P問題，利用NP-Complete問題的特性，我們可以化約所有NP問題為NP-Complete問題，在把這個NP-Complete問題用 polynomial time 解掉，所以P=NP=NP-Complete。&lt;/p&gt;
&lt;p&gt;&lt;img alt="P_np_np-complete_np-hard" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/P_np_np-complete_np-hard.svg/800px-P_np_np-complete_np-hard.svg.png" /&gt;&lt;/p&gt;
&lt;p&gt;事實上，目前科學界普遍相信&lt;span class="math"&gt;\(P\neq NP\)&lt;/span&gt;，所以遇到NP-Complete的問題，就直接標註這是一道難題，使用近似解吧！這是一個不怎麼樂觀的看法，難道說我們真的無法把這樣的難題給解決掉了嗎？也未必啦！仔細想想我們也許還有另外一個方法，只要我們創建一個NTM就可以把這些難題給解決掉啦！期待一下目前正夯的量子電腦看有沒有可能是NTM呢！？&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="CS"></category></entry><entry><title>機器學習技法 學習筆記 (4)：Basic Aggregation Models</title><link href="https://ycc.idv.tw/ml-course-techniques_4.html" rel="alternate"></link><published>2017-03-29T12:00:00+08:00</published><updated>2017-03-29T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-03-29:/ml-course-techniques_4.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋Blending、Bagging、Decision Tree和Random Forest&lt;/p&gt;</summary><content type="html">&lt;h3 id="aggregation-models"&gt;綜觀Aggregation Models&lt;/h3&gt;
&lt;p&gt;如果今天我有很多支的Model，我有辦法融合他們得到更好的效果嗎？&lt;/p&gt;
&lt;p&gt;這就是Aggregation Models的精髓，Aggregation Models藉由類似於投票的方法綜合各個子Models的結果得到效果更好的Model。換個角度看，你可以把整個體系看成一個新的Model，而原本這些子Models當作轉換過後的新Features，&lt;strong&gt;所以Aggregation Model裡頭做了「特徵轉換」，這個特徵轉換產生出許多有預測答案能力的Features，稱為Predictive Features，然後再綜合它們得到最後的Model&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Aggregation Models" src="/media/MachineLearningTechniques/MachineLearningTechniques.007.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;Aggregation Models可以分成兩大類，第一種的作法比較簡單，先Train出一個一個獨立的Predictive Features，然後在綜合它們，&lt;strong&gt;「集合」的動作是發生在得到Train好的Predictive Feature之後，這叫做「Blending Models」&lt;/strong&gt;；第二種作法則是，&lt;strong&gt;「集合」的動作和Training同步進行，這叫做「Aggregation-Learning Models」&lt;/strong&gt;，Aggregation-Learning Models有一個特殊的例子叫做Boost，翻開字典查Boost的意思是「促進」，在這邊的意義是&lt;strong&gt;假設在Training過程所產生的Predictive Feature朝著改善Model的方向前進就叫做Boost&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;從「集合」的方法上也可以進一步細分三種類型，有票票等值的&lt;strong&gt;「Uniform Aggregation Type」&lt;/strong&gt;，有給予Predictive Features不同權重的&lt;strong&gt;「Linear Aggregation Type」&lt;/strong&gt;，甚至還可以用條件或任意Model來分配Predictive Features，這叫做&lt;strong&gt;「Non-linear Aggregation Type」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以兩種類型、三種Aggregation Type，交互產生各類的Aggregation Models。有Blending的三種Aggregation Type，Aggregation-Learning的Uniform Type—Bagging，再加上Aggregation-Learning的Linear Type兩種—AdaBoost和GradientBoost，這兩種也亦是Boost的方法，AdaBoost負責處理Classification的問題，而GradientBoost則負責處理Regression的問題，最後介紹Aggregation-Learning的Non-Linear Type—Decision Tree。然後接著，使用Decision Tree結合其他方法再進一步的產生Random Forest、AdaBoost Decision Tree和GradientBoost Decision Tree。&lt;/p&gt;
&lt;p&gt;我將會分兩篇來介紹Aggregation Models，一篇介紹沒Boost的部分，就是今天這一篇，另外一篇則是來專攻有Boost的部分。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="blending"&gt;Blending&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Blending是泛指在Training結束之後得到幾個Predictive Features，然後再對這些Predictive Features做集合的方法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Blending" src="/media/MachineLearningTechniques/MachineLearningTechniques.008.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;如上圖，基本流程是這樣的，一開始先把Data切成一部分拿來Training，另外一部分拿來Validation，這部份很重要，因為我們待會要利用Validation的Error來決定每筆Predictive Feature對Model的貢獻分配比重；接下來使用不同的方法來產生不同的Predictive Features &lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，來源可能是不同的Model形式、不同的參數變化、不同的隨機情形等等；有了各類的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;之後，我們就可以選擇使用怎樣的方式來結合它們，如果是Uniform Combination，就直接平均所有&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;就可以了，那如果是Linear Combination，想當然爾就是使用線性模型來結合，那如果是Non-Linear Combination，你可以使用任意Model來描述也行；決定好結合方式了，也就同時決定了「特徵轉換」的方法，接下來出動Validation Data，使用這個「特徵轉換」來轉化Validation Data並且做Fitting，最後我們會找到一組解最佳的參數來確定結合的方法，如果是Uniform Combination是不需要這一步的，基本上你得到&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;就直接平均就得到結果了，而Linear Combination則是需要去找出&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;在數學上可以證明Aggregation的效果會比單一一個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的描述的結果還好&lt;/strong&gt;，這很像是在做投票選舉，不同方法可能帶有不一樣的偏見，但是綜合所有意見之後可以找到共識，這個共識是具有較少偏見的，你可以想像偏見就像是Overfitting，&lt;strong&gt;所以Aggregation是具有像Regularizaiton一般抑制Overfitting的效果的&lt;/strong&gt;，但有些時候特別的看法不一定是偏見，也許這一個方法可以看出其他方法看不出來的規律，此時這個部分也不會被完全忽略掉，&lt;strong&gt;所以Aggregation也可以同時擁有像Feature Transform一樣的複雜度。因此Aggregation的方法可以同時增加Model複雜度又同時防止它Overfitting，這個效果是我們以前沒看過的，所以我們會說Aggregation具有截長補短的效果&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="bagging"&gt;Bagging&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Bagging" src="/media/MachineLearningTechniques/MachineLearningTechniques.009.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bagging是一種利用變換原本Data來造出不同&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的簡單方法&lt;/strong&gt;，Bagging的全名稱為Bootstrap Aggregation，其中&lt;strong&gt;Bootstrap指的是「重新取樣原有Data產生新的Data，取樣的過程是均勻且可以重複取樣的」&lt;/strong&gt;，使用Bootstrap我們就可以從一組Data中生出多組Dataset，然後就可以使用這些Dataset來產生多組&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，最後再Uniform Combination這些&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，就完成了Bagging。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="decision-tree"&gt;Decision Tree（決策樹）&lt;/h3&gt;
&lt;p&gt;接下來談Decision Tree這個重要的概念，Decision Tree其實就像是一個多層次的分類，每一次的分類會根據某一個Feature來當作依據判斷它應該繼續往哪一條路走，然後繼續使用可能是另外一個Feature來繼續細分下去。舉個例子好了，假設今天有一個自由式摔跤重量63公斤的女選手Ms. D要參加奧運，所以得透過奧運的分級制度分級，一開始可能根據比賽模式這個Feature下去分類，我查了一下有自由式和古典式兩種，所以Ms. D會被歸類到自由式，再來根據性別這個Feature下去分類，Ms. D是女選手所以分到女選手這一類，再繼續可能會根據體重來細分，體重在奧運分級共有8級，Ms. D可能就被分到62公斤级的那類，這樣的分類精神就是Decision Tree。&lt;/p&gt;
&lt;p&gt;所以，Decision Tree的優點是結果所提供的結構非常容易讓人了解，另外在演算法部分也很容易實現，而且因為具有以條件篩選的結構，所以其實很容易可以做到多類別分類。但是Decision Tree也有一些為人詬病的缺點，Decision Tree整體理論是缺乏基礎的，存在很多是前人的巧思，很多作法都是使用起來感覺效果不錯就延續下去了，目前並不了解背後的原因，也因此沒有一個代表性的演算法存在。&lt;/p&gt;
&lt;p&gt;在講Decision Tree操作方法之前應該要先來講一下Decision Stump，Decision Stump做的事其實就是上述中提到的對某個Feature做切分的這件事，&lt;strong&gt;可以想知Decision Stump是一個預測效果很差的Model，而Aggregation這些Decision Stump形成Decision Tree卻有很好的效果&lt;/strong&gt;，這就是Aggregation的威力。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Decision Tree" src="/media/MachineLearningTechniques/MachineLearningTechniques.010.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;見上圖，我們來看一下Decision Tree的流程，Decision Tree最為人所知的演算法是C&amp;amp;RT，C&amp;amp;RT是一整套的套件，我們今天只是提到它整套套件中的一種特例。Decision Tree產生的函式是這樣的，一開始先判斷進來的這筆資料還能不能繼續分支下去，在三個情況下，我們沒辦法繼續分支下去：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;數據&lt;span class="math"&gt;\(Ɗ\)&lt;/span&gt;只剩一筆數據。&lt;/li&gt;
&lt;li&gt;這群數據&lt;span class="math"&gt;\(Ɗ\)&lt;/span&gt;已經最佳化了，我們會說它的Impurity=0，這個時候我們不知道要從哪裡再切一刀。&lt;/li&gt;
&lt;li&gt;這群數據&lt;span class="math"&gt;\(Ɗ\)&lt;/span&gt;的Feature &lt;span class="math"&gt;\(X_{n}\)&lt;/span&gt;都完全相同。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;當無法再繼續分支下去時，會回傳一個&lt;span class="math"&gt;\(g_{t}(x)=constant\)&lt;/span&gt;，這個常數是一個可以使得這個群體內&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小的數值，在分類問題中這個常數是&lt;span class="math"&gt;\(\{y_{n}\}\)&lt;/span&gt;中佔多數的類別，在Regression問題中這個常數是&lt;span class="math"&gt;\(\{y_{n}\}\)&lt;/span&gt;的平均值。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;大家應該會有點驚訝，Decision Tree也有辦法做Regression？其實是可以的，我們只要讓群裡頭的數字作平均當代表，這們一來要處理實數問題也是可以做到的，不過我們會預期處理Regression問題時會切的比Classification問題來的細和多層。&lt;/p&gt;
&lt;p&gt;那接下來來看假如還可以繼續分支下去應該要怎麼做，這邊假設我們只切一刀分為兩個區塊&lt;span class="math"&gt;\(C=2\)&lt;/span&gt;，我們該根據怎樣的條件來切呢？我們剛剛其實有稍微提到，那就是Impurity，我們&lt;strong&gt;可以根據Impurity Function來衡量「一群資料的不相似程度」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;分類問題的Impurity Function有以下兩種：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(Impurity(Ɗ) = (1/N) 𝚺_{n} ⟦y_{n}≠y^*⟧\)&lt;/span&gt;，其中&lt;span class="math"&gt;\(y^*\)&lt;/span&gt;是&lt;span class="math"&gt;\(Ɗ\)&lt;/span&gt;中佔多數的類別，這個衡量方法就直接的去數出錯誤答案的比例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gini Index: &lt;span class="math"&gt;\(Impurity(Ɗ) = 1 - 𝚺_{k} [ 𝚺_{n}⟦y_{n}=k⟧  / N ]^{2}\)&lt;/span&gt;&lt;/strong&gt;，Gini Index是最為流行的作法，它不同於上一個作法，它是在評估所有的類別後才去計算Impurity，其中 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 代表類別。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而Regression問題有以下方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(Impurity(Ɗ) = (1/N) 𝚺_{n} ( y_{n} - \overline{y} )^{2}\)&lt;/span&gt;&lt;/strong&gt;，其中&lt;span class="math"&gt;\(ȳ\)&lt;/span&gt;代表的是&lt;span class="math"&gt;\(\{y_{n}\}\)&lt;/span&gt;的平均值，式子中使用平方誤差來評估資料的離散程度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有了Impurity Function我們就有了指標，找出應該要使用哪個Feature、應該要怎麼切，才能使得Impurity Function總和最小，決定好這一刀後，接下來就從這一刀切下去，把Data一分為二，然後這兩組Data再各自去長出一棵Decision Tree，經過遞迴式的迭代，我們就可以得到一棵完整的Decision Tree了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Show C&amp;amp;RT" src="/media/MachineLearningTechniques/MachineLearningTechniques.015.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;如果我們讓一棵樹完整的長成了，可以想到的後果想當然爾就是Overfitting，所以我們必須要做Regularization，&lt;strong&gt;Decision Tree常用的Regularization的方法是Pruning&lt;/strong&gt;，就是砍樹，我們將分支的數量&lt;span class="math"&gt;\(Ω(G)\)&lt;/span&gt;加進去&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;中做為Regularization，所以我們問題變成是去找到 &lt;span class="math"&gt;\(argmin\ E_{in}(G)+λΩ(G)\)&lt;/span&gt;，其中的λ可以利用Validation Data來做選擇，你會發現如果真正的要去找到&lt;span class="math"&gt;\(argmin\ E_{in}(G)+λΩ(G)\)&lt;/span&gt;的最佳解，這問題會非常的困難，因為你必須要把所有的可能的樹都考慮進去，所以有一個替代方案，&lt;strong&gt;我們可以先將樹整棵長完，然後在一一的去合併分支，看哪兩個分支合併之後可以使&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小就先合併，使用這樣的作法逐步減少分支的數量&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;順道一提，C&amp;amp;RT可以產生許多替代方案，這些替代方案稱為Surrogate Branch，當有一筆Data缺乏某個Feature，我們仍然有辦法使用替代方案來做決策，這是C&amp;amp;RT的一個大大的優點。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="random-forest"&gt;Random Forest（隨機森林）&lt;/h3&gt;
&lt;p&gt;如果我拿Decision Tree來做Bagging這樣可以嗎？當然OK，Aggregation Model的精髓就是可以綜合子Model，那Decision Tree也可以是看成一個子Model，所以我們在做的就是Aggregation of Aggregation，&lt;strong&gt;這種拿Decision Tree來做Bagging的Model叫做Random Forest&lt;/strong&gt;，這個名字取的很生動，有很多棵數的地方就是森林啦！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Decision Tree和Bagging其實是有互補的作用&lt;/strong&gt;，Decision Tree這種演算法是「變異度」很高的，因為它不像SVM這類的演算法，會去評估與Data之間的距離，空出最大的距離來避免Overfitting，而Bagging正可以拿來減少「變異度」，消除雜訊，所以&lt;strong&gt;Random Forest會比Decision Tree更不易Overfitting&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Random Forest" src="/media/MachineLearningTechniques/MachineLearningTechniques.011.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;見上圖，我們來看一下Random Forest的流程，一開始先做和Bagging裡頭一樣做的事Bootstrap，藉此來產生新的Dataset，另外為了讓我們隨機程度變得更高，我也對我們Features來做點變化，將它乘上一個亂數產生的&lt;span class="math"&gt;\(P\)&lt;/span&gt;，如果&lt;span class="math"&gt;\(P_{i}=0\)&lt;/span&gt;代表我們完全不取這個Feature，如果&lt;span class="math"&gt;\(P_{i}=1\)&lt;/span&gt;代表我們完全取這個Feature，我們更可以以分數來代表我們對某個Feature的重視程度，這個手法叫做Random-subspace。接下來就是把弄的很亂的Dataset放進去長一顆Decision Tree，最後再把所有的Decision Tree平均就是Random Forest的結果。&lt;/p&gt;
&lt;p&gt;Random Forest發展出了一套獨特的Validation方法，我們知道Bootstrap的結果會造成有些Data取用而有些Data不使用，而取用的Data會拿來Training，這讓你想到什麼呢？沒錯，沒有用到的Data可以做Validation，我們可以拿那些沒有被取用的Data來評估Training的好壞，我們會稱那些沒被取用的Date叫做Out-of-Bag Data，而利用Out-of-Bag Data來Validation的Error，稱為Out-of-Bag Error，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Out-of-Bag Error &lt;span class="math"&gt;\(E_{oob}=(1/N) 𝚺_{n} err(y_{n}, {G_{n}}^{-}(x_{n}))\)&lt;/span&gt; &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(where:\ {G_{n}}^{-}(x) = Average(Models\ without\ using\ this\ data)\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Out-of-Bag Error提供一個很方便的Self-validation的方法。&lt;/p&gt;
&lt;p&gt;在以前Linear Model中，權重W代表每筆Feature對Model的貢獻度，我們可以由W的分量大小來評估每個Feature的重要程度。Random Forest則是可以利用&lt;span class="math"&gt;\(E_{oob}\)&lt;/span&gt;和Random-subspace來標示出每個Feature的重要程度，想法是這樣的，如果今天某一個Feature i 對Model很重要，所以說我只對Feature i 做Random-subspace，也就是只有&lt;span class="math"&gt;\(P_{i}\)&lt;/span&gt;是隨機的，可以想知&lt;span class="math"&gt;\(E_{oob}\)&lt;/span&gt;會大幅增加，因此利用這個想法我們可以用來定義Feature的重要程度，
&lt;/p&gt;
&lt;div class="math"&gt;$$
important(i) = E_{oob}(G) - E_{oob}(G with random-subspace at i)
$$&lt;/div&gt;
&lt;p&gt;
&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="_1"&gt;結語&lt;/h3&gt;
&lt;p&gt;在這一篇我們提了幾個基礎的Aggregation Models，從最簡單的Blending，Blending的方法本身不去產生子Model，而是使用兩階段學習，先自行挑選和訓練來產生很多的子Model，而Blending只在這些結果上做不同方式的結合。&lt;/p&gt;
&lt;p&gt;接下來，Learning-Aggregation的方法則化被動為主動，我們先提了Bagging，裡頭使用Bootstrap的技巧來造成資料的隨機性，利用這樣的變異來產生多個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，再接下來我講了Decision Tree，Decision Tree由多個Decision Stump組合而成，每個Decision Stump就是&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，Decision Tree做的事就是，產生Decision Stump、切分Dataset、再產生Decision Stump...接續下去，最後綜合全部的Decision Stump成為Decision Tree。&lt;/p&gt;
&lt;p&gt;最後，我們結合Decision Tree和Bagging產生了Random Forest，利用彼此的互補，讓效果變得更好可以比單純Decision Tree更好。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>讀書手札：大腦解密手冊 The Brain: The Story of You</title><link href="https://ycc.idv.tw/the-brain-the-story-of-you.html" rel="alternate"></link><published>2017-03-24T12:00:00+08:00</published><updated>2017-03-24T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-03-24:/the-brain-the-story-of-you.html</id><summary type="html">&lt;p&gt;大腦的可塑性 / 意識與無意識 / 腦中的交戰網路 / 科技將如何改變大腦的未來&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://www.books.com.tw/products/0010738920"&gt;⟪大腦解密手冊⟫&lt;/a&gt;是一本非常易懂但又相當豐富的書，作者David Eagleman是一名美國的神經科學家，在這本書他嘗試拋開教科書的模式，改以輕鬆聊天的方式來聊神經科學，讓讀者可以很輕易的一探這神秘的大腦。&lt;/p&gt;
&lt;h3 id="_1"&gt;大腦的可塑性&lt;/h3&gt;
&lt;p&gt;人類在剛出生的一刻是非常脆弱的，不能走不能自己吃東西，完全需要依賴他人的照顧，相反的其他的哺乳類在出生的那刻就已經有謀生的能力了，斑馬寶寶出生不到45分鐘就可以奔跑，長頸鹿出生幾個小時就可以學會站立。表面看起來，這對人類生存似乎很不利，事實上卻提供人類大腦更多的彈性，&lt;strong&gt;人類大腦不像其他動物在出生的一開始就已經接好線路了，雖然長大成人的過程腦細胞數量並不會增加，但是突觸的連結卻會有天翻地覆的改變&lt;/strong&gt;，人腦建造過程可以長達25年，過程中會有50%的突觸會被修掉，就算是一個成人突觸連結還是每天不斷的更新，這種彈性使得人類可以比其他動物更能應付環境的各種變化。&lt;/p&gt;
&lt;p&gt;能夠看東西不僅僅需要眼睛，而主要還是靠著眼睛後面的大腦，Mike May在3歲時失明了，過了40年因為幹細胞治療重見光明，不過這個恢復正常的眼睛並沒有讓他恢復視力，雖然能夠看見東西，卻很難說出那是什麼，而且也不清楚這個東西是遠是近，視覺系統不像是照相機，把鏡頭修好了就可以正常使用了，他的大腦長達40年沒有接受光線給的訊號，一時之間是無法辨識視覺給的訊號，也就造成眼睛恢復正常了但是視力並沒有恢復。&lt;/p&gt;
&lt;p&gt;只有感官的訊號是沒有用的，我們還需要大腦去統合和理解這些訊號，大腦幫我們做了很多事，一個有趣的例子，你知道嗎？人在閱讀的時候眼球是不斷的跳動的，一秒鐘會跳4次左右，這種快速的運動稱為「眼球迅速移動」(saccade)，儘管眼球不停的跳動但我們卻可以看到一個穩定的畫面，這是因為大腦存在一個內在模型(internal model)，他會先預測你將會看到什麼，然後視覺訊號才進來作驗證，我們體驗到的視覺很少依賴照進眼睛的光線，較多是依賴腦中既有的東西。大腦幫我們預先處理很多的東西，讓我們可以感受到一個穩定的世界。&lt;/p&gt;
&lt;p&gt;另外一個例子，我們對光線的反應時間大約是190毫秒，而我們對聲音的反應時間比光線快一點是160毫秒，但是我們卻不會感受到這種不同步，因為大腦給你的是一個延遲過的版本，將時間差給隱蔽起來。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;腦其實不在意輸入資訊的細節，他只關注如何有效率的在這個世界活動，並得到它需要的東西。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也因為如此，所以有了&lt;strong&gt;「感官替代」&lt;/strong&gt;這種新科技，也就是利用其他感官體驗來取代失去功能的感官，譬如說失去聽覺的患者，可以穿一個背心，這個背心會把聲音轉換成振動，因為大腦不管進來什麼樣的資料，它都會調整並盡量的處理，天生聽障者使用這個背心大概5天的時間就能正確的辨別別人說出來的字詞，從這裡就可以看出大腦是如何具有彈性。&lt;/p&gt;
&lt;h3 id="_2"&gt;意識與無意識&lt;/h3&gt;
&lt;p&gt;如果說意識決定一個人，倒不如說無意識（或稱潛意識）決定一個人，我們常常把idea的出現歸功於意識，但事實上在你意識到這個idea之前的幾個小時、甚至幾個月，無意識已經開始塑造這個idea，包括鞏固記憶、試驗新組合、評估後果，美國社會心理學家Brett Pelham和他的研究小組從統計結果發現，名字叫做Dennis或Denise的牙醫(dentist)，以及名字叫做Laura或Laurence的律師(Lawyer)特別多，可以說無意識在我們人生的重大決策中扮演相當重要的角色。&lt;/p&gt;
&lt;p&gt;那意識是怎麼形成的？這是一個有趣的哲學問題，意識這種東西有辦法從物質中產生嗎？而如果打造一個人工智慧讓你分不清他是機器人或者是真人，我們該說這樣的機器人有意識嗎？有人會說不！機器人只遵照預先設計好的程式執行，他們不具有意識，作者舉了一個臆想實驗「中文房間」，房間中的人按照詳細的說明書來處理中文符號，並把回答送出房間，這的確可以騙過母語人士，讓他們以為房間裡面的人懂中文，但房間裡的人根本不懂他在做什麼，那機器人是不是也是這樣的。&lt;/p&gt;
&lt;p&gt;不過作者比較傾向於支持另一個反面，認為腦中每個各別神經元並不清楚他們自己在做什麼，但是集體行為讓大腦產生了意識，他舉了螞蟻群的例子，每隻螞蟻只做一些簡單的事，不過一群螞蟻卻可以打造出相當複雜的系統，譬如：蟻窩&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一旦夠多的螞蟻聚集在一起，超生物就出現了，這種集體擁有的特性比個別基礎部分更精緻、複雜。這種現象稱為突現（emergence）; 當簡單的單元以適當的方式交互作用，產生更大的格局，這就是突現。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;每個神經元一輩子只負責回應訊號，他不知道現在在彈奏貝多芬，他不知道你的存在，他不知道你的意圖，不過因為突現他們共同產生了你的意識。  &lt;/p&gt;
&lt;p&gt;我們每天都有一大段時間是處於無意識的狀態，那就是睡眠，威斯康辛大學教授Giulio Tononi認為，我們在清醒的的時候皮質的不同區域間會跨區溝通，而睡眠的無意識則是缺乏跨區溝通，也就是說，在有意識的情況下，我們大腦的各個部分會爭執不休的對話，也就造成你可能會猶豫不決，相反的，在無意識的情況下，你將會進入自動導航模式，大腦根據你既有的突觸迴路來執行任務。&lt;/p&gt;
&lt;h3 id="_3"&gt;腦中的交戰網路&lt;/h3&gt;
&lt;p&gt;如果無意識是一輛直行的車子，意識可以說是這輛車子的方向盤，大腦這一部從衝突中打造出來的機器，每天要下成千上萬的決策。&lt;/p&gt;
&lt;p&gt;作者重新詮釋了Michael Sandel在正義課中提及著名的電車難題，假設今天你是一名鐵路維修工人，遇到一輛失控的火車，在既定的軌道上會撞死正在修理鐵路的5名工人，不過你剛好站在鐵軌控制桿前面，但是如果你扳動控制桿則會造成另一名工人被撞死，幾乎所有人都會扳動這個控制桿，因為1人死亡總比5人好，但換一個情境，如果今天你遇到這個失控火車的時候剛好站在火車上方的橋上，旁邊又剛好有一個胖子，如果你把這個胖子推下去，火車就會因此被他擋住，然後可以拯救5條人命，同樣是1換5的狀況，這個時候你就發現你開始猶豫了。&lt;/p&gt;
&lt;p&gt;作者認為，對腦來說，第一種情境只涉及單純的數學問題，此時活化的是腦中解決邏輯問題的區域，&lt;strong&gt;但在第二種情境，你必須去碰觸那個人，把他推下去，這會激起額外的網路&lt;/strong&gt;，也就是腦中與情緒相關的區域，所以這兩個網路，解決邏輯問題的和情緒相關的，會開始爭辯不休，因此你開始產生猶豫。&lt;/p&gt;
&lt;p&gt;這一種腦中衝突有時候會讓你做出錯誤的決策，今天明明要健身的，不過想想有一部影集還沒看完，好想知道接下來劇情會怎麼走下去，算了！明天再去健身好了，我相信大家常常有類似的經驗，雖然健身對你的好處是多於追劇的，但對於大腦而言純粹在大腦裡模擬的好處，比不上此時此刻真實感覺到的好處，所以作者建議用一種方式來克服這種意志力的不足—&lt;strong&gt;尤里西斯合約&lt;/strong&gt;，希臘神話中的尤里西斯是一名凱旋歸國的戰士，在回國的途中，他們會在經過一座小島，這座小島住著美麗的賽蓮女妖，據說會唱出美麗的歌聲迷惑水手們不自覺得把船開到礁岩去，所以需要用蜂蠟把耳朵塞住，但是尤里西斯實在是很想聽聽看女妖們的聲音，於是他就心生一計命令水手們把他綁在船桅上，其他人把耳朵塞住，到時候不管尤里西斯怎麼叫，怎麼崩潰，都不要理他，船正常開就好了&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;尤里西斯知道未來的自己沒資格作良好的決策，所以在頭腦清楚的時候就把事情安排妥當，以防止自己作錯事，於是「現在的你」和「未來的你」之間的這一種協議稱為「尤里西斯合約」&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;運用在健身的這個例子，你可以約朋友一起在固定的時間健身，讓未來的你沒有任何理由可以推遲。&lt;/p&gt;
&lt;h3 id="_4"&gt;科技將如何改變大腦的未來&lt;/h3&gt;
&lt;p&gt;這本書的最後，作者探討了大腦科技的未來。舉幾個我覺得有趣的例子，奧爾科生命延長基金會一直致力於一種技術，他們將死亡人的大腦冰封保存下來，以期盼有一天科技進步可以讓這大腦重新擁有年輕的身體，這樣這些被冰封的大腦將擁有並享受第二生命週期。&lt;/p&gt;
&lt;p&gt;不過有些人可能不太喜歡這種感覺，那還有另外一種延續生命的方法，稱為數位的不朽，如果大腦運算只單純是神經元之間的運算，我們將可以使用這個運算的概念來創造一個大腦，實際的作法是利用大腦切片去紀錄每個神經元的狀態，並用數位的方式保存下來，然後也可以透過模擬去讓這顆大腦重新活過來，但是這非常的困難，一般神經元有多達一萬條分枝，要繪製完整人類連結體的圖譜預計還需要十年的時間，目前連建立一個大鼠的大腦都做不到。&lt;/p&gt;</content><category term="Reading"></category></entry><entry><title>Python玩數據 (1)：安裝Python, IPython, Numpy, Pandas</title><link href="https://ycc.idv.tw/python-play-with-data_1.html" rel="alternate"></link><published>2017-03-20T12:00:00+08:00</published><updated>2017-03-20T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-03-20:/python-play-with-data_1.html</id><summary type="html">&lt;p&gt;安裝Python, IPython, Numpy, Pandas&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;相較於R，我比較喜歡在工作上使用python來作數據處理&lt;/strong&gt;，主要原因有四個，&lt;strong&gt;第一點，python是一個簡潔的語言&lt;/strong&gt;，讓我們可以在不寫註解的情況下還可以很容易的看出每一行code在做哪些事，這可以省去了不少時間在；&lt;strong&gt;第二點，python可以更容易的寫成物件導向編程&lt;/strong&gt;，物件導向編程可以讓code看起來更為直覺，而且更易於修改、重構或套用，如果是大型軟體開發的話，需要多人協作，此時物件導向便是絕對必要的；&lt;strong&gt;第三點，python是一個通用語言&lt;/strong&gt;，不僅僅只可以作資料處理而已，你可以用python寫一套視窗程式，或者當作網站的後台（這個網站就是建基在python上），如果要做一些平行運算也很容易，&lt;strong&gt;最後一點，也是相當重要的一點，目前常見的deep learning套件TensorFlow或Keras都是架構在python上面&lt;/strong&gt;，所以如果你的數據處理結束要作deep learning的話，直接用python處理是相當理想的。講了python這麼多優點，其實它是有一項缺點是不如R的，R是一個專為資料科學設計的語言，所以背後有強大的社群，也就是說能直接取得資料分析方法的套件會比python來的多，不過這方面在這幾年已經漸漸的改善了。&lt;/p&gt;
&lt;p&gt;講了這麼多python的強大，不過在這個系列我並不會著墨太多在python上，這個部分我會在其他的文章中分享，這系列文章主要聚焦在python的資料處理這部份，我會從基礎講起，讓不懂python的人也可以聽懂。  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="_1"&gt;最困難的第一步：安裝&lt;/h3&gt;
&lt;p&gt;不要以為我在開玩笑，安裝往往是最困難的一步，有些時候安裝一些套件的時候，你必須要先行安裝另外幾個相依套件，如果程式在安裝的過程無法自己補足這些相依套件的話，你就得自己安裝，一般來說如果是python的套件的話，你可以先用待會要介紹的&lt;code&gt;pip install&lt;/code&gt;來安裝，如果不幸在上面找不到的話，就只好上網Google了，另外有些時候安裝還會遇到bug，這個時候Google也同樣是你的好朋友，或者到&lt;a href="https://stackoverflow.com"&gt;Stack Overflow&lt;/a&gt; 上找答案（一個好的coder要培養自己上網找答案的能力），不過大家先不用擔心，以下我會帶大家一步一步的安裝。&lt;/p&gt;
&lt;p&gt;我們將會用到python 2.7版（你也可以選擇更新的版本，不會差距太大），以及他的套件IPython, Numpy和Pandas。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="python27"&gt;Python2.7&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mac&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;python2.7已經是內建的程式了！打開「終端機」，直接輸入&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ python2.7 -V

Python &lt;span class="m"&gt;2&lt;/span&gt;.7.13
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;就會顯示他的版本。&lt;/p&gt;
&lt;p&gt;如果沒有的話，或者你想要自己安裝一份的話，可以參考&lt;a href="https://stringpiggy.hpd.io/mac-osx-python3-dual-install/"&gt;這篇&lt;/a&gt;的說明，或者跟著我往下作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; 安裝 Xcode：打開你的App Store，搜尋Xcode並安裝。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; 安裝 &lt;a href="https://brew.sh"&gt;Homebrew&lt;/a&gt; 這個Mac上好用的套件管理，打開「終端機」，輸入&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt; 1&lt;/span&gt;
&lt;span class="normal"&gt; 2&lt;/span&gt;
&lt;span class="normal"&gt; 3&lt;/span&gt;
&lt;span class="normal"&gt; 4&lt;/span&gt;
&lt;span class="normal"&gt; 5&lt;/span&gt;
&lt;span class="normal"&gt; 6&lt;/span&gt;
&lt;span class="normal"&gt; 7&lt;/span&gt;
&lt;span class="normal"&gt; 8&lt;/span&gt;
&lt;span class="normal"&gt; 9&lt;/span&gt;
&lt;span class="normal"&gt;10&lt;/span&gt;
&lt;span class="normal"&gt;11&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ /usr/bin/ruby -e &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;

&lt;span class="o"&gt;==&lt;/span&gt;&amp;gt; This script will install:
/usr/local/bin/brew
/usr/local/share/doc/homebrew
/usr/local/share/man/man1/brew.1
/usr/local/share/zsh/site-functions/_brew
/usr/local/etc/bash_completion.d/brew
/usr/local/Homebrew

Press RETURN to &lt;span class="k"&gt;continue&lt;/span&gt; or any other key to abort
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;(附註：我用 &lt;code&gt;＄&lt;/code&gt; 代表終端機的輸入起始字元，後面才是你需要輸入的指令)&lt;/p&gt;
&lt;p&gt;按下Enter就會開始安裝了。&lt;/p&gt;
&lt;p&gt;安裝完畢你就可以直接在「終端機」上使用它，我們試著搜尋python&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;
&lt;span class="normal"&gt;4&lt;/span&gt;
&lt;span class="normal"&gt;5&lt;/span&gt;
&lt;span class="normal"&gt;6&lt;/span&gt;
&lt;span class="normal"&gt;7&lt;/span&gt;
&lt;span class="normal"&gt;8&lt;/span&gt;
&lt;span class="normal"&gt;9&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ brew search python

app-engine-python               boost-python@1.59               micropython                    
python-markdown                 wxpython
boost-python                    gst-python                      python ✔                       
python3 ✔                       zpython
homebrew/apache/mod_python            
Caskroom/cask/kk7ds-python-runtime          
Caskroom/cask/mysql-connector-python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;因為我的電腦已經安裝了python2.7和python3.0，所以你會看到他們已經是打勾的狀態，我們的目標就是安裝「python」。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; 安裝python2.7：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ brew install python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;安裝完畢後檔案會被放在底下這個路徑，你可以打開來看一下&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ open /usr/local/Cellar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;應該就會看到python的資料夾了。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; 設定路徑 $PATH（不跟系統 Python 打架）&lt;/p&gt;
&lt;p&gt;這是什麼呢？當你輸入&lt;code&gt;brew&lt;/code&gt; , &lt;code&gt;open&lt;/code&gt; , &lt;code&gt;python2.7&lt;/code&gt; 這些指令到「終端機」上，為什麼「終端機」會認的了這些指令，原因就出在於這個PATH上，又稱為「環境變數」，我們把它叫出來看看&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="nv"&gt;$PATH&lt;/span&gt;
/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;你可以看到有五個路徑分別被字元 &lt;code&gt;:&lt;/code&gt; 隔開，由前到後分別為&lt;code&gt;/user/local/bin&lt;/code&gt;、&lt;code&gt;/usr/bin&lt;/code&gt;、&lt;code&gt;/bin&lt;/code&gt;、&lt;code&gt;/usr/sbin&lt;/code&gt;、&lt;code&gt;/sbin&lt;/code&gt;，這一些都是裝有執行檔的資料夾，今天你如果輸入某個指令，他就會從第一個資料夾下面開始找起，也就是&lt;code&gt;/user/local/bin&lt;/code&gt;，沒有找到再依序往下找，直到找不到為止，如果今天&lt;code&gt;/usr/bin&lt;/code&gt;底下有python，而你剛剛用brew安裝的另一個python放在&lt;code&gt;/user/local/bin&lt;/code&gt; 底下，在這個例子中，你會執行到的就是第一個路徑&lt;code&gt;/user/local/bin&lt;/code&gt; 下的python，那這也是我們要的結果，我們想要執行我們自己安裝的，而不是系統原有的。&lt;/p&gt;
&lt;p&gt;如果&lt;code&gt;/user/local/bin&lt;/code&gt;不是在第一個的話，就必須去修改PATH的順序。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sudo emacs /etc/paths
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;輸入密碼後，就會進入修改模式，然後開始修改順序，利用以下指令把&lt;code&gt;/user/local/bin&lt;/code&gt; 放到最上面&lt;/p&gt;
&lt;p&gt;control + k：把一行字剪下來&lt;/p&gt;
&lt;p&gt;control + y：把字貼上&lt;/p&gt;
&lt;p&gt;control + x + s：存檔&lt;/p&gt;
&lt;p&gt;control + x + c：關掉 emacs&lt;/p&gt;
&lt;p&gt;修改完成重開「終端機」，讓環境變數重載，在輸入一次 &lt;code&gt;echo $PATH&lt;/code&gt; 應該就可以看到修改後正確的環境變數了。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; 那就安裝完畢啦！最後檢查一下你下&lt;code&gt;python2.7&lt;/code&gt;的時候是不是來自於&lt;code&gt;/user/local/bin&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ which python2.7
/usr/local/bin/python2.7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;看起來很正常，Great!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;請參考&lt;a href="https://tecadmin.net/install-python-2-7-on-ubuntu-and-linuxmint/"&gt;這篇&lt;/a&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; 先安裝一些相依套件&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sudo apt-get install build-essential checkinstall
$ sudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; 從網路上下載python2.7 source code&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ &lt;span class="nb"&gt;cd&lt;/span&gt; /usr/src
$ wget https://www.python.org/ftp/python/2.7.13/Python-2.7.13.tgz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; 解壓縮並進去資料夾&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ tar xzf Python-2.7.13.tgz
$ &lt;span class="nb"&gt;cd&lt;/span&gt; Python-2.7.13
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; 依環境配置並安裝&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ sudo ./configure
$ sudo make altinstall
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;make altinstall&lt;/code&gt; 是為了避免你去取代掉預設的python在/usr/bin/python。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; 在這個&lt;a href="https://www.python.org/downloads/release/python-2713/"&gt;網站&lt;/a&gt;依照你的CPU架構下載安裝檔，並安裝。&lt;/p&gt;
&lt;p&gt;&lt;img alt="python_win_install_01" src="/media/PlayDataWithPython/python_win_install_01.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; 設定環境變數&lt;/p&gt;
&lt;p&gt;打開 控制台 &amp;gt; 系統及安全性 &amp;gt; 系統 &amp;gt; 進階系統設定 &amp;gt; 環境變數&lt;/p&gt;
&lt;p&gt;選Path，並按下 編輯，將&lt;code&gt;C:\Python27;C:\Python27＼Scripts&lt;/code&gt; 加到後面，並儲存。環境變數的說明請參考上面Mac安裝的第四步，原理是一樣的，不過在windows裡的區分的符號是&lt;code&gt;;&lt;/code&gt;不是&lt;code&gt;:&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="ipython-numpy-pandas"&gt;IPython, Numpy, Pandas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mac ＆Ubuntu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; 安裝pip&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ curl &lt;span class="s2"&gt;&amp;quot;https://bootstrap.pypa.io/get-pip.py&amp;quot;&lt;/span&gt; -o &lt;span class="s2"&gt;&amp;quot;get-pip.py&amp;quot;&lt;/span&gt;
$ python2.7 get-pip.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; 安裝套件&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;
&lt;span class="normal"&gt;3&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ pip2.7 install ipython
$ pip2.7 install numpy
$ pip2.7 install pandas
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;雖然不建議在windows下開發程式，不過我還是提供一個方法，讓你在接下的文章可以正常作操作。有一個好用的軟體—Anaconda，這個軟體不只可以在windows上使用，在linux和mac都有辦法使用。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;安裝windows版的Anaconda(python 2.7)：&lt;a href="https://www.continuum.io/downloads#windows"&gt;網址&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;安裝結束，就已經安裝好「IPython」的程式，直接打開就可以使用。&lt;/li&gt;
&lt;li&gt;安裝Numpy和Pandas：打開「Anaconda Prompt 」，輸入&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;&lt;span class="normal"&gt;1&lt;/span&gt;
&lt;span class="normal"&gt;2&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;$ conda install numpy
$ conda install pandas
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="ipython"&gt;開啟IPython&lt;/h3&gt;
&lt;p&gt;IPython將會是未來我們這系列會用的一個介面，只要能夠開啟它，我們今天就大功告成了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mac ＆Ubuntu:  在終端機輸入 &lt;code&gt;$ ipython2&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Windows：直接打開「IPython」程式&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;試著import numpy和pandas進來，如果都正常，就代表成功了！&lt;/p&gt;
&lt;p&gt;&lt;img alt="ipython" src="/media/PlayDataWithPython/ipython.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;Ctrl + D 就可以結束跳出啦～ 今天就到這～&lt;/p&gt;</content><category term="CS"></category><category term="Python"></category></entry><entry><title>從《如何閱讀一本書》想像一種不同的知識呈現方法</title><link href="https://ycc.idv.tw/how-to-read-books.html" rel="alternate"></link><published>2017-03-18T12:00:00+08:00</published><updated>2017-03-18T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-03-18:/how-to-read-books.html</id><summary type="html">&lt;p&gt;書籍與網路的PK / 閱讀的層次 / 檢視閱讀 / 分析閱讀 / 主題閱讀 / 書籍與網路的第二回合PK&lt;/p&gt;</summary><content type="html">&lt;p&gt;⟪如何閱讀一本書⟫是美國哲學家Mortimer Jerome Adler在1940年所著的一本教你如何讀書的書，雖然是70年前的書，不過卻是歷久彌新，隨便google都有好幾十篇的文章在講這本書，其中的一篇就是現在你讀的這一篇。&lt;/p&gt;
&lt;p&gt;書中作者教你讀書的策略，像在練功一樣，必須循序漸進的，掌握了一階技巧在前進下一階，作者把閱讀分為四個階段，至於是哪四階段呢？在這邊賣個關子，我待會會好好的解釋。&lt;/p&gt;
&lt;p&gt;這裡先打個岔，我們先來談談書籍紀錄的知識呈現方式。&lt;/p&gt;
&lt;h3 id="pk"&gt;書籍與網路的PK&lt;/h3&gt;
&lt;p&gt;現今科技革命當中，對我們影響最深的就屬網際網路的發展了，它是人類歷史上資訊傳播方式的大變革，人類技術發展史你可以簡單看成是一部人類如何延長自己的手腳及大腦的故事，斧頭被發明來延長補足手的不足，眼鏡被發明來補足眼睛的不足，書籍被發明來延長溝通，同樣的，網際網路也一樣的是延長溝通的發明，使得資訊傳遞更有效率。&lt;/p&gt;
&lt;p&gt;雖然書籍和網際網路一樣是資訊傳播的方式，不過看起來更為便利的網路並沒有使書籍被淘汰，網路上的知識仍然走不進學校，撇開使用體驗不談，網路知識的缺點之一是過於零碎化，書籍是作者整合資訊和自己的認知而成的體系，有一個比較完整的結構，不是網路上零碎知識的總和所能取代的。&lt;/p&gt;
&lt;p&gt;不過網路上的確有許多整合得相當好的資訊，但如果仔細看這些知識的傳播方式不脫書籍的那一套，甚至有些有名的部落客將他們的部落格文章整理成書，通常在轉換上也不會遇到太大的困難，因為這些資訊的呈現方式和書籍的呈現方式並無太大差異，等於是把書搬到網路上而已。&lt;/p&gt;
&lt;p&gt;不過這樣感覺白白浪費電腦或網路不同於書籍的彈性，譬如說超連結或者是動態回饋，不過一定有人會想起來，維基百科不就是一個好例子嗎？它利用超連結把眾多知識給串起來，讓它更有結構性，不同於書籍的好處是，你可以在自己不懂的地方，延伸出去找到答案，延伸出去的頁面如果有不懂的還可以繼續向下延伸。&lt;/p&gt;
&lt;p&gt;但也因為可以一直延伸下去，如果要學習的東西本身很複雜和有很強的連貫性，往往你會迷失在一片頁面海裡，這就是維基百科式的知識呈現方式的弱點。反觀，書籍有作者帶著你走，把後面需要用到的知識在前面為你補足，連結起每個知識節點，還會告訴你哪些地方太困難可以先不要碰，建立起易於學習的體系，這是目前網路知識無法做到的。&lt;/p&gt;
&lt;p&gt;讀到這裡你一定會覺得我應該是要吹捧完書籍閱讀有多棒，然後順著下去講如何閱讀。不過，你猜錯了，&lt;strong&gt;我想做的是了解如何有效率的閱讀一本書，然後回過頭想網路的知識呈現方式可以有怎樣的改善，既然有一本教你如何閱讀一本書的書這麼暢銷，也就是代表閱讀一本書本身還是有所不足，說不定我們可以用電腦或網路來補足這樣的不足&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="_1"&gt;閱讀的層次&lt;/h3&gt;
&lt;p&gt;這本書對我來說，除了重新審視多年來我的讀書方法外，另外也可以進一步探討我上述的疑問，想找尋更好的「知識體系呈現方式」，必須先了解如何有效地取得知識，才能進一步的構思更好的方式來幫助讀者學習知識，本書雖然是一本教你如何讀一本書的書，其實也是在談如何獲取知識的方法。&lt;/p&gt;
&lt;p&gt;作者將閱讀分為四個層次，分別是基礎閱讀(Elementary Reading)、檢視閱讀(Inspectional Reading)、分析閱讀(Analytical Reading)和主題閱讀(Comparative Reading)，基礎閱讀就是指識字能力，也就是閱讀的基礎能力，下一個層次是檢視閱讀，培養能在閱讀中主動地去思考作者想說的話，更進階是分析閱讀，看完一本書有辦法綜觀一本書的整體性和複雜性，並透徹的了解作者的疑問還有解答，最後研究所等級的主題閱讀，這個時候就不只是一本書的事了，你必須有可以比較好幾本書的能力，去學習一個領域的知識，成為那個領域的專家。&lt;/p&gt;
&lt;h3 id="_2"&gt;檢視閱讀&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;檢視閱讀的首要條件就是化被動為主動，主動去思考檢視一本書&lt;/strong&gt;，那該怎麼著手呢? 首先拿到書，先不要就直接一股腦的從第一頁讀起，就像是古代有經驗的將領一定在會戰前會看看對方擺怎樣的陣式，所以先看看書名，再看看序和索引，了解這本書大概在講什麼，然後針對自己有興趣或不了解的篇章去大略的看過，或者隨機東翻翻西翻翻，這個時候你就了解這本書大概在講什麼了。&lt;/p&gt;
&lt;p&gt;這個過程花不了多少時間，可能大概半小時到一個小時之間，不過這個方法可以讓你大致了解一本書的架構，接下來你就可以決定要不要看這本書，如果你在書店，就可以決定要不要買這本書，有些書並不值得花時間閱讀，或者有些書你只需要知道個大概就好了，雖然有些書是公認的好書，但可能不適合你或你不需要，那這一個小時的閱讀也就足夠啦!&lt;/p&gt;
&lt;p&gt;如果大略掃過一本書，你覺得這本書值得一讀，並且還有一些不懂的地方，那就開始完整的讀一遍吧! 在已經知道這本書大致架構的情況下，開始進行完整的閱讀會更為流暢，依循著架構你可以調配你閱讀的速度，簡單的地方你可以快速的讀過，較難的地方就把速度放慢，也就是說你可以依照自己理解來配速。&lt;/p&gt;
&lt;p&gt;那有什麼依循的標準嗎? 有的，但是一樣的你必須保持主動的思考，你可以藉由檢視以下四個要素來判斷是不是真的理解作者想說的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;現在作者在談什麼?&lt;/li&gt;
&lt;li&gt;作者細說了哪些東西?&lt;/li&gt;
&lt;li&gt;作者說得有道理嗎?&lt;/li&gt;
&lt;li&gt;這些跟我有什麼關係?   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;時時不斷的問這些問題，知識才有辦法進到你的腦袋中，在過程中你可能會劃一些線或做一些筆記，去輔助你達到這四個要素，這樣的閱讀層次稱為檢視閱讀，讀完了你能大概能了解作者想說的，有些簡單的書只需要做到檢視閱讀就夠了，但如果想理解更複雜的書就必須開始另一個層次—分析閱讀。&lt;/p&gt;
&lt;h3 id="_3"&gt;分析閱讀&lt;/h3&gt;
&lt;p&gt;分析閱讀必須用檢視閱讀的精神來完成三個步驟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一個步驟，讀者必須結構性的回答一本書再說什麼。&lt;/strong&gt;首先你要盡快的了解這本書應該歸於哪一類，是文學、戲劇、歷史、傳記，還是科學、數學、哲學、社會科學，是虛構的還是紀實的，是實用類型的還是理論類型的書，這通常可以從書名還有前言中看出來，不過有些書就很難以界定，譬如《飄》是愛情小說抑或是歷史故事。&lt;/p&gt;
&lt;p&gt;了解書的類別有助於你依照相應的分析方法來讀書，書中有詳列各種類型書籍的閱讀方式，在這邊不一一舉例，有興趣的人可以去翻翻這本書。了解一本書的類別就可以開始分析閱讀，你的目標應該放在做到了解書中的三個面向：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;整體性： 能用幾句話寫出這本書主要在說些什麼&lt;/li&gt;
&lt;li&gt;複雜性： 為這本書擬大綱說明書中的篇章架構&lt;/li&gt;
&lt;li&gt;作者的意圖：找出作者的問題和他的答案。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;第二個步驟，讀者必須細部的去詮釋一本書的內容。&lt;/strong&gt;閱讀完一本書你需要回答有哪一些keyword，而這些keyword在作者心中代表的是什麼意思，藉由文字間的連結來產生與作者相同的共識，有哪一些重要的句子當中隱含著重要的主旨。接下來主動出擊，重構作者論述來明白他的主張，也就是以自己的話寫出作者的主張，並且重新審視作者在提出的疑問中解決哪些問題? 還有那些問題沒有被解決?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第三個步驟 ，讀者必須有能力去評論一本書。&lt;/strong&gt;當我們已經客觀的了解作者的想法後，我們可以開始評論一本書，也許有一些論述你是認可的，而另外一些是你不認可的，而當你要指出作者的錯誤，可以從四個方向去著手：證明作者知識不足、知識錯誤、不合邏輯或分析與理由不完整。&lt;/p&gt;
&lt;p&gt;這三個步驟都需要搭配前面所提到的檢視閱讀，完成這三個步驟你對於一本書已經有了深入的了解了! 但如果你想要了解一個領域，或成為那領域的專家，那就不能只是讀一本書而已，每個作者的著眼點不同，觀點也相異，甚至是針鋒相對的，就算是同一個作者，也無法在一本書之中闡述他所有的想法，譬如：如果你看了亞當．斯密的《國富論》，卻沒有看他的《道德情操論》，你會覺得有一些地方似乎很沒有說服力。因此接下來要談的是最後一個層次—主題閱讀。&lt;/p&gt;
&lt;h3 id="_4"&gt;主題閱讀&lt;/h3&gt;
&lt;p&gt;開始主題閱讀前，你先要有一份書單，這份書單可以囊括你想要研究的主題，這份書單可以是從一些書裡的參考書單，或者是同一個作者的其他書，有了這份書單，要再進一步的建立起這些書彼此間的連結，最好還是有簡單的翻過這些書，了解這些書大概在講什麼，建立起這些書大致的連結。&lt;/p&gt;
&lt;p&gt;有了這個書單當作地圖，我們就可以開始主題閱讀了。主題閱讀分為五個步驟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一個步驟，找出你的書單中最重要的章節&lt;/strong&gt;，並且去讀這些章節，在強調一下喔！是章節喔！不是最重要的一本書喔！在主題閱讀中，我們的重點應該放在主題上而不是單一書籍上，雖然作者也覺得很難以做到，所以如果你對這個領域還不夠了解，還是先利用分析閱讀好好的K幾本書在說，有了基本認識再來做主題閱讀。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二步驟，是建立與書單上的作者們之間的共識&lt;/strong&gt;，如同之前在分析閱讀所提到的，找出重要的keyword，並且了解作者所表示這個keyword的含意，你才有辦法和作者產生共識，但現在書單上有很多位作者，每位作者也許所用的語言都不同，所以你需要建立起每個作者之間的連結。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第三步驟，有了字義上的共識後，我們就可以開始釐清每本書所討論的中心主旨。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第四步驟，藉由每本書的中心主旨，我們就可以界定他們在討論的議題&lt;/strong&gt;，哪一些是主要的議題，哪一些是次要的議題，哪一些議題作者彼此間有差不多的看法，而哪些議題是爭論不休的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最後一個步驟，把自己丟進去討論，開始評論、分析這一些議題的討論，然後提出自己的看法。&lt;/strong&gt; 事實上，主題閱讀有點像分析閱讀的擴大版，只是分析閱讀是只對一本書，而主題閱讀則是對一群書，方法上差異不大。&lt;/p&gt;
&lt;h3 id="pk_1"&gt;書籍與網路的第二回合PK&lt;/h3&gt;
&lt;p&gt;回到最初的問題，我們藉由這本書來了解書籍閱讀本身有什麼弱點，我們需要用怎樣的策略來使得書籍閱讀更有效率，而我們是否能夠使用網路或電腦去改善這些缺點，讓網路可以在下一輪PK中扳回一城。&lt;/p&gt;
&lt;p&gt;先總結一下這本書，這本書所告訴我們的閱讀法則，其實不外乎是三個面向，第一，主動去閱讀，能用自己的話來講一本書的內容，第二，與作者建立相同的共識，第三，針對作者討論的議題，加以評論，提出自己的看法。&lt;/p&gt;
&lt;p&gt;從這幾個面向，所以我腦中就有了一個未來知識傳遞的另一種可能。在讀者方面，我們可以用超連結的方式來主動去找尋我們想要了解的部分，就像是維基百科一樣，但你所閱讀的網頁本身應該有所結構，不然很可能會使得讀者迷失在其中。&lt;/p&gt;
&lt;p&gt;所以不像一般的網路資訊，我們需要一個作者來整合，但在作者寫作方面要有所改變，與電腦技術相結合，我們之所以需要學習這本書的這些閱讀技巧，很大的原因是因為書籍本身的結構和架構並不是那麼容易被理解，往往不懂的作者真正想說什麼，書籍沒有明寫出作者心中的keyword和主旨，作者必須很費力的分章節分段落的解釋他的脈絡，好的章節分法會讓書籍更容易閱讀，作者也要很小心的在使用一個keyword的時候要先向讀者解釋清楚，避免造成歧意。&lt;/p&gt;
&lt;p&gt;於是，一件很奇妙的事情發生了，作者必須要有一個架構，從這個架構出發，然後巧妙的打平寫成一段段的文字，然後讀者在從這一段段的文字，試著了解作者想要表達的架構，等於繞了一大圈，如果一開始作者在寫作的時候就有軟體輔助他，作者給軟體一個架構，作者依照脈絡寫作，而這脈絡可以由軟體完整的呈現在讀者面前，另外，keyword的解釋可以放在一個獨立的頁面，用超連結連接進來，讓讀者在不清楚這個keyword的時候有更多的說明可以看，並且軟體可以依照作者所給的結構和段落，連貫成一本書，作者只需要再審視一下有哪個地方不通順，稍作修改，一本書就完成了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有了這樣的軟體，可以輕鬆的讓作者把心中的結構給「存」進文件裡，並且很智慧化的「呈現」給讀者，作者與讀者之間理解的鴻溝將會被弭平。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外，如果讀者想要做主題閱讀，網路資訊更容易重構，你可以把不同的書籍談論相同的章節給連結起來，相互比較閱讀，甚至這個軟體還可以幫你預先整理一個脈絡，這個軟體比你更了解每一位作者心中的脈絡，所以他可以連結比你好，也許還可以幫你準備一個學習順序，就像是一個老師一樣。&lt;/p&gt;
&lt;p&gt;還有可以加入社群的力量，在閱讀的過程，可以針對不清楚的地方進行提問，又或者看看別人寫下的某段的註解或評論，在書中寫下自己的看法，也可以看見其他讀者的評論，此時你不只是一個人在讀書，而是和一群人一起讀書，思考的廣度也就更大了。&lt;strong&gt;如果用這樣的方式來傳遞知識，可以同時兼顧書籍的完整性，和網路知識的彈性，我想知識傳遞將會更有效率。&lt;/strong&gt;&lt;/p&gt;</content><category term="Reading"></category></entry><entry><title>機器學習技法 學習筆記 (3)：Kernel Regression</title><link href="https://ycc.idv.tw/ml-course-techniques_3.html" rel="alternate"></link><published>2017-03-15T12:00:00+08:00</published><updated>2017-03-15T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-03-15:/ml-course-techniques_3.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋Probabilistic SVM、Kernel Logistic Regression、Kernel Ridge Regression、Support Vector Regression (SVR)&lt;/p&gt;</summary><content type="html">&lt;p&gt;在上一篇當中我們看到了Kernel Trick的強大，我們繼續運用這個數學工具在其他的Regression上看看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="soft-margin-svml2-regularized-logistic-regression"&gt;Soft-Margin SVM其實很像L2 Regularized Logistic Regression&lt;/h3&gt;
&lt;p&gt;上一篇中提到的Soft-Margin SVM其實很像&lt;a href="https://gitycc.github.io/YCNote/tag/ji-qi-xue-xi-ji-shi.html"&gt;《機器學習基石》&lt;/a&gt;裡頭提到的L2 Regularized Logistic Regression，如果你還記得的話，Logistic Regression是為了因應雜訊而給予每筆資料的描述賦予「機率」的性質，讓Model在看Data的時候不那麼的非黑及白，那時候有提到這叫做Soft Classification，而這個概念就非常接近於Soft-Margin的概念。&lt;/p&gt;
&lt;p&gt;從數學式來看會更清楚，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Soft-Margin SVM：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. (W^{T}W/2) + C×𝚺_{n} ξ_{n}\ \ \ s.t.\ \ \ y_{n}×(W^{T}Z_{n}+b) ≥ 1-ξ_{n}\ and\ ξ_{n} ≥ 0,\ n=1\cdots N\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的式子中，可以將限制條件由max取代掉，轉換成下面的Unbounded的表示方法，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Soft-Margin SVM：&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. C×𝚺_{n} Err_{hinge,n} + (W^{T}W/2)\)&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其中，&lt;span class="math"&gt;\(Err_{hinge,n}=max[0,1-y_{n}×(W^{T}Z_{n}+b)]\)&lt;/span&gt;，稱之為Hinge Error Measure&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;接下來比較一下L2 Regularized Logistic Regression，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;L2 Regularized Logistic Regression：&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. (1/N)×𝚺_{n} Err_{ce,n} +  (λ/N)×W^{T}W\)&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;span class="math"&gt;\(Err_{ce,n}=ln[1+exp(-y_{n}×(W^{T}Z_{n}))]\)&lt;/span&gt;，為Cross-Entropy Error Measure。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;你會發現Soft-Margin SVM和L2 Regularized Logistic Regression兩個式子的形式是很接近的，都有&lt;span class="math"&gt;\(W^{T}W\)&lt;/span&gt;這一項，只是意義上不同，在Soft-Margin SVM裡頭&lt;span class="math"&gt;\(W^{T}W\)&lt;/span&gt;所代表的是反比於空白區大小距離的函式，而在L2 Regularized Logistic Regression裡頭則是指Regularization。&lt;/p&gt;
&lt;p&gt;另外，我們來疊一下&lt;span class="math"&gt;\(Err_{hinge,n}\)&lt;/span&gt;和&lt;span class="math"&gt;\(Err_{ce,n}\)&lt;/span&gt;來看看這兩個函數像不像，&lt;/p&gt;
&lt;p&gt;&lt;img alt="compare:hinge and ce" src="/media/MachineLearningTechniques/MachineLearningTechniques.000_03.png" /&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(Err_{hinge,n}\)&lt;/span&gt;和&lt;span class="math"&gt;\(Err_{ce,n}\)&lt;/span&gt;是非常接近的，所以我們可以說做Soft-Margin SVM，很像是在做L2 Regularized Logistic Regression。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;雖然說Soft-Margin SVM和L2 Regularized Logistic Regression非常的像，但是我在做完Soft-Margin SVM後，仍然沒辦法像Logistic Regression一樣得到一個具有機率分布的Target Function，以下提供了兩種方法，第一種是間接的方法，使用兩階段學習來達成Logistic的效果；第二種是直接將L2 Regularized Logistic Regression加入有如Soft-Margin SVM的Kernel性質。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="svmlogistic-regressionprobabilistic-svm"&gt;使用SVM做Logistic Regression：Probabilistic SVM&lt;/h3&gt;
&lt;p&gt;要讓Soft-Margin SVM在最後呈現的Target Function時具有機率性質，最簡單的作法就是透過兩階段的學習來達成，第一階段先用Soft-Margin SVM去解出切分資料的平面，第二階段再將Logistic Function套在這個平面上，並做Fitting，最後我們就得到一個以Logistic Function表示的Target Function，這個稱之為Probabilistic SVM。實際操作方法如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;使用Soft-Margin SVM解出切平面&lt;span class="math"&gt;\(W_{SVM}^{T}Z+b_{SVM}=0\)&lt;/span&gt;，並將所有Data進一步的轉換到 &lt;span class="math"&gt;\(Z'_{n}=W_{SVM}^{T}Z(X_{n})+b_{SVM}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;接下來用轉換後的結果&lt;span class="math"&gt;\(\{Z'_{n},\ y_{n}\}\)&lt;/span&gt;做Logistic Regression得到係數A和B。&lt;/li&gt;
&lt;li&gt;最後的Target Function就是 &lt;span class="math"&gt;\(g(x)=Θ(A\cdot (W_{SVM}^{T}Z(X_{n})+b_{SVM})+B)\)&lt;/span&gt;，&lt;span class="math"&gt;\(Θ\)&lt;/span&gt;為Logistic Function。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的方法有一個缺點，就是如果B的值不接近0時，SVM的切平面就會和Logistic Regression的邊界就會不同，而且一個Model要Fitting兩次也相當的麻煩，以下還有另外一個可以達到一樣的具有機率性質的效果的方法—Kernel Logistic Regression。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="kernel-trickrepresenter-theorem"&gt;Kernel Trick的真正精髓：Representer Theorem&lt;/h3&gt;
&lt;p&gt;在說明Kernel Logistic Regression之前我們先來複習一下Kernel的概念，並且從中將他的重要觀念萃取出來。&lt;/p&gt;
&lt;p&gt;再來看一眼我們怎麼解Kernel Soft-Margin SVM的，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kernel Soft-Margin SVM：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;在&lt;span class="math"&gt;\(0 ≤ α_{n} ≤ C;\ 𝚺_{n} α_{n}y_{n} = 0\)&lt;/span&gt;的限制條件下，求解&lt;span class="math"&gt;\(min. [(1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}K(X_{n},X_{m})-𝚺_{n} α_{n}]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;得到&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;，然後&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(W = 𝚺_{n} α_{n}y_{n}Z_{n}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(b=y_{sv}-𝚺_{n} α_{n}y_{n}K(X_{n},X_{sv})\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其中W可以想成是由&lt;span class="math"&gt;\(Z_{n}\)&lt;/span&gt;所組合而成的，而決定貢獻程度則反應在放在它前面的係數&lt;span class="math"&gt;\((α_{n}y_{n})\)&lt;/span&gt;，&lt;span class="math"&gt;\(y_{n}\)&lt;/span&gt;決定貢獻的方向，&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;決定影響的程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;數學上，有個理論Representer Theorem可以告訴我們，所有的最佳化問題中，&lt;span class="math"&gt;\(W\)&lt;/span&gt;的最佳解都是由&lt;span class="math"&gt;\(Z_{n}\)&lt;/span&gt;所組合而成的，以線性代數的角度，就是&lt;span class="math"&gt;\(W\)&lt;/span&gt;由&lt;span class="math"&gt;\(Z_{n}\)&lt;/span&gt;所展開(span)，數學上表示成&lt;span class="math"&gt;\(W^*=𝚺_{n} β_{n}Z_{n}\)&lt;/span&gt;。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;這個性質為Kernel Trick提供了一個良好的基礎，每次我們只要遇到&lt;span class="math"&gt;\(W^{*T}Z\)&lt;/span&gt;的部分，我們就可以使用Representer Theorem把問題轉換成&lt;span class="math"&gt;\(W^{*T}Z=𝚺_{n} β_{n}Z_{n}Z=𝚺_{n} β_{n}K(X_{n},X)\)&lt;/span&gt;，就可以使用Kernel Function了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="kernel trick" src="/media/MachineLearningTechniques/MachineLearningTechniques.000_04.png" /&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上圖是老師在上課時列出來SVM、PLA和Logistic Regression的W的展開式，你會發現都可以表現成Representer Theorem的形式。&lt;/p&gt;
&lt;p&gt;有了這個概念，我們就可以把很多問題都利用Representer Theorem來轉換，並且套上Kernel Trick。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="kernel-logistic-regression"&gt;Kernel Logistic Regression&lt;/h3&gt;
&lt;p&gt;那我們有了Representer Theorem就可以直接來轉換L2 Regularized Logistic Regression，讓它有擁有Kernel的效果，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;L2 Regularized Logistic Regression：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. (1/N)×𝚺_{n} ln[1+exp(-y_{n}×(W^{T}Z_{n}))] +  (λ/N)×W^{T}W\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;使用&lt;span class="math"&gt;\(W^*=𝚺_{n} β_{n}Z_{n}\)&lt;/span&gt;代入得，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kernel Logistic Regression: &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(min. (1/N)×𝚺_{n} ln[ 1+exp(-y_{n}×𝚺_{n} β_{n}K(X_{n},X)) ] +  (λ/N)×𝚺_{n}𝚺_{m} β_{n}β_{m}K(X_{n},X_{m})\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的式子可以使用Grandient Descent來求解&lt;span class="math"&gt;\(β_{n}\)&lt;/span&gt;，進而得到&lt;span class="math"&gt;\(W^*=𝚺_{n} β_{n}Z_{n}\)&lt;/span&gt;。而且在Kernel Function的幫助之下，我們更容易可以做到非常高次的特徵轉換。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="kernel-ridge-regression"&gt;Kernel Ridge Regression&lt;/h3&gt;
&lt;p&gt;同理，我們也可以把相同技巧套用到Ridge Regression，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ridge Regression：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. (1/N)×𝚺_{n} (y_{n}-W^{T}Z_{n})^{2} +  (λ/N)×W^{T}W\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;使用&lt;span class="math"&gt;\(W^*=𝚺_{n} β_{n}Z_{n}\)&lt;/span&gt;代入得，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kernel Ridge Regression：&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(min. (1/N)×𝚺_{n} (y_{n}-𝚺_{m} β_{m}K(X_{n},X_{m}))^{2} +  (λ/N)×𝚺_{n}𝚺_{m} β_{n}β_{m}K(X_{n},X_{m})\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的式子也可以使用Grandient Descent來求解&lt;span class="math"&gt;\(β_{n}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;另外，這個式子有辦法推出解析解，先把上式可以寫成矩陣形式，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kernel Ridge Regression：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. E_{aug}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(E_{aug}=(1/N)×(β^{T}K^{T}Kβ-2β^{T}K^{T}y+y^{T}y) +  (λ/N)×β^{T}Kβ)\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以，由&lt;span class="math"&gt;\(∇E_{aug}=0\)&lt;/span&gt;就可以得到最小值成立的條件為&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(β^*=(λI+K)^{-1}y\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其實這個式子非常像之前在線性模型時使用的Pseudo-Inverse，&lt;/p&gt;
&lt;p&gt;Pseudo-Inverse：&lt;span class="math"&gt;\(W=(X^{T}X)^{-1}X^{T}y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;不過現在更為強大了，可以求得非線性模型+Regularization下的解析解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我們可以使用Kernel Ridge Regression來做分類問題，稱之為Least-Squares SVM (LSSVM) 。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="support-vector-regression-svr"&gt;Support Vector Regression (SVR)&lt;/h3&gt;
&lt;p&gt;其實，不管是Kernel Logistic Regression還是Kernel Ridge Regression，這種直接套用Representer Theorem在Regression上的都有一個缺點。&lt;/p&gt;
&lt;p&gt;那就是它們的&lt;strong&gt;&lt;span class="math"&gt;\(β_{n}\)&lt;/span&gt;並不確保大多數是0&lt;/strong&gt;，如果Data筆數非常多的話，這在計算上會是一種負荷。在之前我們討論Kernel SVM時有提到只有Support Vector的數據才會對Model最後的結果有所貢獻，Support Vector的&lt;span class="math"&gt;\(α_{n}&amp;gt;0\)&lt;/span&gt;；而不是Support Vector的數據則沒有貢獻，Non-Support Vector的&lt;span class="math"&gt;\(α_{n}=0\)&lt;/span&gt;。所以你可以想見的是，&lt;strong&gt;&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;大多數是0除了Support Vector外，我們稱這叫做「Sparse &lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;」性質&lt;/strong&gt;，有這樣的性質可以大大的減少計算量。&lt;/p&gt;
&lt;p&gt;因此接下來我們打算&lt;strong&gt;讓Regression具有Support Vector的性質，稱之為Support Vector Regression (SVR)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="SVR" src="/media/MachineLearningTechniques/MachineLearningTechniques.006.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;見上圖說明，Support Vector Regression簡稱SVR，以往的Linear Regression是求一條擬合直線能使所有數據點到直線的Error最小，而現在我們賦予它Soft-Margin的能力，&lt;strong&gt;SVR將擬合直線向外擴張距離ε，在這個擴張的區域裡頭的數據點不去計算它的Error，只有在超出距離ε外的才去計算Error&lt;/strong&gt;，此時這個擬合直線有點像一條水管，水管外我們才計算Error，所以又稱之為Tube Regression。&lt;/p&gt;
&lt;p&gt;這個概念和Soft-Margin SVM有點像，都是在邊界給予犯錯的機會，不同的是Soft-Margin SVM因為是分類問題，所以不允許錯誤的數據超過界，所以評估Error的方向是向內的，而SVR是向外評估Error，在水管外部上方的Error我們記作&lt;span class="math"&gt;\(ξ_{n}^{⋀}\)&lt;/span&gt;，在水管外部下方的Error我們記作&lt;span class="math"&gt;\(ξ_{n}^{⋁}\)&lt;/span&gt;，&lt;strong&gt;所以SVR的目的就是在Regularization之下使得&lt;span class="math"&gt;\(ξ_{n}^{⋀}+ξ_{n}^{⋁}\)&lt;/span&gt;最小，並且調整距離ε和C來決定對Error的容忍程度&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;這個問題同樣的可以化作Dual問題，問題變成只需要最佳化&lt;span class="math"&gt;\(α_{n}^{⋀}\)&lt;/span&gt;和&lt;span class="math"&gt;\(α_{n}^{⋁}\)&lt;/span&gt;，再使用最佳化後的&lt;span class="math"&gt;\(α_{n}^{⋀}\)&lt;/span&gt;和&lt;span class="math"&gt;\(α_{n}^{⋁}\)&lt;/span&gt;就可以得到&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(b\)&lt;/span&gt;。其中&lt;span class="math"&gt;\(W=𝚺_{n} (α_{n}^{⋀}-α_{n}^{⋁}) Z_{n}\)&lt;/span&gt;這式子裡頭隱含著Representer Theorem，每筆數據的貢獻程度&lt;span class="math"&gt;\(β_{n}=(α_{n}^{⋀}-α_{n}^{⋁})\)&lt;/span&gt;，&lt;strong&gt;因此在管子內的&lt;span class="math"&gt;\(α_{n}^{⋀}=0\)&lt;/span&gt;且&lt;span class="math"&gt;\(α_{n}^{⋁}=0\)&lt;/span&gt;，不會有所貢獻，這使得SVR具有Sparse的性質，可以大大的減少計算&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="_1"&gt;結語&lt;/h3&gt;
&lt;p&gt;這一篇中，我們一開始揭露了「Soft-Margin SVM其實很像L2 Regularized Logistic Regression」的這個現象，所以在SVM中最小化&lt;span class="math"&gt;\(W^{T}W\)&lt;/span&gt;有點像是Regression中的Regularization，也因為形式上相當的接近，所以在SVM裡頭用到的數學技巧同樣的可以套到這些有Regularized的Regression上。&lt;/p&gt;
&lt;p&gt;然後，我們從Kernel Soft-Margin SVM中萃取出Kernel Trick的精華—Representer Theorem，最佳化的W可以由Data的Feature &lt;span class="math"&gt;\(Z_{n}\)&lt;/span&gt;所組成，記作&lt;span class="math"&gt;\(W^*=𝚺_{n} β_{n}Z_{n}\)&lt;/span&gt;，這提供了Kernel Trick背後的實踐基礎，接下來我們就開始運用Representer Theorem在L2 Regularized Logistic Regression和Ridge Regression上，讓這些Regression可以輕易的做非線性特徵轉換。&lt;/p&gt;
&lt;p&gt;最後，我們指出了直接套用Representer Theorem在Regression上的缺點就是參數並不Sparse，所以造成計算量大大增加。因此Support Vector Regression (SVR)參照Soft-Margin SVM的形式重新設計Regression，並且使用Dual Transformation和Kernel Function來轉化問題，最後SVR就具有Sparse的特性了。&lt;/p&gt;
&lt;p&gt;上一篇跟這一篇，談的是「Kernel Models」，在這樣的形式下我們可以讓我們的「特徵轉化」變得更為複雜，甚至是無窮多次方還是做得到的。下一篇，我們會進到另外一個主題—Aggregation Models。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (2)：Support Vector Machine (SVM)</title><link href="https://ycc.idv.tw/ml-course-techniques_2.html" rel="alternate"></link><published>2017-02-20T12:00:00+08:00</published><updated>2017-02-20T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-02-20:/ml-course-techniques_2.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋Hard-Margin Support Vector Machine (SVM)、Kernel Function、Kernel Hard-Margin SVM、Soft-Margin SVM、Kernel Soft-Margin SVM、拉格朗日乘子法（Lagrange Multiplier）、Lagrangian Dual Problem&lt;/p&gt;</summary><content type="html">&lt;p&gt;在&lt;a href="/ml-course-techniques_1.html"&gt;上一篇文章&lt;/a&gt;當中，我們掃過了《機器學習技法》 將會包含的內容，今天我們正式來看SVM。&lt;/p&gt;
&lt;p&gt;如果我想要使用無窮次高次方的非線性轉換加入我的Model，可以做到嗎？上一篇，我告訴大家，只要使用Dual Transformation加上Kernel Function等數學技巧就可以做到，我們今天就來看一下這是怎麼一回事。&lt;/p&gt;
&lt;p&gt;本篇文章分為兩個部分，第一部分我盡量不牽扯太多數學計算，而將數學證明放在第二個部分，數學證明的部分非常複雜，但我並不打算把它們忽略掉，因為這些數學計算是相當重要的，它所帶來的方法和概念是可以重複使用的，也有助於你了解和創造其他演算法，所以有心想要成為專家的你請耐心的把後半段的數學看完。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="hard-margin-support-vector-machine-svm"&gt;Hard-Margin Support Vector Machine (SVM)&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Hard-Margin SVM" src="/media/MachineLearningTechniques/MachineLearningTechniques.001.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;回到我們最熟悉的二元分類問題，如果問題的答案是線性可分的話，我們可以找到一條直線把兩類Data給切開來，而在以前PLA的方法，切在哪裡其實是沒辦法決定的，PLA只能幫你找到可以分開兩類的一刀，但不能幫你把這刀切的更好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我們希望這個切開兩類的邊界可以離兩類Data越遠越好，讓邊界到Data有一個較大的空白區，這就是Hard-Margin SVM做的事&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;我們先來看一下如何計算切平面到任意Data的距離，首先我先假設切平面的方程式為
&lt;/p&gt;
&lt;div class="math"&gt;$$
W^T X+b = 0 (切平面)
$$&lt;/div&gt;
&lt;p&gt;
回想一下高中數學，這個平面的法向量是W，垂直於平面，所以垂直於平面的單位法向量是 &lt;span class="math"&gt;\(W/|W|\)&lt;/span&gt;，今天如果我有一點Data Point落在&lt;span class="math"&gt;\(X\)&lt;/span&gt;，另外在平面上任意再找一點&lt;span class="math"&gt;\(X_0\)&lt;/span&gt;，從&lt;span class="math"&gt;\(X_0\)&lt;/span&gt;到&lt;span class="math"&gt;\(X\)&lt;/span&gt;的向量表示為&lt;span class="math"&gt;\(X-X_0\)&lt;/span&gt;，這個向量如果投影到單位法向量上，這個向量的大小正是Data Point到平面的最短距離，表示成
&lt;/p&gt;
&lt;div class="math"&gt;$$
d = |W\cdot (X - X_0)| / |W|
$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(X_0\)&lt;/span&gt;符合切平面的方程式&lt;span class="math"&gt;\(W^T X_0+b = 0\)&lt;/span&gt;代入，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
d = |W\cdot X + b| / |W|
$$&lt;/div&gt;
&lt;p&gt;
所以假如我有一群線性可分的二元分類Data，這個切平面我希望可以離兩類Data越遠越好，所以我會有一段全部都沒有Data的空白區，這邊假設這個空白區的邊界為
&lt;/p&gt;
&lt;div class="math"&gt;$$
W^TX+b = ±1
$$&lt;/div&gt;
&lt;p&gt;
這個假設是可以做到的，因為我們可以以比例去調整&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(b\)&lt;/span&gt;來達到縮放的效果，而不會影響切平面&lt;span class="math"&gt;\(W^T X+b = 0\)&lt;/span&gt; 。從上面的距離公式，我們知道在這個假設之下，空白區邊界距離切平面為
&lt;/p&gt;
&lt;div class="math"&gt;$$
margin = 1 / |W|
$$&lt;/div&gt;
&lt;p&gt;
而剛好落在這空白區邊界的Data會符合以下方程式&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(y_n\times (W^T X_n+b) = 1\ (Support\ Vector)\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;的正負剛好和&lt;span class="math"&gt;\((W^T X_n+b)\)&lt;/span&gt;相抵消，&lt;strong&gt;這些落在空白區邊界的Data被稱為Support Vector，就字面上的意義就像是空白區由這一些數據給「撐」起來，而切平面只由這些Support Vector的數據點所決定，和其他的數據點無關&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果考慮所有Data的話，應該要滿足
&lt;/p&gt;
&lt;div class="math"&gt;$$
y_n\times (W^T X_n+b) ≥ 1\ (All\ Data)
$$&lt;/div&gt;
&lt;p&gt;
&lt;strong&gt;綜合上述，Hard-Margin SVM的目標就是，在符合&lt;span class="math"&gt;\(y_n\times (W^T X_n+b) ≥ 1 ,\ n=1~N\)&lt;/span&gt;的條件下，求&lt;span class="math"&gt;\(Margin (1 / |W|)\)&lt;/span&gt;最大的情形，也可以等價於求&lt;span class="math"&gt;\((W^T W/2)\)&lt;/span&gt; 最小的情形，這個問題有辦法使用QP Solver來求解，詳見&lt;a href="https://en.wikipedia.org/wiki/Quadratic_programming"&gt;這裡&lt;/a&gt;，我就不多加介紹這個數學工具。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="kernel-function"&gt;Kernel Function&lt;/h3&gt;
&lt;p&gt;Kernel Function是最終可以讓我們有無限多次方特徵的數學工具，但這個工具非常容易理解。&lt;/p&gt;
&lt;p&gt;假設考慮一個非線性轉換，將&lt;span class="math"&gt;\(X\)&lt;/span&gt;空間轉換到&lt;span class="math"&gt;\(Z\)&lt;/span&gt;空間，那如果我需要計算轉換過的兩個新Features相乘&lt;span class="math"&gt;\(Z_n (X_n)\times Z_m(X_m)\)&lt;/span&gt;，我有辦法&lt;strong&gt;不需要先做特徵轉換再相乘&lt;/strong&gt;，而是直接使用原有的Features &lt;span class="math"&gt;\(X_n\)&lt;/span&gt;和&lt;span class="math"&gt;\(X_m\)&lt;/span&gt;求出&lt;span class="math"&gt;\(Z_n(X_n)×Z_m(X_m)\)&lt;/span&gt;的最後結果？這種情形數學可以表示成&lt;span class="math"&gt;\(K(X_n,X_m)=Z_n(X_n)×Z_m(X_m)\)&lt;/span&gt;，這個函式就叫Kernel Function。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果有了Kernel Function這樣的數學工具，就可以簡化和優化因為「特徵轉換」所帶來的複雜計算。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我列出以下幾種Kernel Function：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Polynomial Kernel：&lt;span class="math"&gt;\(K_Q(X_n,X_m)=(ζ+γ X_n^T X_m)^Q\)&lt;/span&gt;等價於 「Q次方非線性轉換後的兩個新特徵相乘」。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Guassian Kernel：&lt;span class="math"&gt;\(K(X_n,X_m)=exp(-γ|X_n-X_m|^2)\)&lt;/span&gt;等價於 「無窮次方非線性轉換後的兩個新特徵相乘」。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此有了Guassian Kernel的幫忙，我們完全不需要管特徵轉換有多複雜，我們可以直接使用原有的Features 來計算「無窮次方的非線性轉換」。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最後給予Kernel Function一個物理解釋，Kernel Function說穿了就是兩個向量轉換到Z空間後的「內積」，「內積」可以約略想成是「相似程度」，當兩個向量同向，內積是正的，相似度高，但當兩個向量反向，內積是負的，相似度極低，所以你會發現Guassian Kernel在&lt;span class="math"&gt;\(X_n=X_m\)&lt;/span&gt;會出現最大值，因為代表這兩個位置相似度極高。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="kernel-hard-margin-svm"&gt;Kernel Hard-Margin SVM&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Kernel Hard-Margin SVM" src="/media/MachineLearningTechniques/MachineLearningTechniques.002.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;那我們如何使用Kernel Function來使得Hard-Margin SVM更厲害呢？我們必須額外引入另外的數學工具，包括：Lagrange Multiplier和Lagrange Dual Problem，才有辦法把Kernel Function用上，不過這部份的數學有一些複雜，我將這部份的證明放在後面的附錄上，這邊就直接從結果講起。&lt;/p&gt;
&lt;p&gt;Kernel Hard-Margin SVM的公式是，在&lt;span class="math"&gt;\(α_n  ≥ 0; 𝚺_n α_n y_n = 0\)&lt;/span&gt;的限制條件下，求解&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;使得 &lt;span class="math"&gt;\([(1/2)𝚺_n 𝚺_m  α_n α_m y_n y_m K(X_n,X_m)-𝚺_n α_n]\)&lt;/span&gt;為最小值，&lt;/p&gt;
&lt;p&gt;其中&lt;span class="math"&gt;\(K(X_n,X_m)\)&lt;/span&gt;就是Kernel Function，由你的特徵轉換方式來決定，這個問題一樣可以使用QP Solver來求解。&lt;/p&gt;
&lt;p&gt;當我們已經有了每筆數據點的&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;了，接下來可以利用&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;求出切平面的W和b，在那之前來看一下&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;的意義，&lt;strong&gt;&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;可以看作是某個數據點對切平面的貢獻程度，&lt;span class="math"&gt;\(α_n=0\)&lt;/span&gt;的這些數據點為非Support Vector，而&lt;span class="math"&gt;\(α_n&amp;gt;0\)&lt;/span&gt;的這些數據點是Support Vector，所以對切平面有貢獻的只有Support Vector而已&lt;/strong&gt;，這和剛剛的結論相同。因此，W和b可由Support Vector決定，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(W = 𝚺_{n=sv} α_n y_n Z_n\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(b=y_{sv}-𝚺_n α_n y_n K(X_n,X_{sv})\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;最後提一個非常重要的概念，是什麼原因讓我們不需要管特徵轉換的複雜度？以往我們的作法是這樣的，我們有每筆Data的Features，接下來對每筆Data做特徵轉換，然後在用特徵轉換後的新Features去Train線性模型，這麼一來如果特徵轉換的次方非常高的話，計算的複雜度就會全落在特徵轉換上。&lt;strong&gt;所以我們巧妙的使用數學工具，讓我們可以單單使用Data的Labels來做優化，而將複雜的特徵轉換利用Kernel Function的方式「嵌入」到優化的過程裡頭，此時計算量就只與Data數量有關，所以可以完全不管特徵轉換所帶來的複雜度&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="kernel-hard-margin-svm_1"&gt;Kernel Hard-Margin SVM: 無窮次方的特徵轉換效果如何?&lt;/h3&gt;
&lt;p&gt;終於我們可以使用無窮次方的特徵轉換了，只要使用Kernel Hard-Margin SVM搭配上Guassian Kernel：&lt;span class="math"&gt;\(K(X_n,X_m)=exp(-γ|X_n-X_m|^2)\)&lt;/span&gt;就可以辦到，下圖是模擬的結果，是不是看起來很強大，隨著γ的不同會有不一樣的切分方法，&lt;strong&gt;你會發現γ越大時看起來的結果越接近Overfitting，所以必須小心挑選γ的大小。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Guassian Kernel in Hard-Margin SVM" src="/media/MachineLearningTechniques/MachineLearningTechniques.000_01.png" /&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/203_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/203_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="soft-margin-svm"&gt;Soft-Margin SVM&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Soft-Margin SVM" src="/media/MachineLearningTechniques/MachineLearningTechniques.003.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;剛剛Hard-Margin SVM會很容易Overfitting的原因在於它的機制無法&lt;strong&gt;容忍雜訊&lt;/strong&gt;，所以接下來要講的Soft-Margin SVM可以容忍部份的Data違反規則，讓它們可以超出空白區的邊界。&lt;/p&gt;
&lt;p&gt;見上圖，可以發現我們稍微修改了Hard-Margin SVM，加入了參數&lt;span class="math"&gt;\(ξ_n\)&lt;/span&gt;，&lt;span class="math"&gt;\(ξ_n\)&lt;/span&gt;代表錯誤的Data離空白區邊界有多遠，而我們將&lt;span class="math"&gt;\(ξ_n\)&lt;/span&gt;的總和加進去Cost裡面，在優化的過程中將使違反的狀況不會太多和離邊界太遠，&lt;strong&gt;而參數C負責控制&lt;span class="math"&gt;\(ξ_n\)&lt;/span&gt;總和的影響程度，如果C很大，代表不大能容忍雜訊；如果C很小，則代表對雜訊的容忍很寬鬆&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;因此我們現在有兩種Support Vector，一種是剛好落在空白區邊界的，稱為Free Support Vector；另外一種是違反規則並超出空白區的，稱為Bounded Support Vector，切平面一樣是由這些Support Vector所決定。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="kernel-soft-margin-svm"&gt;Kernel Soft-Margin SVM&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Kernel Soft-Margin SVM" src="/media/MachineLearningTechniques/MachineLearningTechniques.004.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;接下來同樣的對Soft-Margin SVM做數學上Lagrange Multiplier和Lagrange Dual Problem的轉換，再將Kernel Function用上，一樣的，我將這部份的證明放在後面的附錄上，這邊就直接從結果講起。&lt;/p&gt;
&lt;p&gt;Kernel Soft-Margin SVM的公式是，在&lt;span class="math"&gt;\(0 ≤ α_n ≤ C;\ 𝚺_n α_n y_n = 0\)&lt;/span&gt;的限制條件下，求解&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;使得 &lt;span class="math"&gt;\([(1/2)𝚺_n 𝚺_m α_n α_m y_n y_m K(X_n ,X_m)-𝚺_n α_n]\)&lt;/span&gt;為最小值，&lt;/p&gt;
&lt;p&gt;你會發現和Kernel Hard-Margin SVM唯一只差在&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;被&lt;span class="math"&gt;\(C\)&lt;/span&gt;所限制。&lt;/p&gt;
&lt;p&gt;當我們已經有了每筆數據點的&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;了，接下來可以利用&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;求出切平面的&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(b\)&lt;/span&gt;，&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;一樣的可以看作是某個數據點對切平面的貢獻程度，&lt;span class="math"&gt;\(α_n=0\)&lt;/span&gt;的這些數據點為非Support Vector，而&lt;span class="math"&gt;\(α_n&amp;gt;0\)&lt;/span&gt;的這些數據點是Support Vector，可以進一步細分，&lt;span class="math"&gt;\(α_n &amp;lt; C\)&lt;/span&gt;為Free Support Vector，而&lt;span class="math"&gt;\(α_n＝C\)&lt;/span&gt;為Bounded Support Vector。相同的，W和b可由Support Vector (Free Support Vector和Bounded Support Vector)決定，跟Kernel Hard-Margin SVM公式一模一樣&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(W = 𝚺_{n=sv} α_n y_n Z_n\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(b=y_{sv} -𝚺_n α_n y_n K(X_n,X_{sv})\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="kernel-soft-margin-svm_1"&gt;Kernel Soft-Margin SVM: 容忍雜訊的無窮次方特徵轉換&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Guassian Kernel in Soft-Margin SVM" src="/media/MachineLearningTechniques/MachineLearningTechniques.000_02.png" /&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/204_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/204_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;來看看Kernel Soft-Margin SVM搭配上Guassian Kernel的效果如何，上圖是模擬的結果，我們會發現有部分Data違反分類規則，所以Soft-Margin SVM確實可以容忍雜訊，而且&lt;span class="math"&gt;\(C\)&lt;/span&gt;越小，容忍雜訊的能力越強，所以要特別注意&lt;span class="math"&gt;\(C\)&lt;/span&gt;的選取，如果沒有選好還是可能造成Overfitting的。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="_1"&gt;結語&lt;/h3&gt;
&lt;p&gt;在這一篇當中，我們介紹了Hard-Margin SVM和Soft-Margin SVM，並且成功的利用數學工具將問題轉換成，可以單單使用Data的Labels來做優化，而將複雜的特徵轉換利用Kernel Function的方式「嵌入」到優化的過程裡頭，此時計算量就只與Data數量有關，所以可以完全不管特徵轉換所帶來的複雜度，因此利用Guassian Kernel就可以做到「無窮多次的特徵轉換」了。最後再次強調數學的部分非常重要，它提供的方法和概念是可以重複使用的，而這部份的數學是少不了的，所以有興趣的可以繼續往下看下去。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="lagrange-multiplier"&gt;[進階] 拉格朗日乘子法（Lagrange Multiplier）&lt;/h3&gt;
&lt;p&gt;如果是物理系學生修過古典力學，應該對這個數學工具不陌生。&lt;strong&gt;Lagrange Multiplier是用在有限制條件之下的求極值問題&lt;/strong&gt;，步驟如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;問題：在限制 &lt;span class="math"&gt;\(g_i (x_1,x_2, … , x_n) = 0,\ i=1\cdots k\)&lt;/span&gt;  之下，求 &lt;span class="math"&gt;\(f(x_1,x_2, … , x_n)\)&lt;/span&gt; 的極值&lt;/li&gt;
&lt;li&gt;假設Lagrange Function：   &lt;span class="math"&gt;\(L(x_1,x_2, … , x_n,λ_i) = f(x_1,x_2, … , x_n) + 𝚺_i λ_i × g_i(x_1,x_2, … , x_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;聯立方程式求解：&lt;/li&gt;
&lt;li&gt;找L的極值：&lt;span class="math"&gt;\(∇L = 0\)&lt;/span&gt;  [Stationarity Condition]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(g_i (x_1,x_2, … , x_n) = 0,\ i=1\cdots k\)&lt;/span&gt;  [Primal Feasibility Condition]&lt;/li&gt;
&lt;li&gt;求解以上聯立方程式得到最佳解 &lt;span class="math"&gt;\(x_{1},x_{2}, … , x_{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上面的聯立方程式不難理解，Primal Feasibility Condition就是我們的限制式，然後Stationarity Condition就是求極值的方法，非常直觀，滿足上面的式子我們就可以在限制上面找極值。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;上面是一般的Lagrange Multiplier，只有考慮到限制式是等式的情形，假如限制條件是不等式呢？我們來看一下加強版的Lagrange Multiplier：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;問題：在限制 &lt;span class="math"&gt;\(g_{i}(x_{1},x_{2}, … , x_{n}) = 0,\ i=1\cdots k\)&lt;/span&gt; 且  &lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) ≤ 0,\ j=1\cdots r\)&lt;/span&gt; 之下，求 &lt;span class="math"&gt;\(f(x_{1},x_{2}, … , x_{n})\)&lt;/span&gt; 的極值&lt;/li&gt;
&lt;li&gt;假設Lagrange Function：   &lt;span class="math"&gt;\(L(x_{1},x_{2}, … , x_{n}, λ_{i},μ_{j}) = f(x_{1},x_{2}, … , x_{n}) + 𝚺_{i} λ_{i} × g_{i}(x_{1},x_{2}, … , x_{n}) + 𝚺_{j} μ_{j} × h_{j}(x_{1},x_{2}, … , x_{n})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;聯立方程式求解：&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;找&lt;span class="math"&gt;\(L\)&lt;/span&gt;的極值：&lt;span class="math"&gt;\(∇L = 0\)&lt;/span&gt;  [Stationarity Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(g_{i}(x_{1},x_{2}, … , x_{n}) = 0,\ i=1\cdots k\)&lt;/span&gt; 且 &lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) ≤ 0,\ j=1\cdots r\)&lt;/span&gt;  [Primal Feasibility Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(μ_{j}  × h_{j} (x_{1},x_{2}, … , x_{n}) = 0,\ j=1\cdots r\)&lt;/span&gt;  [Complementary Slackness Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;求&lt;span class="math"&gt;\(L\)&lt;/span&gt;的最小值時 &lt;span class="math"&gt;\(μ_{j} ≥ 0,\ j=1\cdots r\)&lt;/span&gt;；求&lt;span class="math"&gt;\(L\)&lt;/span&gt;的最大值時 &lt;span class="math"&gt;\(μ_{j} ≤ 0,\ j=1\cdots r\)&lt;/span&gt; [Dual Feasibility Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;以上的條件包括Stationarity、Primal Feasibility、Complementary Slackness、Dual Feasibility通稱 KKT (Karush-Kuhn-Tucker) Conditions&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;加強版的Lagrange Multiplier和一般版的一樣有Stationarity Condition和Primal Feasibility Condition。唯一增加的是Complementary Slackness Condition和Dual Feasibility Condition。&lt;/p&gt;
&lt;p&gt;先來講一下Complementary Slackness Condition怎麼來的，我們來考慮不等式條件&lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) ≤ 0\)&lt;/span&gt;，會有兩個情形發生，一個是壓到邊界，也就是&lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) = 0\)&lt;/span&gt;，這個時候問題就回到一般版的Lagrange Multiplier，此時&lt;span class="math"&gt;\(μ_{j}\)&lt;/span&gt;和&lt;span class="math"&gt;\(λ_{i}\)&lt;/span&gt;效果是一樣的，&lt;span class="math"&gt;\(μ_{j}\)&lt;/span&gt;可以是任意值；另外一種情況是我沒壓到邊界，也就是&lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) &amp;lt; 0\)&lt;/span&gt;，這個時候我可以把這個限制看作不存，最簡易的方法就是令&lt;span class="math"&gt;\(μ_{j}=0\)&lt;/span&gt;，他在&lt;span class="math"&gt;\(L(x_{1},x_{2}, … , x_{n}, λ_{i},μ_{j})\)&lt;/span&gt; 中就不參與作用了。&lt;strong&gt;所以綜合壓到邊界和不壓到兩種情況，我們可以寫出一個有開關效果的方程式 &lt;span class="math"&gt;\(μ_{j} × h_{j}(x_{1},x_{2}, … , x_{n}) = 0\)&lt;/span&gt;，這就是Complementary Slackness Condition。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外一個是Dual Feasibility Condition，這個限制一樣是在不等式條件才會發生，&lt;span class="math"&gt;\(μ_{j}\)&lt;/span&gt;的正負號取決於&lt;span class="math"&gt;\(L\)&lt;/span&gt;是要求最大還是求最小值，稍微解釋一下，找極值我們用&lt;span class="math"&gt;\(∇L = 0\)&lt;/span&gt;這個式子來求，代入Lagrange Function後得&lt;span class="math"&gt;\(∇L = ∇f +𝚺_{i}λ_{i}×∇g_{i}+𝚺_{j}μ_{j}×∇h_{j}=0\)&lt;/span&gt;，先定性來看，假設不計&lt;span class="math"&gt;\(∇g_{i}\)&lt;/span&gt;的影響，當最後解落在&lt;span class="math"&gt;\(h ≤ 0\)&lt;/span&gt;的邊界上時&lt;span class="math"&gt;\(∇f＝- μ×∇h\)&lt;/span&gt;，因為&lt;span class="math"&gt;\(h ≤ 0\)&lt;/span&gt;的關係，所以&lt;span class="math"&gt;\(∇h\)&lt;/span&gt;是朝向可行區的外面，如果今天是求&lt;span class="math"&gt;\(f\)&lt;/span&gt;的極小值，那們&lt;span class="math"&gt;\(∇f\)&lt;/span&gt;應當朝著可行區才合理，如果不是的話則可行區內部有更小更佳的解，所以求極小值時&lt;span class="math"&gt;\(μ ≥ 0\)&lt;/span&gt;；如果是求&lt;span class="math"&gt;\(f\)&lt;/span&gt;的極大值，那&lt;span class="math"&gt;\(∇f\)&lt;/span&gt;應當朝著可行區的外面，所以&lt;span class="math"&gt;\(μ ≤ 0\)&lt;/span&gt;，這個條件待會會用在對偶問題上面。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;其實我們之前在《機器學習基石》裡的Regularization有偷用了Lagrange Multiplier的產物。&lt;/p&gt;
&lt;p&gt;Regularization將W的長度限制在一個範圍，表示成
&lt;/p&gt;
&lt;div class="math"&gt;$$
|W|^{2} ≤ C
$$&lt;/div&gt;
&lt;p&gt;
在這個條件下我們要找&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的極小值，使用加強版的Lagrange Multiplier：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;問題：在限制  &lt;span class="math"&gt;\(|W|^{2} - C ≤ 0\)&lt;/span&gt; 之下，求 &lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt; 的極小值&lt;/li&gt;
&lt;li&gt;假設Lagrange Function：   &lt;span class="math"&gt;\(L = E_{in} + μ × ( |W|^{2} - C)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;聯立方程式求解：&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉W = 𝞉E_{in} / 𝞉W + 2μ × |W| = 0\)&lt;/span&gt;  [Stationarity Condition]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(|W|^{2} - C ≤ 0\)&lt;/span&gt;  [Primal Feasibility Condition]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(μ × ( C - |W|^{2} ) = 0\)&lt;/span&gt;  [Complementary Slackness Condition]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stationarity Condition的結果就是Regularization的結果了，可以&lt;a href="/ml-course-foundations_4.html"&gt;回去參照一下&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="lagrangian-dual-problem"&gt;[進階] Lagrangian Dual Problem&lt;/h3&gt;
&lt;p&gt;接下來來講對偶問題，這個部分很難，我也是反覆在網路上看了很多篇介紹才弄懂，推薦大家看&lt;a href="http://www.eng.newcastle.edu.au/eecs/cdsc/books/cce/Slides/Duality.pdf"&gt;這一篇&lt;/a&gt;，這篇介紹的很清楚，應該會對大家理解Lagrangian Dual有幫助。&lt;/p&gt;
&lt;p&gt;來考慮一下待會會用到的求極小值問題，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在限制 &lt;span class="math"&gt;\(g_{i}(x_{1},x_{2}, … , x_{n}) = 0,\ i=1\cdots k\)&lt;/span&gt; 且  &lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) ≤ 0,\ j=1\cdots r\)&lt;/span&gt; 之下，求 &lt;span class="math"&gt;\(f(x_{1},x_{2}, … , x_{n})\)&lt;/span&gt; 的極小值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果我們利用剛剛的解法，稱之為Lagrangian Primal Problem。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;而這個問題可以等效轉換成Lagrangian Dual Problem，利用以下關係式&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(Minimum Problem ≡ min. L  ≡ min. [max._{μ ≥ 0} L] ≥ max._{μ ≥ 0} [min. L(μ)]\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我們在將原本&lt;span class="math"&gt;\(min. L\)&lt;/span&gt; 換成&lt;span class="math"&gt;\(min. [max._{μ ≥ 0} L]\)&lt;/span&gt; 是不影響結果的，因為我們剛剛分析過了在求最小值時&lt;span class="math"&gt;\(μ ≥ 0\)&lt;/span&gt;是合理的，相反的如果&lt;span class="math"&gt;\(μ &amp;lt; 0\)&lt;/span&gt;，則求&lt;span class="math"&gt;\(max._{μ ≥ 0} L\)&lt;/span&gt;時會產生無限大的結果，接下來就是交換&lt;span class="math"&gt;\(min.\)&lt;/span&gt;和&lt;span class="math"&gt;\(max.\)&lt;/span&gt;的部分，數學上可以證明&lt;span class="math"&gt;\(min. [max._{μ ≥ 0} L] ≥ max._{μ ≥ 0} [min. L(μ)]\)&lt;/span&gt;這樣的關係，我們就稱左式轉到右式為Dual轉換。&lt;/p&gt;
&lt;p&gt;而上面式子右側的求法，我們可以先求出&lt;span class="math"&gt;\(Θ(λ_{i},μ_{j}) =\ given\ λ_{i},μ_{j}\ to\ find\ min. L(x_{1},x_{2}, … , x_{n}, λ_{i},μ_{j})\)&lt;/span&gt; ，作法是使用&lt;span class="math"&gt;\(∇L = 0\)&lt;/span&gt;所產生符合極值的參數代入&lt;span class="math"&gt;\(L(x_{1},x_{2}, … , x_{n}, λ_{i},μ_{j})\)&lt;/span&gt;，換成以&lt;span class="math"&gt;\(λ_{i}\)&lt;/span&gt;,&lt;span class="math"&gt;\(μ_{j}\)&lt;/span&gt;表示的&lt;span class="math"&gt;\(Θ(λ_{i},μ_{j})\)&lt;/span&gt;。然後，再求&lt;span class="math"&gt;\(Θ(λ_{i},μ_{j})\)&lt;/span&gt;的最大值，就可以了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;經過Dual轉換後，我們將原本在&lt;span class="math"&gt;\(x_{1},x_{2}, … , x_{n}\)&lt;/span&gt;的問題轉換到&lt;span class="math"&gt;\(λ_{i},μ_{j}\)&lt;/span&gt;的空間上。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;這個轉換我們可以使用下面的圖來解釋，&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lagrangian Dual Geometric Interpretation" src="/media/MachineLearningTechniques/MachineLearningTechniques.005.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;我們先不管&lt;span class="math"&gt;\(g(x)\)&lt;/span&gt;的部分只看&lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(h(x)\)&lt;/span&gt;的部分，假設所有的Data &lt;span class="math"&gt;\(x\)&lt;/span&gt;映射到&lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(h(x)\)&lt;/span&gt;會產生一塊區域&lt;span class="math"&gt;\(G\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;在Primal Problem中我們可以很容易的找出&lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) ≤ 0\)&lt;/span&gt;的限制之下&lt;span class="math"&gt;\(f(x_{1},x_{2}, … , x_{n})\)&lt;/span&gt; 的最小值，見上圖左側。&lt;/p&gt;
&lt;p&gt;見上圖中間，Dual Problem採取另外一個方法，它先去找
&lt;/p&gt;
&lt;div class="math"&gt;$$
Θ(μ) = given\ μ\ to\ find\ min. L(x,μ),\ where: L(x,μ) = f(x)+μh(x)。
$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(f(x)+μh(x)=α\)&lt;/span&gt;在圖中的平面上是一條直線，而&lt;span class="math"&gt;\(f(x)+μh(x)\)&lt;/span&gt;的值也就是&lt;span class="math"&gt;\(α\)&lt;/span&gt;也正好是它的「截距」，所以在給定&lt;span class="math"&gt;\(μ\)&lt;/span&gt;後要最小化&lt;span class="math"&gt;\(f(x)+μh(x)\)&lt;/span&gt;的方法，就等效於固定直線斜率最小化截距，所以最後這個直線就必須要切於&lt;span class="math"&gt;\(G\)&lt;/span&gt;才能使得截距最小，所以我們得到一條切於&lt;span class="math"&gt;\(G\)&lt;/span&gt;且斜率&lt;span class="math"&gt;\((-μ)\)&lt;/span&gt;的直線， 因此我們就順利的得到&lt;span class="math"&gt;\(Θ(μ)\)&lt;/span&gt;的關係式了，接下來我要找出&lt;span class="math"&gt;\(Θ(μ)\)&lt;/span&gt;的最大值，所以就必須往上推，這個時候你就發現答案和前面Primal Problem答案一模一樣，這種最佳化答案相同的情況稱為「Strong Duality」，而最佳化答案不相同的情況就叫做「Weak Duality」，見上圖右側，在這種&lt;span class="math"&gt;\(G\)&lt;/span&gt;的形狀下，就會產生最佳化答案不相同的情況。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="hard-margin-svm-dual-kernel-function-kernel-hard-margin-svm"&gt;[進階] Hard-Margin SVM Dual + Kernel Function = Kernel Hard-Margin SVM&lt;/h3&gt;
&lt;p&gt;那我們現在可以正式的把Lagrangian Dual的東西放到Hard-Margin SVM上面。&lt;/p&gt;
&lt;p&gt;回想一下Hard-Margin SVM的問題是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在&lt;span class="math"&gt;\(y_{n}×(W^{T}X_{n}+b) ≥ 1 ,\ n=1\cdots N\)&lt;/span&gt;的條件下，求&lt;span class="math"&gt;\((W^{T}W/2)\)&lt;/span&gt; 最小的情形。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;那如果加上非線性轉換，從&lt;span class="math"&gt;\(X\)&lt;/span&gt;空間轉到&lt;span class="math"&gt;\(Z\)&lt;/span&gt;空間，則問題變成&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在&lt;span class="math"&gt;\(y_{n}×(W^{T}Z_{n}+b) ≥ 1 ,\ n=1\cdots N\)&lt;/span&gt;的條件下，求&lt;span class="math"&gt;\((W^{T}W/2)\)&lt;/span&gt; 最小的情形。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以我們可以使用Lagrangian Multiplier來解決問題，依以下步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;假設Lagrange Function：   &lt;span class="math"&gt;\(L(W,b,α) = (W^{T}W/2) +  𝚺_{n} α_{n} × [1-y_{n}×(W^{T}Z_{n}+b)]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;考慮Primal Feasibility、Complementary Slackness、Dual Feasibility的限制&lt;/li&gt;
&lt;li&gt;Primal Feasibility Condition：&lt;span class="math"&gt;\(1-y_{n}×(W^{T}Z_{n}+b) ≤ 0\)&lt;/span&gt; [式1-1]&lt;/li&gt;
&lt;li&gt;Complementary Slackness Condition：&lt;span class="math"&gt;\(α_{n}  × [1-y_{n}×(W^{T}Z_{n}+b)] = 0\)&lt;/span&gt; [式1-2]&lt;/li&gt;
&lt;li&gt;Dual Feasibility Condition：&lt;span class="math"&gt;\(α_{n}  ≥ 0\)&lt;/span&gt; [式1-3]&lt;/li&gt;
&lt;li&gt;先求出&lt;span class="math"&gt;\(Θ(α) = given α to find min. L(W,b,α)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉b = - 𝚺_{n} α_{n}y_{n} = 0\)&lt;/span&gt; [式1-4]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉W_{n} =  |W|- 𝚺_{n} α_{n}y_{n}Z_{n} = 0\)&lt;/span&gt;，&lt;span class="math"&gt;\(y_{n}Z_{n}\)&lt;/span&gt;應該和&lt;span class="math"&gt;\(W\)&lt;/span&gt;同向，所以
     &lt;span class="math"&gt;\(W = 𝚺_{n} α_{n}y_{n}Z_{n}\)&lt;/span&gt; [式1-5]&lt;/li&gt;
&lt;li&gt;因此&lt;span class="math"&gt;\(L(W,b,α)\)&lt;/span&gt;只要滿足[式1-4]和[式1-5]就代表是極小值了&lt;/li&gt;
&lt;li&gt;所以[式1-4]和[式1-5]代入得&lt;span class="math"&gt;\(Θ(α,β) = (-1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}Z_{n}Z_{m}+𝚺_{n} α_{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;求&lt;span class="math"&gt;\(Θ(α)\)&lt;/span&gt;極大值&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(max.[Θ(α)]＝min.[-Θ(α)]=min.[(1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}Z_{n}Z_{m}-𝚺_{n} α_{n}]\)&lt;/span&gt; —[式1-6]&lt;/li&gt;
&lt;li&gt;綜合上述[式1-3]、[式1-4]、[式1-6]並改寫成Kernel的形式得，&lt;span class="math"&gt;\(min. [(1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}K(X_{n},X_{m})-𝚺_{n} α_{n}], s.t. α_{n} ≥ 0 ; \ 𝚺_{n} α_{n}y_{n} = 0\)&lt;/span&gt;，使用QP Solver可以求出 &lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;可以用&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;來求&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(b\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;涵義：觀察[式1-2]可得 (1) &lt;span class="math"&gt;\(α_{n} = 0\)&lt;/span&gt; 為Non-Support Vector； (2) &lt;span class="math"&gt;\(α_{n} &amp;gt; 0\)&lt;/span&gt; 代表&lt;span class="math"&gt;\(y_{n}×(W^{T}Z_{n}+b)=1\)&lt;/span&gt;，為Support Vector。&lt;/li&gt;
&lt;li&gt;由[式1-5]得，&lt;span class="math"&gt;\(W = 𝚺_{n} α_{n}y_{n}Z_{n}\)&lt;/span&gt;，從式子中你會發現對W有貢獻的只有Support Vector &lt;span class="math"&gt;\((α_{n}&amp;gt;0)\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;假設在某個Support Vector(&lt;span class="math"&gt;\(α_{n}&amp;gt;0\)&lt;/span&gt;)上，由[式1-2]可推得，&lt;span class="math"&gt;\(b=y_{sv}-𝚺_{n} α_{n}y_{n}K(X_{n},X_{sv})\)&lt;/span&gt;  (at Support Vector)。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="soft-margin-svm-dual-kernel-function-kernel-soft-margin-svm"&gt;[進階] Soft-Margin SVM Dual + Kernel Function = Kernel Soft-Margin SVM&lt;/h3&gt;
&lt;p&gt;考慮Soft-Margin SVM和特徵轉換：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在&lt;span class="math"&gt;\(y_{n}×(W^{T}Z_{n}+b) ≥ 1-ξ_{n}\)&lt;/span&gt;且&lt;span class="math"&gt;\(ξ_{n} ≥ 0,\ n=1\cdots N\)&lt;/span&gt;的條件下，求&lt;span class="math"&gt;\((W^{T}W/2) + C 𝚺_{n} ξ_{n}\)&lt;/span&gt;最小的情形。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以我們可以使用Lagrangian Dual Problem來解決問題，依以下步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;假設Lagrange Function：   &lt;span class="math"&gt;\(L(W,b,ξ,α,β) = (W^{T}W/2) + C 𝚺_{n} ξ_{n} +  𝚺_{n} α_{n} × [1-ξ_{n}-y_{n}×(W^{T}Z_{n}+b)] + 𝚺_{n} β_{n} × [-ξ_{n}]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;考慮Primal Feasibility、Complementary Slackness、Dual Feasibility的限制&lt;/li&gt;
&lt;li&gt;Primal Feasibility Condition：&lt;span class="math"&gt;\(1-ξ_{n}-y_{n}×(W^{T}Z_{n}+b) ≤ 0\)&lt;/span&gt; [式2-1]；&lt;span class="math"&gt;\(-ξ_{n} ≤ 0\)&lt;/span&gt; [式2-2]&lt;/li&gt;
&lt;li&gt;Complementary Slackness Condition：&lt;span class="math"&gt;\(α_{n}  × [1-ξ_{n}-y_{n}×(W^{T}Z_{n}+b)] = 0\)&lt;/span&gt; [式2-3]；&lt;span class="math"&gt;\(β_{n} × [-ξ_{n}] = 0\)&lt;/span&gt; [式2-4]&lt;/li&gt;
&lt;li&gt;Dual Feasibility Condition：&lt;span class="math"&gt;\(α_{n}  ≥ 0\)&lt;/span&gt; [式2-5]；&lt;span class="math"&gt;\(β_{n}  ≥ 0\)&lt;/span&gt; [式2-6]&lt;/li&gt;
&lt;li&gt;先求出&lt;span class="math"&gt;\(Θ(α,β) = given\ α,β\ to\ find\ min. L(W,b,ξ,α,β)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉b = - 𝚺_{n} α_{n}y_{n} = 0\)&lt;/span&gt; [式2-7]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉W_{n} =  |W|- 𝚺_{n} α_{n}y_{n}Z_{n} = 0\)&lt;/span&gt;，&lt;span class="math"&gt;\(y_{n}Z_{n}\)&lt;/span&gt;應該和&lt;span class="math"&gt;\(W\)&lt;/span&gt;同向，所以
     &lt;span class="math"&gt;\(W = 𝚺_{n} α_{n}y_{n}Z_{n}\)&lt;/span&gt; [式2-8]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉ξ_{n} = C - α_{n} - β_{n} = 0\)&lt;/span&gt; [式2-9]&lt;/li&gt;
&lt;li&gt;因此&lt;span class="math"&gt;\(L(W,b,ξ,α,β)\)&lt;/span&gt;只要滿足[式2-7]、[式2-8]和[式2-9]就代表是極小值了&lt;/li&gt;
&lt;li&gt;所以[式2-7]、[式2-8]和[式2-9]代入得&lt;span class="math"&gt;\(Θ(α,β) = (-1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}Z_{n}Z_{m}+𝚺_{n} α_{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;求&lt;span class="math"&gt;\(Θ(α,β)\)&lt;/span&gt;極大值&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(max.[Θ(α,β)]＝min.[-Θ(α,β)]=min.[(1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}Z_{n}Z_{m}-𝚺_{n} α_{n}]\)&lt;/span&gt; —[式2-10]&lt;/li&gt;
&lt;li&gt;綜合上述[式2-5]、[式2-6]、[式2-9]、[式2-10]並改寫成Kernel的形式得，&lt;span class="math"&gt;\(min. [(1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}K(X_{n},X_{m})-𝚺_{n} α_{n}],\ s.t. 0 ≤ α_{n} ≤ C;\  𝚺_{n} α_{n}y_{n} = 0\)&lt;/span&gt;，使用QP Solver可以求出 &lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;可以用&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;來求&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(b\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;涵義：觀察[式2-3]和[式2-4]可得 (1) &lt;span class="math"&gt;\(α_{n} = 0\)&lt;/span&gt; 為Non-Support Vector； (2) &lt;span class="math"&gt;\(0 &amp;lt; α_{n} &amp;lt; C\)&lt;/span&gt; 代表&lt;span class="math"&gt;\(y_{n}×(W^{T}Z_{n}+b)=1\)&lt;/span&gt;，為Free Support Vector；(3) &lt;span class="math"&gt;\(α_{n} = C\)&lt;/span&gt; 代表&lt;span class="math"&gt;\(y_{n}×(W^{T}Z_{n}+b)=1-ξ_{n}\)&lt;/span&gt;，為Bounded Support Vector。&lt;/li&gt;
&lt;li&gt;由[式2-8]得，&lt;span class="math"&gt;\(W = 𝚺_{n} α_{n}y_{n}Z_{n}\)&lt;/span&gt;，從式子中你會發現對W有貢獻的只有Support Vector (&lt;span class="math"&gt;\(α_{n}&amp;gt;0\)&lt;/span&gt;)。&lt;/li&gt;
&lt;li&gt;假設在某個Support Vector(&lt;span class="math"&gt;\(α_{n}&amp;gt;0\)&lt;/span&gt;且&lt;span class="math"&gt;\(β_{n}&amp;gt;0\)&lt;/span&gt;)上，由[式2-3]和[式2-4]可推得，&lt;span class="math"&gt;\(b=y_{sv}-𝚺_{n} α_{n}y_{n}K(X_{n},X_{sv})\)&lt;/span&gt;  (at Support Vector)。&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (1)：我們將會學到什麼? 先見林再來見樹</title><link href="https://ycc.idv.tw/ml-course-techniques_1.html" rel="alternate"></link><published>2017-01-12T12:00:00+08:00</published><updated>2017-01-12T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-01-12:/ml-course-techniques_1.html</id><summary type="html">&lt;p&gt;有什麼特徵可以使用？ / Embedding Numerous Features ：Kernel Models / Combining Predictive Features：Aggregation Models / Distilling Implicit Features：Extraction Models&lt;/p&gt;</summary><content type="html">&lt;p&gt;在之前四篇文章中，我總結了台大教授林軒田在Coursera上的《機器學習基石》16堂課程，我覺得這是機器學習初學很重要的基礎課程，接下來我要接續更進階的課程。&lt;/p&gt;
&lt;p&gt;林軒田教授的機器學習是兩學期的課，第一學期是《機器學習基石》，第二學期就是接下來這個系列要講的《機器學習技法》，這兩堂課程是有相當大的銜接關係的，所以如果想看這系列的文章，請先看&lt;a href="/tag/ji-qi-xue-xi-ji-shi.html"&gt;這四篇《機器學習基石》的介紹&lt;/a&gt;或者&lt;a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations"&gt;直接到Coursera上學習&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;《機器學習技法》課程影片可以到老師的Youtube [ &lt;a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2"&gt;https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&lt;/a&gt; ]上收看，投影片可以到老師的個人網站上下載 [ &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/&lt;/a&gt; ]。&lt;/p&gt;
&lt;p&gt;以前，我曾經和實驗室的英國學長聊英國的教育方法，然後我驚人的發現，他的學校在大一就已經學過量子場論（物理上很難的學科XDD）了，我就很好奇量子場論不是需要很深厚的數學基礎嗎？大一是要怎麼教啊？他告訴我，他們大一就會完整走過物理的各大領域，不過是用非常概念的方式來學習，不牽涉到太困難的數學，但這概念的一系列課程卻是四年大學中相當重要的基礎，讓他在開始學細節前就可以知道這些東西未來會用在哪裡？產生了連結讓學習更有效率。&lt;/p&gt;
&lt;p&gt;所以，《機器學習技法》中會介紹很多厲害的機器學習的方法，但這一篇我不直接進去看每個方法的細節，我想帶大家坐著直升機來先看看這遊樂園中有哪些遊樂設施，先來見林再來見樹，會更容易了解。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="_1"&gt;有什麼特徵可以使用？&lt;/h3&gt;
&lt;p&gt;在之前《機器學習基石》中，我們講到了Features（特徵）的選擇，&lt;strong&gt;Features（特徵）就是我的Model描述Data的方法，也可以說是影響Data的變數&lt;/strong&gt;，那在之前我們講過Features（特徵）的選擇可以是線性的，那也可以使用「特徵轉換」來產生非線性。&lt;/p&gt;
&lt;p&gt;在這系列文章，我們會看到更多種類的Features，可以分為三類：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Embedding Numerous Features（嵌入大量特徵）&lt;/li&gt;
&lt;li&gt;Combining Predictive Features（綜合預測結果的特徵）&lt;/li&gt;
&lt;li&gt;Distilling Implicit Features（抽取隱含特性的特徵）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我已經盡力用我的理解翻譯上面的英文，哈！&lt;/p&gt;
&lt;p&gt;這些不同種類的Features就會造成不同的Models，這些Models分別是&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Embedding Numerous Features ：Kernel Models（Kernel模型）&lt;/li&gt;
&lt;li&gt;Combining Predictive Features：Aggregation Models（集合模型）&lt;/li&gt;
&lt;li&gt;Distilling Implicit Features：Extraction Models（萃取模型）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;讓我們依序來看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="embedding-numerous-features-kernel-models"&gt;Embedding Numerous Features ：Kernel Models&lt;/h3&gt;
&lt;p&gt;還記得《機器學習基石》中，我們講了哪些Model嗎？我們一開始講了二元分類問題，然後提出了Perceptron Learning Algorithm (PLA)來解決這個問題（&lt;a href="/ml-course-foundations_1.html"&gt;詳見《機器學習基石》第一篇&lt;/a&gt;），如果數據是線性可分的話，我們就可以使用PLA劃分出一條邊界來區分兩種種類。&lt;/p&gt;
&lt;p&gt;接下來提到我們可以使用Regression的方法來做二元分類問題，其中Logistic Regression考慮了雜訊造成每個Label的出現呈機率分布，給予一個較為寬鬆的區分方法，我們會稱PLA為Hard Classification，而Logistic Regression為Soft Classification。（&lt;a href="/ml-course-foundations_3.html"&gt;詳見《機器學習基石》第三篇&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;最後，我們引入「特徵轉換」將我們原本的線性區分推到非線性區分，讓我的Model有更大的複雜度，也因為如此，我們需要使用Regularization和Validation來避免 Overfitting。（&lt;a href="/ml-course-foundations_4.html"&gt;詳見《機器學習基石》第四篇&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;那如果我想要使用無窮個高次方的非線性Features來當作我的Model，可以做到嗎？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;來看一下之前我們做特徵轉換怎麼做的？其實我們沒有多做什麼功夫，我們只是把高次項先產生出來，然後在把這每一項當作線性模型的Features去處理，我們就用線性模型的方法產生了非線性的效果。&lt;/p&gt;
&lt;p&gt;那如果非線性項目的個數無窮多個，顯然這種方法就做不了了啊！&lt;/p&gt;
&lt;p&gt;不過，數學總是會拯救我們，&lt;strong&gt;我們可以使用Dual Transformation加上Kernel Function的技巧，帶我們走捷徑，直接用解析解讓我們得出答案，繞過要考慮無窮多個Features後再處理的窘境。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第一堂課「Linear Support Vector Machine」中，提出Hard-Margin Support Vector Machine (SVM)的架構，他和PLA非常相近，屬於Hard Classification，不同的是Hard-Margin SVM還會讓這個切分的邊界落在最佳的位置上。&lt;/p&gt;
&lt;p&gt;第二堂課 「Dual Support Vector Machine」中，我們開始使用Dual Transformation，把大部分與Data中Features有關的計算，取代成計算與Data中Labels有關的計算，讓我們朝不需要計算Features邁進一步，但是因為有另外一部分還是需要計算Features，所以一樣的我們還是無法讓Features有無窮多個。&lt;/p&gt;
&lt;p&gt;第三堂課「Kernel Support Vector Machine」中，我們引入Kernel Function來幫助我們，現在真的可以不需去列出所有Features也能算出答案，所以我們就可以讓Features有無窮多項，但也因為Model太過複雜，我們不得不去面對Overfitting的問題。&lt;/p&gt;
&lt;p&gt;第四堂課「Soft-Margin Support Vector Machine」中，提出Soft-Margin SVM，它是一種Soft Classification，讓我們可以允許部分錯誤發生，並且同樣的使用Dual Transformation加上Kernel Function的技巧，來讓我可以使用無窮多項的Features，而且因為Soft-Margin SVM可以允許錯誤，也就是對雜訊有容忍度，因此可以幫助我們抑制Overfitting的發生。&lt;/p&gt;
&lt;p&gt;第五堂課「Kernel Logistic Regression」中，我們將Kernel的方法引入Logistic Regression當中來用不同於Soft-Margin SVM的方式做二元分類。&lt;/p&gt;
&lt;p&gt;第六堂課「Support Vector Regression」中，會介紹如何使用Kernel Model來做各類Regression的問題。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這6堂課，主要做的事是把《機器學習基石》裡面學到的東西，全部引入數學工具讓Model的Features可以擴展到無窮多項，產生更強大的Kernel Model。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="combining-predictive-featuresaggregation-models"&gt;Combining Predictive Features：Aggregation Models&lt;/h3&gt;
&lt;p&gt;那如果今天我有很多支的Model，我有辦法融合他們得到更好的效果嗎？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這就是Aggregation Models的精髓，Aggregation Models藉由類似於投票的方法綜合各個子Models的結果得到效果更好的Model。換個角度看，你可以把整個體系看成一個新的Model，而原本這些子Models當作轉換過後的新Features，所以Aggregation Model裡頭做了「特徵轉換」，這個轉換產生出許多有預測答案能力的Features，稱為Predictive Features，然後再綜合它們。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Aggregation Models可以分成兩大類，第一種的作法比較簡單，先Train出一個一個獨立的Predictive Features，然後在綜合它們，&lt;strong&gt;「集合」的動作是發生在得到Train好的Predictive Feature之後，這叫做「Blending Models」&lt;/strong&gt;；第二種作法則是，&lt;strong&gt;「集合」的動作和Training同步進行，這叫做「Aggregation-Learning Models」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;從「集合」的方法上也可以進一步細分三種類型，有票票等值的&lt;strong&gt;「Uniform Aggregation Type」&lt;/strong&gt;，有給予Predictive Features不同權重的&lt;strong&gt;「Linear Aggregation Type」&lt;/strong&gt;，甚至還可以用條件或任意Model來分配Predictive Features，這叫做&lt;strong&gt;「Non-linear Aggregation Type」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以兩種類型、三種Aggregation Type，交互產生六種Aggregation Models。&lt;/p&gt;
&lt;p&gt;第七堂課「Bootstrip Aggregation」中，一開始介紹Blending Models的三種Aggregation Type，第一種是直接平均所有的Predictive Features，第二種則是藉由每個Predictive Feature的預測能力，使用線性模型去調配它們的權重，第三種則是使用任意模型分配權重。接著又介紹了Aggregation-Learning Models的Uniform Aggregation Type，稱之為Bagging，它的特點在於它可以利用變換Dataset來造出很多個Predictive Features，並接著做Aggregation。&lt;/p&gt;
&lt;p&gt;第八堂課「Adaptive Boosting」中，介紹Aggregation-Learning Models的Linear Aggregation Type，稱之為AdaBoost，它的特點在於它可以使得每個Predictive Features彼此間可以截長補短。&lt;/p&gt;
&lt;p&gt;第九堂課「Decision Tree」中，介紹Aggregation-Learning Models的Non-linear Aggregation Type，稱之為Decision Tree。&lt;/p&gt;
&lt;p&gt;第十堂課「Random Forest」中，使用Bagging來做Decision Tree，這叫做Random Forest。&lt;/p&gt;
&lt;p&gt;第十一堂課「Gradient Boosted Decision Tree」中，會介紹AdaBoost的Regression版本稱為GradientBoost，並且運用AdaBoost和GradientBoost在Decision Tree上面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這5堂課，我們將會介紹Aggregation Models，引入綜合、集合Predictive Feature的概念來使我們造出更好的Model。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="distilling-implicit-featuresextraction-models"&gt;Distilling Implicit Features：Extraction Models&lt;/h3&gt;
&lt;p&gt;那最後這個部分則是介紹現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，在這裡我們通稱Extraction Models。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extraction Models的特色在於它「特徵轉換」的方法，使用一層一層神經元來做非線性的特徵轉換，如果具有多層神經元，那就是做了多次的非線性特徵轉換，這就是「深度學習」，藉由Data機器會自行學習出這每一層的特徵轉換，找出隱含的Features。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第十二堂課「Neural Network」中，介紹Neural Network，並介紹Neural Network的演算法—Back-Propagation（反向傳遞法），在概念上Gradient Descent就是Back-Propagation的源頭，另外介紹避免Overfitting的方法—Early Stopping。&lt;/p&gt;
&lt;p&gt;第十三堂課「Deep Learning」中，開始介紹「深度學習」，考慮多層神經元的Neural Network就叫做Deep Learning，我們會探討如何在Deep Learning中加入Regularization，並介紹一種叫做Auto-encoder的特殊Deep Learning方法。&lt;/p&gt;
&lt;p&gt;第十四堂課「Radial Basis Function Network」中，介紹Radial Basis Function (RBF) Network，並且介紹K-means等非監督分類法。&lt;/p&gt;
&lt;p&gt;第十五堂課「Matrix Factorization」中，我們會探討類別的匹配問題，例如：我想要知道用戶喜歡看什麼電影，而我的Data只有用戶的ID和電影的編號。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這4堂課，我們將會介紹Extraction Model，使用神經元的概念來萃取出Data中的Features。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="_2"&gt;後話&lt;/h3&gt;
&lt;p&gt;最後總結一下《機器學習技法》會講哪些東西？我們會講具有三種不同「特徵轉換」方式的Models。&lt;strong&gt;Kernel Model的「特徵轉換」是將非線性Features擴張到無窮多個；Aggregation Model的「特徵轉換」是產生出有預測能力的Features；Extraction Model的「特徵轉換」是利用神經元的方式來做到萃取出隱含的資訊。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;跟《機器學習基石》不一樣的地方，《機器學習技法》中介紹更厲害的「特徵轉換」來產生更厲害的Model，不過因為會有Overfitting的狀況，所以我們還需要介紹相應的配套措施。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在未來一系列的文章，我會帶大家一一的來看這些內容，不過和之前一樣，我不會以課堂當作單位來講，而是以單元式的方式，而且我主要的目的是去點出概念，並盡可能的不去牽涉太多的數學計算，但是數學計算的部分是很重要的，這會影響到你真正的實作，數學的部份可以去看林軒田老師的影片或投影片，裡頭都有很詳細的介紹。&lt;/p&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習基石 學習筆記 (4)：機器可以怎麼學得更好?</title><link href="https://ycc.idv.tw/ml-course-foundations_4.html" rel="alternate"></link><published>2016-09-18T12:00:00+08:00</published><updated>2016-09-18T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2016-09-18:/ml-course-foundations_4.html</id><summary type="html">&lt;p&gt;特徵轉換 / Overfitting / Regularization / Validation&lt;/p&gt;</summary><content type="html">&lt;h3 id="_1"&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回中，我們已經了解了機器學習基本的操作該怎麼做。而這一篇中，我們來看&lt;strong&gt;機器可以怎麼學得更好?&lt;/strong&gt; 基本上有三招：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），我們來看看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="feature-transformation"&gt;Feature Transformation（特徵轉換）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="/media/MachineLearningFoundations/MachineLearningFoundations.013.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;在上一回當中我們講了很多的線性模型，大家有沒有懷疑說，數據呈現的方式一定可以用線性描述嗎？我的答案是通常線性描述會表現不錯，但不是絕對，&lt;strong&gt;那我們怎麼用非線性的方法來描述我們的數據，這邊提供一個方法叫做「非線性轉換」，或者又稱為「特徵轉換」（還記得變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;又可以稱為特徵Features）&lt;/strong&gt;，聽起來有點困難齁～其實不會啦！&lt;/p&gt;
&lt;p&gt;假設今天你的Data分布是圓圈狀的分布，顯而易見的你很難用一條線去區分他們，那我們應該怎麼做呢？假設今天有一個轉換可以把這個圓圈狀分布的空間轉換到另外一個空間，而且在這個新的空間，我們可以做到線性可分，這樣問題就解決了，我們非常擅長做線性可分啊！怎麼做呢？我們知道這個空間分布是圓圈分布，所以套用以前學過的公式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(x_1, x_2) = sign(-A\times x_1^2-B\times x_2^2+C)
$$&lt;/div&gt;
&lt;p&gt;，如此一來，令 &lt;span class="math"&gt;\(z_1=-x_1^2\)&lt;/span&gt;; &lt;span class="math"&gt;\(z_2=-x_2^2\)&lt;/span&gt;，所以問題就變成一個線性問題
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(z_1, z_2) = sign(A\times z_1+B\times z_2+C)
$$&lt;/div&gt;
&lt;p&gt;
在做這個操作時，我們會用到非線性項，也就是高次項或交叉項，所以會稱這個轉換叫做「非線性轉換」。&lt;strong&gt;藉由人為觀察數據，並給予適當的特徵轉換，找出其中隱藏的非線性項當作新的特徵，又稱為特徵工程（Feature Engineering）。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但如果我們需要去人為定義這個「非線性轉換」，這就很弱啦！我們當然希望機器可以自行從Data中學習到這個轉換，作法是這樣的，我們先把變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;做個變化和擴充，讓它們互相的相乘創造出高次項，再把這些項等價的放到Linear Model裡，所以我們就用了線性的作法來做到Non-linear Model，而因為有權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;在非線性項前面的關係，所以機器會針對Data自行去調配非線性項或線性項的權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;，這效果就等同於機器自行學習到「非線性轉換」。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;機器自己學習特徵轉換的這個概念應該是現今ML最重要的概念之一，最近很夯的深度學習就擁有強大的特徵轉換功能，這些轉換都是機器從Data自行學來的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特徵轉換讓ML變得很強大，但要特別注意，因為我們增加了非線性項，所以等於是增加了模型的複雜度，這麼做的確可以壓低&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;沒有錯，但也可能使得&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;不再成立，也就是Overfitting，所以建議要逐步的增加非線性項，從低次方的項開始加起，避免Overfitting。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id="overfitting"&gt;Overfitting&lt;/h3&gt;
&lt;p&gt;Overfitting是一個大怪獸，在學習怎麼對付牠之前，我們先來好好的了解牠！&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overfitting" src="/media/MachineLearningFoundations/MachineLearningFoundations.000.03.png" /&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上面這張圖用很簡單的方法說明了Overfitting是怎麼一回事，假設藍色的線是Target，也就是我們抽樣的母群體，因為雜訊的關係，抽樣出來的點可能會稍微偏離Target，而如果這個時候我們用二次式來描述這些抽樣出來的Data（上圖中的左側）會發現&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;不能壓到0，所以這個時候可能有人想說加進去更高次項來試試看（上圖中的右側），此時會發現&lt;span class="math"&gt;\(E_{in}=0\)&lt;/span&gt;，所有數據都可以被完整描述了，但是你會發現Fit的曲線已經完全偏離了Target，反而是使用低次項還描述比較好，低次項描述的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和 &lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;(Target Function) 比較接近，所以結論是&lt;strong&gt;如果我們把「隨機雜訊」（Stochastic Noise）Fit進去Model裡面就會因此產生Overfitting，要避免這種情況發生，就要小心使用高次項&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overfitting2" src="/media/MachineLearningFoundations/MachineLearningFoundations.000.04.png" /&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;但可別以為沒有「隨機雜訊」鬧場就不會出現Overfitting，上圖假設一個沒有「隨機雜訊」的情形，但是這次Target Function的複雜度很高（上圖右側），當我們從中採樣一些Data來進行Fitting，如上圖左側，我們分別使用2次和10次來做Fitting，這個時候你會發現雖然2次和10次都和Target曲線差很遠，但是小次方的還是Fit的比較好一點，造成Overfitting的原因是因為當Target很複雜的情況下，如果採樣的數據不大，根本無法反應Target本身，所以就算使用了和Target一樣複雜的Model，也只是在瞎猜而已。&lt;strong&gt;這種因為Target本身的複雜度所帶來的雜訊，我們稱為「決定性雜訊」(Deterministic Noise)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Noise" src="/media/MachineLearningFoundations/MachineLearningFoundations.000.05.png" /&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們來看一下「隨機雜訊」（Stochastic Noise）和「決定性雜訊」（Deterministic Noise）怎麼造成Overfitting的，上圖中的兩張漸層圖表示的是Overfitting的程度，越接近紅色代表Overfitting越嚴重；反之，越接近藍色則Overfitting越輕微。左邊的漸層圖是考慮「隨機誤差」的影響，右邊的漸層圖則是考慮「決定性雜訊」的影響。從這兩張圖我們可以觀察出下面四點特性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data數量&lt;span class="math"&gt;\(N\)&lt;/span&gt;越少，越容易Overfitting&lt;/li&gt;
&lt;li&gt;「隨機雜訊」越多，越容易Overfitting&lt;/li&gt;
&lt;li&gt;「決定性雜訊」越多，越容易Overfitting&lt;/li&gt;
&lt;li&gt;Model本身越複雜，越容易Overfitting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;那有什麼方法可以防止Overfitting嗎？有的，包括之前講過的一些方法，我們來看一下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;從簡單的模型開始做起，從低次模型開始做起，在慢慢加入高次項&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提升資料的正確性：Data Cleaning/Pruning（資料清洗）將錯誤的Data修正或刪除&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Hinting（製造資料），使用合理的方法擴增原有的資料，例如：在圖形辨識問題中，可以用平移和旋轉來擴增出更多Data&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regularization（正規化）：限制權重W的大小以控制高次的影響。&lt;/strong&gt;（接下來會詳述...）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Validation（驗證）：將部分Data保留不進去Fitting，然後用這個Validation Data來檢驗Overfitting的程度。&lt;/strong&gt;（接下來會詳述...）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id="regularization"&gt;Regularization（正規化）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="regularation" src="/media/MachineLearningFoundations/MachineLearningFoundations.014.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;剛剛我們提到了Overfitting所造成的影響很大一部分是因為Model複雜度所造成的，但是為了可以把&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;給壓下去，我們又的確需要去增加高次項，所以依照建議需要從低次項開始慢慢的加，這樣感覺很麻煩啊！&lt;strong&gt;有沒有辦法讓機器自己去限制高次項的出現呢？有的，這就是Regularization（正規化）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;還記得剛剛在講「特徵轉換」時，有提到一點，ML有辦法自行學習「特徵轉換」的關鍵是因為高次項前面有一個可調控的權重，而機器會針對Data來調整權重大小，那其實就是等價於機器自己學習到了「特徵轉換」，同理可知，&lt;strong&gt;我們只要限制權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;的大小就等同於限制了機器無所忌憚的使用高次項&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;經數學證明，&lt;strong&gt;限制權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;的大小可以等價於在&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;上面加上「&lt;span class="math"&gt;\(W\)&lt;/span&gt;大小的平方」乘上定值&lt;span class="math"&gt;\(λ\)&lt;/span&gt;，&lt;span class="math"&gt;\(λ\)&lt;/span&gt;越大代表&lt;span class="math"&gt;\(W\)&lt;/span&gt;大小限制越緊；&lt;span class="math"&gt;\(λ\)&lt;/span&gt;越小代表&lt;span class="math"&gt;\(W\)&lt;/span&gt;大小限制越鬆&lt;/strong&gt;，這也非常容易想像，訓練Model的方法是去降低&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;，但是如果使用了大的&lt;span class="math"&gt;\(W\)&lt;/span&gt;，就會使得&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;增大，自然而然在訓練的過程中，機器會去尋找小一點的&lt;span class="math"&gt;\(W\)&lt;/span&gt;，也就等同於限制了&lt;span class="math"&gt;\(W\)&lt;/span&gt;的大小。&lt;/p&gt;
&lt;p&gt;見上圖左側，我們修改了Gradient Descent讓它受到Regularization的限制。&lt;/p&gt;
&lt;p&gt;而上圖左側下方，顯示了在&lt;span class="math"&gt;\(λ\)&lt;/span&gt;增大的同時，限制&lt;span class="math"&gt;\(W\)&lt;/span&gt;的大小會越來越緊，所以Fitting的結果從原本的Overfitting變成Underfitting。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Underfitting所代表的是Model本身的複雜度不夠，不足以使得&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;降的夠小，如果你經過Validation（待會會講）後發現沒有Overfitting的現象，但是你的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;始終壓不下來，那就有可能是Underfitting，那你該考慮的是增加Model複雜度或者放寬Regularization，反而不是Regularization。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regularizer的選擇常見的有兩種L2和L1，L2使用「&lt;span class="math"&gt;\(W\)&lt;/span&gt;大小的平方」，L1則使用「&lt;span class="math"&gt;\(W\)&lt;/span&gt;大小的絕對值」。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;當Linear Regression使用Regularization限制，統計上有一個名稱稱為Ridge Regression，你可以使用Gradient Descent來做，又或者使用解析解的方法。&lt;/p&gt;
&lt;p&gt;最後提一個Regularization的細節，你會發現因為高次項是彼此兩兩相乘的結果，所以項目的個數會隨著次方增加而增加，這麼一來在做Regularization時可能會過度懲罰高次項，因此，我們可以將Feature轉換成Legendre Polynomials來避免這個問題。&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id="validation"&gt;Validation（驗證）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="validation" src="/media/MachineLearningFoundations/MachineLearningFoundations.015.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;講了這麼多Overfitting，但到底要怎麼去量化Overfitting呢？Overfitting就是&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;不成立，但是&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;我們不會知道啊！因為我們不會知道Target Function是什麼，那該怎麼得到量化Overfitting的值呢？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有一個方法叫做Validation可以拿來量化Overfitting的值，這個方法是先將採樣的數據做分離，一部分將會拿來做Model Fitting（Model Training），另外一部分保留起來評估訓練完畢的Model，因為保留的這一部分源自於母群體，而且又沒有被Model給看過，所以它可以很客觀的反應出&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;的大小。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我們的Model和Algorithm從以前講到現在已經是越來越複雜了，來複習一下Model和Algorithm受哪些參數影響，Algorithm的選擇就有很多了，包括：PLA、Linear Regression、Logistic Regression；Learning Rate &lt;span class="math"&gt;\(η\)&lt;/span&gt;也需要去選擇大小決定學習速率；Feature Transformation中Feature的決定和次方大小的決定；Regularization也有L2、L1 Regularizer的選擇；還有Regularization的&lt;span class="math"&gt;\(λ\)&lt;/span&gt;值也必須被決定。&lt;/p&gt;
&lt;p&gt;這些條件彼此交互搭配會產生很多組的Model，那該如何挑選Model呢？我們就可以使用Validation來當作一個依據來選擇Model，選擇出&lt;span class="math"&gt;\(E_{val}\)&lt;/span&gt;最小的Model，如上圖所示。&lt;/p&gt;
&lt;p&gt;另外實作上有一些方法：Leave-One-Out Cross Validation和V-Fold Cross Validation，他們的精髓就是保留&lt;span class="math"&gt;\(k\)&lt;/span&gt;筆Data當作未來Validation用，另外一些拿下去Train Model，然後再用這k筆去評估並得到&lt;span class="math"&gt;\(E_{val,1}\)&lt;/span&gt;，還沒結束，為了讓&lt;span class="math"&gt;\(E_{val}\)&lt;/span&gt;盡可能的正確，所以我們會在把Data作一個迴轉，這次使用另外一組k組Data來Validation，其餘的再拿去Train Model，然後在評估出，&lt;span class="math"&gt;\(E_{val,2}\)&lt;/span&gt; … 以此類推，當轉完一輪之後，在把這些&lt;span class="math"&gt;\(E_{val,1}\)&lt;/span&gt;, &lt;span class="math"&gt;\(E_{val,2}\)&lt;/span&gt;, ...做平均得到一個較為精確&lt;span class="math"&gt;\(E_{val}\)&lt;/span&gt;。那Leave-One-Out Cross Validation顧名思義就是&lt;span class="math"&gt;\(k=1\)&lt;/span&gt;，但這樣做要付出的代價就是計算量太大了，所以V-Fold Cross Validation則使用&lt;span class="math"&gt;\(k=V\)&lt;/span&gt;來做。實務上，我常常做Validation時根本不會去Cross它們，我大都只是保留一部分的Data來驗證而已，給大家參考。&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id="_2"&gt;總結&lt;/h3&gt;
&lt;p&gt;來到了這四篇有關於林軒田教授機器學習基石學習筆記的尾聲了，讓我們重溫看看我們學會了什麼？&lt;/p&gt;
&lt;p&gt;一開始我帶大家初探ML的基本架構，建立Model、使用Data訓練、最後達到描述Target Function的目的，也帶大家認識各種機器學習的類型。&lt;/p&gt;
&lt;p&gt;接下來，我們用理論告訴大家，ML是不是真的可以做到，那在什麼時候可以做到？要符合哪些條件？我們知道要有好的Model，VC Dimension越小越好，也就是可調控的參數越少越好，才會使得&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;成立；要有足夠的Data；要有好的Learning Algorithm能把&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;壓低，這三種條件成立後，如此一來Model在描述訓練數據很好的同時也可以很好的去預測母群體，但我們發現&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;壓低和可調控的參數越少越好兩者是Trade-off，所以我們必須取適當的VC Dimension。&lt;/p&gt;
&lt;p&gt;再接下來我們開始看實際上ML該怎麼做，引入相當重要的Learning Algorithm，也就是Gradient Descent，並且說明了Linear Regression和Logistic Regression，而且還可以使用這兩種Regression來做分類問題。&lt;/p&gt;
&lt;p&gt;那最後就真正亮出ML的三大絕招啦：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），Feature Transformation使得Model更為強大，所以&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;更能夠壓低，但是為了避免Overfitting我們必須去限制它，Regularization可以限制高次項的貢獻，另外，Validation可以量化Overfitting的程度，有了這個我們就可以去選出體質健康而且&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;又小的Model。&lt;/p&gt;
&lt;p&gt;機器學習基石的這些概念都很重要，往後如果你開始學習其他的ML技巧，例如：深度學習，這些知識都是你強大的基礎，所以多看幾次吧！&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習基石"></category></entry><entry><title>機器學習基石 學習筆記 (3)：機器可以怎麼樣學習?</title><link href="https://ycc.idv.tw/ml-course-foundations_3.html" rel="alternate"></link><published>2016-08-07T12:00:00+08:00</published><updated>2016-08-07T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2016-08-07:/ml-course-foundations_3.html</id><summary type="html">&lt;p&gt;Gradient Descent / Linear Regression / Logistic Regression / 使用迴歸法做二元分類問題&lt;/p&gt;</summary><content type="html">&lt;h3 id="_1"&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回中，我們已經了解了機器學習在理論上有怎樣的條件才可以達成，所以接下來我們就可以正式的來看有哪一些機器學習的方法。&lt;/p&gt;
&lt;p&gt;在這一篇中，我會帶大家初探：&lt;strong&gt;機器可以怎麼樣學習?&lt;/strong&gt; 內容包括：Gradient Descent、Linear Regression、Logistic Regression、使用迴歸法做二元分類問題等等。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="gradient-descent"&gt;Gradient Descent（梯度下降）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="/media/MachineLearningFoundations/MachineLearningFoundations.009.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;還記得上一回我們歸納出了一套ML的流程，複習一下&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;準備好足夠的數據&lt;/li&gt;
&lt;li&gt;把Model建立好，&lt;span class="math"&gt;\(d_{VC}\)&lt;/span&gt;必須要是有限的，而且大小要適中&lt;/li&gt;
&lt;li&gt;定義好評估&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的Error Measurement&lt;/li&gt;
&lt;li&gt;使用演算法找出最佳參數把&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;降低&lt;/li&gt;
&lt;li&gt;最後評估一下是否有Overfitting的狀況，確保&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;請容許我先不管Model這部份該怎麼建立，我們先來看如何找到最佳參數這部份，&lt;strong&gt;假設今天我知道&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的評估方法，我該如何找到最佳的參數來使得&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;更小？有一套普遍的方法叫做Gradient Descent&lt;/strong&gt;，很強大，甚至連現今流行的「深度學習」找最佳解的機制也是從Gradient Descent衍生出來的。&lt;/p&gt;
&lt;p&gt;想像一下你是一位登山客，你在爬一座由&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;所決定的高山，你的目標是去這座山最低的山谷，也就是&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小的地方，因為村莊正在那裡，但是很不幸的你沒有地圖，這個時候有什麼方法可以知道低谷在哪裡呢？最簡單的答案就是一直下坡就對啦！反正我知道村莊在山谷裡，那我就一路下山應該就可以找到村莊了，這就是Gradient Descent的精髓。&lt;/p&gt;
&lt;p&gt;在數學上有一個衡量函數變化的東西，這就是Gradient（梯度），Gradient是一個向量，它的「方向」指向函數值增加量最大的方向，而它的「大小」反應這個變化有多大，其實就是一次微分啦！只不過Gradient推廣到高維度而已。所以我們和這個登山客做一樣的事情，我們朝著下降最多的方向前進，這就是Gradient Descent（梯度下降法），我剛剛說了，梯度是指向函數值增加量最大的方向，那顯然我們往反方向走就可以達到最大下降，所以如果我們有一個Error函數&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;，它的Gradient就是&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;，那我們的下降方向就是&lt;span class="math"&gt;\(- \nabla E_{in}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;來看一下上圖中Gradient Descent的流程，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定義出Error函數&lt;/li&gt;
&lt;li&gt;Error函數讓我們可以去評估&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;算出它的梯度&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;朝著&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;的反方向更新參數W，而每次只跨出&lt;span class="math"&gt;\(η\)&lt;/span&gt;大小的一步&lt;/li&gt;
&lt;li&gt;反覆的計算新參數&lt;span class="math"&gt;\(W\)&lt;/span&gt;的梯度，並一再的更新參數&lt;span class="math"&gt;\(W\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;這邊要特別注意，流程中的第四項中，有提到&lt;span class="math"&gt;\(η\)&lt;/span&gt;，&lt;strong&gt;&lt;span class="math"&gt;\(η\)&lt;/span&gt;稱為Learning Rate，它影響的是更新步伐的大小&lt;/strong&gt;，&lt;span class="math"&gt;\(η\)&lt;/span&gt;的選擇要適當，如果&lt;span class="math"&gt;\(η\)&lt;/span&gt;太小的時候，我們可能要花很多時間才可以走到低點，但如果&lt;span class="math"&gt;\(η\)&lt;/span&gt;太大的話，又可能導致我們在兩個山腰間跳來跳去，甚至越更新越往高處跑，&lt;strong&gt;所以選擇適當的&lt;span class="math"&gt;\(η\)&lt;/span&gt;相當的重要，所以下次如果你發現&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;一直降不下來甚至在增大，試著將&lt;span class="math"&gt;\(η\)&lt;/span&gt;減小看看&lt;/strong&gt;。另外&lt;span class="math"&gt;\(η\)&lt;/span&gt;也可以不是定值，我們可以直接設&lt;span class="math"&gt;\(η＝|\nabla E_{in}|\)&lt;/span&gt;，這麼一來遇到陡坡的時候它就會跨大一點的步伐，遇到緩坡的時候就會跨小步一點，隨狀況調整&lt;span class="math"&gt;\(η\)&lt;/span&gt;的值。&lt;/p&gt;
&lt;p&gt;Gradient Descent (GD, 梯度下降) 有兩個變形，分別為Stochastic Gradient Descent (SGD, 隨機梯度下降) 和 Batch Gradient Descent (BGD, 批次梯度下降)，這差別只在於評估&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;的時候所考慮的Data數量，正常來說必須要考慮所有的Data，我們才會得到真正的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;，才有辦法算出正確的&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;，但這樣所要付出的代價就是較大的計算量。&lt;/p&gt;
&lt;p&gt;所以&lt;strong&gt;Stochastic Gradient Descent的作法是一次只拿一筆Data來求&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;，並且更新參數&lt;span class="math"&gt;\(W\)&lt;/span&gt;&lt;/strong&gt;，這樣的更新方法顯然會比較不穩定，但我們假設，經過好幾輪的更新後，已經完整看過整個數據了，所以平均來說效果和一般的Gradient Descent一樣。&lt;/p&gt;
&lt;p&gt;另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，&lt;strong&gt;Batch Gradient Descent一次評估k組數據，並更新參數W&lt;/strong&gt;，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的計算方法你可以想像成在做矩陣計算，矩陣元素在計算的時候往往是可以拆開計算的，此時GPU利用它強大的平行化運算將這些元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，&lt;strong&gt;所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Batch Gradient Descent，是既有效率又穩定的作法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Gradient Descent求最佳解其實不是完美的，還記得我們的目標嗎？我們希望可以走到最低點的山谷裡，所以我們採取的策略是不斷的下降，這個時候如果遇到兩種情形就會導致還沒到山谷就已經動彈不得，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;小山谷，數學上稱為&lt;strong&gt;Local Minimum&lt;/strong&gt;，雖然在那點看起來，那邊的確是相對的低點，所以&lt;span class="math"&gt;\(\nabla E_{in}=0\)&lt;/span&gt;，但卻不是整個&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的最低點，但也因為&lt;span class="math"&gt;\(\nabla E_{in}=0\)&lt;/span&gt;的關係，更新就不會再進行。&lt;/li&gt;
&lt;li&gt;平原，數學上稱為&lt;strong&gt;Saddle Point（鞍點）&lt;/strong&gt;，在一片很平的區域，&lt;span class="math"&gt;\(\nabla E_{in}=0\)&lt;/span&gt;，所以就停止不動了。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;針對這些問題有一些改良後的演算法，在這裡不詳述，請參考&lt;a href="http://ruder.io/optimizing-gradient-descent/"&gt;S. Ruder的整理&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;好！我們已經了解了怎麼使用Gradient Descent去找到&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小的最佳參數，那我們可以回頭看Model有哪一些？Error Measure該怎麼定？&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="linear-regression"&gt;Linear Regression&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="/media/MachineLearningFoundations/MachineLearningFoundations.010.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;先從最簡單的看起，那就是線性迴歸（Linear Regression），假設今天我要用三種變數&lt;span class="math"&gt;\((x_1, x_2, x_3)\)&lt;/span&gt;來建立一個簡單的線性模型，那就是&lt;/p&gt;
&lt;div class="math"&gt;$$
w_0+w_1 x_1+w_2 x_2+w_3 x_3
$$&lt;/div&gt;
&lt;p&gt;這個又稱為Score，標為&lt;span class="math"&gt;\(s\)&lt;/span&gt;，為了方便起見，我們會額外增加&lt;span class="math"&gt;\(x_0=1\)&lt;/span&gt;的參數，這麼一來Score就可以寫成矩陣形式&lt;/p&gt;
&lt;div class="math"&gt;$$
s = w_0 x_0+w_1 x_1+w_2 x_2+w_3 x_3=W^T x
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
where: W = [w_0, w_1, w_2, w_3], x = [x_0=1, x_1, x_2, x_3]
$$&lt;/div&gt;
&lt;p&gt;
在線性模型中，這個 s 就正好是我們Model預測的值，通常我們會把預測得來的 &lt;span class="math"&gt;\(y\)&lt;/span&gt; 記作&lt;span class="math"&gt;\(\widehat{y}\)&lt;/span&gt; (y hat)，如果今天這個 y 和 ŷ 是實數的話，那這就是一個標準的Linear Regression問題，那如何去衡量預測的好或不好呢？&lt;strong&gt;我們可以使用Squared Error來衡量，&lt;span class="math"&gt;\(err(\widehat{y},y)=(\widehat{y}-y)^2\)&lt;/span&gt;&lt;/strong&gt;，所以 &lt;span class="math"&gt;\(\widehat{y}\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(y\)&lt;/span&gt; 越靠近Error就越小。&lt;/p&gt;
&lt;p&gt;Squared Error的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;平面是一個單純的開口向上的拋物線，所以它的最低點其實是有解析解的，我們可以靠著數學上的&lt;strong&gt;Pseudo-Inverse方法&lt;/strong&gt;把最佳參數給算出來，但是Pseudo-Inverse計算非常龐大，當數據量很大時這個方法是不可行的，而剛剛介紹的Gradient Descent計算複雜度只有&lt;span class="math"&gt;\(O(N)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="logistic-regression"&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="/media/MachineLearningFoundations/MachineLearningFoundations.011.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;在上一回討論二元分類問題時，我們的評估模型都是非黑即白的&lt;/p&gt;
&lt;p&gt;在上一回討論二元分類問題時，我們考慮的狀況是「沒有雜訊」的情形，不過在實際情況下，「雜訊」是一定需要考慮的。在「沒有雜訊」的情形下，一筆Data只會有一個確定的答案要嘛是 &lt;span class="math"&gt;\(- 1\)&lt;/span&gt; 不然就是 &lt;span class="math"&gt;\(+ 1\)&lt;/span&gt;，&lt;strong&gt;如果考慮「雜訊」，一筆Data出現的答案可能呈現機率分布&lt;/strong&gt;，介於 &lt;span class="math"&gt;\(- 1\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(+ 1\)&lt;/span&gt; 之間，舉例可能會產生像下面一樣的情況，
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}(◯|X^{(1)}) = 0.9,\  \mathbb{P}(✕|X^{(1)}) = 0.1
$$&lt;/div&gt;
&lt;p&gt;之前PLA的分類方法是屬於非黑及白的，預測的結果不存在模糊地帶，這種分類法我們稱為Hard Classification，這種分類法並不能描述機率分布，所以我們來考慮另外一種分類法，稱之為Soft Classification。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Soft Classification看待每個答案不是非黑及白的，而是去評估每個答案出現的機會有多大，以此作為分類&lt;/strong&gt;，我們打算使用Regression的連續特性來產生Soft Classification，我們需要引入一個重要的函數—Logistic Function，這個函數可以將所有實數映射到0到1之間，如上圖下方中間的圖示所示，&lt;strong&gt;Logistic Function會將極大的值映射成1，而將極小值映射成0，這個0到1的值剛剛好可以拿來當作機率的大小&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以我們就可以來建立一個有機率概念的模型，這個Model的預測值是一個機率，一樣的先給予輸入變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;和權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;求出Score &lt;span class="math"&gt;\(s\)&lt;/span&gt;，再把 &lt;span class="math"&gt;\(s\)&lt;/span&gt; 放到Logistic Function當中，我們就可以映射出在一個機率空間，我們藉由調整&lt;span class="math"&gt;\(W\)&lt;/span&gt;來改變Model來擬合我們的Data，有了這個新的Model，我們就可以用機率的方式來描述二元分類，&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}(◯|X^{(1)}) = Θ(s)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\mathbb{P}(✕|X^{(1)}) = 1 - Θ(s) = Θ(-s)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
where:\ Θ(s)=1/[1+e^{-s}]
$$&lt;/div&gt;
&lt;p&gt;OK! 決定好Model，我們就可以來定義它的Error Measurement的方式了，這個時候如果使用Squared Error來作為Error Measurement你會發現這種評估方式有一點失焦了，如果採用Squared Error，我們做的事是將機率的值給擬合精準，但我們知道這個機率的產生是來自於雜訊，預測雜訊是沒有意義的，要做的事應該是要在考慮雜訊之下盡可能提升模型會產生取樣資料的可能機率。&lt;/p&gt;
&lt;p&gt;這就是Max Likelihood的概念，&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}(likelihood) = \mathbb{P}(x^{(1)})\mathbb{P}(◯|x^{(1)},H) \times \mathbb{P}(x^{(2)})\mathbb{P}(✕|x^{(2)},H) \times … \times \mathbb{P}(x^{(N)})\mathbb{P}(◯|x^{(N)},H)
$$&lt;/div&gt;
&lt;p&gt;我們的任務就是找一個function set &lt;span class="math"&gt;\(H\)&lt;/span&gt;使得我可以最大化likelihood，
&lt;/p&gt;
&lt;div class="math"&gt;$$
argmax_{({H})} \mathbb{P}(likelihood)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=argmax_{({H})} \sum_{y^{(n)}\ is\ ◯} \{ln[\mathbb{P}(x^{(n)})]+ln[\mathbb{P}(◯|x^{(n)},H)] \}+ \sum_{y^{(n)}\ is\ ✕} \{ln[\mathbb{P}(x^{(n)})]+ln[\mathbb{P}(✕|x^{(n)},H)]\}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=argmax_{({H})} \sum_{y^{(n)}\ is\ ◯} ln[\mathbb{P}(◯|x^{(n)},H)] + \sum_{x^{(n)}\ is\ ✕} ln[\mathbb{P}(✕|y^{(n)},H)]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=argmin_{({H})} \sum_{y^{(n)}\ is\ ◯} -ln[\mathbb{P}(◯|x^{(n)},H)] + \sum_{y^{(n)}\ is\ ✕} -ln[\mathbb{P}(✕|x^{(n)},H)]
$$&lt;/div&gt;
&lt;p&gt;假設 &lt;span class="math"&gt;\(◯ \equiv  (y=+1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(✕ \equiv  (y=0)\)&lt;/span&gt;，則上式可以化簡，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
=argmin_{({H})} \sum_{n} -y^{(n)}ln\mathbb{P}(◯|x^{(n)},H) -(1-y^{(n)})ln[1-\mathbb{P}(◯|x^{(n)},H)]
$$&lt;/div&gt;
&lt;p&gt;其中&lt;span class="math"&gt;\(E_{ce}=-yln\mathbb{P}(◯|x,H)-(1-y)ln[1-\mathbb{P}(◯|x,H)]\)&lt;/span&gt; 就是Cross-Entropy Error。&lt;/p&gt;
&lt;p&gt;而對於logistic regression model而言，&lt;span class="math"&gt;\(\mathbb{P}(◯|x,H)=Θ(s)\)&lt;/span&gt;，代入Cross-Entropy Error得
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{ce,logistic}=-ylnΘ(s)-(1-y)ln(1-Θ(s))=-ylnΘ(s)-(1-y)lnΘ(-s)
$$&lt;/div&gt;
&lt;p&gt;
&lt;strong&gt;我們可以使用Gradient Descent來降低Cross-Entropy，這又稱為Logistic Regression，在這個問題中就沒有簡單的解析解可以直接算，只能使用Gradient Descent來求取近似解。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3 id="_2"&gt;使用迴歸法做二元分類問題&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="/media/MachineLearningFoundations/MachineLearningFoundations.012.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;剛剛介紹了Logistic Regression，其實我們是可以將Logistic Regression運用來做二元分類問題。&lt;/p&gt;
&lt;p&gt;線性模型的標準方法，我們會將變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;做線性組合得到Linear Scoring Function — &lt;span class="math"&gt;\(s\)&lt;/span&gt;，線性組合的係數和Threshold稱為權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;，我們可以調整權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;來改變Model，那針對看待&lt;span class="math"&gt;\(s\)&lt;/span&gt;的不同方式就衍生出不同的方法。那為了可以將Regression問題轉換成二元分類問題，所以通常我們會假設&lt;span class="math"&gt;\((y=+1)\)&lt;/span&gt;為&lt;span class="math"&gt;\(◯\)&lt;/span&gt;，&lt;span class="math"&gt;\((y=-1)\)&lt;/span&gt;為&lt;span class="math"&gt;\(✕\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;先回顧一下之前&lt;a href="/ml-course-foundations_1.html"&gt;PLA的作法&lt;/a&gt;，我們把 &lt;span class="math"&gt;\(s&amp;gt;0\)&lt;/span&gt; 的狀況視為&lt;span class="math"&gt;\(◯\)&lt;/span&gt;，也就是&lt;span class="math"&gt;\((y=+1)\)&lt;/span&gt;；然後把&lt;span class="math"&gt;\(s&amp;lt;0\)&lt;/span&gt; 的狀況視為&lt;span class="math"&gt;\(✕\)&lt;/span&gt;，也就是&lt;span class="math"&gt;\((y=-1)\)&lt;/span&gt;，把這個概念畫成上圖右側的圖，圖中藍色的階梯函數就是PLA的Error Measurement，正是因為它是一個階梯函數，所以我們不能使用Gradient Descent等Regression方法來處理，&lt;strong&gt;因為在階梯的每一點&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;都是0（除了原點外），也就是如此PLA在更新的過程才無法確保趨近於最佳解，而需要使用Pocket PLA來解決這個問題&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那如果我們用Linear Regression來做這件事呢？我們把Squared Error畫在上圖右側小圖的紅線，你會發現它的低點會落在&lt;span class="math"&gt;\(y\times s=1\)&lt;/span&gt;的地方，這應該不是我們要的結果，雖然它一樣可以把錯誤的判斷修正回正確，但是面對過度確定的正確答案，它反而會去修正它往錯誤的方向，很顯然這不是我們想要的。&lt;/p&gt;
&lt;p&gt;最好的方式就是Logistic Regression了，我們將&lt;span class="math"&gt;\(s\)&lt;/span&gt;做Logistic Function的轉換，轉換成機率，並在評估最大化Likelihood的條件下定義出Cross-Entropy來當作Error Measurement，在上圖右側的小圖，我們稍微調整Cross-Entropy，使得它的Error Function可以在&lt;span class="math"&gt;\(y\times s=0\)&lt;/span&gt;的地方和Squared Error相切，&lt;strong&gt;這張圖告訴我們的是隨著Grandient Descent每次的更新，Logistic Regression會把分類做的越來越好&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id="_3"&gt;後話&lt;/h3&gt;
&lt;p&gt;在這一篇當中，我們介紹了Grandient Descent這一個相當重要的演算法，並且運用在兩種Regression上：Linear Regression和Logistic Regression，Linear Regression是最簡單的Regression方法，甚至它還可以使用Pseudo-Inverse的方法直接算出最佳解，Logistic Regression考慮了有雜訊的Data產生的機率分布，我們可以用Logistic Regression做Soft Binary Classification，而且我們也說明了Logistic Regression為何適合拿來用在二元分類上。本篇我們對於ML的實際作法有了基本認識，在下一篇，我們繼續討論還有沒有什麼方式可以讓ML做的更好。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習基石"></category></entry><entry><title>機器學習基石 學習筆記 (2)：為什麼機器可以學習?</title><link href="https://ycc.idv.tw/ml-course-foundations_2.html" rel="alternate"></link><published>2016-06-26T12:00:00+08:00</published><updated>2016-06-26T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2016-06-26:/ml-course-foundations_2.html</id><summary type="html">&lt;p&gt;機器可以學習嗎? / &lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;的差異 / VC Generalization Bound / 機器要能學習的三要素 / 學習架構&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;h3 id="_1"&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們初探了機器學習，了解了什麼時候適合使用機器學習，而不是一般的Hard Coding，那今天這篇文章要繼續問下去。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;為什麼機器可以學習(Why Can Machines Learn?)&lt;/strong&gt;，本篇會介紹學理上機器學習（ML）必須要有哪些條件才可行，這些理論有非常多的數學，但卻是了解機器學習非常重要的內功，我會盡量避開繁複的數學運算，而帶大家直接的了解式子所要告訴我們的觀念。&lt;/p&gt;
&lt;h3 id="_2"&gt;機器可以學習嗎?&lt;/h3&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.001" src="/media/MachineLearningFoundations/MachineLearningFoundations.001.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;還記得上面這張圖嗎? 上次帶大家初探了Machine Learning(ML)的基本架構，可以把整個概念總結成上面這張圖。&lt;/p&gt;
&lt;p&gt;我們來複習一下，先從最上面的盆子開始看起，我們用Target Function代表你想要學習的技能，在非常理想的情況下，也就是沒有noise的情況，每組輸入變數 &lt;span class="math"&gt;\(X_n\)&lt;/span&gt;都會找到一組精確的輸出 &lt;span class="math"&gt;\(y_n\)&lt;/span&gt;，而這個Target Function能產生多個Data，圖中那些小球就是代表由Target Function產生的Data，今天我從中隨機抽取出&lt;span class="math"&gt;\(N\)&lt;/span&gt;組Data來做機器學習，接下來Learning Algorithm會利用這些取出的Data去找出最吻合的Hypothesis，那這組Hypothesis就成了我們學習出來的結果，我們可以利用這個結果來預測新的問題。&lt;/p&gt;
&lt;p&gt;那麼上面這張圖真的合理嗎? 我們真的有辦法用上面的方法讓機器學習嗎? &lt;/p&gt;
&lt;p&gt;先介紹幾個名詞，我們會稱&lt;strong&gt;抽樣的Data為In-sample Data&lt;/strong&gt;，並且稱&lt;strong&gt;Hypothesis預測In-sample Data的誤差為In-sample Error，記作&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;&lt;/strong&gt;，因此Learning Algorithm的目的就是找出那組Hypothesis使得&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小。&lt;/p&gt;
&lt;p&gt;回想一下二元分類問題，在上一篇當中我們使用PLA來挑選Hypothesis Set，還記得我們做了什麼事來確保我們可以得到最佳解嗎? 那就是Pocket的方法，Pocket的目的就是去留住一組能預測最好的Hypothesis，也就是能保留一組最佳參數使得&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小。&lt;/p&gt;
&lt;p&gt;但如果&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;真的已經可以壓到0了，我們就可以說機器學習已經完成了嗎？&lt;/p&gt;
&lt;p&gt;並不是這樣的，回到目的，我們真正希望的是機器有辦法預測新的問題，所以真正的目標是能將「沒有看過的Data」也可以預測好，而不是單單將取樣的Data預測好就夠了。&lt;/p&gt;
&lt;p&gt;我們會稱&lt;strong&gt;未被取樣的Data為Out-of-sample Data&lt;/strong&gt;，並且稱&lt;strong&gt;Hypothesis預測Out-of-sample Data的誤差為Out-of-sample Error，記作&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;，我們最終目的就是把&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;壓下來，也就代表可以預測新的問題&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;但遺憾的是我們不會真正知道&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;，除非我們知道Target Function，所以我們只能評估&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;來選取Model參數，因此重要的是需要&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;這個條件要成立，否則一切的學習都是無效的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;總結一下機器學習的條件，我們必須建立一個 Learning Model可以確保&lt;span class="math"&gt;\(E_{in}\approx E_{out}\)&lt;/span&gt;，所以在Learning Algorithm選出最小&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的Hypothesis，同時這組Hypothesis也可以很好的預測Out-sample，我們就可以說機器已經會學習了。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="e_ine_out"&gt;&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;的差異&lt;/h3&gt;
&lt;p&gt;&lt;img alt="image" src="/media/MachineLearningFoundations/MachineLearningFoundations.005.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;剛剛我們已經提到了如果機器能學習，那就必須先確保&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，下面我會引入Hoeffding不等式來說明這個條件怎麼成立。&lt;/p&gt;
&lt;p&gt;先想像一下我有一個桶子，這個桶子裝了兩種顏色的小球，分別為橘色和綠色，今天如果桶子內橘色球佔的比例為&lt;span class="math"&gt;\(μ\)&lt;/span&gt;，而今天我們從中隨機抽樣出&lt;span class="math"&gt;\(N\)&lt;/span&gt;顆小球，並且計算出這&lt;span class="math"&gt;\(N\)&lt;/span&gt;顆小球中橘色佔的比例為&lt;span class="math"&gt;\(ν\)&lt;/span&gt;，此時我們可以想像的到，&lt;span class="math"&gt;\(μ=ν\)&lt;/span&gt;不一定會成立，但&lt;span class="math"&gt;\(μ\)&lt;/span&gt;也不至於離&lt;span class="math"&gt;\(ν\)&lt;/span&gt;太遠，所以Hoeffding不等式就告訴我們&lt;span class="math"&gt;\(|μ-ν|\)&lt;/span&gt;會被限制在一個範圍內，表示為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}[|ν-μ|&amp;gt;ε] \leq 2 exp(-2ε^2N)
$$&lt;/div&gt;
&lt;p&gt;
當&lt;span class="math"&gt;\(ε\)&lt;/span&gt;越大，出現的機率就越低。&lt;/p&gt;
&lt;p&gt;接下來我們再把橘球和綠球的意義換成是，一組Hypothesis預測每筆Data的好或壞，預測正確的是綠球，預測失敗的是橘球，所以對於In-Sample來說，&lt;span class="math"&gt;\(μ\)&lt;/span&gt; 就是 &lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
μ = (1/N) \sum_{n=1}^N ⟦h(x)\neq y_n⟧ = E_{in}(h)
$$&lt;/div&gt;
&lt;p&gt;
對於Out-Sample來說，&lt;span class="math"&gt;\(ν\)&lt;/span&gt; 就是 &lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
ν =  ε_{x \sim P} ⟦h(x)\neq f(x)⟧ = E_{out}(h)
$$&lt;/div&gt;
&lt;p&gt;
套入剛剛的不等式，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}[|E_{in}(h)-E_{out}(h)|&amp;gt;ε] \leq 2 exp(-2ε^2N)
$$&lt;/div&gt;
&lt;p&gt;
上面這個式子告訴我們&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;差距超過&lt;span class="math"&gt;\(ε\)&lt;/span&gt;的可能性是被限制住的，只要抽樣的數量&lt;span class="math"&gt;\(N\)&lt;/span&gt;夠多，基本上&lt;span class="math"&gt;\(E_{in}\approx E_{out}\)&lt;/span&gt;就成立，我們這邊定義那些超出&lt;span class="math"&gt;\(ε\)&lt;/span&gt;的Data為Bad Data(不好的數據)，Bad Data出現的可能是被Bound住的，所以機器學習是有可能的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="/media/MachineLearningFoundations/MachineLearningFoundations.006.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;而事實上，我們的hypothesis不會只有一個，所以接下來來考慮如果有M個Hypotheses的情況下我們的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;的差異會怎麼被參數影響。&lt;/p&gt;
&lt;p&gt;如果我們考慮M組Hypotheses，就會發現每種Hypothesis出現Bad Data的地方可能不一樣，因此大大的減少能使用的Data，如上圖左側所示。&lt;/p&gt;
&lt;p&gt;今天如果我有1000份從Target Function取&lt;span class="math"&gt;\(N\)&lt;/span&gt;個Data的情形，然後只用一個Hypothesis來衡量，根據Hoeffding's Inequality，1000份裡面假設大概5份會出現Bad Data，但今天我再增加一組Hypothesis來衡量，對於這個Hypothesis也可能有自己的5份Bad Data，如果很不幸的，剛剛好這5份Bad Data和前5份沒有重疊，因此用這兩個hypotheses來評估的話，1000份裡頭將會出現10份的Bad Data，由此類推，如果有&lt;span class="math"&gt;\(M\)&lt;/span&gt;組Hypotheses，最差的情況會發生在什麼時候呢? 那就是&lt;span class="math"&gt;\(M\)&lt;/span&gt;個Hypotheses的每份Bad Data彼此都沒有交集，夠慘吧! 所以把這些出現Bad Data的機率取聯集得到以下式子：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}[\exists h\in \mathbb{H}\ s.t.\ |E_{in}(h)-E_{out}(h)|&amp;gt;ε] \leq 2M exp(-2ε^2N)
$$&lt;/div&gt;
&lt;p&gt;
大家現在回想一下上一篇所提到的Perceptron Hypothesis Set就會發現，糟糕了! Perceptron Hypothesis Set 裡有無限多組的Hypotheses，也就是&lt;span class="math"&gt;\(M→∞\)&lt;/span&gt;，那我們不就需要無限多的Data才能做到&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，否則機器根本不會學習，所以前一篇的內容都在亂講，PLA根本無法學習，因為&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，就算&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;很小也不代表學習成立，機器學習是不可能的。等一下！先沉住氣，聽我接下來慢慢解釋，你就會發現還有一線生機。&lt;/p&gt;
&lt;h3 id="vc-generalization-bound"&gt;VC Generalization Bound&lt;/h3&gt;
&lt;p&gt;&lt;img alt="image" src="/media/MachineLearningFoundations/MachineLearningFoundations.007.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;問題出在這裡，我們在Multi-Bin Hoeffding’s Inequality中採用了一個假設，就是假設每組Hypotheses的Bad Data彼此間都沒有重疊，所以在&lt;span class="math"&gt;\(M→∞\)&lt;/span&gt;的情況下，當然會有一個無限大的上限值，但如果考慮了Bad Data重疊的情形，縱使&lt;span class="math"&gt;\(M→∞\)&lt;/span&gt;的情況下還是有機會把Bad Data的出現機率壓在一個有限的定值之下。&lt;/p&gt;
&lt;p&gt;我們回到二元分類問題，看一下上圖中左側的圖例，如果今天在二維平面上做二元分類，當數據量只有1個&lt;span class="math"&gt;\(n=1\)&lt;/span&gt;時，就算你的切法有無窮多種，但對於一組Data來說就只有兩類Hypotheses而已，再來看&lt;span class="math"&gt;\(n=2\)&lt;/span&gt;的情況，一樣的無限多組的切法但Hypotheses也只能歸類成4類。&lt;/p&gt;
&lt;p&gt;所以Hypotheses用來描述數據的情況是彼此有所重疊的，也就是Bad Data出現的情形在許多Hypotheses是相同的。&lt;/p&gt;
&lt;p&gt;但是聰明的你一定想到，如果今天&lt;span class="math"&gt;\(n\)&lt;/span&gt;的數量不斷的增加，則Hypotheses被分類的數量就會成指數 &lt;span class="math"&gt;\(2^n\)&lt;/span&gt; 增加，Hypotheses彼此之間Bad Data的重疊情況就會漸漸減少，因此仍然無法限制住Bad Data的數量。&lt;/p&gt;
&lt;p&gt;先別緊張，我們繼續看下去，當&lt;span class="math"&gt;\(n=3\)&lt;/span&gt;，沒有意外的Hypotheses會被分類為8類，那接下來&lt;span class="math"&gt;\(n=4\)&lt;/span&gt;時，你就會發現一個有趣的現象，開始有一些分類情況是不會出現的，因為它無法被一分為二，因此我們擔心因為Data數量增加而造成Hypotheses的種類暴增的情形被排除了，有一些狀況是不會出現的，Hypotheses是有重疊的。&lt;/p&gt;
&lt;p&gt;剛剛所提到的分類方式的數量稱為Dichotomy。在&lt;span class="math"&gt;\(n=1\)&lt;/span&gt;、&lt;span class="math"&gt;\(n=2\)&lt;/span&gt;到&lt;span class="math"&gt;\(n=3\)&lt;/span&gt;的情形，所有列得出來的方式都可被完整分類開來，我們稱這情形為Shatter，但是到了&lt;span class="math"&gt;\(n=4\)&lt;/span&gt;的時候，有些不可能被分類的情形出現了，稱為不可被Shatter，另外我們又稱此情形開始發生的那點為Break Point，這邊注意一下喔! 會不會存有Break Point取決於你的Hypothesis Set長怎麼樣，現在這個例子的Break Point在&lt;span class="math"&gt;\(n=4\)&lt;/span&gt;，其他的Hypothesis Set就不一定了。&lt;/p&gt;
&lt;p&gt;Break Point的出現非常重要，他所代表的是Bad Data的出現機率不會無所限制的大下去，因此把這概念帶入Multi-Bin Hoeffding’s Inequality，經過繁複的計算，就可以得到以下公式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}[\exists h\in \mathbb{H}\ s.t.\ |E_{in}(h)-E_{out}(h)|&amp;gt;ε] \leq 4m_{\mathbb{H}}(2N) exp(-ε^2N/8)
$$&lt;/div&gt;
&lt;p&gt;
，原本的&lt;span class="math"&gt;\(M\)&lt;/span&gt;消失了，取而代之的是Growth Function  &lt;span class="math"&gt;\(m_{\mathbb{H}}(2N)\)&lt;/span&gt;，Growth Function與Data數量&lt;span class="math"&gt;\(N\)&lt;/span&gt;有關，這就是我們剛剛解說的，決定Hypothesis Set的種類的其實是 Data的數量&lt;span class="math"&gt;\(N\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;那麼Growth Function要怎麼和Break Point連結起來呢？&lt;/p&gt;
&lt;p&gt;先定義一下VC Dimension：&lt;span class="math"&gt;\(d_{VC}= Break Point-1\)&lt;/span&gt;，Break Point代表首次出現不Shatter的情況，那比它小一級代表的正是最大可以Shatter的點，上面的例子中&lt;span class="math"&gt;\(d_{VC}=3\)&lt;/span&gt;。而這個VC Dimension就可以和我們在意的Growth Function連接起來，經過數學推倒可以得到以下關係式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
m_{\mathbb{H}}(n) = n^{d_{VC}},\ d_{VC} = BreakPoint-1
$$&lt;/div&gt;
&lt;p&gt;
所以我們就知道啦！&lt;strong&gt;只要有Break Point存在，VC Dimension就是一個有限的值，也因此Growth Function是一個有限的值，VC Bound就產生了，就可以確保Bad Data出現的機率被壓在一個定值之下，所以一樣的只要資料量&lt;span class="math"&gt;\(N\)&lt;/span&gt;夠多就可以確保&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，機器將可以學習。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外一件重要的事，VC Dimension在數學上是有意義的，&lt;strong&gt;&lt;span class="math"&gt;\(d_{VC} \approx 可調控變數的個數\)&lt;/span&gt;&lt;/strong&gt;，像是上述的二維二元分類問題，它的可調控變數有&lt;span class="math"&gt;\(w_0\)&lt;/span&gt;, &lt;span class="math"&gt;\(w_1\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(w_2\)&lt;/span&gt;，總共3個，所以&lt;span class="math"&gt;\(d_{VC}=3\)&lt;/span&gt;。&lt;strong&gt;也就是說Hypothesis Set的可調變參數如果是有限，大部分都可以做機器學習。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="_3"&gt;機器要能學習的三要素&lt;/h3&gt;
&lt;p&gt;前面拉哩拉雜的講了一堆，終於要推出我們的結論了! 所以如果剛剛的數學讓你感到很挫敗，沒關係，讀懂這段那就足夠了。&lt;/p&gt;
&lt;p&gt;從VC Generalization Bound，我們可以知道機器學習是可能的，只要它具備三點要素：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Good Hypothesis Set: Hypothesis Set 必須有Break Point的存在，也意味著VC Dimension是有限的，而且越小越好，在意義上代表可以調控的變數不要太多。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Good Data: 數據量越大越好，可以壓低VC Generalization Bound&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Good Learning Algorithm: 以上兩點可以確定的是&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，接下來好的Learning Algorithm要有能力找到&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt; 最小的參數。很直觀的，當我們可以調控的變數越多，我們的選擇就越多，也就是我們可以找到更小&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt; 的機會變多了，所以可以調控的變數不可以太少。&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;眼尖的你有沒有發現矛盾啊! 可以調控的變數很少，我們能確保&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，但是如果我想要找到更小的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt; 又必須有更多的調控變數，這個矛盾是機器學習上一個重要的課題，&lt;strong&gt;解法是我們必須要能找到適當的調控變數數量，也就是適當大小的&lt;span class="math"&gt;\(d_{VC}\)&lt;/span&gt; &lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="/media/MachineLearningFoundations/MachineLearningFoundations.000.02.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://d396qusza40orc.cloudfront.net/ntumlone/lecture_slides/07_handout.pdf"&gt;https://d396qusza40orc.cloudfront.net/ntumlone/lecture_slides/07_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上圖中，我們把VC Generalization Bound公式帶入Growth Function和&lt;span class="math"&gt;\(d_{VC}\)&lt;/span&gt;的關係式，並且設&lt;span class="math"&gt;\(δ\)&lt;/span&gt; 為最大可以容忍的Bad Data出現機率，把它帶入取代掉&lt;span class="math"&gt;\(ε\)&lt;/span&gt;，整理一下，就可以推出上圖的公式，&lt;span class="math"&gt;\(\Omega (N,\mathbb{H},δ)\)&lt;/span&gt;稱為Model Complexity，這一項代表的是Hypothesis Set的大小造成的模型複雜度，它隨著&lt;span class="math"&gt;\(d_{VC}\)&lt;/span&gt;增加而增加。Model Complexity越大代表Bad Data更容易出現，所以&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;開始被帶開了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這個現象有一個很常見的名字叫做Overfitting，指的是使用非常複雜的Model來Fitting，雖然可以把手頭上的數據Fit的很漂亮，但是拿到其他的數據來看就會發現這Model的預測性非常的差，原因就是因為Model Complexity造成&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;脫鉤了，所以選擇一個複雜度適中的Model是很重要的。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id="_4"&gt;機器學習架構一般化&lt;/h3&gt;
&lt;p&gt;&lt;img alt="image" src="/media/MachineLearningFoundations/MachineLearningFoundations.008.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;最後我們來總結一下機器學習的流程，上圖中是之前提到的機器學習的架構並額外考慮一些真實情形，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每筆Data出現的機會不一定，同樣的採樣結果也是會受機率的影響，所以上圖中標示為&lt;span class="math"&gt;\(\mathbb{P} (x)\)&lt;/span&gt;，這個修改並不會影響機器學習的流程和結果。&lt;/li&gt;
&lt;li&gt;Data可能會受到Noise的影響，所以給定&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;並不一定會百分之一百得到&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;，他存在著可能會出錯，上圖標示為&lt;span class="math"&gt;\(\mathbb{P}(y|x)\)&lt;/span&gt;，我們可以增大我們採樣的數量&lt;span class="math"&gt;\(N\)&lt;/span&gt;來減少Noise的影響。&lt;/li&gt;
&lt;li&gt;我們是採用&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;來當作選擇Model參數的指標，因此我們需要訂出Error的評估方式，常見的有Squared Error &lt;span class="math"&gt;\(E_{squared} = (y_n - y_{prediction})^2\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;跟著架構我們就有一套機器學習的&lt;strong&gt;標準流程&lt;/strong&gt;，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;準備好足夠的數據&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;把Model建立好，&lt;span class="math"&gt;\(d_{VC}\)&lt;/span&gt;必須要是有限的，而且大小要適中&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定義好評估&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的Error Measurement&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;使用演算法找出最佳參數把&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;降低&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最後評估一下是否有Overfitting的狀況，確保&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;&lt;/strong&gt;（未來會講怎麼做）&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習基石"></category></entry><entry><title>機器學習基石 學習筆記 (1)：何時可以使用機器學習?</title><link href="https://ycc.idv.tw/ml-course-foundations_1.html" rel="alternate"></link><published>2016-06-06T12:00:00+08:00</published><updated>2016-06-06T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2016-06-06:/ml-course-foundations_1.html</id><summary type="html">&lt;p&gt;什麼是Machine Learning / ML的使用時機 / 二元分類問題 / 多元學習&lt;/p&gt;</summary><content type="html">&lt;h3 id="_1"&gt;前言&lt;/h3&gt;
&lt;p&gt;經過幾個月的努力，終於完成田神在Coursera上machine learning的兩門課中的第一門課—&lt;a href="https://www.coursera.org/course/ntumlone"&gt;機器學習基石&lt;/a&gt;，田神不愧為田神的名號，整門課上起來非常流暢，每個觀念講得非常得清晰，考究學理，但是又不會單單只有理論而已，課程中會舉很多實用的例子，讓你了解每個觀念如何實踐。因此，非常推薦大家去把Coursera上面的課程完整聽一次，應該會收益良多，接下來一系列的文章，我會摘要出《機器學習基石》之中主要的概念，適合對Machine Learning（ML）有興趣的初學者來一窺它的脈絡。&lt;/p&gt;
&lt;p&gt;《機器學習基石》一共有16堂課，主要分為四個方向，第一個方向，&lt;strong&gt;何時可以使用機器學習(When Can Machines Learn? )&lt;/strong&gt;，點出什麼是機器學習，適合在哪些情形下使用，並引入貫穿整個課程的二元分類問題，第二個方向，&lt;strong&gt;為什麼機器可以學習(Why Can Machines Learn?)&lt;/strong&gt;，介紹學理上機器學習必須要有哪些條件才可行，這些理論是了解機器學習非常重要的內功，第三個方向，&lt;strong&gt;機器可以怎麼樣學習(How Can Machines Learn?)&lt;/strong&gt;，學習完了學理，我們來看機器學習有哪些的使用方法，最後一個方向，&lt;strong&gt;機器可以怎麼樣學得更好(How Can Machines Learn Better?)&lt;/strong&gt;，探討哪些問題會造成機器學不好，然後怎麼去改善。&lt;/p&gt;
&lt;h3 id="machine-learning-ml"&gt;什麼是Machine Learning (ML)&lt;/h3&gt;
&lt;p&gt;在了解機器學習之前，我們不妨來想想「你」從小是怎麼學習的，有人會說學習就是一個不斷記憶的過程，但這樣的說法顯然不夠全面，你總不會認為把考題的所有答案都背起來的學生就已經學會一門知識了吧！所以，考題只是表象，我們真正要學習的是它背後的觀念，可以拿來推敲未知的知識。&lt;/p&gt;
&lt;p&gt;同樣的，ML的學習方式也有點類似於人類的學習，機器從Data中開始學習起，這些Data就像是一道一道的考題，而ML做的事正是去學習Data後面的觀念，而不是單純把Data給儲存起來，有了Data背後的觀念才能舉一反三，才算是真正的學會了。&lt;/p&gt;
&lt;p&gt;所以，做ML有點像是手把手的造一顆大腦，並且訓練它學會Data背後的知識。那這個大腦要怎麼設計呢？這個大腦用我們學物理的人的說法就是建一個Model，而餵給它Data的過程就是Model Fitting。&lt;/p&gt;
&lt;p&gt;那什麼是Model呢？讓我來解釋一下，&lt;strong&gt;所謂的Model就是給一個未知現象的框架來試圖描述它&lt;/strong&gt;，舉個例子，我們都知道力的公式是&lt;span class="math"&gt;\(F=ma\)&lt;/span&gt;（力＝質量x加速度），但如果你今天拿一顆皮球來，你就會發現這個公式並不那麼正確，因為皮球會形變，那怎麼辦呢？我們可以假設形變會把部份的力給抵消掉，所以式子改寫成&lt;span class="math"&gt;\((F-F_1)=ma\)&lt;/span&gt;，在這邊&lt;span class="math"&gt;\(F_1\)&lt;/span&gt;就是那個抵消的力，這樣就是設計了一個Model來描述這個現象，而&lt;span class="math"&gt;\(F_1\)&lt;/span&gt;是一個未知的值，我們可以用實驗數據來推估&lt;span class="math"&gt;\(F_1\)&lt;/span&gt;，這就是所謂的Model Fitting。&lt;/p&gt;
&lt;p&gt;物理上的Model通常是這樣做的，我們先觀察未知現象，然後從中猜測可能造成這現象的原因，總結這些原因來設計一個Model，Model中可能有一些參數還沒被決定，此時我們就可以用數據來決定它，這就是Model Fitting。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.001" src="/media/MachineLearningFoundations/MachineLearningFoundations.001.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;了解了Model的概念就相當好了解ML的架構，上圖是ML的基本架構，&lt;strong&gt;假設我們今天要讓機器學一樣技術，這個技術我們用一個函數來表示，稱之為Target Function，這個Target Function就是隱藏在Data後面的真正道理&lt;/strong&gt;，每個變數&lt;span class="math"&gt;\(X\)&lt;/span&gt;會有相應的正確答案&lt;span class="math"&gt;\(Y\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;今天我從Target Function中取出&lt;span class="math"&gt;\(N\)&lt;/span&gt;組當作Data來給我的機器學習，那目標是什麼?&lt;strong&gt;目標當然是讓機器學習出這個Target Function啦！&lt;/strong&gt;所以我們要先設計我們的Model，最終目的是決定Model裡的參數之後，這個被選擇的Model就是Target Function。&lt;/p&gt;
&lt;p&gt;Model就是上圖中的Hypothesis Set，在Model參數還沒被決定之前，你可以想像它就像一個集合包含很多可以選擇的函數，而使用數據Model Fitting以後，選出一組最佳化的參數，就好像從這個集合中挑選一組函數一樣。&lt;/p&gt;
&lt;p&gt;在這個找最佳化參數的過程，我們需要一個機制，這個機制可以評估Hypothesis Set中每組函數描述Data的好壞，並且找出描述Data最好的那組參數，這個機制就是上圖中的Learning Algorithm。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;建立Model，使用Data加上Learning Algorithm找出最佳參數，這就是ML的架構輪廓&lt;/strong&gt;。當然這邊要補充一下，物理上的Model通常是建基在已知的知識之上，而常見的ML強大之處是不需要太多的人為的智慧，機器可以自行學習，所以我這裡指的Model是比物理上的Model更加廣義的。&lt;/p&gt;
&lt;h3 id="machine-learning-ml_1"&gt;Machine Learning (ML)的使用時機&lt;/h3&gt;
&lt;p&gt;剛剛帶大家初探了ML的架構，接下來帶大家了解什麼時候我們適合使用ML。&lt;/p&gt;
&lt;p&gt;舉幾個例子，大家可能比較有感覺，譬如說Netflix曾辦過一場競賽，競賽的內容是利用客戶的影片評分紀錄，來預測未評分影片的得分，如果可以增進預測率10%，就可以獨得100萬美元獎金，這個問題就可以使用ML，Data是過去得評分紀錄，Target Function是用戶評分的規律，如此一來，機器學到了這個技術，未來就可以舉一反三的推出未評分影片的分數，和用戶喜歡的影片可能有哪些。&lt;/p&gt;
&lt;p&gt;再多看幾個例子，例如設計火星勘查機，人類目前對火星的了解仍相當有限，所以我們沒辦法完全猜測勘查機在火星會遇到什麼問題，所以必須讓勘查機有ML的能力去學習各種問題的解決方法。&lt;/p&gt;
&lt;p&gt;再來個例子，現在很夯的汽車自動駕駛也需要ML技術，機器去學習辨識交通號誌。&lt;/p&gt;
&lt;p&gt;看了這麼多例子，我們會發現這些例子都很難以寫出簡單的規則，但是卻又存在著一種規律，這種情形正是適合用ML來做。&lt;/p&gt;
&lt;p&gt;在以往電腦工程幾乎都是由工程師用嚴謹的邏輯去逐條的把規則一一的寫上，這樣的機器不具有學習能力，或稱得上人工智慧，因為它只是單純反應工程師的工人智慧而已，但如果遇到一些困難的問題，譬如告訴機器什麼是狗，這時候你就會發現很難用人為規則來描述它，有尾巴，可是是怎樣的尾巴？有耳朵，那這耳朵怎麼和貓的耳朵區分開來？此時要用人為寫出規則就太困難了，我們不這麼做，反過來我們設計架構讓機器自己去從Data中學習。&lt;/p&gt;
&lt;p&gt;總結一下上面的重點，ML的最佳使用時機包含下面三種情形&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;你想要學習的技術存在一種模式&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;要學習的技術不容易簡單的列出規則&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;存在可以代表這個要學習的模式的Data&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="_2"&gt;二元分類問題&lt;/h3&gt;
&lt;p&gt;&lt;img alt="img" src="/media/MachineLearningFoundations/MachineLearningFoundations.000.01.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://class.coursera.org/ntumlone-003/lecture/17"&gt;https://class.coursera.org/ntumlone-003/lecture/17&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;好! 大家現在應該對於機器學習有一些認識了，那接下來我們來實作一些例子來了解機器學習架構怎麼運作。像個小學生一樣，我們先從簡單的是非題來學起，是非題學究一點的講法就是「二元分類問題」。&lt;/p&gt;
&lt;p&gt;舉個例子，今天有一家銀行想要開發一款ML的軟體，這個軟體可以根據過去信用卡核發用戶的資料，去判斷要不要核發信用卡給這個新的申請人，這些過去的資料可能包括：用戶年齡、用戶性別、用戶年薪等等，讓機器藉由這些資料去學習判斷要不要核發信用卡。把這樣的二元分類問題化作&lt;/p&gt;
&lt;p&gt;Target Function：&lt;span class="math"&gt;\(f: X → y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(X\)&lt;/span&gt;有年齡、性別和年薪這些變數，而&lt;span class="math"&gt;\(y\)&lt;/span&gt;則是個二元類別，不是&lt;span class="math"&gt;\(y=1\)&lt;/span&gt;(核發)就是&lt;span class="math"&gt;\(y= -1\)&lt;/span&gt;(不核發)。&lt;/p&gt;
&lt;p&gt;那接下來，我們就要決定我們的Learning Model，也就是Hypothesis Set。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.002" src="/media/MachineLearningFoundations/MachineLearningFoundations.002.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;引入&lt;strong&gt;Perceptron(感知器) Hypothesis Set&lt;/strong&gt;來當作我們的Hypothesis Set，如上圖，我們給予我們的輸入變數個別的權重，然後相加起來，並且看這個值是正還是負，來決定輸出值是&lt;span class="math"&gt;\(+1\)&lt;/span&gt;或&lt;span class="math"&gt;\(-1\)&lt;/span&gt;，&lt;span class="math"&gt;\(sign\)&lt;/span&gt;函數的作用是假設輸入的值為正則輸出&lt;span class="math"&gt;\(+1\)&lt;/span&gt;，反之則輸出&lt;span class="math"&gt;\(-1\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;對應核發信用卡這個例子，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(x_1\)&lt;/span&gt; = 用戶年齡; &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; = 用戶性別; &lt;span class="math"&gt;\(x_3\)&lt;/span&gt; = 用戶年薪，&lt;/p&gt;
&lt;p&gt;在分別乘上weight &lt;span class="math"&gt;\(w_1\)&lt;/span&gt;, &lt;span class="math"&gt;\(w_2\)&lt;/span&gt;, &lt;span class="math"&gt;\(w_3\)&lt;/span&gt;，這個變數前面的weight代表這個變數對於答案&lt;span class="math"&gt;\(Y\)&lt;/span&gt;有什麼影響，如果是正向影響，&lt;span class="math"&gt;\(weight &amp;gt; 0\)&lt;/span&gt;，如果沒有影響，&lt;span class="math"&gt;\(weight = 0\)&lt;/span&gt;，如果負向影響，&lt;span class="math"&gt;\(weight &amp;lt; 0\)&lt;/span&gt;，舉個例子，高年薪也許可以提升核發信用卡的機會，那它前面的weight應該就是正的，也許性別並不影響核發信用卡的機會，則&lt;span class="math"&gt;\(weight = 0\)&lt;/span&gt;，那麼考慮到這些input變數對結果影響的評估，我們會得到一個數值 &lt;span class="math"&gt;\((w_1\times x_1+w_2\times x_2+...)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;此時我們要用這個數值去做「二元分類」，也就是一分為二，怎麼做呢? 很簡單，給他一分水嶺，高於一個閥值我就給他 &lt;span class="math"&gt;\(y=+1\)&lt;/span&gt;，低於一個閥值我就給他&lt;span class="math"&gt;\(y=-1\)&lt;/span&gt;，假設這個閥值為&lt;span class="math"&gt;\((-w_0)\)&lt;/span&gt;，則分類依據就可以表示為 &lt;span class="math"&gt;\(sign(w_0+w_1\times x_1+w_2\times x_2+...)\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;上圖中的 &lt;span class="math"&gt;\(s = w_0+w_1\times x_1+w_2\times x_2+...\)&lt;/span&gt; 就像一個分數(score)一樣，高分 &lt;span class="math"&gt;\(s&amp;gt;0\)&lt;/span&gt; 的我就核發(&lt;span class="math"&gt;\(+1\)&lt;/span&gt;)，低分 &lt;span class="math"&gt;\(s &amp;lt; 0\)&lt;/span&gt; 的我就不核發(&lt;span class="math"&gt;\(-1\)&lt;/span&gt;)，其中權重 &lt;span class="math"&gt;\(w_0, w_1, w_2, ...\)&lt;/span&gt; 都可以由機器學習去調整，這些不同的weight就構成了Hypothesis Set，也就是Model，那接下來我們還需要Learning Algorithm來取出最佳參數，也就是決定一組最佳weight來選出最吻合數據的Hypothesis。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.003" src="/media/MachineLearningFoundations/MachineLearningFoundations.003.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;如上圖所示，&lt;strong&gt;Perceptron Learning Algorithm(PLA)&lt;/strong&gt;是用於處理Perceptron Hypothesis Set的一種演算法。&lt;/p&gt;
&lt;p&gt;它的作法簡單來講是，藉由一筆一筆的數據去逐步的更新它的weight使得Model可以描述這筆數據，直到不需要再更新為止，此時所有的Data都可以用這個Model表示，更新的方法是先判斷進來的這筆數據是否符合目前的Model預測，如果不符合，此時&lt;span class="math"&gt;\(\left[...\right]\)&lt;/span&gt;為&lt;span class="math"&gt;\(+ 1\)&lt;/span&gt;，則必須朝變數向量&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;的方向，前進或後退大小為Learning Rate的一步來更新weight，前進還是後退端看你的Data是&lt;span class="math"&gt;\(y=-1\)&lt;/span&gt;或&lt;span class="math"&gt;\(+1\)&lt;/span&gt;，&lt;span class="math"&gt;\(y=+1\)&lt;/span&gt;就往前進，&lt;span class="math"&gt;\(y=-1\)&lt;/span&gt;就往後退。&lt;/p&gt;
&lt;p&gt;因此，這個跨步更新的動作必須可以使Model接近正確答案，這麼神奇，真的假的？不太直覺，先從score來想起，假設有一筆資料為&lt;span class="math"&gt;\((X_n,y_n)\)&lt;/span&gt;，則Score：&lt;span class="math"&gt;\(s = W_t・X_n\)&lt;/span&gt;，在&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;和&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;向量彼此有同向分量的情況下，&lt;span class="math"&gt;\(s &amp;gt; 0\)&lt;/span&gt;，如果這個時候&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;剛好為&lt;span class="math"&gt;\(+1\)&lt;/span&gt;，則&lt;span class="math"&gt;\(sign(s)=y_n\)&lt;/span&gt;，這個時候&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;描述這個數據就很好啊，我們就不需要去更新它；如果相反&lt;span class="math"&gt;\(y_n=-1\)&lt;/span&gt;，這個&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;描述這個數據就不正確，也就是說&lt;span class="math"&gt;\(W_t\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(X_n\)&lt;/span&gt;不應該同向，所以我們讓&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;加上&lt;span class="math"&gt;\(-X_n\)&lt;/span&gt;(&lt;span class="math"&gt;\(=y_n\times X_n\)&lt;/span&gt;)，把&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;從原本與&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;同向的狀態反向拉離開來。那如果在&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;和&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;向量彼此不同向的情況下，&lt;span class="math"&gt;\(s &amp;lt; 0\)&lt;/span&gt;，這個時候如果&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;剛好為&lt;span class="math"&gt;\(-1\)&lt;/span&gt;，則&lt;span class="math"&gt;\(sign(s)=y_n\)&lt;/span&gt;，很好我們不去更新它；如果相反&lt;span class="math"&gt;\(y_n=+1\)&lt;/span&gt;，這個&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;描述這個數據不正確，也就是說&lt;span class="math"&gt;\(W_t\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(X_n\)&lt;/span&gt;不應該反向，所以我們讓&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;加上&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;(&lt;span class="math"&gt;\(=y_n\times X_n\)&lt;/span&gt;)，把&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;拉到和&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;同向一點。這就是PLA找到更好&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;的機制。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.004" src="/media/MachineLearningFoundations/MachineLearningFoundations.004.jpeg" /&gt;&lt;/p&gt;
&lt;p&gt;Seeing is believing，上面這張圖帶我們來看PLA如何運作，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initially: 在最一開始的時候，我們weight &lt;span class="math"&gt;\(W_t\)&lt;/span&gt;先設成零向量&lt;/li&gt;
&lt;li&gt;Update 1: PLA更新把零向量的&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;拉成&lt;span class="math"&gt;\(W_{t+1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Update 2: 上一輪的&lt;span class="math"&gt;\(W_{t+1}\)&lt;/span&gt;已經是這一輪的&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;，也就是紅色的那個向量，&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;決定了一條壁壘分明的二元分類邊界，這條線的方程式其實就是 &lt;span class="math"&gt;\(w_0+w_1x_1+... = 0\)&lt;/span&gt;，如果你還記得高中數學的話，這條邊界必然會和&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;垂直，如圖所示，而&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;的方向是屬於&lt;span class="math"&gt;\(y=+1\)&lt;/span&gt;的區域，這一輪剛剛好找到一個圈(&lt;span class="math"&gt;\(y=+1\)&lt;/span&gt;)落在&lt;span class="math"&gt;\(y=-1\)&lt;/span&gt;的區域，因此我們需要更新weight，做法是把&lt;span class="math"&gt;\(W_t\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(y_n\times X_n\)&lt;/span&gt;(=&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;)相加成為新的weight &lt;span class="math"&gt;\(W_{t+1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;...........以此類推&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;如果資料線性可分的話，PLA在迭代多次後，是可以用一條線完全區分兩種數據&lt;/strong&gt;。但如果數據不是線性可分，不存在一條線來區分數據，此時最佳解就必須評估整體犯錯有多少，找出犯錯最少的那條直線就是最佳解，但可惜的是PLA方法並不會在迭代中趨向於犯錯最少的那條線，什麼時候該停止迭代是個世紀難解的NP-Hard問題（如果不了解這個名詞，&lt;a href="/algorithm-complexity-theory.html"&gt;詳見&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;因此要改變一下PLA，這個方法我們稱之為Pocket，當每次得到一組weight的時候，都拿它來評估它對所有Data的區分能力好或壞，而只留下一組最好的放進口袋裡，所以當迭代次數做多了，保留在口袋的這組解就可以看成是最佳解，就這麼簡單。&lt;/p&gt;
&lt;h3 id="_3"&gt;多元學習&lt;/h3&gt;
&lt;p&gt;機器學習和人類學習一樣，有各式各樣的學習型態。剛剛的&lt;strong&gt;「二元分類問題」&lt;/strong&gt;就像考「是非題」一樣，答案要嘛是Yes不然就是No，表示為 &lt;strong&gt;&lt;span class="math"&gt;\(y=\{-1, 1\}\)&lt;/span&gt;&lt;/strong&gt;，這就像是機器在小學時代的問題，較為簡單。&lt;/p&gt;
&lt;p&gt;現在機器脫離國小來到了國中，考試題目開始出現「選擇題」，這和機器學習中的&lt;strong&gt;「多元分類問題」&lt;/strong&gt;一樣，必須從兩個以上有限的答案中作選擇，表示為 &lt;strong&gt;&lt;span class="math"&gt;\(y=\{1, 2, ... , k\}\)&lt;/span&gt;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;另外機器還可能遇到傷透腦筋的「計算題」，在機器學習裏頭稱為&lt;strong&gt;「Regression 問題」&lt;/strong&gt;，這個時候答案已經放寬到整個實數系了，表示為 &lt;strong&gt;&lt;span class="math"&gt;\(y∈R\)&lt;/span&gt;&lt;/strong&gt;，舉個例子，譬如利用過去天氣的數據去預測明日氣溫，或者利用歷史股價資料預測未來股價，都是Regression的應用。&lt;/p&gt;
&lt;p&gt;此時，機器到了大學，開始碰到不那麼容易回答，甚至不存在單一答案的「申論題」，這在ML中像是&lt;strong&gt;「Structure Learning 問題」&lt;/strong&gt;，答案的選擇換成了各種結構，表示為 &lt;strong&gt;&lt;span class="math"&gt;\(y=\{structures\}\)&lt;/span&gt;&lt;/strong&gt;，舉個例子可能比較好理解，例如：自然語言，我們都希望有一天電腦可以理解我們的語言，我們可以不再需要以機器語言來和電腦溝通，而是用人類的語言直接和電腦溝通，聽起來很棒對吧! 這個部分的ML就需要Structure Learning來學習語言的文法結構。&lt;/p&gt;
&lt;p&gt;我們教機器學習也有各種不同的教育方法。&lt;/p&gt;
&lt;p&gt;有像是填鴨式教育的&lt;strong&gt;「Supervised Learning」(監督式學習）&lt;/strong&gt;，直接告訴機器考題和答案，讓機器從中學習，這種情況下每筆資料&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;對應的&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;都有明確Label，Data中有明顯的答案。&lt;/p&gt;
&lt;p&gt;有像是培養科學家教育方法一樣的&lt;strong&gt;「Unsupervised Learning」(非監督式學習）&lt;/strong&gt;，此時每筆資料&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;對應的&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;都沒有Label，所以機器要自己歸納整理，然後從中學到規律，通常用於分群問題，對資料做分類找出規律性。&lt;/p&gt;
&lt;p&gt;那還有折衷於上述兩種方法的啟發式教育，&lt;strong&gt;「Semi-supervised Learning」(半監督式學習）&lt;/strong&gt;，在這個情形下有部分資料&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;是有Label的，機器可以藉由有Label的正確答案和資料的規律性來做更好的學習，一個有名的例子是Facebook的人臉辨識標記功能，有部分已經被用戶標記的照片，這屬於有Label的&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;，但有更多沒有標記的照片，這些照片也可以幫助ML學習。&lt;/p&gt;
&lt;p&gt;那還有像是訓練小狗的方法，當我跟小狗說坐下，如果牠真的坐下了，這個時候我就給牠獎勵，譬如說餵牠好吃的食物，久而久之牠就會學會聽從這個命令，&lt;strong&gt;「Reinforcement Learning」(強化式學習）&lt;/strong&gt;就是不直接表明&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;的Label，但是機器能在嘗試中得到&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;結果的好壞，再從這個好壞當作回饋去優化它的學習。&lt;/p&gt;
&lt;p&gt;Data給的方法也可以有很多種類。&lt;/p&gt;
&lt;p&gt;剛剛舉的ML例子都是屬於&lt;strong&gt;「Batch Learning」&lt;/strong&gt;，也就是一次給你所有的Data。另外一種給Data的方法叫做&lt;strong&gt;「Online Learning」&lt;/strong&gt;，這個情形下Data會一個一個以序列的方式餵給機器，這麼方式下的Model可以隨時更新。最後一種方式是&lt;strong&gt;「Active Learning」&lt;/strong&gt;，機器不僅是被動的接受 Data，而是會根據它自己的需求向使用者索取它想要的Data。&lt;/p&gt;
&lt;p&gt;另外，除了有輸出值&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;有多種種類之外，輸入的變數&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;的來源也有很多種，我們稱之為Features。&lt;/p&gt;
&lt;p&gt;如果具有物理意義的輸入變數，稱之為&lt;strong&gt;「Concrete Features」&lt;/strong&gt;，這些變數建立在人類知識的預先處理。還有輸入變數並不具有物理含意的情形，這稱之為&lt;strong&gt;「Abstract Features」&lt;/strong&gt;。那有些情形下直接採用不加以處理的原始數據，稱為&lt;strong&gt;「Raw Features」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;而使用工人智慧由人力從Raw Features中萃取出Concrete Features，這叫做Feature Engineering。相反的，現在很夯的Deep Learning厲害的地方是他可以自行從Data中學習 Features。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;總結一下，機器學習有很多種型態，從Data的給予方式可分為Batch Learning、Online Learning和Active Learning。Data的表達形式由輸入變數&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;和輸出值&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;所決定，從輸入變數&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;的來源可分為Concrete Features、Raw Features和Abstract Features，從輸出值&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;的種類上可以分為二元分類、多元分類、Regression和Structured Learning 問題，從輸出值&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;的Label給予情況可分為Supervised Learning、Unsupervised Learning、Semi-supervised Learning 和 Reinforcement Learning。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;順道一提，這16堂課裡頭主要聚焦在探討Batch Supervised Learning with Concrete Features。&lt;/p&gt;
&lt;h3 id="_4"&gt;後話&lt;/h3&gt;
&lt;p&gt;這篇文章帶大家初探了一眼機器學習，介紹了機器學習的架構和種類，以及它的使用時機，還有介紹了整門課非常重要的二元分類問題。但是講這麼多，機器學習真的可能嗎? 那如果可以做到，會需要哪一些要素呢? 這就必須深入理論之中，才能找到答案，在下一篇文章裡，我將介紹這門課的第二個部分：Why Can Machines Learn? &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習基石"></category></entry></feed>