<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>YC Note - AI.ML</title><link href="https://ycc.idv.tw/" rel="alternate"></link><link href="https://ycc.idv.tw/feeds/aiml.atom.xml" rel="self"></link><id>https://ycc.idv.tw/</id><updated>2022-11-26T12:00:00+08:00</updated><subtitle>ML/DL Tech Blog</subtitle><entry><title>極簡說明Multi-thread/Multi-process、CPU-bound/IO-bound和GIL</title><link href="https://ycc.idv.tw/multithread-multiprocess-gil.html" rel="alternate"></link><published>2022-11-26T12:00:00+08:00</published><updated>2022-11-26T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2022-11-26:/multithread-multiprocess-gil.html</id><summary type="html">&lt;p&gt;Python GIL 究竟怎麼影響我們應該採用Multi-thread或Multi-process？ Multi-thread和Multi-process又是什麼？什麼是CPU-bound和IO-bound呢？&lt;/p&gt;</summary><content type="html">&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Multi-Thread中每個Thread會共享資源，因此其優點是：資源利用較有效率、共享資源存取較快速&lt;/li&gt;
&lt;li&gt;Multi-process將這些資源複製且隔離，其優點是：Process間不共享記憶體、程式碼比較好寫&lt;/li&gt;
&lt;li&gt;特別注意：不管在 Multi-Thread 和 Multi-process 都可以吃到完整的多顆CPU，因為所有工作都會丟到OS Thread Scheduler，Scheduler會管控CPU的使用，但是Python可能有GIL需另外討論。&lt;/li&gt;
&lt;li&gt;等待CPU計算較多的工作稱為CPU-bound Task，等待DMA拉資訊進記憶體較多的工作稱為IO-bound Task。&lt;/li&gt;
&lt;li&gt;Python 在原始 default 的編譯器是採用CPython，而 CPython 採用了GIL，這導致了一個Python Job（一個Process）同一時間只能跑一個Thread。最終導致，CPython 的 Multi-thread 不能同時吃到多顆CPU，因此無法解決CPU-bound Task的問題。&lt;/li&gt;
&lt;li&gt;粗略的原則：在CPython的情況下，遇到CPU-bound Task使用Multi-process，遇到 IO-bound Task使用Multi-thread。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Multi-Thread and Multi-Process&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/CS/thread_and_process.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single Process Single Thread&lt;ul&gt;
&lt;li&gt;Single Process中記憶體存放Code、Data&lt;/li&gt;
&lt;li&gt;Single Thread 進行中需使用 Stack 來追蹤紀錄&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Single Process Multi-Thread&lt;ul&gt;
&lt;li&gt;單一Process中的多個Thread共享資源（記憶體存放的Code、Data）&lt;/li&gt;
&lt;li&gt;也因為共享資源的緣故，要小心撰寫程式碼避免dead lock和race condition等問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multi-process&lt;ul&gt;
&lt;li&gt;每個Process都有自己一份的Code、Data，process彼此間不共享這些資源&lt;/li&gt;
&lt;li&gt;所以開許多Process比較吃記憶體&lt;/li&gt;
&lt;li&gt;而且如果想要讓Process之間共享資訊也會比較不容易，需要要做 serialize、memory copy、de-serialize 等等 OS 的開銷&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;譬喻：&lt;ul&gt;
&lt;li&gt;Process 像是一間工廠，工廠裡面有許多器具（Code、Data）&lt;/li&gt;
&lt;li&gt;Thread 像是裡頭的工人&lt;/li&gt;
&lt;li&gt;Multi-Thread就像是一間工廠有多個工人，這樣的話那就需要避免他們互搶設備或互相等待&lt;/li&gt;
&lt;li&gt;Multi-Process就像是蓋多間工廠，蓋更多工廠意味著更多的資本支出，而且工廠和工廠間要資源交換比較不容易&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Multi-Thread v.s. Multi-process&lt;ul&gt;
&lt;li&gt;Multi-Thread 優點：資源利用較有效率；共享資源存取較快速&lt;/li&gt;
&lt;li&gt;Multi-Thread 缺點：因為共用記憶體，所以使用上要注意dead lock和race condition等問題；程式碼比較難寫&lt;/li&gt;
&lt;li&gt;Multi-processing 優點：process之間不共享記憶體；程式碼比較好寫&lt;/li&gt;
&lt;li&gt;Multi-processing 缺點：資源利用比較沒效率，較佔記憶體；共享資源需要額外開銷，有一些東西沒辦法serialize處理上就很麻煩；如果有跨Process的分享資源的話，仍需注意dead lock和race condition等問題&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;特別注意：不管在 Multi-Thread 和 Multi-process 都可以吃到完整的多顆CPU，因為所有工作都會丟到OS Thread Scheduler，Scheduler會管控CPU的使用，但是Python情況特殊我們待會討論。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;CPU-bound and IO-bound&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;CPU-bound&lt;ul&gt;
&lt;li&gt;當某個工作高度仰賴CPU計算，我們會稱之為CPU-bound Task&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IO-bound&lt;ul&gt;
&lt;li&gt;當某個工作大量等待外部存取（例如：從硬碟讀寫、網路溝通），我們會稱之為IO-bound Task&lt;/li&gt;
&lt;li&gt;外部存取：意味著從外部將資訊拉近記憶體，此工作主要由DMA來完成，CPU在這過程並不需要參與，所以會有一段等待的時間&lt;/li&gt;
&lt;li&gt;DMA（Direct Memory Access）允許某些電腦內部的硬體子系統，可以獨立地直接讀寫系統記憶體，而不需CPU介入處理 。很多硬體的系統會使用DMA，包含硬碟控制器、繪圖顯示卡、網路卡和音效卡。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;GIL (Global Interpreter Lock)&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/CS/cpython_thread_and_process.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 在原始 default 的編譯器是採用CPython，而 CPython 採用了GIL，這導致了一個Python Job（一個Process）同一時間只能跑一個Thread，這意味著就算有多顆CPU的存在，同時間還是沒辦法吃一顆以上的CPU，也因此會有以下情況。&lt;/li&gt;
&lt;li&gt;當我們需要處理CPU-bound Task時，如果採用Multi-thread，因為GIL的緣故導致無法同時使用多顆CPU，也因此計算效能無法提升。相反如果採用Multi-process，因為開了好幾個Process，每個Process都有自己獨立的GIL，因此才能吃到多顆CPU，進而解決CPU-bound的問題。&lt;/li&gt;
&lt;li&gt;當我們需要處理IO-bound Task時，如果採用Multi-thread，因為CPU的使用量並不高，大部分thread的時間都是在等待IO，所以多個thread其實就足夠解決IO-bound問題。當然如果使用Multi-process也是可以做到，但是這會多造成資源的浪費。&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI.ML"></category><category term="CS"></category></entry><entry><title>Tesla AI Day 2022 筆記</title><link href="https://ycc.idv.tw/tesla-aiday2022.html" rel="alternate"></link><published>2022-10-15T12:00:00+08:00</published><updated>2022-10-15T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2022-10-15:/tesla-aiday2022.html</id><summary type="html">&lt;p&gt;在2022年舉辦的Tesla AI Day上，他們推出了人形機器人Optimus，以及介紹他們在自動駕駛上的技術推進。這篇是YC觀賞完AI Day後的筆記，摘要了一些重點，並且在最後提出我的觀點。&lt;/p&gt;</summary><content type="html">&lt;p&gt;原始影片：&lt;a href="https://www.youtube.com/watch?v=ODSJsviD_SU&amp;amp;t=9274s"&gt;youtube&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Tesla Bot: Optimus&lt;/h2&gt;
&lt;h3&gt;Demo&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/optimus.gif"&gt;&lt;/p&gt;
&lt;p&gt;上次 Tesla AI Day 預告的人形機器人 Optimus 終於亮相了，Musk 說開發這個機器主要是要幫助人類完成一些枯燥或危險的工作，為了達到這個目的，Optimus 的手指有特別設計，可以靈巧的完成人類的工作，Musk 強調說目前市面上大部分的機器人都缺乏大腦，而且售價昂貴，Optimus 未來售價將壓在2萬美元以下，並且預計在明年開始量產。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/optimus_2.png"&gt;&lt;/p&gt;
&lt;p&gt;Optimus 目前已經可以完成一些基本動作。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/optimus_3.png"&gt;&lt;/p&gt;
&lt;p&gt;Optimus 採用在自動駕駛上使用的視覺系統，含有語義分割的功能，所以在 Optimus 的眼中它可以認得出它自身的部分、地板、花圃、澆水器、等。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/optimus_4.png"&gt;&lt;/p&gt;
&lt;h3&gt;Hardware Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/human_form.png"&gt;&lt;/p&gt;
&lt;p&gt;Optimus 以人的身體來打造，全身上下有多於200個自由度，自由度可以想像可以彎曲的地方，類比於人類的關節，而手有多達27個自由度，從影片仔細觀察 Optimus 的大拇指甚至可以向內折去碰觸小指頭，相當的細緻。而 Optimus 的重量也跟一個成人差不多約73公斤。並且 Tesla 團隊特別設計讓 Optimus 在待機的狀態下可以比較省電。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/latest_generation.png"&gt;&lt;/p&gt;
&lt;p&gt;上圖橘色的部分為 Actuators（致動器，是一種將能源轉換成機械動能的裝置），而藍色的部分為電力系統，Optimus 同樣裝備著如電動車上的晶片，這晶片可以用來做影像處理、路線規劃，甚至他們希望未來我們可以跟 Optimus 溝通，所以裝備著無線連線器和聲音接收器。&lt;/p&gt;
&lt;h3&gt;Hardware Simulation&lt;/h3&gt;
&lt;p&gt;延續 Tesla 造車的經驗，Tesla 透過物理引擎模擬來設計機械結構，舉例：利用物理引擎模擬電動車撞擊的狀況來安全的設計車體結構。同樣的，可以透過這個物理引擎來模擬 Optimus 跌倒的狀況，並且設計 Optimus 在各種跌倒狀態下能避免電池和電路板等重要地方嚴重損壞。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/structure_for_mass_production.png"&gt;&lt;/p&gt;
&lt;p&gt;同樣的，可以利用物理引擎來模擬機器人走路的受力狀況，並且設計出好的結構來應付各類動作。物理引擎可以有效的節省實體試錯的時間。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/knee_joint.png"&gt;&lt;/p&gt;
&lt;p&gt;例如在設計膝關節也得益於物理引擎的幫忙，為了仿造膝蓋關節的結構 Tesla 經過模擬後採用「四軸系統」。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/knee_requirement_from_tasks.png"&gt;&lt;/p&gt;
&lt;p&gt;上圖綠色的曲線代表四軸系統，而藍色的曲線代表兩軸系統，其呈現的是在關節不同角度下的模擬受力狀況，可以發現兩軸系統在膝蓋彎曲的時候受力相當大，而四軸系統則能穩定維持低受力狀況，因此四軸系統比較有效率。&lt;/p&gt;
&lt;h3&gt;Actuators（致動器）＆ Hand&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/actuator_1.png"&gt;&lt;/p&gt;
&lt;p&gt;由電驅動的致動器已經用於電動車上，但電動車只需要前後兩個致動器，而為了支援人形機器人的各類動作，例如：走路、跑步、搬重物、爬樓梯等，Optimus 需要使用到28個致動器。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/actuator_2.png"&gt;&lt;/p&gt;
&lt;p&gt;為了要選擇和設計這28個致動器，透過讓機器人走路和轉彎模擬並畫出各致動器的力矩-速度的軌跡，有了這個軌跡圖我們就可以計算某個致動器的耗能狀況，進而推算出其系統成本。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/actuator_3.png"&gt;&lt;/p&gt;
&lt;p&gt;針對某一個位置，Tesla 團隊模擬了好幾十萬顆致動器並得到所有致動器在系統成本-致動器重量的分布，我們想要選擇系統成本越低、重量越輕的致動器，所以會選擇最靠近左下角原點的致動器，透過 Pareto Front 我們可以找出那最佳的致動器。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/actuator_4.png"&gt;&lt;/p&gt;
&lt;p&gt;在28個位置都可以做同樣的計算，因此我們得到28種最佳的致動器，但是考慮到未來量產，28種款式顯然太多了，縱使因為左右對稱，可以省一半，但仍然還是太多款式，因此 Tesla 團隊透過 Commonality Study（共性研究）在權衡之下，使用6種致動器裝配在這28個地方。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/actuator_5.png"&gt;&lt;/p&gt;
&lt;p&gt;這6種致動器如上圖所示，上排為旋轉式、下排為線性式。而講者特別展示他們的致動器有能力舉起一架三腳鋼琴，這並不是花俏的功能，這是必須要有的，因為人類的某些肌肉也同樣有此張力。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/hands.png"&gt;&lt;/p&gt;
&lt;p&gt;為了要打造像人類一樣靈巧的雙手，Optimus 手部使用了6個致動器，並提供了11個自由度。擁有可調整的抓握能力，可以依照情境去調整抓握的方法與力氣，可以提取約9公斤的重物，並且使用工具，甚至是相當小的物件。而特別有趣的一點是，Optimus 的手有嵌入 Sensors，所以在抓握的時候 Optimus 可以了解它與物件接觸的狀況來調整抓握的方法。&lt;/p&gt;
&lt;h3&gt;Optimus Software&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/robot_occupancy.png"&gt;&lt;/p&gt;
&lt;p&gt;Optimus 的視覺系統採用和自駕車相同的 Occupancy Network（詳見下方介紹），而因為通常在室內，所以沒有GPS可以參考，因此 Optimus 更仰賴於視覺系統。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/visual_navigation.png"&gt;&lt;/p&gt;
&lt;p&gt;為了能在室內行走而不去撞到東西，Optimus 建基在 Occupancy Network 上預測圖上藍色的高頻關注點，可以控制避免 Optimus 去撞上這些點的同時，也可以可以利用這些點的實時變化來了解機器人自身的動作。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;讓機器人走路並不是件容易的事情，要考慮以下幾件事：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;對物理自身的了解：譬如腳的長度、重量、腳掌的長度&lt;/li&gt;
&lt;li&gt;怎樣的走路姿態是節能的&lt;/li&gt;
&lt;li&gt;平衡&lt;/li&gt;
&lt;li&gt;協同動作&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/locomaotion_plan_1.png"&gt;&lt;/p&gt;
&lt;p&gt;為了考量這些，Tesla 的作法是利用模擬去訂定移動計畫，並且規劃出軌跡，包括：左腳掌和右腳掌要踏的位置、腳掌的移動軌跡、骨盆的移動軌跡、等等。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/locomaotion_plan_real_world.png"&gt;&lt;/p&gt;
&lt;p&gt;但縱使使用模擬周全考慮後，在真實世界還是存在著誤差，例如：機器人可能會振動偏移、Sensors 也存在著觀測誤差，所以有許多狀況是超出模擬之外的，這導致一開始機器人是不穩定的，很容易跌倒。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/motion_control.png"&gt;&lt;/p&gt;
&lt;p&gt;所以 Tesla 提出了修正，建基在模擬預測之上，Optimus 使用 Sensors 去估計當下狀態，計算當下狀態與理想值的差異，並進行實時調整，才終於讓 Optimus 可以在正常世界行走。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/natrual_motion_reference.png"&gt;&lt;/p&gt;
&lt;p&gt;為了要讓 Optimus 可以做許多人類的操作，Tesla 團隊蒐集了許多由人類示範的操作動作紀錄，把它們存成一個 Library。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/online_motion_adaption.png"&gt;&lt;/p&gt;
&lt;p&gt;而就算是拿箱子的這一個動作有存在 Library 中，但是實際狀況箱子可能有大有小、有輕有重，也可能如上圖所示放在不同的地方、高度，所以我們不能直接使用 Library 中的示範動作，因此需要透過一個軌跡優化程式來調整雙手的軌跡，才能正確的完成任務。&lt;/p&gt;
&lt;h2&gt;Full Self Driving&lt;/h2&gt;
&lt;h3&gt;Intro.&lt;/h3&gt;
&lt;p&gt;全自動駕駛從去年的2千個使用者，到今年2022已經提升到16萬個使用者，總共經歷了35個版本、訓練了近7.5萬個模型，大約每8分鐘就有一個新的模型。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/fsd_2022.png"&gt;&lt;/p&gt;
&lt;p&gt;這是今天的大綱，Neural Networks 的部分包含了 Occupancy Networks 和 Lanes &amp;amp; Objects Networks，Occupancy Networks 作為底層描述幾何關係的預測，目標是從影像資料轉成實體世界，可以預測樹、牆、建築物、車，甚至預測它未來的移動方向；除此之外，Lanes &amp;amp; Objects Networks 提供了更詳細的語義預測。Training Data 的部分有去年提過的 Auto Labeling 和 Simulation，Data Engine 的部分則是描述他們針對錯誤的預測做改正的系統性作法。Planning 則是自動駕駛計畫它軌跡的程式。&lt;/p&gt;
&lt;h3&gt;Planning&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/assert_to_pedestrain.png"&gt;&lt;/p&gt;
&lt;p&gt;計畫行車軌跡是相當複雜的，其需要考慮交互關係，如上圖，紅色的車輛想要左轉，卻遇到左前方的行人和右前方疾駛的車輛，它應該怎麼行駛呢？當下急轉顯然不是一個好行為，可能會撞到行人，所以人類駕駛會選擇讓行人先通過，再左轉從行人後方通過，但如果選擇這樣的路徑，右方疾駛的車輛可能會撞上，所以更好的作法是等待行人通過以及等待右側車輛通過再左轉。所以好的自動駕駛應該要能考慮接下來路徑的所有交互作用，但是這一切計算只能在50ms內完成，如果有大約20個物件要考慮，大概需要考慮100種的交互作用，要如何在這麼短的時間內將一切都考慮進去呢？Tesla 採用平行計算的樹搜尋法。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/interaction_search_1.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/interaction_search_2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/interaction_search_3.png"&gt;&lt;/p&gt;
&lt;p&gt;基於預測的 3D Vector Space，可以計算出多條可能的路徑，針對每一種路徑 Tesla 會逐一的檢查所有可能的交互作用，最後挑選出最佳的路徑，這個過程可以看作一個 Tree Search，並且可以採用平行運算來加速這整個過程。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/interaction_search_4.png"&gt;&lt;/p&gt;
&lt;p&gt;在樹中的每一個節點都會經歷三個歷程：路徑生成、路徑評分、基於交互作用或中間目標的分岔。&lt;/p&gt;
&lt;p&gt;路徑生成的部分，他們曾經嘗試使用物理數值優化，但這過程太過緩慢，平均一個動作需要花費1-5ms，如果有100個動作則最少需花費100ms，這超出了我們的限制，所以他們接著考慮使用神經網路的方法，讓神經網路學習人類的操作示範或數值優化後的路徑，如此一來便可以將一個動作壓在約100us，成功的解決問題。&lt;/p&gt;
&lt;p&gt;路徑生成過後，他們會針對路徑作評分，會考慮剛剛提到的「可能撞擊檢查」，好的路徑要禮讓行人、避免與其他車輛擦撞，同時也考慮了「行車舒適度」，並且使用神經網路的方法去評估「車主取消自動駕駛接管的可能性」和「這路徑像不像人類的行駛方法」。&lt;/p&gt;
&lt;h2&gt;Occupancy Network&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/occupancy_network.png"&gt;&lt;/p&gt;
&lt;p&gt;Occupancy Network 作為底層描述幾何關係的預測，目標是從8顆鏡頭影像資料轉成實體世界，並提供所需的語義。它有以下特色：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;能描述 Volumetric Occupancy （體積佔用）&lt;/li&gt;
&lt;li&gt;考慮了多鏡頭、影片的上下文&lt;/li&gt;
&lt;li&gt;能穩定的預測物件，不容易出現物件跳躍的情況&lt;/li&gt;
&lt;li&gt;同時能預測物件的語義&lt;/li&gt;
&lt;li&gt;同時能預測物件的動向&lt;/li&gt;
&lt;li&gt;有效率的記憶體使用和計算能力&lt;/li&gt;
&lt;li&gt;~10ms 的更新率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而上圖為其結構，其實跟去年相去不遠，可以說是將去年提及的 HydraNets 和類似 NeRF 描述 Voxels (可以想成在3D中的Pixels) 的技術相結合，去年提及的部分就不贅述了（請詳見 &lt;a href="https://www.ycc.idv.tw/tesla-aiday2021.html"&gt;Tesla AI Day 2021 筆記&lt;/a&gt;），讓我們專注的討論今年值得一提的更新或更詳細的內容，如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 Deconvolutions 後，會進行 Volume Outputs 和 Surface Outputs 的預測，除了預測幾何關係外，還會預測語義和動向&lt;/li&gt;
&lt;li&gt;為了增加預測的解析度，他們設計了 Queryable Outputs 將原本 Volume Outputs 的資訊壓入 MLP 中，接著就可以查詢這個 MLP 來得到解析度更高的資訊，你可以想像在這個過程模型考量了許多資訊後給出了良好的內插結果，所以可以得到更高解析度的資訊&lt;/li&gt;
&lt;li&gt;同時，他們也使用類似 NeRF 的方法來描述 Voxels，今年他們給出了相關的論文— &lt;a href="https://arxiv.org/abs/2112.05131"&gt;Plenoxels: Radiance Fields without Neural Networks, Alex Yu et al., 2022&lt;/a&gt;，這個技術不需要神經網路也能訓練 NeRF，並且在品質些微提升的情況下降低了時間複雜度兩個數量級&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Training Infra.&lt;/h3&gt;
&lt;p&gt;為了要應付龐大的計算量，Tesla 裝備著14K顆GPU，其中4K顆作 Auto Labeling 之用、10K顆作模型訓練之用；並且裝備著30PB的分散式影片快取記憶體。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/optimizing_video_model_training.png"&gt;&lt;/p&gt;
&lt;p&gt;但是並不是將大量的GPU和大容量的記憶體裝上去就完事了，Tesla 團隊還為底層做了很多的優化，如上圖所示，例如，擴充原本的 PyTorch 加速影片訓練速度、設計新的儲存格式 .smol 來降低容量和減少輸出寫入的操作次數。在優化過後，訓練 Occupancy Network 的速度提升了2.3倍。&lt;/p&gt;
&lt;h3&gt;Lanes&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/fsd_lanes.png"&gt;&lt;/p&gt;
&lt;p&gt;縱使我們已經使用 Occupancy Network 重建了整個實體世界，包括標線、車輛、紅綠燈，但行車的路線仍然不是顯而易見的，如上圖所示你可以了解行車路線有多麼複雜，它包含著行車方向、變換車道、道路切換，所以我們希望使用神經網路來預測所有可能的路線。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/fsd_lanes_nn.png"&gt;&lt;/p&gt;
&lt;p&gt;FSD Lanes Neural Network 分為三個部分，Vision Component 設計的如同 Occupancy Network 的 Backbone 一樣；Map Component 則是希望融入 Navigation Map 的資訊，包括：路的幾何和拓撲、行車方向、車道數量、車道的拓撲、是否為公車道、是否為高乘載車道、.. 等；Language Component 則是希望以稀疏的形式輸出所有可能的路線，&lt;strong&gt;這邊的作法是借鏡了語言生成（如：GPT3）的方法，將路線化作一種語言，希望模型在參考視覺之後以 Autoregression 的方式生成這個關於路線的語言&lt;/strong&gt;，以下詳細說明。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/lang_of_lanes_1.png"&gt;&lt;/p&gt;
&lt;p&gt;右側是鳥瞰圖，我們要在其上預測所有路徑，左側是 Autoregressive Decoder，這邊採用多層次的預測，要注意每一層的預測皆有使用 Cross Attention 參考影像及地圖的資訊。首先要預測節點位置，但因為計算量的考量，所以採用先在大的網格上預測位置，這次輸出是 index 18，我們可找到它並反白它的位置。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/lang_of_lanes_2.png"&gt;&lt;/p&gt;
&lt;p&gt;接下來接續著預測它細緻的位置，此時輸出index 31，我們可以找到它並反白，此時已經確認了點位。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/lang_of_lanes_3.png"&gt;&lt;/p&gt;
&lt;p&gt;接著預測這個點的「拓撲類型」，這邊預測的是 "Start"，也就是道路的起始點。我們可以將這些所有輸出組合成為一個 Character，然後接續的輸出下一個 Character 直到結束，是不是很像一種語言！&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/lang_of_lanes_4.png"&gt;&lt;/p&gt;
&lt;p&gt;當我們接續的輸出下一個 Character，這次是輸出黃色的點位，其拓撲類型為 "Continue"，這意味著與上一個點相連，而相連的軌跡則由 Spline Coefficient Predictor 預測，它會透過預測係數來描述兩點連接的平滑曲線的形狀，如此一來我們就使用語言的方式連出一條「路線」。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/lang_of_lanes_5.png"&gt;&lt;/p&gt;
&lt;p&gt;當然一個點可能有包含多條岔路，上面的例子預測藍綠色的點，其拓撲類型為 "Fork"，而 Fork Point Predictor 預測 index 0，代表是從第0個位置的點分岔出來，而 Spline Coefficient Predictor 同樣的預測兩點連線的曲線。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/lang_of_lanes_6.png"&gt;&lt;/p&gt;
&lt;p&gt;這個操作一路的進行下去，直到出現拓撲類型為 "End of sentence" 為止，我們就成功的建構了所有的路現，是不是很聰明的想法！順道一提這個技術同樣的用在機器人 Optimus 上。&lt;/p&gt;
&lt;h3&gt;Auto Labeling&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/auto_labeling_for_lanes_net.png"&gt;&lt;/p&gt;
&lt;p&gt;在2018年 Tesla 還在 Image Space 上作標注，那時每一個 Clip 需要標注533小時。而目前使用的 multi-trip 版本每個 Clip 只需標注不到6分鐘，Auto Labeling 的技術帶來巨大的改變。其作法和去年的差不多（請詳見 &lt;a href="https://www.ycc.idv.tw/tesla-aiday2021.html"&gt;Tesla AI Day 2021 筆記&lt;/a&gt;），他這邊特別強調三個步驟來達到這樣的效果：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;高精度的軌跡：使用模型在每個軌跡上預測高精度的資訊，在目前任務中這個資訊就是指 Lanes&lt;/li&gt;
&lt;li&gt;多旅程重建：因為一條路徑可能有多台 Tesla 電動車在不同時間不同天氣開過無數遍，所以可以將這些資訊疊加在一起去雜訊，建構出更穩固的 Label&lt;/li&gt;
&lt;li&gt;運用於新路徑：當然 Auto Labeling 也可以運用在新的路徑的 Label 生成&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/auto_labeling_failed_case.png"&gt;&lt;/p&gt;
&lt;p&gt;當然還有一些時機 Auto Labeling 並不能表現的很好，例如：缺乏光線、濃霧、遮擋和雨天，還有待加強，不過不用擔心的是，因為在同一個路段會有不同時機的 Clip，所以縱使當下表現不好，也可以拿過去的資訊來彌補。&lt;/p&gt;
&lt;h3&gt;Simulation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;可以亂數生成行車路徑，再基於此路徑生成路面、行道樹、建築、天空，並在合理的位置生成紅綠燈，並且創造車輛行駛在可能的路徑上，這一切只要幾分鐘就可以用模擬器自動生成&lt;/li&gt;
&lt;li&gt;同一個場景，模擬器可以產生不同天氣、日光&lt;/li&gt;
&lt;li&gt;甚至可以基於 Google 地圖去生成模擬城市&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Data Engine&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/how_data_helps.png"&gt;&lt;/p&gt;
&lt;p&gt;Tesla 建構一個標注的流程，讓標注人員針對模型預測錯誤的部分進行修正，並且將這些資料放到資料集當中。舉例，預測停車的問題，上圖中模型預測車子不處於停車的狀態，但標注人員從 Clip 看出車內沒有人，所以他手動標注為停車狀態，而這筆資料就相當有價值，會放進去讓模型重新訓練。和學術界不同，Tesla 不固定他的資料集，而是持續擴增他的資料集，為的就是確保在各種情況都能安全的駕駛。&lt;/p&gt;
&lt;h2&gt;我的觀點&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tesla 花不到一年的時間打造了人形機器人 Optimus，雖然相比於 Boston Dynamics 的 Atlas 機器人來說，Optimus 像個小嬰兒一樣，但不到一年就有如此成果，個人覺得已經是相當快速了，要知道雙足行走是相當困難的。&lt;/li&gt;
&lt;li&gt;我們從設計來看，Tesla 確實想要打造一台可為人類工作、低價、可量產的機器人。Optimus 具備純影像視覺系統，可以感知物件並了解其語義，並且仿造人類來設計機身，手部的靈活度也是少見於機器人的，因此這樣的機器人才有機會可以做到像人的動作，才有機會讓它來幫助人類完成一些枯燥或危險的工作。並且在設計初期，他們已經在考慮未來如何降低售價和量產了，我們看到他們如何利用模擬來找到低成本和低重量的致動器，我們看到他們使用共性研究將28種致動器降至6種。因此我是相當期待 Optimus 接下來的發展的！&lt;/li&gt;
&lt;li&gt;個人認為如果想要打造可為人類工作的機器人還有一件很重要的事情，很可惜它沒在這場演講被提及和說明，那就是「互動介面」，例如我想要機器人幫我洗衣服，我可能必須指示它一些步驟，例如：把衣服從籃子拿出來、放到洗衣機、加入洗衣劑、等，但我怎麼告訴機器人。是想要透過語音輸入嗎？可是語言理解是個大坑，在語音和語言理解這塊，Tesla 在資料上和 Know-how 上並沒有優勢，所以也許要透過如此自然的方法互動還需要好幾年的時間開發。抑或是想要透過程式操作？為了要能靈活操作各類任務，這勢必要設計一種程式框架來描述這模糊的世界，並且這樣的框架還可以跟影像視覺系統緊密結合，我會非常期待看到這樣的程式框架。期待明年我的疑問可以得到解答。&lt;/li&gt;
&lt;li&gt;自動駕駛的部分與去年相去不遠，但是今年有一些新東西和更仔細的說明。路徑規劃的部分有說明應該如何考慮與其他物件的交互作用，Occupancy Network 的部分有提到他們生成 Voxels 技術是採用不需要神經網路也能訓練的 NeRF。最令人驚艷的是 Lane Networks 採用語言生成的方法來預測所有可能的行車路線。&lt;/li&gt;
&lt;li&gt;去年我留下的疑問（怎麼利用駕駛人操作的資料來優化？）今年得到了解答，今年有提到使用物理引擎來規劃路徑速度太慢，所以他們使用神經網路來預測路徑，而這神經網路就是訓練在駕駛人操作的資料之上的。&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI.ML"></category><category term="Tesla"></category></entry><entry><title>Tesla AI Day 2021 筆記</title><link href="https://ycc.idv.tw/tesla-aiday2021.html" rel="alternate"></link><published>2022-10-09T12:00:00+08:00</published><updated>2022-10-09T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2022-10-09:/tesla-aiday2021.html</id><summary type="html">&lt;p&gt;在2021年舉辦的Tesla AI Day上，Tesla 揭露了他們開發自動駕駛的技術，並且預告他們將開發人形機器人。這篇是YC觀賞完AI Day後的筆記，我摘要了一些重點，並且在最後提出我的觀點。&lt;/p&gt;</summary><content type="html">&lt;p&gt;原始影片：&lt;a href="https://www.youtube.com/watch?v=j0z4FweCy4M&amp;amp;t=7750s"&gt;youtube&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How do we make a car autonomous?&lt;/h2&gt;
&lt;h3&gt;Vision&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/convert_to_vector_space.jpg"&gt;&lt;/p&gt;
&lt;p&gt;在 Full Self-Driving（FSD）的任務當中，需要利用多顆鏡頭的影像來重建出 3-D 的 Vector Space，而在這 Vector Space 中我們就可以靠著 Planing Algorithm 來駕駛汽車。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/detection_head.png"&gt;&lt;/p&gt;
&lt;p&gt;在簡單的 FSD 原型中，可以輸入一個 Frame 的圖片進入 Feature Extractor，這裡採用RegNet [&lt;a href="https://arxiv.org/abs/2003.13678"&gt;Designing Network Design Spaces, Radosavovic et al., 2020&lt;/a&gt;]，並且採用 Bi-directional Feature Pyramid Network (BiFPN) [&lt;a href="https://arxiv.org/abs/1911.09070"&gt;EfficientDet: Scalable and Efficient Object Detection, Mingxing Tan et al., 2019&lt;/a&gt;] 來融合不同規模的資訊，Andrej 舉例當一輛車子在低解析度時無法被確認，此時如果使用高解析度的資訊就可以判別。最後就可以在其上設計 Detection Head，可以做分類任務或迴歸任務。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/hydranet.png"&gt;&lt;/p&gt;
&lt;p&gt;而在 FSD 中需要更多的資訊，需要物件偵測、交通號誌偵測並判別、車道預測等等，所以需要多任務的訓練，所以 Tesla 採用多個 Detection Head，整個結構稱為 HydraNets。這樣的結構帶來三點好處，第一，因為共享 Feature，這會在 Inference 時更為有效率；第二，因為各項任務的解耦合，我們可以在不影響其他任務的情況下微調單一任務；第三，因為抽出了 Multi-scale Features 的緣故，我們可以將它存起來，這帶來了使用上的彈性，有時使用暫存的 Multi-scale Features 來微調任務，而有時可以進行 End-to-end 的訓練。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/hydranet_first_demo.gif"&gt;&lt;/p&gt;
&lt;p&gt;上面的demo片段是最初版本的 HydraNets 結果，特別強調這邊使用單一 Frame 的圖片進行預測，從畫面中你可以看到，模型標示著停車指示、停止線、車道邊緣、其他車輛（並指示是否為停車的狀態）、交通號誌等，看起來一切好像不錯，但是當我們想將所有鏡頭的資訊綜合轉成 Vector Space 就出現了問題。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/problem_per_camera_detection_then_fusion.jpg"&gt;&lt;/p&gt;
&lt;p&gt;上圖揭露了問題，在每一個鏡頭下的每一個 Frame 都能清楚的預測標線，但是當我們想要利用這些資訊建立 3-D 的 Vector Space 時候，就會發現其匹配的狀況並不良好，從鳥瞰圖看來所有的線並不能正確的貼齊，效果相當差，原因在於每個圖片在預測深度時存在著誤差，誤差疊加之後就造成 Vector Space 的訊號不穩定。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/learning_where_to_look_end_to_end.png"&gt;&lt;/p&gt;
&lt;p&gt;於是乎 Tesla 的研發團隊打算另闢蹊徑，&lt;strong&gt;假設我們直接 End-to-End 的讓模型直接輸出 Vector Space，是不是就可以解決這個問題&lt;/strong&gt;，舉個例子：當一輛卡車出現在八個鏡頭中的五個，如果使用每個鏡頭都獨立判斷的方式，模型難以感知這是同一輛卡車，但是如果我們可以綜合八顆鏡頭的資訊並且直接輸出 Vector Space，就有機會讓模型學習到這五個鏡頭內的卡車是同一輛，並且落在 Vector Space 的某個地方。要往這方向前進會先遇到兩個問題：1. 怎麼從 Image Space 轉成 Vector Space？ 2. 要做到 Vector Space 的預測，我們要有在 Vector Space 標注的資料集。問題2我們會在後續討論，而問題1的解法是引入 Self-attention [&lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention Is All You Need, Vaswani et al., 2017&lt;/a&gt;] 來結合八顆鏡頭的 Multi-scale Features 產生 Vector Space 的 Features。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/rectify_to_a_common_virtual_camera.png"&gt;&lt;/p&gt;
&lt;p&gt;由於每輛出場的車輛其八顆鏡頭的參數可能存在著差異，如果將這些有差異的影像輸入到單一模型就可能達不到原有的效果。而 Tesla 的作法是將每顆鏡頭先做 Camera Calibration (Rectify Layer)，具體的作法是將八顆鏡頭轉換到 Synthetic Virtual Camera 作校正。上圖右側顯示校正前後的差異，其中將後側鏡頭（Repeater）的影像疊加，可以發現校正後圖片變得清晰了，這意味著所有車輛在調整完鏡頭後影像更為一致。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/vector_space_edges_and_lines.jpg"&gt;&lt;/p&gt;
&lt;p&gt;上圖右側的鳥瞰圖是 End-to-end 的結果，預測在 Vector Space 成效就明顯的提升了。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/detection_singlecam_multicam.gif"&gt;&lt;/p&gt;
&lt;p&gt;在多顆鏡頭一同學習的幫助下，可以有效的解決遮擋的問題，能更穩定的預測周遭車輛。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/video_nn.png"&gt;&lt;/p&gt;
&lt;p&gt;Tesla 的研究小組發現有一些預測需要上下文的資訊，於是他們設計一個 Feature Queue 並將每一個時間段 Frame 的 Feature 推入，並且利用 Video Module 去綜合萃取 Feature，Video Module 能更加穩定的預測周遭車輛。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/spatial_rnn.png"&gt;&lt;/p&gt;
&lt;p&gt;除了時間尺度之外，他們還考慮了空間尺度，譬如左轉或右轉的標誌可能在先前的路上顯示，我們不能因為車輛等個紅燈就忘記了先前這些重要的資訊，所以除了時間尺度上的紀錄，我們還需要空間尺度上的紀錄，於是他們提出了 Spatial RNN 的方法在每一個空間點持續演化其 Feature，更棒的是不同 Tesla 車輛是可以共用這個空間 Feature Queue 的，所以經過不同車輛、不同時間、不同地點的大量蒐集資料，Tesla 精準的掌握豐富的資訊地圖。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/tesla_vision_summary.png"&gt;&lt;/p&gt;
&lt;p&gt;以上就是 Tesla Vision 模型的大架構。&lt;/p&gt;
&lt;h3&gt;Planning &amp;amp; Control&lt;/h3&gt;
&lt;p&gt;我們已經有了由 Tesla Vision 建立的 Vector Space，接下來我們就可以在這個空間裡開車，但這不是一件簡單的事，會遇到 Non-Convex    和 High-Dimensional 兩個問題。Non-Convex 意味著路線規劃中存在著多條足夠好的路徑，但是想要找到全局最優的那個路徑會變得困難，很有可能會卡在局部最優解；High-Dimensional 的原因是因為車輛需要為接下來10-15秒做規劃，要估計這一個時間區間的位置、速度、加速度，因此要描述一整條路徑是高維的。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/planning_and_control.gif"&gt;&lt;/p&gt;
&lt;p&gt;優化方法有兩種策略 — Discrete Search 和 Continuous Function Optimization，但這兩種策略都有侷限性，Discrete Search 用在 High-Dimensional 的情況下會造成計算複雜度相當高，而 Continuous Function Optimization 用在 Non-Convex 容易陷在局部最佳解，因此 Tesla 的解法是採用混合式，&lt;strong&gt;先使用 Discrete Search 來建立一條可走的通道，來侷限路徑，稱為 Convex Corridor，再使用 Continuous Function Optimization 的方法找出一條平穩的路徑&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/need_consider_other.png"&gt;&lt;/p&gt;
&lt;p&gt;另外，我們在做行車規劃時還需要考慮其他車輛的移動，例如遇到會車的狀況，自動駕駛要能分辨來車是否要讓道，如果對方讓道，我們的車子前進；如果對方不讓道前進，我們的車子要讓道。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/example_parking_problem.png"&gt;&lt;/p&gt;
&lt;p&gt;在一般道路，尤其是高速公路，路徑搜索還算簡單，但是有一些狀況卻是相當棘手的，例如：停車問題。如上圖，停車場充斥著許多障礙物，在路徑搜索中最常見的方法是使用 A* 演算法，但是這個簡單的方法展開次數要多達近40萬次，這樣的運算速度是不能接受的，所以 Tesla 再進一步加入行進方向導航，可以讓 A* 演算法進步到展開次數大概2萬次，但這仍然不夠快速。原因是因為目前作法並不能綜觀全局（使用鳥瞰圖）來找路徑，於是我們借助神經網路的幫忙，&lt;strong&gt;利用類似AlphaGO的方式來做路徑搜索，結果展開次數可以降到近300次而已，差異相當的巨大&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;How do we generate training data?&lt;/h2&gt;
&lt;h3&gt;Manual labeling&lt;/h3&gt;
&lt;p&gt;演算法僅僅提供了效能的上界，我們還是需要充足的資料來調整演算法的參數，所以建構一個品質良好且量大的資料集是重要的，因此 &lt;strong&gt;Tesla 聘僱了大約1000名專業的標注人員進行標注，並且開發了專屬的標注軟體&lt;/strong&gt;。在一開始他們是在2維的圖片上面標注，後來如上述所提及的，模型要直接預測 Vector Space 才能表現的夠好，所以他們開始在 Vector Space 上標注，直接標注在3維的空間加上時間尺度。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/labeding_tools.png"&gt;&lt;/p&gt;
&lt;h3&gt;Auto Labeling&lt;/h3&gt;
&lt;p&gt;人與電腦擅長的東西不同，人擅長語意相關的東西，而電腦擅長幾何、重建、追蹤等等，所以 Tesla 的標注軟體充分利用兩方的所長，進行人機協作，來更快速的建立良好的資料集，這就是Auto Labeling的技術。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/restructuring_the_road.png"&gt;&lt;/p&gt;
&lt;p&gt;Tesla 會從車輛上蒐集影片的資料，並上傳至伺服器 Dojo，利用這些資料我們就可以重建整個道路，如上圖左側的圖例，輸入 (x,y) 座標查詢地面高度、道路指示線、路緣等等。你可能會想利用三維網格來描述路面，但因為拓撲的限制，這樣的表示是不可微分的，並不適合重建或生成，所以他們實際的作法是用類似於Neural Radiance Fields (NeRF) [&lt;a href="https://arxiv.org/abs/2003.08934"&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis, Ben Mildenhall et al., 2020&lt;/a&gt;] ，&lt;strong&gt;NeRF 能做到利用多個角度鏡頭的拍攝來重建三維資訊（當然，必須要有這些鏡頭與拍攝物的空間相對關係）&lt;/strong&gt;，而 Tesla 的八顆鏡頭正提供了這個設置，NeRF 的精髓在於將物體和場景的資訊以隱性的方法編碼進 MLP 中，所以當我們輸入 (x,y) 座標到 MLP，我們可以獲得我們想要的空間資訊。經過大量的查詢 (x,y) 得到 z，我們就可以將路面給建構起來。&lt;/p&gt;
&lt;p&gt;更棒的是，每一輛行走的 Tesla 車輛都會持續蒐集並重建這些路段，有些路段還很有可能由多輛 Tesla 電動車一起建構來消除雜訊，所以愈多 Tesla 電動車在路面上開，獲得的資料就越全面、越精確，自動駕駛的穩定度就更高。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/walls_barriers_and_everythingelse.png"&gt;&lt;/p&gt;
&lt;p&gt;同樣的技術要重建靜態物也是沒問題的。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/and_that_solve_the_problem.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;利用這樣的 Auto Labeling 技術 Tesla 成功的在三個月內將光雷達給移除&lt;/strong&gt;，初期因為資料不夠充足，如果遇到視線不佳的情況，預測將會失準，如上圖左側所示。為了解決這個問題，Tesla 團隊從大量車輛中找尋類似視線不佳的影片一萬枚，並且利用 Auto Labeling 技術自動標注，大概只花了一週就完成，而這在過去採用純人工標記可能需要好幾個月，結果模型在加上這樣的資料集訓練後，就能在視線不佳的情況下做出良好的預測，如上圖右測所示，縱使視線不佳也能穩定的預測前方車輛，這為 Tesla 帶來移除光雷達的信心。&lt;/p&gt;
&lt;h3&gt;Simulation&lt;/h3&gt;
&lt;p&gt;Tesla 團隊也嘗試的使用模擬的方式人工合成影像資料來訓練模型。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/simulation.png"&gt;&lt;/p&gt;
&lt;p&gt;使用模擬的一大好處是 Label 會同時產生，他可以幫助我們創造一些不容易出現的場景，例如：有人在高速公路跑步，或者創造出不容易標注的場景，例如：有一群人在走動同時存在著大量的遮擋問題。人工合成資料可以補足真實世界資料的不足，那要怎麼創造以假亂真的合成資料呢？有五個特點：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;精確傳感器的模擬：模擬的目的不是為了好看，而是要能反應真實，例如在不同相機曝光度的調整下，模擬的要盡可能接近實際的狀況，為了做到這一點，他們對傳感器進行建模，包括：傳感器噪音、運動模糊、光學失真、前燈傳輸、擋風玻璃折射特性。&lt;/li&gt;
&lt;li&gt;保持模擬的擬真：使用抗鋸齒算法，甚至是光線追蹤技術，來讓模擬更真實&lt;/li&gt;
&lt;li&gt;多樣的場景配置&lt;/li&gt;
&lt;li&gt;基於算法的場景生成：盡量減少在場景生成時的人員介入，讓這項技術可以大量生成多樣的場景，雖然可能大部分的合成資料模型都預測的不錯，但是他們會針對那些模型沒辦法預測好的場景多產生一些資料讓模型學習&lt;/li&gt;
&lt;li&gt;場景重建：更神的是，他們可以將真實世界的片段轉化成為虛擬的場景，如此一來就可以針對困難的場景多生成多筆資料&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Dojo&lt;/h2&gt;
&lt;p&gt;為了應付自動駕駛的龐大計算量，Tesla 還自建了他們自己的超級電腦—Dojo。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/dojo.png"&gt;&lt;/p&gt;
&lt;p&gt;Tesla 幾乎重頭打造整個系統，從晶片設計，到集成電路，到計算叢集，還有軟體設計。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/dojo_training_node.png"&gt;&lt;/p&gt;
&lt;p&gt;上圖展示了訓練節點的內部結構，這是一個64位Superscalar CPU，圍繞著矩陣運算單元和向量SIMD進行優化。這個節點可以提供每秒1萬億次的浮點計算（FLOP）。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/dojo_chip.png"&gt;&lt;/p&gt;
&lt;p&gt;集成354個 Training Node，就得到中間黃色的計算陣列。在計算陣列周圍使用了576個高速低功耗的SerDes圍繞，使得該晶片擁有極高的I/O帶寬。結合這些就得到了 D1 Chip ，其採用 7 nm 製程，在645平方毫米下可容納500億個電晶體。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/dojo_chip_scaling.png"&gt;&lt;/p&gt;
&lt;p&gt;Dojo D1幾乎是業界第一。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/dojo_system.png"&gt;&lt;/p&gt;
&lt;p&gt;含有多個Dojo D1的集成電路，可提供每秒9萬億次的浮點計算（FLOP）。&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/dojo_software.png"&gt;&lt;/p&gt;
&lt;p&gt;得益於 Tesla 構建的強大的編譯器，只需要少許的改動原本的 Pytorch 就可以使用 Dojo。&lt;/p&gt;
&lt;h2&gt;Tesla Bot — Optimus&lt;/h2&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Tesla/tesla_bot.png"&gt;&lt;/p&gt;
&lt;p&gt;Tesla 將打造機器人，他們想延伸自動駕駛上面技術來打造一個通用機器人。&lt;/p&gt;
&lt;h2&gt;我的觀點&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;End-to-end 是個重要的趨勢，當預測目標與真實目標貼近時往往表現得更好。Tesla 一開始嘗試想從影像的預測去推算出 Vector Space 的預測，但因為誤差累積的緣故，無法得到好的預測結果，當他們採用 End-to-end 直接預測在 Vector Space 後才得以解決問題。&lt;/li&gt;
&lt;li&gt;讓模型合理的掌握所有資訊是重要的，例如在這裡：綜合八顆鏡頭的影像、在時間尺度和空間尺度上讓模型使用這些上下文的資訊。&lt;/li&gt;
&lt;li&gt;Tesla 將自己打造成一家資料公司，當資料愈多，訓練出來的模型就越準；自動駕駛越優質，更能吸引市場大眾購買他們的電動車，又反過來增強他們的資料，這形成一個良性循環。&lt;/li&gt;
&lt;li&gt;為了有效的利用龐大的資料，Tesla 發展了 Auto Labeling 的技術以及打造超級電腦 Dojo，這同樣形成一個良性循環，Auto Labeling 讓標注速度大幅度提升，可以快速迭代模型；而模型更準確，Auto Labeling 的重建技術就越準確。在這個循環下，Tesla 可以漸漸的不需要那麼多標注人員。&lt;/li&gt;
&lt;li&gt;Auto Labeling 技術應該是推動這整個專案最重要的技術。&lt;/li&gt;
&lt;li&gt;你是否有發現 Tesla 幾乎從頭打造開發自動駕駛的一切，晶片自己設計、集成電路自己打造、編譯器自己設計、標注軟體自行打造、創造自己的模擬引擎。這是 Musk 一貫的方法，重新打造並精練整個開發過程，最終會帶來精準打擊目標同時降低成本的效果，並且創造公司技術的護城河。&lt;/li&gt;
&lt;li&gt;Tesla 確實有創造可量產人形機器人的底氣在，自動駕駛技術就是電機械技術與AI的交會，而且 Tesla 還懂得如何降低成本及量產，善用這些Know-how來打造人形機器人是個聰明的決策，也同時創造公司的另外一個上升曲線。&lt;/li&gt;
&lt;li&gt;原本我也是一個對於移除光雷達的懷疑者，但聽完這個演講後我漸漸能理解 Tesla 為何採用純視覺的方法，一般駕駛員也是憑藉著視覺開車，更何況 Tesla 電動車有八顆鏡頭，其視野能力已經比人還好了，並且模型的預測是經過大量資料的驗證，反觀人類開車上路前的驗證是相對少的。而配置光雷達是需要成本的，如果大量資料都在在的顯示模型的有效性，拿掉光雷達的決策也是可以理解的。&lt;/li&gt;
&lt;li&gt;我覺得每個駕駛自行開車的操作也是一個重要的資訊，這場演講沒有講到他們怎麼使用這些資訊，有點想知道！&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI.ML"></category><category term="Tesla"></category></entry><entry><title>擴散模型（Diffusion Model）：生成模型的新成員</title><link href="https://ycc.idv.tw/diffusion-model.html" rel="alternate"></link><published>2022-05-20T12:00:00+08:00</published><updated>2022-05-20T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2022-05-20:/diffusion-model.html</id><summary type="html">&lt;p&gt;本篇從概念到深入數學的介紹擴散模型（Diffusion Model）。&lt;/p&gt;</summary><content type="html">&lt;h2&gt;生成模型 (Generative Model) 家族&lt;/h2&gt;
&lt;p&gt;在過去，作為生成模型的 GAN (Generative Adversarial Network) 最廣為人使用，GAN是在2014年由 Goodfellow 所提出來的方法，其結構由兩個網路所組成：生成器網路和鑑別器網路，生成器網路負責生成以假亂真的合成樣本，而鑑別器網路負責仔細區分出真實樣本和合成樣本，經由兩者交替對抗學習，最終我們可以得到一個好的生成器。這個生成器網路通常輸入為一組取樣自高斯分布的亂數，而輸出就是合成樣本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;為何我們需要取樣於一個「分布」？&lt;/strong&gt;以圖片為例，假設是長32寬32的一張圖片可以由一組長度為32x32x3的向量來表示，這並不意味著一組長度為32x32x3的隨機向量就能產生一張「有意義的」圖片，所以在32x32x3的空間中存在著能產生有意義圖片的不同機率分布，我們只要能把這個機率分布估準了，就可以從這個機率分布抽樣出有意義的圖片，這就是為何生成模型往往需要抽樣至某個分布，可想而知要用簡單的數學式來表示這樣的分布是多們困難的一件事，因此科學家們設計了一個簡單分布—高斯分布，並希望透過若干複雜的轉換（通常使用深度網路）後可以得到這個複雜的分布，通常我們會稱這個高斯分布為Hidden Space，因此「從樣本空間抽樣」等效於「從Hidden Space抽樣再轉換」。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Generative/hidden-to-sample-space.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;繼GAN以後有一個後起之秀 — Flow-based Generative Model，不同於 GAN 需要一個鑑別器網路來輔助訓練生成器網路，Flow-based Generative Model 透過一個可逆推的網路結構來訓練，這個可逆推網路的兩端就是Hidden Space跟我們想要得到的Sample Space，既然網路是可以逆推的，我們就可以輸入一群真實樣本訓練網路將其分布轉換為高斯分布的Hidden Space，待網路訓練完成我們就可以逆推來作為生成器使用。&lt;/p&gt;
&lt;p&gt;&lt;img alt="ddpm" src="http://www.ycc.idv.tw/media/Generative/ddpm.png"&gt;&lt;/p&gt;
&lt;p&gt;近期，生成模型的家族又多了一個新的模型，就是本篇要介紹的擴散模型（Diffusion Model），Diffusion Model 的中心思想是使用若干個微幅轉換來轉換Hidden Space成為Sample Space，如上圖所示， &lt;span class="math"&gt;\(\pmb{x}_T\)&lt;/span&gt; 代表抽樣自高斯分布Hidden Space的圖片，&lt;span class="math"&gt;\(\pmb{x}_0\)&lt;/span&gt; 代表抽樣自Sample Space的圖片，從 &lt;span class="math"&gt;\(\pmb{x}_0\)&lt;/span&gt; 到 &lt;span class="math"&gt;\(\pmb{x}_T\)&lt;/span&gt; 是模糊化的過程，中間經過若干事先定義好的操作 &lt;span class="math"&gt;\(q(\pmb{x}_{t}\mid \pmb{x}_{t-1})\)&lt;/span&gt;，而生成模型旨於學習模糊化的逆向轉換 &lt;span class="math"&gt;\(p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_t)\)&lt;/span&gt; ，當我們有了&lt;span class="math"&gt;\(p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_t)\)&lt;/span&gt; 就可以隨機抽樣自Hidden Space並轉成一張合成的圖片。實際操作上，&lt;span class="math"&gt;\(p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_t)\)&lt;/span&gt; 是一個神經網路，其輸入為圖片 &lt;span class="math"&gt;\(\pmb{x}_t\)&lt;/span&gt; 和其所在的step &lt;span class="math"&gt;\(t\)&lt;/span&gt;，其輸出為預測模糊化中被添加的雜訊 &lt;span class="math"&gt;\(\pmb{z}_\theta(\pmb{x}_t ,t)\)&lt;/span&gt; ，經理論的推導證明：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{x}_{t-1}=\frac{1}{\sqrt{\bar{\alpha}_t}}(\pmb{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\pmb{z}_\theta(\pmb{x}_t ,t))+\sigma_t\pmb{z}\ \ \text{ ; }\pmb{z}\sim\mathcal{N}(0;\pmb{I})
$$&lt;/div&gt;
&lt;p&gt;
(詳見【2.22】)，因此只要能成功預測隨機數 &lt;span class="math"&gt;\(\pmb{z}_\theta(\pmb{x}_t ,t)\)&lt;/span&gt; 就可以得到逆推模糊化的反轉換。&lt;/p&gt;
&lt;p&gt;閱讀到這裡的你已經把它的概念弄懂八成了，接下來要進入到可怕的數學時間，Are you ready?&lt;/p&gt;
&lt;h2&gt;從 Variational Inference 到 Evidence Lower Bound (ELBO)&lt;/h2&gt;
&lt;p&gt;近年來面對極其複雜（e.g. NN）的機率模型，傳統的優化方式變得不可行，如： Expectation-Maximization Algorithm ，所以接下來要跟大家介紹的 Variational Inference (VI) 就變得開始廣為人使用。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Generative/variational_inference.png"&gt;&lt;/p&gt;
&lt;p&gt;常見的生成模型可以表示成如上圖所示，&lt;span class="math"&gt;\(\pmb{x}\)&lt;/span&gt;  為生成之樣本，而 &lt;span class="math"&gt;\(\pmb{z}\)&lt;/span&gt; 座落於 latent space &lt;span class="math"&gt;\(Z\)&lt;/span&gt; 上的一個點，這個 latent space 可以具有各類可能的分布，為求方便通常會定義為一個 &lt;a href="https://www.ycc.idv.tw/deep-dl_1.html"&gt;高斯分佈&lt;/a&gt;，即 &lt;span class="math"&gt;\(p (\pmb{z})\sim\mathcal{N}(\pmb{z}:\pmb{0};\pmb{I})\)&lt;/span&gt; 。假設 &lt;span class="math"&gt;\(p (\pmb{x}\mid \pmb{z})\)&lt;/span&gt; (給定&lt;span class="math"&gt;\(\pmb{z}\)&lt;/span&gt; 之後求 &lt;span class="math"&gt;\(\pmb{x}\)&lt;/span&gt;  的分布) 是已知的，則聯合機率為
&lt;/p&gt;
&lt;div class="math"&gt;$$
p (\pmb{x},\pmb{z})=p (\pmb{x}\mid \pmb{z})p (\pmb{z})   \ \ 【1.1】
$$&lt;/div&gt;
&lt;p&gt;
也可推得
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\pmb{z}\mid \pmb{x})=\frac{p (\pmb{x},\pmb{z})}{p (\pmb{x})}  \ \ 【1.2】
$$&lt;/div&gt;
&lt;p&gt;
【1.2】是無法輕易求得的，這是雞生蛋蛋生雞的問題，分母的 &lt;span class="math"&gt;\(p(\pmb{x})\)&lt;/span&gt; 不正是我們想要學習的目標，所以在缺乏這一項的情況下求取 &lt;span class="math"&gt;\(p(\pmb{z}\mid \pmb{x})\)&lt;/span&gt; 是做不到的。&lt;/p&gt;
&lt;p&gt;Variational Inference 的技巧就是引入 &lt;span class="math"&gt;\(q(\pmb{z}\mid \pmb{x})\)&lt;/span&gt; 來近似 &lt;span class="math"&gt;\(p(\pmb{z}\mid \pmb{x})\)&lt;/span&gt;，這麼做可以得到一個較易計算的 Evidence Lower Bound (ELBO)。接下來我們來推導一下，由於我們希望 &lt;span class="math"&gt;\(q(\pmb{z}\mid \pmb{x})\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(p(\pmb{z}\mid \pmb{x})\)&lt;/span&gt;分布盡可能的靠近，所以需要最小化他們之間的 KL Divergence：
&lt;/p&gt;
&lt;div class="math"&gt;$$
min\ D_{KL}[q(\pmb{z}\mid \pmb{x})\mid\mid p(\pmb{z}\mid \pmb{x})]  \ \ 【1.3】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}[q(\pmb{z}\mid \pmb{x})\mid\mid p(\pmb{z}\mid \pmb{x})]\\
= \int q(\pmb{z}\mid \pmb{x})\ log\ \frac{q(\pmb{z}\mid \pmb{x})}{p(\pmb{z}\mid \pmb{x})}d\pmb{z}\\
= \int q(\pmb{z}\mid \pmb{x})\ log\ \frac{p(\pmb{x})q(\pmb{z}\mid \pmb{x})}{p(\pmb{x},\pmb{z})}d\pmb{z}\ \ \ \text{;因為 }p(\pmb{x},\pmb{z})=p (\pmb{z}\mid \pmb{x})p(\pmb{x})\\
=log\ p(\pmb{x})-\int q(\pmb{z}\mid \pmb{x})\ log\ \frac{p(\pmb{x},\pmb{z})}{q(\pmb{z}\mid \pmb{x})}d\pmb{z}
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
ELBO_{\pmb{x},\pmb{z}}=\int q (\pmb{z}\mid \pmb{x})\ log\ \frac{p(\pmb{x},\pmb{z})}{q(\pmb{z}\mid \pmb{x})}d\pmb{z} \ \ 【1.4】
$$&lt;/div&gt;
&lt;p&gt;
這一項被稱為 Evidence Lower Bound (ELBO)，因為 &lt;span class="math"&gt;\(p(\pmb{x})\)&lt;/span&gt; 是樣本空間機率，應該是一個上帝決定好的定值，所以如果想要讓 &lt;span class="math"&gt;\(q(\pmb{z}\mid \pmb{x})\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(p(\pmb{z}\mid \pmb{x})\)&lt;/span&gt;分布盡可能的靠近，就需要最大化ELBO。而這項是可以計算的，將其寫成抽樣估計的形式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
ELBO_{\pmb{x},\pmb{z}}=E_{q (\pmb{z}\mid \pmb{x})}[log\ \frac{p(\pmb{x}\mid\pmb{z})p(\pmb{z})}{q(\pmb{z}\mid \pmb{x})}]\ \ 【1.5】
$$&lt;/div&gt;
&lt;p&gt;
上式中的 &lt;span class="math"&gt;\(p(\pmb{z})\)&lt;/span&gt; 是定義好的分布，通常為高斯分布，&lt;span class="math"&gt;\(p(\pmb{x}\mid\pmb{z})\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(q(\pmb{z}\mid \pmb{x})\)&lt;/span&gt; 也是兩個已知的函式，所以【1.5】是可求得的。&lt;/p&gt;
&lt;p&gt;回過頭來看Diffusion Model，每一個 Step 中模糊化 &lt;span class="math"&gt;\(q(\pmb{x}_{t}\mid \pmb{x}_{t-1})\)&lt;/span&gt; 與逆模糊化 &lt;span class="math"&gt;\(p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_t)\)&lt;/span&gt; 應該存在ELBO的限制，在待會的推倒中我們會看到這一點。&lt;/p&gt;
&lt;h2&gt;擴散模型的Loss Function&lt;/h2&gt;
&lt;p&gt;接著我們來完整推導Diffusion Model 吧！每一次的模糊化我們可以定義為
&lt;/p&gt;
&lt;div class="math"&gt;$$
q(\pmb{x}_t\mid\pmb{x}_{t-1})=\mathcal{N}(\pmb{x}_t:\sqrt{1-\beta_t}\pmb{x}_{t-1};\beta_t\pmb{I}) \ \ 【2.1】
$$&lt;/div&gt;
&lt;p&gt;
其中 &lt;span class="math"&gt;\(\beta\)&lt;/span&gt;是介於0到1之間，這項可以是學來的，也可以是事前定義的定值，在 DDPM 論文中，&lt;span class="math"&gt;\(\beta\)&lt;/span&gt; 是一個定好的值。而經過 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 次（事先定義）的模糊化後，我們希望最終的 &lt;span class="math"&gt;\(\pmb{x}_T\)&lt;/span&gt; 可以接近高斯分布，即：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{x}_T\sim\mathcal{N}(\pmb{x}_T:0;\pmb{I}) \ \ 【2.2】
$$&lt;/div&gt;
&lt;p&gt;
這個過程我們稱之為正向擴散過程（forward diffusion process）。而逆模糊化我們定義成：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_\theta (\pmb{x}_{t-1}\mid\pmb{x}_{t})\sim\mathcal{N}(\pmb{x}_{t-1}:\pmb{\mu}_\theta (\pmb{x}_t ,t);\pmb{\Sigma}_\theta (\pmb{x}_t ,t)) \ \ 【2.3】
$$&lt;/div&gt;
&lt;p&gt;
其中 &lt;span class="math"&gt;\(p_\theta (\pmb{x}_{t-1}\mid\pmb{x}_{t})\)&lt;/span&gt; 用來近似 &lt;span class="math"&gt;\(q(\pmb{x}_{t-1}\mid\pmb{x}_{t})\)&lt;/span&gt;，&lt;span class="math"&gt;\(\pmb{\mu}_\theta\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\pmb{\Sigma}_\theta\)&lt;/span&gt; 代表模型 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 預測的平均值和標準差。&lt;/p&gt;
&lt;p&gt;然而模糊化 &lt;span class="math"&gt;\(q(\pmb{x}_{t}\mid \pmb{x}_{t-1})\)&lt;/span&gt; 與逆模糊化 &lt;span class="math"&gt;\(p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_t)\)&lt;/span&gt; 應該存在ELBO的限制，從 【1.4】出發：
&lt;/p&gt;
&lt;div class="math"&gt;$$
ELBO_{\pmb{x}_{0:T}}=\int q (\pmb{x}_{1:T}\mid \pmb{x}_0)\ log\ \frac{p_{\theta}(\pmb{x}_{0:T})}{q (\pmb{x}_{1:T}\mid \pmb{x}_0)}d\pmb{x}_{1:T}\\
=-\int q (\pmb{x}_{1:T}\mid \pmb{x}_0)\ [log\frac{q (\pmb{x}_{T}\mid \pmb{x}_{0})}{p_{\theta}(\pmb{x}_{T})}+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}-log\ p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})]d\pmb{x}_{1:T}\\
=-\{D_{KL}[q (\pmb{x}_{T}\mid \pmb{x}_{0})\mid\mid p_{\theta}(\pmb{x}_{T})]+\sum_{t=2}^{T}D_{KL}[q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})\mid\mid p_{\theta}(\pmb{x}_{t-1}\mid\pmb{x}_{t})]-log\ p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})\} \ \ 【2.4】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
log\ \frac{p_{\theta}(\pmb{x}_{0:T})}{q (\pmb{x}_{1:T}\mid \pmb{x}_0)}\\
=-log\ \frac{q (\pmb{x}_{1:T}\mid \pmb{x}_0)}{p_{\theta}(\pmb{x}_{0:T})}\\
=-log\ \frac{\prod_{t=1}^{T} q (\pmb{x}_{t}\mid \pmb{x}_{t-1})}{p_{\theta}(\pmb{x}_{T})\prod_{t=1}^{T}p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}\\
=-[-log\ p_{\theta}(\pmb{x}_{T})+\sum_{t=1}^{T}log\frac{q (\pmb{x}_{t}\mid \pmb{x}_{t-1})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}]\\
=-[-log\ p_{\theta}(\pmb{x}_{T})+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t}\mid \pmb{x}_{t-1})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}+log\frac{q (\pmb{x}_{1}\mid \pmb{x}_{0})}{p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})}] \\
=-[-log\ p_{\theta}(\pmb{x}_{T})+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}\frac{q (\pmb{x}_{t}\mid \pmb{x}_{0})}{q(\pmb{x}_{t-1}\mid \pmb{x}_{0})}+log\frac{q (\pmb{x}_{1}\mid \pmb{x}_{0})}{p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})}]\\
\text{;因為 }q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})=\frac{q (\pmb{x}_{t-1}, \pmb{x}_{t},\pmb{x}_{0})}{q (\pmb{x}_{t},\pmb{x}_{0})}=\frac{q (\pmb{x}_{t}\mid\pmb{x}_{t-1})q (\pmb{x}_{t-1}\mid\pmb{x}_{0})q (\pmb{x}_{0})}{q (\pmb{x}_{t}\mid\pmb{x}_{0})q (\pmb{x}_{0})}\\
=-[-log\ p_{\theta}(\pmb{x}_{T})+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t}\mid \pmb{x}_{0})}{q(\pmb{x}_{t-1}\mid \pmb{x}_{0})}+log\frac{q (\pmb{x}_{1}\mid \pmb{x}_{0})}{p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})}]\\
=-[-log\ p_{\theta}(\pmb{x}_{T})+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}+log\frac{q (\pmb{x}_{T}\mid \pmb{x}_{0})}{q(\pmb{x}_{1}\mid \pmb{x}_{0})}+log\frac{q (\pmb{x}_{1}\mid \pmb{x}_{0})}{p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})}]\\
=-[log\frac{q (\pmb{x}_{T}\mid \pmb{x}_{0})}{p_{\theta}(\pmb{x}_{T})}+\sum_{t=2}^{T}log\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})}{p_\theta(\pmb{x}_{t-1}\mid \pmb{x}_{t})}-log\ p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1})]
$$&lt;/div&gt;
&lt;p&gt;
我們需要最大化ELBO，從【2.4】可得優化任務為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta^*=\text{argmin}_{\theta}\ -ELBO_{\pmb{x},\pmb{z}}=\text{argmin}_{\theta}\{L_T+L_{T-1}+...+L_{1}+L_0\} \ \ 【2.5】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_T=D_{KL}[q (\pmb{x}_{T}\mid \pmb{x}_{0})\mid\mid p_{\theta}(\pmb{x}_{T})] \ \ 【2.6】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
L_{1\leq t\leq T-1}=D_{KL}[q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})\mid\mid p_{\theta}(\pmb{x}_{t-1}\mid\pmb{x}_{t})] \ \ 【2.7】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
L_0=-log\ p_\theta(\pmb{x}_{0}\mid \pmb{x}_{1}) \ \ 【2.8】
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;計算&lt;span class="math"&gt;\(L_T\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(L_T\)&lt;/span&gt; 與 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 無關，不需要優化，可以忽略。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;計算&lt;span class="math"&gt;\(L_0\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因為【2.3】，所以 &lt;span class="math"&gt;\(p_\theta(\pmb{x}_0\mid\pmb{x}_1)\)&lt;/span&gt; 是可以由模型預測而得的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;計算&lt;span class="math"&gt;\(L_{1\leq t\leq T-1}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(L_t\)&lt;/span&gt; 這一項先從 &lt;span class="math"&gt;\(q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})\)&lt;/span&gt; 開始做起，先給公式後面再補上證明：
&lt;/p&gt;
&lt;div class="math"&gt;$$
q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})=\mathcal{N}(\pmb{x}_{t-1}:\tilde{\pmb{\mu}}_t (\pmb{x}_t ,\pmb{x}_0);\tilde{\beta}_t\pmb{I}) \ \ 【2.9】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{\pmb{\mu}}_t (\pmb{x}_t ,\pmb{x}_0)=\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_{t}}\pmb{x}_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{(1-\bar{\alpha}_{t})}\pmb{x}_t \ \ 【2.10】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\tilde{\beta}_t=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_t \ \ 【2.11】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\alpha_t=1-\beta_t \ \ 【2.12】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\bar{\alpha}_t=\prod_{s=1}^t\alpha_s \ \ 【2.13】
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;接下來回過頭來，我們要來證明【2.9】，在那之前我們要來證明一個好用的式子，由【2.1】搭配【2.12】、【2.13】置換變數可得
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{x}_t=\sqrt{\alpha_t}\pmb{x}_{t-1}+\sqrt{1-\alpha_t}\pmb{\epsilon}_{t-1}\\
=\sqrt{\alpha_t}(\sqrt{\alpha_{t-1}}\pmb{x}_{t-2}+\sqrt{1-\alpha_{t-1}}\pmb{\epsilon}_{t-2})+\sqrt{1-\alpha_t}\pmb{\epsilon}_{t-1} \\
=\sqrt{\alpha_t\alpha_{t-1}}\pmb{x}_{t-2}+[\sqrt{\alpha_t-\alpha_t\alpha_{t-1}}\pmb{\epsilon}_{t-2}+\sqrt{1-\alpha_t}\pmb{\epsilon}_{t-1}]\\
=\sqrt{\alpha_t\alpha_{t-1}}\pmb{x}_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\bar{\pmb{\epsilon}}_{t-2}\\
...\\
=\sqrt{\bar{\alpha}_t}\pmb{x}_{0}+\sqrt{1-\bar{\alpha}_t}\pmb{\epsilon}\text{ ; }\pmb{\epsilon}\sim\mathcal{N}(0;\pmb{I}) \ \ 【2.14】\\
\Rightarrow q (\pmb{x}_{t}\mid \pmb{x}_{0})=\mathcal{N}(\sqrt{\bar{\alpha}_t}\pmb{x}_{0};(1-\bar{\alpha}_t)\pmb{I}) \ \ 【2.15】
$$&lt;/div&gt;
&lt;p&gt;
上式中 &lt;span class="math"&gt;\(\bar{\epsilon}\)&lt;/span&gt; 代表兩個 Guaissan 的相加，其分布遵循 &lt;span class="math"&gt;\(\sum_{i=1}^{n}a_i\cdot \mathcal{N}(z:\mu_i;\sigma^2_i)=\mathcal{N}(z:\sum_{i=1}^{n}a_i\mu_i;\sum_{i=1}^{n}a_i^2\sigma_i^2)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;如此一來就可以來計算目標了，引入【2.1】、【2.15】可得
&lt;/p&gt;
&lt;div class="math"&gt;$$
q (\pmb{x}_{t-1}\mid \pmb{x}_{t},\pmb{x}_{0})=q (\pmb{x}_{t}\mid \pmb{x}_{t-1},\pmb{x}_{0})\frac{q (\pmb{x}_{t-1}\mid \pmb{x}_{0})}{q (\pmb{x}_{t}\mid \pmb{x}_{0})}\\
\propto exp\{-\frac{1}{2}[\frac{(\pmb{x}_{t}-\sqrt{\alpha_t}\pmb{x}_{t-1})^2}{\beta_t}+\frac{(\pmb{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}}\pmb{x}_{0})^2}{1-\bar{\alpha}_{t-1}}-\frac{(\pmb{x}_{t}-\sqrt{\bar{\alpha}_{t}}\pmb{x}_{0})^2}{1-\bar{\alpha}_{t}}]\}\\
=exp\{-\frac{1}{2}[(\frac{\alpha_t}{\beta_t}+\frac{1}{1-\bar{\alpha}_{t-1}})\pmb{x}_{t-1}^2-(\frac{2\sqrt{\alpha_t}}{\beta_t}\pmb{x}_{t}+\frac{2\sqrt{\bar{\alpha}_t}}{1-\bar{\alpha}_t}\pmb{x}_{0})\pmb{x}_{t-1}+C(\pmb{x}_{t},\pmb{x}_{0})]\}
$$&lt;/div&gt;
&lt;p&gt;
整理可得平均值和方差為【2.10】和【2.11】。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下來為求方便計算，我們假設 &lt;span class="math"&gt;\(\pmb{\Sigma}_{\theta}\)&lt;/span&gt; 為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{\Sigma}_\theta (\pmb{x}_t ,t)=\sigma_{t,\theta} \pmb{I} \ \ 【2.16】
$$&lt;/div&gt;
&lt;p&gt;【2.16】、【2.9】、【2.3】和【A.1】代入【2.7】可得
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_{1\leq t\leq T-1}=D_{KL}[\mathcal{N}(\pmb{x}_{t-1}:\tilde{\pmb{\mu}}_t (\pmb{x}_t ,\pmb{x}_0);\tilde{\beta}_t\pmb{I})\mid\mid \mathcal{N}(\pmb{x}_{t-1}:\pmb{\mu}_\theta (\pmb{x}_t ,t);\pmb{\Sigma}_\theta (\pmb{x}_t ,t))] \\
=\sum_{j=1}^{J} D_{KL}[\mathcal{N}(x_{t-1,j}:\tilde{\mu}_{t,j} (\pmb{x}_t ,\pmb{x}_0);\tilde{\beta}_{t,j})\mid\mid \mathcal{N}(x_{t-1,j}:\mu_{\theta,j} (\pmb{x}_t ,t);\sigma_{t,\theta} (\pmb{x}_t ,t)\pmb{I})]\\
=\sum_{j=1}^{J} log\frac{\sigma_{t,\theta}}{\tilde{\beta}_{t,j}}+\frac{\tilde{\beta}_{t,j}^2+[\tilde{\mu}_{t,j} (\pmb{x}_t ,\pmb{x}_0)-\mu_{\theta,j} (\pmb{x}_t ,t)]^2}{2\sigma_{t,\theta}^2}-\frac{1}{2} \ \ 【2.17】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(\sigma_{t,\theta}\)&lt;/span&gt; 這一項在DDPM當中設為定值，作者實驗了兩種假設 &lt;span class="math"&gt;\(\sigma_{t,\theta}=\beta_t\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\sigma_{t,\theta}=\tilde{\beta}_t\)&lt;/span&gt; 發現對成效來說沒太大的差別。而Improved DDPM這一項則是用學的，作者假設 &lt;span class="math"&gt;\(\sigma_{t,\theta}=exp(v\ log\beta_t+(1-v)\ log\tilde{\beta}_t)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;觀察【2.17】可發現平均值的優化：
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_{t,mean}=\frac{1}{2\sigma_{t,\theta}^2}[\tilde{\pmb{\mu}}_{t} (\pmb{x}_t ,\pmb{x}_0)-\pmb{\mu}_{\theta} (\pmb{x}_t ,t)]^2\ \ 【2.18】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\tilde{\pmb{\mu}}_{t} (\pmb{x}_t ,\pmb{x}_0) =\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_{t}}\pmb{x}_0+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{(1-\bar{\alpha}_{t})}\pmb{x}_t
$$&lt;/div&gt;
&lt;p&gt;
我們可以藉由變換上式來得到優化目標，我們可以優化還原狀況（也就是原圖 &lt;span class="math"&gt;\(\pmb{x}_0\)&lt;/span&gt;），也可以預測添加的雜訊，而 DDPM作者實驗發現預測添加的雜訊得到的效果比較好，因此我們使用【2.14】替換掉 &lt;span class="math"&gt;\(\pmb{x}_0\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_{t}}\times\frac{1}{\sqrt{\bar{\alpha}_t}}[\pmb{x}_t-\sqrt{1-\bar{\alpha}_t}\pmb{\epsilon}]+\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{(1-\bar{\alpha}_{t})}\pmb{x}_t\\
=\frac{1}{\sqrt{\bar{\alpha}_t}}(\pmb{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\pmb{\epsilon})\ \ 【2.19】
$$&lt;/div&gt;
&lt;p&gt;
【2.18】式中 &lt;span class="math"&gt;\(\pmb{\mu}_{\theta} (\pmb{x}_t ,t)\)&lt;/span&gt; 是我們可以假設的，假設我讓它預測添加的雜訊，我們可以假設為 
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{\mu}_{\theta} (\pmb{x}_t ,t)=\frac{1}{\sqrt{\bar{\alpha}_t}}(\pmb{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\pmb{z}_\theta(\pmb{x}_t ,t)) \ \ 【2.20】
$$&lt;/div&gt;
&lt;p&gt;
因此，我們可以推得逆模糊的關鍵公式，【2.20】代入【2.3】得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_\theta (\pmb{x}_{t-1}\mid\pmb{x}_{t})\sim\mathcal{N}(\pmb{x}_{t-1}:\frac{1}{\sqrt{\bar{\alpha}_t}}(\pmb{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\pmb{z}_\theta(\pmb{x}_t ,t));\pmb{\Sigma}_\theta (\pmb{x}_t ,t)) \ \ 【2.21】
$$&lt;/div&gt;
&lt;p&gt;
上式也可以寫作：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\pmb{x}_{t-1}=\frac{1}{\sqrt{\bar{\alpha}_t}}(\pmb{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\pmb{z}_\theta(\pmb{x}_t ,t))+\sigma_t\pmb{z}\ \ \text{ ; }\pmb{z}\sim\mathcal{N}(0;\pmb{I}) \ \ 【5.22】
$$&lt;/div&gt;
&lt;p&gt;【2.19】和【2.20】代入【2.18】得
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_{t,mean}=\frac{\beta^2_t}{2\bar{\alpha}_t(1-\bar{\alpha}_t)\sigma_{t,\theta}^2}\mid\mid\pmb{\epsilon}-\pmb{z}_\theta(\sqrt{\bar{\alpha}_t}\pmb{x}_{0}+\sqrt{1-\bar{\alpha}_t}\pmb{\epsilon},t)\mid\mid^2\ \ 【2.23】
$$&lt;/div&gt;
&lt;p&gt;
在DDPM中，作者發現使用去除上式Weighting的優化式效果更好，寫作：
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_{t,mean,simple}=\mid\mid\pmb{\epsilon}-\pmb{z}_\theta(\sqrt{\bar{\alpha}_t}\pmb{x}_{0}+\sqrt{1-\bar{\alpha}_t}\pmb{\epsilon},t)\mid\mid^2\ \ 【2.24】
$$&lt;/div&gt;
&lt;p&gt;
因此DDPM的訓練和取樣示例代碼如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/Generative/DDPM-algo.png"&gt;&lt;/p&gt;
&lt;p&gt;其中應用到【2.24】和【2.22】。&lt;/p&gt;
&lt;h2&gt;Appendix A: KL Divergence between two Gaussians&lt;/h2&gt;
&lt;div class="math"&gt;$$
D_{KL}[\mathcal{N}(z:\mu_1;\sigma^2_1)\mid\mid\mathcal{N}(z:\mu_2;\sigma^2_2)]\\
=\int \mathcal{N}(z:\mu_1;\sigma^2_1)[log\ \mathcal{N}(z:\mu_1;\sigma^2_1)-log\ \mathcal{N}(z:\mu_2;\sigma^2_2)]dz\\
=\int \mathcal{N}(z:\mu_1;\sigma^2_1)\{-\frac{1}{2}[log\ 2\pi\cdot \sigma^2_1+\frac{(z-\mu_1)^2}{\sigma_1^2}]+\frac{1}{2}[log\ 2\pi\cdot \sigma^2_2+\frac{(z-\mu_2)^2}{\sigma_2^2}]\}dz \\
\text{;因為 }\mathcal{N}(\mu;\sigma^2)=\frac{1}{\sqrt{2\pi\cdot \sigma^2}}exp(-\frac{(z-\mu)^2}{2\sigma^2})\\
=-\frac{1}{2}[log\ 2\pi\cdot \sigma^2_1+\frac{(\sigma_1^2+\mu_1^2)-2\mu_1\mu_1+\mu_1^2}{\sigma_1^2}]+\frac{1}{2}[log\ 2\pi\cdot \sigma^2_2+\frac{(\sigma_1^2+\mu_1^2)-2\mu_2\mu_1+\mu_2^2}{\sigma_2^2}] \\
\text{;因為 }\int z\cdot\mathcal{N}(\mu;\sigma^2)dz=\mu\text{ 且 }\int z^2\cdot\mathcal{N}(\mu;\sigma^2)dz=\sigma^2+\mu^2\\
=log\frac{\sigma_2}{\sigma_1}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}-\frac{1}{2} \ \ 【A.1】
$$&lt;/div&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Auto-Encoding Variational Bayes: https://arxiv.org/pdf/1312.6114.pdf&lt;/li&gt;
&lt;li&gt;An Introduction to Variational Inference: https://arxiv.org/pdf/2108.13083.pdf&lt;/li&gt;
&lt;li&gt;From Autoencoder to Beta-VAE: https://lilianweng.github.io/posts/2018-08-12-vae/&lt;/li&gt;
&lt;li&gt;Denoising Diffusion Probabilistic Models: https://arxiv.org/pdf/2006.11239.pdf&lt;/li&gt;
&lt;li&gt;What are Diffusion Models: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="生成模型"></category></entry><entry><title>OCR：CRNN+CTC開源加詳細解析</title><link href="https://ycc.idv.tw/crnn-ctc.html" rel="alternate"></link><published>2020-10-12T12:00:00+08:00</published><updated>2020-10-12T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-10-12:/crnn-ctc.html</id><summary type="html">&lt;p&gt;Pytorch CRNN+CTC 開源囉！並且在這篇中會仔細介紹 CRNN 的架構，以及 CTC 的架構、訓練的參數優化和其三種 Inference 方法（greedy decode, beam search decode, prefix beam search decode）&lt;/p&gt;</summary><content type="html">&lt;p&gt;場景文字辨識 OCR (Optical Character Recognition) 應用場景非常多，例如：Evernote提供的名片辨識、諸多銀行內部使用的存摺影像辨識、新型停車場提供的車牌辨識系統、證件識別工具、旅遊業會用到護照識別，不勝枚舉！凡是想要將生活場景中的文字藉由機器去讀取的都是OCR的範疇，它會為企業省下許多人工判讀的人力成本。&lt;/p&gt;
&lt;p&gt;在場景文字辨識中相當經典的模型就是 &lt;a href="http://arxiv.org/abs/1507.05717"&gt;CRNN+CTC&lt;/a&gt;，Github 上也有許多相關的 Repo.，但 YC 發現沒有一個將這個模型寫的夠好、夠容易修改的 pytorch 開源程式碼，所以 YC 決定自己幹一個並且將它開源，目前搭配資料集 Synth90k 已經可以在英文辨識上做到 93.9 % 的 Sequence Accuracy，歡迎大家來嘗試使用！&lt;/p&gt;
&lt;h3&gt;開源：pytorch版本的CRNN+CTC&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/GitYCC/crnn-pytorch"&gt;GitYCC/crnn-pytorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;👆🏼👆🏼👆🏼👆🏼👆🏼&lt;/p&gt;
&lt;p&gt;麻煩大家不吝給予星星 ⭐️，並且分享給更多人知道！&lt;/p&gt;
&lt;p&gt;&lt;img alt="reading" src="http://www.ycc.idv.tw/media/CV/170_READING_62745.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="showtime" src="http://www.ycc.idv.tw/media/CV/178_Showtime_70541.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="novel" src="http://www.ycc.idv.tw/media/CV/78_Novel_52433.jpg"&gt;&lt;/p&gt;
&lt;h3&gt;網路架構&lt;/h3&gt;
&lt;p&gt;CRNN+CTC 結構源於論文&lt;a href="http://arxiv.org/abs/1507.05717"&gt;An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition (2015), Baoguang Shi et al.&lt;/a&gt;，其網路架構其實並不複雜，講白了就是 CNN 的 Backbone 再搭配 Bi-directional RNN，最後對每個時間點作Softmax分類問題，但是在衡量輸出時則是需要綜觀每個時間點的 CTC Loss。&lt;/p&gt;
&lt;p&gt;&lt;img alt="crnn_structure" src="http://www.ycc.idv.tw/media/CV/crnn_structure.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="crnn_structure_detail" src="http://www.ycc.idv.tw/media/CV/crnn_structure_detail.png"&gt;&lt;/p&gt;
&lt;p&gt;首先，將圖片轉成灰階並且伸縮至高度32，通過多層的Conv2D、MaxPool和BatchNormalization抽取圖片相關的特徵。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;img_height&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;img_width&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="n"&gt;channels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;256&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;kernel_sizes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;strides&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;paddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;cnn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# shape of input: (batch, input_channel, height, width)&lt;/span&gt;
    &lt;span class="n"&gt;input_channel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;output_channel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;channels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;kernel_sizes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;paddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;batchnorm&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BatchNorm2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_channel&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;relu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LeakyReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;leaky_relu&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# size of image: (channel, height, width) = (img_channel, img_height, img_width)&lt;/span&gt;
&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pooling0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# (64, img_height // 2, img_width // 2)&lt;/span&gt;

&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pooling1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="c1"&gt;# (128, img_height // 4, img_width // 4)&lt;/span&gt;

&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;pooling2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# (256, img_height // 8, img_width // 4)&lt;/span&gt;

&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_norm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;cnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;pooling3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# (512, img_height // 16, img_width // 4)&lt;/span&gt;

&lt;span class="n"&gt;conv_relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# (512, img_height // 16 - 1, img_width // 4 - 1)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;接下來藉由一個 DenseLayer 去將圖片轉成維度為 &lt;code&gt;map_to_seq_hidden&lt;/code&gt; 的序列，這樣就可以接續的使用 RNN 來萃取序列的特徵。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map_to_seq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_channel&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;output_height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;map_to_seq_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;這邊特別注意一下，當你經過 CNN Backbone 之後的維度是：&lt;code&gt;(batch, channel, new_height, new_width)&lt;/code&gt; ，但是你希望進RNN前的維度是：&lt;code&gt;(seq_len, batch, map_to_seq_hidden)&lt;/code&gt; ，所以需要將 &lt;code&gt;channel, new_height&lt;/code&gt; 攤平再過 DenseLayer，如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;channel&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;permute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# (width, batch, feature)&lt;/span&gt;
&lt;span class="n"&gt;seq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map_to_seq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="map_to_seq" src="http://www.ycc.idv.tw/media/CV/map_to_seq.png"&gt;&lt;/p&gt;
&lt;p&gt;最後再過兩層 Bi-directional LSTM 萃取序列相關的特徵，並且過 DenseLayer 來將維度轉成與分類的數量相同，再過 Softmax 就大功告成！注意：分類的類別是所有要預測的字元再加上 blank ε ，一般我們會讓 blank ε 在Onehot Encoding 時放在 index=0 的位置。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/CV/biLSTM.png"&gt;&lt;/p&gt;
&lt;p&gt;詳細模型架構請詳見：&lt;a href="https://github.com/GitYCC/crnn-pytorch/blob/master/src/model.py"&gt;src/model.py&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;CTC (Connectionist Temporal Classification)&lt;/h3&gt;
&lt;p&gt;RNNs 與其他傳統方法相比，如：HMMs (Hidden Markov Model) 和 CRFs (Conditional Random Field) 有以下優勢：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNNs 無須多餘的先驗假設&lt;/li&gt;
&lt;li&gt;RNNs 提供了相當強大且一般化的機制去描述時間序列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是 RNNs 只能訓練在已經分配好的序列上，所以當你僅有 Sequence Label 是不夠的，還需要知道這些 Label 要怎麼被&lt;strong&gt;分配&lt;/strong&gt;。舉個例子：如果今天你要辨識的圖片包含 &lt;code&gt;"CAT"&lt;/code&gt; 這個詞，你想要使用 RNN 進行辨識，而這個 RNN 長度假設為 7，為了要能訓練 RNN 我們需要給它 Ground True，此時你就需要將 &lt;code&gt;"CAT"&lt;/code&gt; 這 3 個字&lt;strong&gt;分配&lt;/strong&gt;到 7 格中，但是這樣的標注是費工的、不切實際的，&lt;strong&gt;我們希望可以直接訓練在沒有分配前的 Sequence Label 上&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;CTC 設計了一個機制讓我們可以做到這件事：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在原本要預測的字元中多加入了 blank ε &lt;/li&gt;
&lt;li&gt;透過映射機制 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 將 RNN 的輸出轉化成 Sequence Prediction&lt;/li&gt;
&lt;li&gt;映射機制 &lt;span class="math"&gt;\(B\)&lt;/span&gt;：合併相鄰的字元並且除去 blank ε &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="ctc mapping" src="http://www.ycc.idv.tw/media/CV/ctc_mapping.png"&gt;&lt;/p&gt;
&lt;p&gt;有了這個映射機制 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 我們就可以不需要事先分配 Sequence Label 也能訓練網路，但是要注意：能映射到一組 Sequence Label 的可能性是有很多組合的，例如：&lt;code&gt;"hee-l-lloo"&lt;/code&gt; 和 &lt;code&gt;"hheel-lloo"&lt;/code&gt; 都會映射到 &lt;code&gt;"hello"&lt;/code&gt;，所以要特別去設計它的 Loss 讓其可以考慮各種可能性，這會在下一節中仔細闡述。&lt;/p&gt;
&lt;h3&gt;CTC Loss: Forward-Backward Algorithm&lt;/h3&gt;
&lt;p&gt;接下來這一個部分將會是本篇最難理解的部分，而且也最為數學，所以在陷進去數學漩渦之前，我們先來概念性的了解 CTC Loss 究竟是做了什麼。簡言之，&lt;strong&gt;我們需要讓 CTC Loss 降低的同時，等同於做到提升產生 Ground True Sequence Label 的機率，而在映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 的作用下產生這個 Ground True Sequence Label 會有多種來源組合，這些組合都必須被考慮進去，然後我們有了 CTC Loss 與參數的關係式就可以使用梯度下降進行優化。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以我們需要窮舉所有可能的組合才能正確的將產生 Ground True Sequence Label 的機率算出來，我們就來試著羅列。假設我們的 Sequence Label 是 &lt;code&gt;"CAT"&lt;/code&gt; ，見圖一，所以我們的 &lt;span class="math"&gt;\(\ell\)&lt;/span&gt; 為 &lt;code&gt;"CAT"&lt;/code&gt;，為了把 blank ε 列入考慮，我們設計了 &lt;span class="math"&gt;\(\ell'\)&lt;/span&gt;，其長度關係為 &lt;span class="math"&gt;\(|\ell'|=2|\ell|+1\)&lt;/span&gt; 。參照映射函數的規則，只有圖中兩個紅點是可能的出發點、兩個藍點是可能的結束點，因此我們需要羅列從出發點到結束點可能的路徑 &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;，這些路徑都可以在經過映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 後產生 Sequence Label &lt;code&gt;"CAT"&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/CV/IMG_ctc_example_01.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖一
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;加總所有可能透過 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 映射至 Ground True Sequence Label &lt;span class="math"&gt;\(\ell\)&lt;/span&gt; 的條件機率：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\ell|y)=\sum_{π:B(π)=\ell}p(π|y)\ \text{ ↪︎【1】}
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(π|y)=\prod_{t=1}^{T}y^{t}_{π_t}\ \text{ ↪︎【2】}
$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(y^{t}_{π_t}\)&lt;/span&gt; 表示在時間 &lt;span class="math"&gt;\(t\)&lt;/span&gt; Label 為 &lt;span class="math"&gt;\(π_t\)&lt;/span&gt; 的機率。&lt;/p&gt;
&lt;p&gt;而我們的目標就是想要最大化 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt; ，取 &lt;span class="math"&gt;\(ln\)&lt;/span&gt; 在加上負號，就會變換成為最小化問題：
&lt;/p&gt;
&lt;div class="math"&gt;$$
min_{y}\ -ln[p(\ell|y)]\ \text{ ↪︎【3】}
$$&lt;/div&gt;
&lt;p&gt;
只要算其微分，就可以使用梯度下降的方法進行優化：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial}{\partial y}\{ -ln[p(\ell|y)]\}=\frac{-1}{p(\ell|y)}\frac{\partial p(\ell|y)}{\partial y}\ \text{ ↪︎【4】}
$$&lt;/div&gt;
&lt;p&gt;
所以只要解決兩件事：如何計算 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\frac{\partial p(\ell|y)}{\partial y}\)&lt;/span&gt; ，就可以優化參數了！&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;但是窮舉各種可能路徑有這麼簡單嗎？幸好我們有 &lt;a href="https://zh.wikipedia.org/zh-tw/动态规划"&gt;Dynamic Programming&lt;/a&gt; ，這裡借鏡解 HMM 優化問題會使用的 Forward-Backward Algorithm 來解決這個問題。 &lt;a href="https://zh.wikipedia.org/zh-tw/动态规划"&gt;Dynamic Programming&lt;/a&gt; 是一種遞迴的演算法，只要有初始值和遞迴式就可以一路算下去。運用在這個問題上，我們只需要找到在 &lt;span class="math"&gt;\(y_t\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(y_{t-1}\)&lt;/span&gt; 的機率累加遞迴關係，以及 &lt;span class="math"&gt;\(y_1\)&lt;/span&gt; 時刻的機率分布，就可以一路算到最後一個時刻點 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 的機率累積。&lt;/p&gt;
&lt;p&gt;首先，我們先來定義「Forward Algorithm」的 &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; ：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\alpha_t(s)\equiv\sum_{π:B(π_{1:t})=\ell_{1:s}}\prod_{t'=1}^{t}y^{t'}_{π_{t'}}\ \text{ ↪︎【5】}
$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(\alpha_t(s)\)&lt;/span&gt; 表示從可能的起始點累積到 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 時刻且 Label &lt;span class="math"&gt;\(\ell'=s\)&lt;/span&gt;  那點的總機率，可以從式子的右式看出，加總所有能將 &lt;span class="math"&gt;\(π_{1:t}\)&lt;/span&gt; (圖一中的衡欄) 映射到 &lt;span class="math"&gt;\(\ell_{1:s}\)&lt;/span&gt; (圖一中的縱欄) 的路徑 &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; ，並且將這路徑 &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; 上的各點機率相乘 (因為互為獨立事件)。&lt;/p&gt;
&lt;p&gt;再參照圖一和【1】式，我們知道 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt; 可以用 &lt;span class="math"&gt;\(\alpha_t(s)\)&lt;/span&gt; 表示：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\ell|y)=\alpha_T(|\ell'|)+\alpha_T(|\ell'|-1)\ \text{ ↪︎【6】}
$$&lt;/div&gt;
&lt;p&gt;
右式的兩項代表結束的兩個藍點。&lt;/p&gt;
&lt;p&gt;從【5】式我們可以得到 &lt;span class="math"&gt;\(t=1\)&lt;/span&gt; 的初始值，因為只有兩個紅點是可能的路徑，所以得到：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \alpha_1(1)=y_\epsilon^1 \\
    \alpha_1(2)=y_{\ell_1}^1 \\
    \alpha_1(s)=0,\ \forall s&amp;gt; 2
\end{cases}\ \text{ ↪︎【7】}
$$&lt;/div&gt;
&lt;p&gt;
從【5】式我們也可以得到遞迴式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\alpha_t(s)=[\sum_{π:B(π_{1:t-1})=\ell_{1:s}}\prod_{t'=1}^{t}y^{t'}_{π_{t'}}]y^{t}_{l'_{s}}=[\sum_{s'\in connected\ to\ (t,s)}\alpha_{t-1}(s')]y^{t}_{l'_{s}}\ \text{ ↪︎【8】}
$$&lt;/div&gt;
&lt;p&gt;
連接到 &lt;span class="math"&gt;\((t,s)\)&lt;/span&gt; 可分為三種路徑情況，如下圖：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/CV/IMG_ctc_example_02.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖二：Forward Algorithm可能路徑
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;所以我們可以把【8】遞迴式寫得更加清楚：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\alpha_t(s)= 
\begin{cases}
    [\alpha_{t-1}(s)+\alpha_{t-1}(s-1)]y_{l'_s}^t&amp;amp; \text{if } l'_s=\epsilon\text{ or }l'_{s-2}=l'_s\\
    [\alpha_{t-1}(s)+\alpha_{t-1}(s-1)+\alpha_{t-1}(s-2)]y_{l'_s}^t   &amp;amp; \text{otherwise}
\end{cases}\ \text{ ↪︎【9】}
$$&lt;/div&gt;
&lt;p&gt;
善用【9】式遞迴式和【7】式初始值，你就可以求得圖一中所有格子的 &lt;span class="math"&gt;\(\alpha_t(s)\)&lt;/span&gt; 值，再藉由【6】式就可以得到 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我們順利的解決了【4】式中的 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt;，那接下來只剩下 &lt;span class="math"&gt;\(\frac{\partial p(\ell|y)}{\partial y}\)&lt;/span&gt;，為了算這一項我們還需要「Backward Algorithm」，「Backward Algorithm」的 &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; 定義如下： 
&lt;/p&gt;
&lt;div class="math"&gt;$$
\beta_t(s)\equiv\sum_{π:B(π_{t:T})=\ell_{s:|\ell|}}\prod_{t'=t}^{T}y^{t'}_{π_{t'}}\ \text{ ↪︎【10】}
$$&lt;/div&gt;
&lt;p&gt;
從【10】式我們可以得到 &lt;span class="math"&gt;\(t=T\)&lt;/span&gt; 的初始值，因為只有兩個藍點是可能的路徑，所以得到：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{cases}
    \beta_T(|\ell'|)=y_\epsilon^T \\
    \beta_T(|\ell'|-1)=y_{\ell_{|\ell|}}^T \\
    \beta_T(s)=0,\ \forall s&amp;lt; |\ell'|-1
\end{cases}\ \text{ ↪︎【11】}
$$&lt;/div&gt;
&lt;p&gt;
從【10】式我們也可以得到遞迴式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\beta_t(s)=y^{t}_{l'_{s}}[\sum_{π:B(π_{t:T})=\ell_{s:|\ell|}}\prod_{t'=t+1}^{T}y^{t'}_{π_{t'}}]=y^{t}_{l'_{s}}[\sum_{s'\in connected\ from\ (t,s)}\beta_{t+1}(s')]\ \text{ ↪︎【12】}
$$&lt;/div&gt;
&lt;p&gt;
從 &lt;span class="math"&gt;\((t,s)\)&lt;/span&gt; 連接的情況一樣也可分為三種情況，如下圖：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/CV/IMG_ctc_example_03.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖三：Backward Algorithm可能路徑
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;所以我們可以把【12】遞迴式寫得更加清楚：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\beta_t(s)= 
\begin{cases}
    [\beta_{t+1}(s)+\beta_{t+1}(s+1)]y_{l'_s}^t&amp;amp; \text{if } l'_s=\epsilon\text{ or }l'_{s+2}=l'_s\\
    [\beta_{t+1}(s)+\beta_{t+1}(s+1)+\beta_{t+1}(s+2)]y_{l'_s}^t   &amp;amp; \text{otherwise}
\end{cases}\ \text{ ↪︎【13】}
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;當我們把 Forward Algorithm 和 Backward Algorithm 合在一起就會得到一個好用的公式，綜【5】和【10】式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\alpha_t(s)\beta_t(s)=\sum_{π:B(π)=\ell\ \&amp;amp;\ \pi_t=\ell'_s}\ y_{\ell'_s}^t\prod_{t'=1}^{T}y^{t'}_{π_{t'}}\ \text{ ↪︎【14】}
$$&lt;/div&gt;
&lt;p&gt;
再考慮【2】式
&lt;/p&gt;
&lt;div class="math"&gt;$$
\Rightarrow \frac{\alpha_t(s)\beta_t(s)}{y_{\ell'_s}^t}=\sum_{π:B(π)=\ell\ \&amp;amp;\ \pi_t=\ell'_s}\ \prod_{t'=1}^{T}y^{t'}_{π_{t'}}＝\sum_{π:B(π)=\ell\ \&amp;amp;\ \pi_t=\ell'_s}p(π|y)\ \text{ ↪︎【15】}
$$&lt;/div&gt;
&lt;p&gt;
再考慮【1】式
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\ell|y)=\sum_{s=1}^{|\ell'|}\sum_{π:B(π)=\ell\ \&amp;amp;\ \pi_t=\ell'_s}p(π|y)=\sum_{s=1}^{|\ell'|}\frac{\alpha_t(s)\beta_t(s)}{y_{\ell'_s}^t}\ \text{ ↪︎【16】}
$$&lt;/div&gt;
&lt;p&gt;
【16】式提供了另外一個求 &lt;span class="math"&gt;\(p(\ell|y)\)&lt;/span&gt; 的方法，而且這個算法可以在任意時間點 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 作計算，更棒的是它可以讓我們很方便的求得 &lt;span class="math"&gt;\(\frac{\partial p(\ell|y)}{\partial y}\)&lt;/span&gt; ：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial p(\ell|x)}{\partial y_k^t}=\frac{\partial}{\partial y_k^t}[\sum_{s=1}^{|\ell'|}\frac{\alpha_t(s)\beta_t(s)}{y_{\ell'_s}^t}]\ \text{ ↪︎【17】}
$$&lt;/div&gt;
&lt;p&gt;
因為微分的關係，上式加總中與 &lt;span class="math"&gt;\(y_k^t\)&lt;/span&gt; 無關的項都會化為零，可以進一步改寫：
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\frac{\partial}{\partial y_k^t}[\sum_{s\in \{\ell'_s=k\}}\frac{\alpha_t(s)\beta_t(s)}{y_{\ell'_s}^t}]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\frac{\partial}{\partial y_k^t}[\sum_{s\in \{\ell'_s=k\}}\frac{(\cdots)y_{l'_s}^t\times y_{l'_s}^t(\cdots)}{y_{l'_s}^t}]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\sum_{s\in \{\ell'_s=k\}}\frac{(\cdots)y_{l'_s}^t\times y_{l'_s}^t(\cdots)}{[y_{l'_s}^{t}]^2}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\Rightarrow \frac{\partial p(\ell|x)}{\partial y_k^t}=\frac{1}{[y_{l'_s}^{t}]^2}\sum_{s\in \{\ell'_s=k\}}\alpha_t(s)\beta_t(s)\ \text{ ↪︎【18】}
$$&lt;/div&gt;
&lt;p&gt;因此綜【6】和【18】我們就可以解【4】式，然後就可以對網路做反向傳播優化參數了！&lt;/p&gt;
&lt;h3&gt;Inference of CTC&lt;/h3&gt;
&lt;p&gt;在 &lt;a href="https://github.com/GitYCC/crnn-pytorch"&gt;GitYCC/crnn-pytorch&lt;/a&gt; 中我有實作了&lt;a href="https://github.com/GitYCC/crnn-pytorch/blob/master/src/ctc_decoder.py"&gt;三種 CTC 的 Inference 方法&lt;/a&gt;，分別為 &lt;code&gt;greedy_decode&lt;/code&gt;、&lt;code&gt;beam_search_decode&lt;/code&gt; 和 &lt;code&gt;prefix_beam_decode&lt;/code&gt;，精確度依序越來越準確，但是隨著精確度提高所花的 Inference 時間也就越長。&lt;/p&gt;
&lt;p&gt;事實上，這三種方式都不是最佳解，真正的最佳解得枚舉出所有可能的 Sequence Label 組合再取最大機率的那一個，它必須要衡量 &lt;span class="math"&gt;\((C+1)^T\)&lt;/span&gt; 條路徑且每條路徑都需要要做 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 次的乘法，所以時間複雜度為 &lt;span class="math"&gt;\(O(T\times (C+1)^T)\)&lt;/span&gt;，這個計算量大到不切實際，假設英文字母 &lt;span class="math"&gt;\(C=26\)&lt;/span&gt; ，然後 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 假設為 &lt;span class="math"&gt;\(20\)&lt;/span&gt;，時間複雜度會達到 &lt;span class="math"&gt;\(8.4e\text{+}29\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;所以接下來我會逐一介紹三種常見的近似方法。&lt;/p&gt;
&lt;h4&gt;Greedy Decode&lt;/h4&gt;
&lt;div class="math"&gt;$$
\ell^*\approx B(\pi^*)\ ;\ \pi^*= \{argmax_{s}\ y^{t}_{s}\ \text{ for t=1..T}\}
$$&lt;/div&gt;
&lt;p&gt;這是最 heuristic 的方法，直接找在每個時間點 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 最大的 &lt;span class="math"&gt;\(s\)&lt;/span&gt; 當作路徑 &lt;span class="math"&gt;\(\pi^*\)&lt;/span&gt; 再過映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt;。這個方法雖然簡單，但在大部分情況下會是正確的。在最佳解時我們希望的是在過完映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 之後得到最大可能的路徑 &lt;span class="math"&gt;\(\ell^*\)&lt;/span&gt; ，所以需要考慮所有可能的路徑並將其機率加總才能得到最佳解，但是  Greedy Decode 則是只考慮了最高機率的一條路徑。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/CV/IMG_ctc_greed_decode.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖四：Greedy Decode
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;greedy_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_reconstruct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;Beam Search Decode&lt;/h4&gt;
&lt;p&gt;既然只選一條機率最大的路徑不那麼精確，那麼我就選最大的前 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 條路徑再將其機率相加，這個就是 Beam Search 的近似方法。為了達到這個目的，我們在每一個時間點 &lt;span class="math"&gt;\(t\)&lt;/span&gt; 都會保留前 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 條累積相乘機率最大的可能路徑，如下圖所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/CV/IMG_ctc_beam_search.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖五：Beam Search Decode (beam size = 2)
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;beam_search_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;beam_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;beam_size&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;emission_threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;emission_threshold&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DEFAULT_EMISSION_THRESHOLD&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

    &lt;span class="n"&gt;beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[([],&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;  &lt;span class="c1"&gt;# (prefix, accumulated_log_prob)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;new_beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accumulated_log_prob&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;beams&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_count&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;log_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;emission_threshold&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;continue&lt;/span&gt;
                &lt;span class="n"&gt;new_prefix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="c1"&gt;# log(p1 * p2) = log_p1 + log_p2&lt;/span&gt;
                &lt;span class="n"&gt;new_accu_log_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;accumulated_log_prob&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;
                &lt;span class="n"&gt;new_beams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;new_prefix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_accu_log_prob&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="c1"&gt;# sorted by accumulated_log_prob&lt;/span&gt;
        &lt;span class="n"&gt;new_beams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_beams&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;beam_size&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# sum up beams to produce labels&lt;/span&gt;
    &lt;span class="n"&gt;total_accu_log_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accu_log_prob&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;beams&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_reconstruct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="c1"&gt;# log(p1 + p2) = logsumexp([log_p1, log_p2])&lt;/span&gt;
        &lt;span class="n"&gt;total_accu_log_prob&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
            &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;accu_log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_accu_log_prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NINF&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

    &lt;span class="n"&gt;labels_beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;accu_log_prob&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accu_log_prob&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;total_accu_log_prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
    &lt;span class="n"&gt;labels_beams&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;labels_beams&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;其中用到 &lt;code&gt;logsumexp&lt;/code&gt; 的用意是因為我們操作在 Log Probability 上面，雖然機率相乘即是 Log Probability 相加，很方便操作。但是如果碰到需要機率相加時，就需要先取 &lt;span class="math"&gt;\(exp\)&lt;/span&gt; 還原後再相加再取 &lt;span class="math"&gt;\(log\)&lt;/span&gt; ，即：&lt;span class="math"&gt;\(log(p1 + p2) = logsumexp([log(p1), log(p2)])\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4&gt;Prefix Beam Search Decode&lt;/h4&gt;
&lt;p&gt;我們可以再進一步讓它更精確一點，剛剛的 Beam Search 是在映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 之前找 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 條路徑，Prefix Beam Search 更進一步拿前 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 條經過映射函數 &lt;span class="math"&gt;\(B\)&lt;/span&gt; 後的 Prefix 當作評估的方式 ，如此會更接近我們想要找到映射後的最高機率的 Sequence。&lt;/p&gt;
&lt;p&gt;在 Prefix 的世界裡不存在 blank ε，但是 blank ε 卻是會影響 Prefix，例如：&lt;code&gt;"A-A"&lt;/code&gt; 映射完會是 &lt;code&gt;"AA"&lt;/code&gt;，但是 &lt;code&gt;"AA"&lt;/code&gt; 映射完則會是 &lt;code&gt;"A"&lt;/code&gt; ，所以在 Prefix Beam Search 存在 &lt;code&gt;ProbabilityWithBlank&lt;/code&gt; 和 &lt;code&gt;ProbabilityNoBlank&lt;/code&gt; 兩種累積機率，這兩種情況分別是結尾有 blank 的&lt;strong&gt;累積&lt;/strong&gt;機率和結尾沒有 blank 的&lt;strong&gt;累積&lt;/strong&gt;機率，特別注意：&lt;strong&gt;累積&lt;/strong&gt;機率代表從開始到目前的總機率。&lt;/p&gt;
&lt;p&gt;有了這個概念，我們來看會遇到什麼樣的狀況，並且在每個狀況下我們要怎麼去計算 &lt;code&gt;ProbabilityWithBlank&lt;/code&gt; 和 &lt;code&gt;ProbabilityNoBlank&lt;/code&gt; ，以下符號 &lt;code&gt;*&lt;/code&gt; 代表上一時刻的 prefix，&lt;code&gt;E&lt;/code&gt; 代表上一時刻 prefix 的最後一個字元，&lt;code&gt;ε&lt;/code&gt; 代表 blank，以下的情況皆是在考慮新字元進來要怎麼去累加 &lt;code&gt;ProbabilityWithBlank&lt;/code&gt; 和 &lt;code&gt;ProbabilityNoBlank&lt;/code&gt; ：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/CV/prefix_beam_search_formula.png"&gt;&lt;/p&gt;
&lt;p&gt;特別注意，以上初始化是遇到新的字元就假設其後會不帶 blank。&lt;/p&gt;
&lt;p&gt;當然每次考慮一個新的時間點，我們都需要去累加可能會產生這個 Prefix 的各種情況，因此在以下的程式碼中，我們的 &lt;code&gt;ProbabilityWithBlank&lt;/code&gt; 和 &lt;code&gt;ProbabilityNoBlank&lt;/code&gt; 是會迭代累加的。當累積完這個時間點的所有 Prefix 機率後，我們會取前 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 大機率的 Prefix 留下來繼續往下一個時間點累加，此時要用 &lt;code&gt;ProbabilityWithBlank&lt;/code&gt; 和 &lt;code&gt;ProbabilityNoBlank&lt;/code&gt; 的合來當作排序的依據。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/CV/IMG_ctc_prefix_beam_search.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  圖六：Prefix Beam Search Decode (beam size = 2)
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;prefix_beam_decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;beam_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;beam_size&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;emission_threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;emission_threshold&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;DEFAULT_EMISSION_THRESHOLD&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;

    &lt;span class="n"&gt;beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NINF&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;  &lt;span class="c1"&gt;# (prefix, (blank_log_prob, non_blank_log_prob))&lt;/span&gt;
    &lt;span class="c1"&gt;# initial of beams: (empty_str, (log(1.0), log(0.0)))&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;new_beams_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NINF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;NINF&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# log(0.0) = NINF&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_nb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;beams&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_count&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;log_prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emission_log_prob&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;emission_threshold&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="k"&gt;continue&lt;/span&gt;

                &lt;span class="n"&gt;end_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

                &lt;span class="c1"&gt;# if new_prefix == prefix&lt;/span&gt;
                &lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_lp_nb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;blank&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_nb&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
                        &lt;span class="n"&gt;new_lp_nb&lt;/span&gt;
                    &lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="k"&gt;continue&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;end_t&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;new_lp_nb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_nb&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                    &lt;span class="p"&gt;)&lt;/span&gt;

                &lt;span class="c1"&gt;# if new_prefix == prefix + (c,)&lt;/span&gt;
                &lt;span class="n"&gt;new_prefix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prefix&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
                &lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;new_lp_nb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;new_prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;end_t&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;new_prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;new_lp_nb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_nb&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                    &lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;new_prefix&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;new_lp_b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;new_lp_nb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lp_b&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;log_prob&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                    &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# sorted by log(blank_prob + non_blank_prob)&lt;/span&gt;
        &lt;span class="n"&gt;beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_beams_dict&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;logsumexp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;beams&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;beams&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;beam_size&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beams&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;我們在這篇文章當中清楚的了解到 CRNN 的架構，以及 CTC 的架構、訓練的參數優化和其三種 Inference 方法。看完了這些原理，也該動手試玩看看，在 &lt;a href="https://github.com/GitYCC/crnn-pytorch"&gt;GitYCC/crnn-pytorch&lt;/a&gt; 中已經有已經 pretrained 的模型可以使用，不妨跟著以下步驟實際動手玩玩看 OCR 場景辨識吧！&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;$ git clone https://github.com/GitYCC/crnn-pytorch.git&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;$ cd crnn-pytorch/&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;$ pip install -r requirements.txt&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;$ python src/predict.py demo/*.jpg&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1507.05717.pdf"&gt;An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition (2015), Baoguang Shi et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf"&gt;Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks (2006), Alex Graves et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1408.2873.pdf"&gt;First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs (2014), Awni Y. Hannun et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://distill.pub/2017/ctc/"&gt;Sequence Modeling With CTC, Awni Hannun.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/39266552"&gt;对《CTC 原理及实现》中的一些算法的解释&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="CV"></category></entry><entry><title>資源整理：跟上AI前沿知識</title><link href="https://ycc.idv.tw/latest_ai_info.html" rel="alternate"></link><published>2020-07-04T12:00:00+08:00</published><updated>2020-07-04T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-07-04:/latest_ai_info.html</id><summary type="html">&lt;p&gt;AI領域日新月異，在這領域的玩家應該要持續的跟上最前沿的知識和技術，本篇文章整理了相關學術研討會、部落格，讓讀者可以輕易的接觸到可靠的新資源。（也歡迎讀者補充更多資訊）(持續更新)&lt;/p&gt;</summary><content type="html">&lt;h3&gt;arXiv&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/arxiv_logo.png"&gt;&lt;/p&gt;
&lt;p&gt;通常在&lt;a href="https://arxiv.org/"&gt;arXiv&lt;/a&gt;上幾乎可以搜到所有AI領域重要的論文，而且還可以拿到第一手的論文，但是arXiv並沒有嚴格的審核機制，所以在尚未經過其他研討會和期刊審核過之前務必要對內容執懷疑的態度。&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://arxiv.org/corr/subjectclasses"&gt;arXiv: Computer Science分類項目&lt;/a&gt;&lt;/h5&gt;
&lt;h5&gt;&lt;a href="https://arxiv.org/list/cs.AI/recent"&gt;AI項目最新論文&lt;/a&gt;&lt;/h5&gt;
&lt;h5&gt;&lt;a href="https://arxiv.org/list/cs.LG/recent"&gt;ML項目最新論文&lt;/a&gt;&lt;/h5&gt;
&lt;h5&gt;&lt;a href="https://arxiv.org/list/cs.CV/recent"&gt;CV項目最新論文&lt;/a&gt;&lt;/h5&gt;
&lt;h5&gt;&lt;a href="https://arxiv.org/list/cs.CL/recent"&gt;NLP項目最新論文&lt;/a&gt;&lt;/h5&gt;
&lt;h3&gt;Conference 學術研討會&lt;/h3&gt;
&lt;p&gt;AI領域大多數的成果都會以學術研討會的方式呈現，以下列出較具權威的學術研討會。&lt;/p&gt;
&lt;p&gt;以下研討會的順序排名以Google Scholar Metrics為主，h-index代表所有發表論文中至少有h篇分別被引用了至少h次；h-median代表被引用最多的h篇（由h-index決定）論文當中引用次數的中位數。舉例：一個研討會有五篇文章，其被引用次數如下：17, 9, 6, 3, 2，其h-index為3，所以其具影響力的h篇文章被引用數如下：17, 9, 6，因此中位數9就是h-median。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/neurips_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://nips.cc"&gt;NeurIPS (Neural Information Processing Systems)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年的12月舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 169&lt;/code&gt;   &lt;code&gt;h5-median: 334&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;NeurIPS 是目前 AI &amp;amp; ML 領域中最大的研討會，前稱為NIPS&lt;/li&gt;
&lt;li&gt;專注於神經網路與深度學習&lt;/li&gt;
&lt;li&gt;同時在學術界和產業界具有名望&lt;/li&gt;
&lt;li&gt;相較於其他 AI &amp;amp; ML 研討會，NeurIPS 較著重在理論和方法論上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/iclr_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://iclr.cc"&gt;ICLR (International Conference on Learning Representations)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年的4-5月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 150&lt;/code&gt;   &lt;code&gt;h5-median: 276&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;顧名思義，ICLR 是最專注於深度學習的研討會&lt;/li&gt;
&lt;li&gt;ICLR 是非常新的研討會，於2013年舉辦第一屆，但是近年來深度學習火熱，再加上兩位神級創辦人 Yoshua Bengio 和 Yann LeCun 的加持之下，ICLR 的聲望自然是水漲船高&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/icml_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://icml.cc"&gt;ICML (International Conference on Machine Learning)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年的7月舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 135&lt;/code&gt;   &lt;code&gt;h5-median: 254&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;與 NeurIPS 和 ICLR 相比，ICML 專注於更廣義的機器學習&lt;/li&gt;
&lt;li&gt;相較於其他 AI &amp;amp; ML 研討會，ICML 較著重在理論和方法論上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/aaai_logo.jpg"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://aaai.org"&gt;AAAI (AAAI Conference on Artificial Intelligence)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年的2月舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 95&lt;/code&gt;   &lt;code&gt;h5-median: 153&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;屬於綜合性的 AI 研討會&lt;/li&gt;
&lt;li&gt;相較於其他 AI &amp;amp; ML 研討會，AAAI 較著重在實務運用&lt;/li&gt;
&lt;li&gt;AAAI 在產業界相當有名氣&lt;/li&gt;
&lt;li&gt;相當古老的研討會，最早自1980年&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/kdd_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://www.kdd.org"&gt;SIGKDD (Special Interest Group on Knowledge Discovery and Data Mining)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年的8月舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 86&lt;/code&gt;   &lt;code&gt;h5-median: 138&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;也就是俗稱的 KDD，由 ACM (電腦協會) 主辦，ACM通過它的35個特別興趣組 (Special Interest Group，SIG) 提供特殊的技術資訊和服務，其中的SIGKDD主要是專注於資料探勘，它是資料探勘領域最好的研討會&lt;/li&gt;
&lt;li&gt;相較於其他 AI &amp;amp; ML 研討會，KDD 較著重在實務運用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/ijcai_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://www.ijcai.org"&gt;IJCAI (International Joint Conference on Artificial Intelligence)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：2015年之前是單數年舉辦，目前已改成每年7-8月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 67&lt;/code&gt;   &lt;code&gt;h5-median: 100&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;屬於綜合性的 AI 研討會&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/colt.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="http://www.learningtheory.org"&gt;COLT (Conference on Learning Theory)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年6-7月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 48&lt;/code&gt;   &lt;code&gt;h5-median: 65&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;這是計算理論最好的研討會，由 ACM (電腦協會) 主辦&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Computer Vision 電腦視覺&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/cvpr_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="http://cvpr2020.thecvf.com"&gt;CVPR (International Conference on Computer Vision and Pattern Recognition)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年6-7月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 240&lt;/code&gt;   &lt;code&gt;h5-median: 383&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最具權威的電腦視覺研討會，歷史悠久，最早自1985年&lt;/li&gt;
&lt;li&gt;電腦視覺三大頂尖研討會之一&lt;/li&gt;
&lt;li&gt;由 IEEE 舉辦&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/eccv.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://eccv2020.eu"&gt;ECCV (European Conference on Computer Vision)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：與ICCV交替舉辦，於偶數年舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 137&lt;/code&gt;   &lt;code&gt;h5-median: 263&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;電腦視覺三大頂尖研討會之一&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/iccv.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="http://iccv2019.thecvf.com"&gt;ICCV (International Conference on Computer Vision)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：與ECCV交替舉辦，於奇數年舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 129&lt;/code&gt;   &lt;code&gt;h5-median: 220&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;電腦視覺三大頂尖研討會之一&lt;/li&gt;
&lt;li&gt;由 IEEE 舉辦&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Natural Language Processing 自然語言處理&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/acl_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://www.aclweb.org"&gt;ACL (Meeting of the Association for Computational Linguistics)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年7-8月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 106&lt;/code&gt;   &lt;code&gt;h5-median: 168&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;&lt;a href="https://www.aclweb.org"&gt;EMNLP (Conference on Empirical Methods in Natural Language Processing)&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;時間：於每年6-7月間舉辦&lt;/code&gt;   &lt;code&gt;h5-index: 88&lt;/code&gt;   &lt;code&gt;h5-median: 157&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Blogs 部落格&lt;/h3&gt;
&lt;p&gt;許多歐美科技大廠都有寫它們的研究部落格，通常較為通俗易懂。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/googleai_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://ai.googleblog.com/"&gt;Google AI blog&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/deepmind_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://deepmind.com/blog/"&gt;Deepmind blog&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/openai_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://openai.com/research/"&gt;OpenAI Research&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/fb_logo.jpg"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://research.fb.com/category/facebook-ai-research/"&gt;Facebook AI Research&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/ms_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://blogs.microsoft.com/ai/"&gt;Microsoft AI Blog&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/amazon_ai_logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://blog.aboutamazon.com/amazon-ai"&gt;Amazon AI&lt;/a&gt;&lt;/h5&gt;
&lt;h5&gt;&lt;a href="https://aws.amazon.com/tw/blogs/machine-learning/"&gt;AWS Machine Learning Blog&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/nvidia_logo.jpg"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://news.developer.nvidia.com/category/artificial-intelligence/"&gt;NVIDIA News AI&lt;/a&gt;&lt;/h5&gt;
&lt;hr&gt;
&lt;p&gt;還有學術界較知名的部落格。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/mit.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="http://news.mit.edu/topic/artificial-intelligence2"&gt;MIT’s artificial intelligence news&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/BAIR_Logo.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://bair.berkeley.edu/blog/"&gt;Berkeley Artificial Intelligence Research (BAIR) Blog&lt;/a&gt;&lt;/h5&gt;
&lt;hr&gt;
&lt;p&gt;最後列一些其他知名的部落格。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/distill.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://distill.pub/"&gt;Distill&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;嚴格來說Distill並不算是部落格，它比較像是線上的期刊。Distill旨在以一種更加易懂的方式展示AI研究，並結合互動式圖表和圖形，幫助讀者更輕鬆地理解研究。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/ml_theory.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="http://hunch.net/"&gt;Machine Learning (Theory)&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;此部落格的作者為微軟首席研究員 &lt;a href="https://www.microsoft.com/en-us/research/people/jcl/"&gt;John Langford&lt;/a&gt; ，Langford 常常提供有關機器學習理論的深刻見解，並提供了有關國際機器學習大會的最新信息。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/ml_mastery.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://machinelearningmastery.com/blog/"&gt;Machine Learning Mastery&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;身為開發者的 Jason Brownlee 早在幾年前就開始撰寫這個部落格，內容較為貼近如何實用機器學習。到目前它仍然是尋求增加ML概念知識的業界人士的首選部落格。 &lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/LatestAIInfo/analytics_vidhya.png"&gt;&lt;/p&gt;
&lt;h5&gt;&lt;a href="https://www.analyticsvidhya.com/blog/"&gt;Analytics Vidhya&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;Analytics Vidhya的網站為數據科學家們提供了許多的乾貨，包括大量有關AI、機器學習和深度學習的材料。&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=jcL5iOjV7as&amp;amp;feature=youtu.be"&gt;youtube: Top Conferences in Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=nPdwLY15Y-o&amp;amp;feature=youtu.be"&gt;youtube: Top Conferences for Natural Language Processing (NLP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=VtjTgSnKb-I"&gt;youtube: Top Conferences for Computer Vision and Image Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scholar.google.com/citations?view_op=top_venues&amp;amp;hl=en&amp;amp;vq=eng"&gt;google scholar rank&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mropengate.blogspot.com/2019/01/ai.html"&gt;AI 深度學習：工作後的學術資源總整理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/What-are-the-best-conferences-and-journals-about-machine-learning"&gt;quora: What are the best conferences and journals about machine learning?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.springboard.com/blog/machine-learning-blog/"&gt;40 Must-Read AI / Machine Learning Blogs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="AI.ML"></category></entry><entry><title>剖析深度學習 (4)：Sigmoid, Softmax怎麼來？為什麼要用MSE和Cross Entropy？談廣義線性模型</title><link href="https://ycc.idv.tw/deep-dl_4.html" rel="alternate"></link><published>2020-03-14T12:00:00+08:00</published><updated>2020-03-14T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-03-14:/deep-dl_4.html</id><summary type="html">&lt;p&gt;學習一段時間深度學習的你是不是有一個疑惑：Activation Function為什麼要用Sigmoid和Softmax？Loss Function為什麼要用MSE和Cross Entropy？其他狀況要用什麼？當然你可以把它們看作是個合理定義，但是學習深度就端看你是不是可以用最少的定義表示最多的東西，如果你仔細google一下就會發現有一個相關的名詞—廣義線性定理，但是大部分的文章和教材都沒辦法將它講的很清楚，原因是因為沒有先介紹「充分統計量」的概念。在本講你會學到如何用「充分統計量」來說明在廣義線性定理中的Canonical Link Function，進而推導出Activation Function，你會學到如何藉由MLE和MAP來推導出Loss Function，學完以後你會對Activation Function和Loss Function有更深的認識。&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;深度學習發展至今已經有相當多好用的套件，使得進入的門檻大大的降低，因此如果想要快速的實作一些深度學習或機器學習，通常是幾行程式碼可以解決的事。但是，如果想要將深度學習或機器學習當作一份工作，深入了解它背後的原理和數學是必要的，才有可能因地制宜的靈活運用，YC準備在這一系列當中帶大家深入剖析深度學習。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;前面的&lt;a href="https://www.ycc.idv.tw/deep-dl_2.html"&gt;第二講&lt;/a&gt;和&lt;a href="https://www.ycc.idv.tw/deep-dl_3.html"&gt;第三講&lt;/a&gt;其實都是為了這一講而存在。&lt;/p&gt;
&lt;p&gt;學習一段時間深度學習的你是不是有一個疑惑：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Activation Function為什麼要用Sigmoid和Softmax？&lt;/li&gt;
&lt;li&gt;Loss Function為什麼要用MSE和Cross Entropy？&lt;/li&gt;
&lt;li&gt;其他狀況要用什麼？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;當然你可以把它們看作是個合理定義，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;但是學習深度就端看你是不是可以用最少的定義表示最多的東西&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果你仔細google一下就會發現有一個相關名詞—廣義線性定理，但是大部分的文章和教材都沒辦法將它講的很清楚，原因是因為沒有先介紹「充分統計量」的概念。&lt;/p&gt;
&lt;p&gt;在本講你會學到如何用「充分統計量」來說明在廣義線性定理中的Canonical Link Function，進而推導出Activation Function，你會學到如何藉由MLE和MAP來推導出Loss Function，學完以後你會對Activation Function和Loss Function有更深的認識。&lt;/p&gt;
&lt;p&gt;這一篇我可以非常自豪的說，網路上的資料在這個議題上找不到寫的比我更詳細的，這是我看過很多書和教材融會貫通而成的，請大家一定要看到最後，必定收穫滿滿。&lt;/p&gt;
&lt;h3&gt;前情提要&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.ycc.idv.tw/deep-dl_3.html"&gt;上一講&lt;/a&gt;中我們清楚的了解頻率學派和貝氏學派各自的觀點，並且從兩者觀點出發去探討機器學習問題。&lt;/p&gt;
&lt;p&gt;頻率學派使用Maximum Likelihood Estimation (MLE) 來優化，優化關係式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)  \ \ ↪︎【1】
$$&lt;/div&gt;
&lt;p&gt;
此式等價於最小化Data與Model之間的Cross Entropy，或等價於最小化Data與Model之間的KL Divergence，與&lt;a href="https://www.ycc.idv.tw/deep-dl_2.html"&gt;第二講的資訊理論&lt;/a&gt;完美契合。&lt;/p&gt;
&lt;p&gt;貝氏學派則使用Maximum A Posterior (MAP) 來優化，優化關係式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)+\operatorname{ln}p(\theta\mid m)  \ \ ↪︎【2】
$$&lt;/div&gt;
&lt;p&gt;
除了第一項與MLE一樣之外，我們還需要考慮第二項，此項考慮了參數的出現分布，當參數分布是均等時，MAP和MLE是等價的。但是我們希望 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 可以接近0，所以一般會去假設 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 為一個Variance有限且平均值為0的分布，如果選擇使用Normal Distribution，則會得到L2 Regularization Term；如果選擇用Laplace Distribution，則會得到L1 Regularization Term。&lt;/p&gt;
&lt;p&gt;所以接下來要讓機器可以學習只剩下最後一哩路，如何將【1】或【2】變換成擬合問題呢？&lt;/p&gt;
&lt;p&gt;其實只需要找到合適的分布代入 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 就可以了。&lt;/p&gt;
&lt;h3&gt;充分統計量 (sufficient statistic)&lt;/h3&gt;
&lt;p&gt;在這之前要引入一個重要的統計工具，那就是「充分統計量」。&lt;/p&gt;
&lt;p&gt;什麼「充分統計量」呢？Wiki的定義是&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在統計學中，關於一個統計模型和其相關的未知參數的充分統計量是指「沒有任何其他可以從同一樣本中計算得出的統計量可以提供任何有關未知參數的額外信息」。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其數學表示式為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_\theta (y_1,y_2,..,y_n)=h(y_1,y_2,..,y_n)g_\theta(T(y_1,y_2,..,y_n))  \ \ ↪︎【3】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(f_\theta(.)\)&lt;/span&gt;為你的Model假定的分布，當中包含決定Model的參數&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;；&lt;span class="math"&gt;\(y_1,y_2,..,y_n\)&lt;/span&gt; 為多筆資料；&lt;span class="math"&gt;\(T(.)\)&lt;/span&gt; 稱為充分統計量，可以是一個單值或矩陣，它可以讓唯一包含 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 資訊的 &lt;span class="math"&gt;\(g_\theta\)&lt;/span&gt; 不在直接depend on &lt;span class="math"&gt;\(y_i\)&lt;/span&gt;，而是depend on &lt;span class="math"&gt;\(T(.)\)&lt;/span&gt; ，也因此做到了「沒有任何其他可以從同一樣本中計算得出的統計量可以提供任何有關未知參數的額外信息」，因為唯一包含未知參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 的 &lt;span class="math"&gt;\(g_\theta(.)\)&lt;/span&gt; 只需要 &lt;span class="math"&gt;\(T(.)\)&lt;/span&gt; 當作Input，其餘的統計量皆不需要，此時我們會稱 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 為充分統計量。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;還是以我們相當熟悉的Normal Distribution當作例子 （如果不熟悉，&lt;a href="https://www.ycc.idv.tw/deep-dl_1.html"&gt;請詳見第一講&lt;/a&gt;） ：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{\theta}(y_1,y_2,..,y_n)=(\frac{1}{\sqrt{2\pi}\sigma_\theta})^n exp\{{\sum_i-\frac{1}{2\sigma_\theta^2}(y_i-\mu_\theta)^2}\}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=(\frac{1}{\sqrt{2\pi}\sigma_\theta})^nexp\{{\sum_i-\frac{1}{2\sigma_\theta^2}(y_i^2-2\mu_\theta y_i+\mu_\theta^2)}\}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=(\frac{1}{\sqrt{2\pi}\sigma_\theta})^nexp\{{-\frac{1}{2\sigma_\theta^2}(\sum_iy_i^2-2\mu_\theta \sum_iy_i+n\mu_\theta^2)}\}  \ \ ↪︎【4】
$$&lt;/div&gt;
&lt;p&gt;當我定義兩個充分統計量 &lt;span class="math"&gt;\(S_1=\sum_iy_i\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(S_2=\sum_iy_i^2\)&lt;/span&gt; ，所以充分統計量為
&lt;/p&gt;
&lt;div class="math"&gt;$$
T=\begin{bmatrix} S_1 \\ S_2 \end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;
代入得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{\theta}(y_1,y_2,..,y_n)=(\frac{1}{\sqrt{2\pi}\sigma_\theta})^n exp\{{-\frac{1}{2\sigma_\theta^2}(\begin{bmatrix} -2\mu_\theta \ 1 \end{bmatrix}\begin{bmatrix} S_1 \\ S_2 \end{bmatrix}+n\mu_\theta^2)}\}  \ \ ↪︎【5】
$$&lt;/div&gt;
&lt;p&gt;
此時整個分布都不需要depend on &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; ，只depend on 充分統計量 &lt;span class="math"&gt;\(T(.)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;將【5】式取 &lt;span class="math"&gt;\(\operatorname{ln}\)&lt;/span&gt; ，就得到它的log probability：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\operatorname{ln}p_{\theta}(y_1,y_2,..,y_n)=n\operatorname{ln}\frac{1}{\sqrt{2\pi}\sigma_\theta}-\frac{1}{2\sigma_\theta^2}(S_2-2\mu_\theta S_1+n\mu_\theta^2)  \ \ ↪︎【6】
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;這樣分離有什麼好處？好處是當我們需要Maximum &lt;span class="math"&gt;\(\operatorname{ln}p_{\theta}(y_1,y_2,..,y_n)\)&lt;/span&gt; 時事情會變得容易。&lt;/p&gt;
&lt;p&gt;假設我想利用數據 &lt;span class="math"&gt;\(y_1,y_2,..,y_n\)&lt;/span&gt; 找一組參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 使 &lt;span class="math"&gt;\(\operatorname{ln}p_{\theta}(y_1,y_2,..,y_n)\)&lt;/span&gt; 最大：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta^*=argmax_\theta\ \operatorname{ln}f_{\theta}(y_1,y_2,..,y_n)=argmax_\theta\ \operatorname{ln}h(y_1,y_2,..,y_n)+\operatorname{ln}g_\theta(\{T(y_1,y_2,..,y_n)\})
$$&lt;/div&gt;
&lt;p&gt;
其優化式為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial}{\partial \theta}\operatorname{ln}f_{\theta}(y_1,y_2,..,y_n)|_{\theta^*}=\frac{\partial}{\partial \theta}\operatorname{ln}g_\theta(\{T(y_1,y_2,..,y_n)\})|_{\theta^*}
$$&lt;/div&gt;
&lt;p&gt;
所以：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial}{\partial \theta}\operatorname{ln}g_\theta(\{T(y_1,y_2,..,y_n)\})|_{\theta^*}=0  \ \ ↪︎【7】
$$&lt;/div&gt;
&lt;p&gt;
這個關係式就足以讓我們找到最佳的 &lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt; ，而其只與充分統計量 &lt;span class="math"&gt;\(\{T(...)\}\)&lt;/span&gt; 有關，也就是說：&lt;strong&gt;當我從數據當中統計出充分統計量 &lt;span class="math"&gt;\(\{T(...)\}\)&lt;/span&gt; 就足以讓我找到最佳的Model參數&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;繼續剛剛的例子，將【6】代入【7】就可以得到Model的最佳參數：
&lt;/p&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial}{\partial \mu_{\theta}}[n\operatorname{ln}\frac{1}{\sqrt{2\pi}\sigma_\theta}-\frac{1}{2\sigma_\theta^2}(S_2-2\mu_\theta S_1+n\mu_\theta^2)]=\frac{1}{2\sigma_\theta^2}(-2S_1+2n\mu_{\theta})
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\Rightarrow \mu_{\theta}=\frac{1}{n}S_1=\frac{1}{n}\sum_iy_i  \ \ ↪︎【8】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial}{\partial \sigma_{\theta}}[n\operatorname{ln}\frac{1}{\sqrt{2\pi}\sigma_\theta}-\frac{1}{2\sigma_\theta^2}(S_2-2\mu_\theta S_1+n\mu_\theta^2)]=-\frac{n}{\sigma_\theta}+\frac{1}{\sigma_\theta^3}(S_2-\frac{1}{n}S_1^2)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\Rightarrow \sigma_\theta^2=\frac{S_2}{n}-(\frac{S_1}{n})^2=(\frac{1}{n}\sum_iy_i^2)-(\frac{1}{n}\sum_iy_i)^2  \ \ ↪︎【9】
$$&lt;/div&gt;
&lt;p&gt;是不是跟我們之前學的東西是自恰的啊！&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;現在你知道我不得不提「充分統計量」這個概念的原因了吧！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有了「充分統計量」的概念，拿到一個分布你可以清楚的知道：我需要哪些必要的統計量才可以擬合這個分布，並且可以透過Maximun Log Probability輕易的找到這些充分統計量對應的Model變數。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;廣義線性模型（Generalized Linear Models, GLM）&lt;/h3&gt;
&lt;p&gt;如果你看其他的介紹文章，通常會先講古典線性模型，再講廣義線性模型，我這邊會反過來講，因為古典線性模型只是廣義線性模型的特例 — 當假設Normal Distribution時，所以只要真正搞懂廣義線性模型，古典線性模型也就懂了。&lt;/p&gt;
&lt;p&gt;我們手上現在會有兩個東西：分布模型和線性擬合模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分布模型：就是 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt;，搭配優化準則MLE和MAP就可以找最佳參數&lt;/li&gt;
&lt;li&gt;擬合模型：寫作為 &lt;span class="math"&gt;\(h(x_i,m,\theta)=\theta^0+\sum_k \theta_i^kx_i^k  \ \ ↪︎【10】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如何將這兩者連繫起來呢？我們透過Mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; 和 Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; 來做到：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu=E_{y\sim p(y_i\mid x_i,m,\theta)}[y]  \ \ ↪︎【11】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
g(\mu)=h(x_i,m,\theta)  \ \ ↪︎【12】
$$&lt;/div&gt;
&lt;p&gt;其中：擬合模型 &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 負責擬合Mean經Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; 轉換後的值。這個Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; 其實限制很少，只需要符合兩點即可：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Link Function必須是單調遞增（monotonic）&lt;/li&gt;
&lt;li&gt;Link Function的值域必須能夠覆蓋理論分布的空間&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;所以接下來作法就容易了，只要依以下步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;決定好分布模型 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 且決定好Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;算出模型的Mean並透過Link Function來連接Mean和擬合模型： &lt;span class="math"&gt;\(\mu=E_{y\sim p(y_i\mid x_i,m,\theta)}[y]=g^{-1}(h(x_i,m,\theta))\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;用上面的關係式將 &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 代換到 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 裡&lt;/li&gt;
&lt;li&gt;利用MLE和MAP來找尋最佳參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;古典線性模型&lt;/h3&gt;
&lt;p&gt;接下來我們就來演示一下廣義線性模型的特例—古典線性模型，&lt;strong&gt;古典線性模型使用Normal Distribution當分布模型 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt;，並且使用Identity Function當作 Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; &lt;/strong&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用Normal Distribution和Identity Link Function
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(y_i-\mu)^2}\}  \ \ ↪︎【13】
   $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
   g(\mu)=\mu  \ \ ↪︎【14】
   $$&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;算出模型的Mean並透過Link Function來連接Mean和線性擬合模型
   &lt;div class="math"&gt;$$
   \mu=g^{-1}(h(x_i,m,\theta))=h(x_i,m,\theta)  \ \ ↪︎【15】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用上面的關係式將 &lt;span class="math"&gt;\(h(x_i,\theta)\)&lt;/span&gt; 代換到 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 裡
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(y_i-h(x_i,m,\theta))^2}\}  \ \ ↪︎【16】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用MLE和MAP來找尋最佳參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;
   將【16】式代入【1】式，經化簡可得：
   &lt;div class="math"&gt;$$
   \theta_{MLE}=argmax_\theta\ \sum_{i}\ -\frac{1}{2\sigma^2}(y_i-h(x_i,m,\theta))^2  \ \ ↪︎【17】
   $$&lt;/div&gt;
   因為擬合模型只對 &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; 感興趣，所以這裡令 &lt;span class="math"&gt;\(\sigma^2=1\)&lt;/span&gt; ，得：
   &lt;div class="math"&gt;$$
   \theta_{MLE}=argmin_\theta\ \sum_{i}\ \frac{1}{2}(y_i-h(x_i,m,\theta))^2  \ \ ↪︎【18】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上面的式子就是Loss Function為Mean Squared Error (MSE) 的Regression，它背後假設的分布就是Normal Distribution。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這裡的 &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 可以是線性的，當然如果將feature space先作非線性轉換得 &lt;span class="math"&gt;\(z_i\)&lt;/span&gt; 再代入得 &lt;span class="math"&gt;\(h(z_i,m,\theta)\)&lt;/span&gt; 也可以是非線性的，當然 &lt;span class="math"&gt;\(h(z_i,m,\theta)\)&lt;/span&gt; 可以是Neural Network，先用前面幾層Hidden Layers做非線性轉換，在拿轉換後的結果與數據做線性擬合。其中：&lt;span class="math"&gt;\(m\)&lt;/span&gt; 代表的是Network的Hyperparameters，包括：Network的結構、Learning Rate的大小、Batch Size、...等等；而 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 則是Network中需要學習的權重。&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下來我要問一個問題：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;其他分布能使用古典線性模型嗎？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;假設今天是Binary Classification的問題，則其分布不再是Normal Distribution，而是Bernoulli Distribution：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(y_i\mid x_i,m,\theta)=\pi^{y_i}(1-\pi)^{1-y_i}  \ \ ↪︎【19】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(y_i \in \{0,1\}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\pi\)&lt;/span&gt; 代表 &lt;span class="math"&gt;\(y_i=1\)&lt;/span&gt; 的機率。&lt;/p&gt;
&lt;p&gt;一樣假設Identity Function當作 Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; 
&lt;/p&gt;
&lt;div class="math"&gt;$$
h(x_i,m,\theta)=E_{y\sim p_{model}}[y_i]=\sum_{y_i=0}^{y_i=1} y_i \pi^{y_i}(1-\pi)^{1-y_i}=\pi
$$&lt;/div&gt;
&lt;p&gt;
這樣對嗎？其實是錯誤的。Link Function沒有對應到正確的值域，所以 &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 可以是整個實數空間，而 &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; 卻只能是落在0到1之間，等式不能成立。&lt;/p&gt;
&lt;p&gt;所以古典線性模型並不能適用於非Normal Distribution，我們必須找其他合適的Link Function。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;事實上，認真找可以找到若干個正確符合的 Link Function，但是只有唯一一種Link Function符合讓Mean符合「充分統計」，這就是 Canonical Link Function。而其他符合的 Link Function 則稱為 Non-Canonical Link Function。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;指數族分布（Exponential Family）與 Canonical Link Function&lt;/h3&gt;
&lt;p&gt;要談Canonical Link Function就必須要談「指數族分布」，指數族分布很好的囊括了常見的分布，包括：Normal Distribution、Bernoulli Distribution、Poisson Distribution、...等等，而且還具備了許多良好的性質。&lt;/p&gt;
&lt;p&gt;指數族分布定義如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(y\mid \eta)=h(y)exp\{\eta^T T(y)-A(\eta)\}  \ \ ↪︎【20】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta\)&lt;/span&gt; : Natural Parameters or Linear Predictor&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A(\eta)\)&lt;/span&gt;: Log Partition Function or Log Normalizer&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y)\)&lt;/span&gt;: sufficient statistics (充分統計量)，通常的分布是 &lt;span class="math"&gt;\(T(y)=y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h(y)\)&lt;/span&gt;: base measure&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因為機率加總為1，所以 &lt;span class="math"&gt;\(A(\eta)\)&lt;/span&gt; 的型式是受其他變數的影響：
&lt;/p&gt;
&lt;div class="math"&gt;$$
A(\eta)=ln[\int h(y)exp\{\eta^T T(y)\}dy]  \ \ ↪︎【21】
$$&lt;/div&gt;
&lt;p&gt;
然後指數族分布有一些重要的數學關係式（證明詳見&lt;a href="https://www.cs.princeton.edu/~bee/courses/scribe/lec_09_02_2013.pdf"&gt;此篇&lt;/a&gt;）
&lt;/p&gt;
&lt;div class="math"&gt;$$
E[T(y)|\eta]=\frac{\partial A(\eta)}{\partial \eta}  \ \ ↪︎【22】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
Var[T(y)|\eta]=\frac{\partial^2 A(\eta)}{\partial \eta\partial \eta}  \ \ ↪︎【23】
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;列下考慮多筆Data的情況：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(y_1,y_2,..,y_n|\eta)=[\prod_i h(y_i)]exp\{\eta^T\sum_i T(y_i)-nA(\eta)\}  \ \ ↪︎【24】
$$&lt;/div&gt;
&lt;p&gt;
仔細觀察【24】式和【3】式：指數族分布的 &lt;span class="math"&gt;\(T(y)\)&lt;/span&gt; 符合「充分統計」，你會發現 &lt;span class="math"&gt;\(exp\{.\}\)&lt;/span&gt; 這裡對映到的是 &lt;span class="math"&gt;\(g_\theta\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;所以我們可以依循著剛剛的套路找到最佳參數與統計量的對映，使用【7】式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial}{\partial \eta}[\eta^T \sum_iT(y_i)-nA(\eta)]=\sum_i T(y_i)-n\frac{\partial A(\eta)}{\partial \eta}=\sum_i T(y_i)-n\cdot E[T(y)|\eta]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\Rightarrow E[T(y)|\eta^*]=\frac{1}{n} \sum_{i=1}^{n} T(y_i)  \ \ ↪︎【24】
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;因此指數族分布隱含著一個相當好的特性：在最好的參數 &lt;span class="math"&gt;\(\eta^*\)&lt;/span&gt; 之下，分布對 &lt;span class="math"&gt;\(T(y)\)&lt;/span&gt; 的期望值就等同於你量測 &lt;span class="math"&gt;\(T(y_i)\)&lt;/span&gt; 的平均值，而且 &lt;span class="math"&gt;\(T(y_i)\)&lt;/span&gt; 還是一個充分統計量，也就是說你已經不需要其他統計量了。&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;當 &lt;span class="math"&gt;\(T(y)=y\)&lt;/span&gt; (大部分情形都是)，可得與Mean的關係式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu=\frac{1}{n} \sum_{i=1}^{n} y_i=E[y|\eta^*]=\frac{\partial A(\eta)}{\partial \eta}|_{\eta^*}  \ \ ↪︎【25】
$$&lt;/div&gt;
&lt;p&gt;
因此我們建立了 Mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\eta\)&lt;/span&gt; 的關係，假設：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial A(\eta)}{\partial \eta}=g^{-1}(\eta)=\mu  \ \ ↪︎【26】
$$&lt;/div&gt;
&lt;p&gt;
&lt;strong&gt;其中： &lt;span class="math"&gt;\(g^{-1}(.)\)&lt;/span&gt; 就是大名鼎鼎的 Activation Function 。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;此時，讓 &lt;span class="math"&gt;\(\eta=h(x_i,m,\theta)\)&lt;/span&gt;，則Link Function &lt;span class="math"&gt;\(g(.)\)&lt;/span&gt; 稱為Canonical Link Function：
&lt;/p&gt;
&lt;div class="math"&gt;$$
g(.)=(\frac{\partial A(\eta)}{\partial \eta})^{-1}  \ \ ↪︎【27】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
g(\mu)=h(x_i,m,\theta)  \ \ ↪︎【28】
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;在符合指數族分布的情況下，採用 &lt;span class="math"&gt;\(\eta\)&lt;/span&gt; 和 Canonical Link Function，會讓Mean變成為充分統計量，這意味著我們很有效率的使用著數據。&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;現在我們可以回頭加深觀念：&lt;strong&gt;為什麼在古典模型當中會選擇 Identity Link Function 呢？因為對於Normal Distribution而言，Identity Link Function 是 Canonical Link Function。&lt;/strong&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{normal}(y)=\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(y-\mu)^2}\}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\frac{1}{\sqrt{2\pi}}exp\{\frac{\mu}{\sigma^2}y-\frac{1}{2\sigma^2}y^2-(ln\sigma+\frac{1}{2\sigma^2}\mu^2)\}  \ \ ↪︎【29】
$$&lt;/div&gt;
&lt;p&gt;對應【20】式得：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;div class="math"&gt;$$
  h(y)=\frac{1}{\sqrt{2\pi}}  \ \ ↪︎【30】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class="math"&gt;$$
  \eta=\begin{bmatrix} \mu/\sigma^2 \\ -1/2\sigma^2 \end{bmatrix}  \ \ ↪︎【31】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class="math"&gt;$$
  T(y)=\begin{bmatrix} y \\ y^2 \end{bmatrix}  \ \ ↪︎【32】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;div class="math"&gt;$$
  A(\eta)=\operatorname{ln}\sigma+\frac{1}{2\sigma^2}\mu^2=-\frac{1}{2}\operatorname{ln}(-2\eta_2) -\frac{\eta_1^2}{4\eta_2}  \ \ ↪︎【33】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再代入【20】式找 Link Function：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial A(\eta)}{\partial \eta}=\begin{bmatrix} -\eta_1/2\eta_2 \\ -1/2\eta_2+\eta_1^2/4\eta_2^2 \end{bmatrix}=\begin{bmatrix} \mu \\ \mu^2+\sigma^2 \end{bmatrix}  \ \ ↪︎【34】
$$&lt;/div&gt;
&lt;p&gt;
有兩項充分統計量，所以要擬合一個Normal Distribution需要兩個統計量（其實我們剛才我們已經知道）。但是對於線性模型我們只需要 Mean &lt;span class="math"&gt;\(\mu\)&lt;/span&gt; ，所以我們只考慮第一項：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial A(\eta)}{\partial \eta}|_0=g^{-1}(\eta)=\mu  \ \ ↪︎【35】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\mu=g^{-1}(\eta) \Rightarrow g(\mu)=\mu  \ \ ↪︎【36】
$$&lt;/div&gt;
&lt;p&gt;得證，確實古典模型在使用Normal Distribution時選擇的 Identity Link Function 是Canonical Link Function。&lt;/p&gt;
&lt;h3&gt;Binary Classification：從GLM推出Sigmoid和(狹義的)Cross Entropy&lt;/h3&gt;
&lt;p&gt;再重新來看Bernoulli Distribution。&lt;/p&gt;
&lt;p&gt;再寫一次【19】式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(y_i\mid x_i,m,\theta)=\pi^{y_i}(1-\pi)^{1-y_i}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=exp\{\operatorname{ln}(\frac{\pi}{1-\pi})y+\operatorname{ln}(1-\pi)\}
$$&lt;/div&gt;
&lt;p&gt;對應【20】式得：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h(y)=1  \ \ ↪︎【37】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta=\operatorname{ln}(\frac{\pi}{1-\pi}) \ \ ↪︎【38】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T(y)=y  \ \ ↪︎【39】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A(\eta)=-\operatorname{ln}(1-\pi)=ln(1+e^\eta)  \ \ ↪︎【40】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再代入【20】式找 Canonical Link Function：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial A(\eta)}{\partial \eta}=\frac{1}{1+e^{-\eta}}=g^{-1}(\eta)=\mu \Rightarrow g(\mu)=\operatorname{ln}(\frac{\mu}{1-\mu})  \ \ ↪︎【41】
$$&lt;/div&gt;
&lt;p&gt;
其中： &lt;span class="math"&gt;\(\frac{1}{1+e^{-\eta}}\)&lt;/span&gt; 就是Sigmoid Function，計作 &lt;span class="math"&gt;\(\sigma(.)\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
\Rightarrow \mu=\sigma(h(x_i,m,\theta))  \ \ ↪︎【42】
$$&lt;/div&gt;
&lt;p&gt;
上式就是我們擬合的關係式， &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 可以是線性方程式，也可以是Neural Network，然後有注意到嗎？&lt;strong&gt;Sigmoid 剛剛好是 &lt;span class="math"&gt;\(h(x_i,m,\theta)\)&lt;/span&gt; 輸出後的最後一層，我們使用廣義線性定理就自然而然的得到Activation Function，這就是為什麼在Binary Classification問題中NN最後一層是Sigmoid的原因 &lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下來就按步驟求出最佳參數：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用Bernoulli Distribution和相應的 Canonical Link Function
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=\pi^{y_i}(1-\pi)^{1-y_i}  \ \ ↪︎【43】
   $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
   g(\mu)=\operatorname{ln}(\frac{\mu}{1-\mu})  \ \ ↪︎【44】
   $$&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;算出模型的Mean並透過Link Function來連接Mean和線性擬合模型
   &lt;div class="math"&gt;$$
   \pi=g^{-1}(h(x_i,m,\theta))=\sigma (h(x_i,m,\theta) ) \ \ ↪︎【45】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用上面的關係式將 &lt;span class="math"&gt;\(h(x_i,\theta)\)&lt;/span&gt; 代換到 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 裡
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=(\sigma (h(x_i,m,\theta) ))^{y_i}(1-\sigma (h(x_i,m,\theta) ))^{1-y_i}  \ \ ↪︎【46】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用MLE和MAP來找尋最佳參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;
   將【46】式代入【1】式，經化簡可得：
   &lt;div class="math"&gt;$$
   \theta_{MLE}=argmin_\theta\ \sum_{i}\ -y_i \operatorname{ln}(p_i)-(1-y_i)\operatorname{ln}(1-p_i) \ \ ↪︎【47】
   $$&lt;/div&gt;
   其中： &lt;span class="math"&gt;\(p_i=\sigma (h(x_i,m,\theta))\)&lt;/span&gt;。沒錯！我們推出了(狹義的)Cross Entropy。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Multi-class Classification：從GLM推出Softmax和(狹義的)Cross Entropy&lt;/h3&gt;
&lt;p&gt;Categorical Distribution的分布：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(y_i\mid x_i,m,\theta)=\prod_{j=1}^{k-1}\phi_j^{\delta(y_i=j)}\cdot \phi_k^{1-\sum_{j=1}^{k-1}\delta(y_i=j)}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=exp\{\sum_{j=1}^{k-1}\delta(y_i=j)\operatorname{ln}\phi_j+(1-\sum_{j=1}^{k-1}\delta(y_i=j))\operatorname{ln}\phi_k \}
$$&lt;/div&gt;
&lt;p&gt;將第二個 &lt;span class="math"&gt;\(\sum\)&lt;/span&gt; 猜開放到前一個&lt;span class="math"&gt;\(\sum\)&lt;/span&gt; 裡：
&lt;/p&gt;
&lt;div class="math"&gt;$$
=exp\{\sum_{j=1}^{k-1}\delta(y_i=j)\operatorname{ln}\frac{\phi_j}{\phi_k}+\operatorname{ln}\phi_k \} \ \ ↪︎【48】
$$&lt;/div&gt;
&lt;p&gt;
對應【20】式得：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(h(y)=1  \ \ ↪︎【49】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\eta_j=\operatorname{ln}\frac{\phi_j}{\phi_k};\ \ \ (j=1,...,k-1) \ \ ↪︎【50】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(T_j(y)=\delta(y=j)  \ \ ↪︎【51】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(A(\eta_1,...,\eta_{k-1})=-\operatorname{ln}\phi_k=\operatorname{ln}[\sum_{j=1}^{k}e^{\eta_j}]  \ \ ↪︎【52】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;[堆導] &lt;span class="math"&gt;\(A(\eta_j)\)&lt;/span&gt; 的計算過程，從【50】式出發：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\eta_j=\operatorname{ln}(\frac{\phi_j}{\phi_k}) \Rightarrow \phi_ke^{\eta_j}=\phi_j  \ \ ↪︎【53】
$$&lt;/div&gt;
&lt;p&gt;胡亂假設 &lt;span class="math"&gt;\(\phi_k\)&lt;/span&gt; 存在，接下來加總所有的 &lt;span class="math"&gt;\(\phi_j\)&lt;/span&gt; 應該為 1：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\Rightarrow \sum_{j=1}^{k}\phi_ke^{\eta_j}=\sum_{j=1}^{k}\phi_j=1  \ \ ↪︎【54】
$$&lt;/div&gt;
&lt;p&gt;
所以：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi_k=\frac{1}{\sum_{j=1}^{k}e^{\eta_j}}  \ \ ↪︎【55】
$$&lt;/div&gt;
&lt;p&gt;回代【53】式，得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\phi_j=\frac{e^{\eta_j}}{\sum_{j=1}^{k}e^{\eta_j}}  \ \ ↪︎【56】
$$&lt;/div&gt;
&lt;p&gt;剛剛我雖然胡亂假設有&lt;span class="math"&gt;\(\phi_k\)&lt;/span&gt;的存在，不過做完的結果並不違和，&lt;span class="math"&gt;\(e^{\eta_j}\)&lt;/span&gt; 作為各項的機率，並且除上所有機率的相加 &lt;span class="math"&gt;\(\sum_{j=1}^{k}e^{\eta_j}\)&lt;/span&gt;，可以確保所有機率總和為 &lt;span class="math"&gt;\(\sum_{j=1}^{k}\phi_j=1\)&lt;/span&gt; 。所以：
&lt;/p&gt;
&lt;div class="math"&gt;$$
A(\eta_1,...,\eta_{k-1})=-\operatorname{ln}\phi_k=\operatorname{ln}[\sum_{j=1}^{k}e^{\eta_j}]  \ \ ↪︎【57】
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;將【52】式代入【20】式找 Canonical Link Function：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{\partial A(\eta_1,...,\eta_{k-1})}{\partial \eta_j}=\frac{e^{\eta_j}}{\sum_{j=1}^{k-1}e^{\eta_j}}=g^{-1}(\eta_j)  \ \ ↪︎【58】
$$&lt;/div&gt;
&lt;p&gt;
其中： &lt;span class="math"&gt;\(\frac{e^{\eta_j}}{\sum_{j=1}^{k-1}e^{\eta_j}}\)&lt;/span&gt; 就是Softmax Function，計作 &lt;span class="math"&gt;\(softmax\{.\}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mu_j=softmax(h_j(x_i,m,\theta))  \ \ ↪︎【59】
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;接下來就按步驟求出最佳參數：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用Categorical Distribution和相應的 Canonical Link Function
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=\prod_{j=1}^{k}\phi_j^{\delta(y_i=j)}  \ \ ↪︎【60】
   $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$
   g^{-1}(\eta_j)=softmax\{\eta_j\}  \ \ ↪︎【61】
   $$&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;算出模型的Mean並透過Link Function來連接Mean和線性擬合模型
   &lt;div class="math"&gt;$$
   \phi_j=g^{-1}(h(x_i,m,\theta))=softmax\{h_j(x_i,m,\theta)\} \ \ ↪︎【62】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用上面的關係式將 &lt;span class="math"&gt;\(h(x_i,\theta)\)&lt;/span&gt; 代換到 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 裡
   &lt;div class="math"&gt;$$
   p(y_i\mid x_i,m,\theta)=\prod_{j=1}^{k}softmax\{h_j(x_i,m,\theta)\}^{\delta(y_i=j)}  \ \ ↪︎【63】
   $$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;利用MLE和MAP來找尋最佳參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;
   將【63】式代入【1】式，經化簡可得：
   &lt;div class="math"&gt;$$
   \theta_{MLE}=argmin_\theta\ \sum_{i}\sum_{j} -\delta(y_i=j)\operatorname{ln}p_{i,j} \ \ ↪︎【64】
   $$&lt;/div&gt;
   其中： &lt;span class="math"&gt;\(p_{i,j}=softmax\{h_j(x_i,m,\theta)\}\)&lt;/span&gt;。我們也推出了Multi-class Cross Entropy。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;結論&lt;/h3&gt;
&lt;p&gt;恭喜大家堅持到這裡，應該會收穫不少。以後別人問你為什麼使用Mean Square Error？為什麼這裡要加Sigmoid？為什麼這裡要用 (狹義的) Cross Entropy？為什麼這裡卻要用Softmax？你都可以輕易的回答，甚至給你另外一個Distribution，例如：Possion Distribution，你也可以推出它的Canonical Link Function，也可以知道應該要用什麼樣的Loss去優化，你已經融會貫通了！&lt;/p&gt;
&lt;p&gt;再複習一下！&lt;/p&gt;
&lt;p&gt;為了將上章節提到的MLE和MAP化作擬合問題實際用數據去訓練Model，我們需要廣義線性定理，廣義線性定理必須藉由 Link Function 來連接擬合模型和分布模型，這樣就可以藉由MLE和MAP來優化擬合模型內的參數， Link Function 的限制只有兩條：單調遞增和值域覆蓋。&lt;/p&gt;
&lt;p&gt;但是任意取的話，其平均值不一定是模型的「充分統計量」，而當 Link Function 為 Canonical Link Function時，平均值正是「充分統計量」，因此意味著我們可以很有效率的使用著數據，而Canonical Link Function的反函數正是大名鼎鼎的Activation Function，所以在廣義線性模型的推導中自然會得到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression問題時，Normal Distribution使用Linear當Activation Function&lt;/li&gt;
&lt;li&gt;Binary Classification問題時，Bernoulli Distribution使用Sigmoid當Activation Function&lt;/li&gt;
&lt;li&gt;Multi-class Classification問題時，Categorical Distribution使用Softmax當Activation Function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;當定義完成含有擬合參數的分布模型後，我們就可以用MLE或MAP來找到擬合的優化方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression問題時，使用Mean Square Error&lt;/li&gt;
&lt;li&gt;Binary Classification問題時，使用（狹義的）Cross Entropy&lt;/li&gt;
&lt;li&gt;Multi-class Classification問題時，使用 Multi-class Cross Entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有這些以前不加解釋的東西，都可以由廣義線性定理推導出來。&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.deeplearningbook.org"&gt;Ian Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Christopher Bishop. Pattern Recognition and Machine Learning. 2006.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/ml-notes-why-the-least-square-error-bf27fdd9a721"&gt;ML notes: why the Least Square Error?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.princeton.edu/~bee/courses/scribe/lec_09_02_2013.pdf"&gt;Introduction: exponential family, conjugacy, and sufficiency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cs.princeton.edu/~bee/courses/scribe/lec_09_02_2013.pdf"&gt;Generalized Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/generalized-linear-models-9cbf848bb8ab"&gt;towardsdatascience: Generalized linear models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Sufficient_statistic"&gt;Wiki: sufficient statistic&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Generalized_linear_model"&gt;Wiki: Generalized linear model&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Exponential_family#Properties"&gt;Wiki: Exponential family&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;http://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf&lt;/li&gt;
&lt;li&gt;http://zhouyichu.com/machine-learning/Generalized-Linear-Models/&lt;/li&gt;
&lt;li&gt;http://www.airc.org.tw/newsfiles/r.pdf&lt;/li&gt;
&lt;li&gt;https://www.flutterbys.com.au/stats/tut/tut10.4.html&lt;/li&gt;
&lt;li&gt;https://stats.stackexchange.com/questions/40876/what-is-the-difference-between-a-link-function-and-a-canonical-link-function&lt;/li&gt;
&lt;li&gt;https://stats.stackexchange.com/questions/288451/why-is-mean-squared-error-the-cross-entropy-between-the-empirical-distribution-a&lt;/li&gt;
&lt;li&gt;https://ithelp.ithome.com.tw/articles/10200862&lt;/li&gt;
&lt;li&gt;&lt;a href="http://benz.nchu.edu.tw/~kucst/數統CH7-CH9.pdf"&gt;Chapter 7 &amp;amp; 8 Sufficient Statistics &amp;amp; More about Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;https://beginningwithml.wordpress.com/2018/06/22/3-4-softmax-regression/&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;[此文章為原創文章，轉載前請註明文章來源]&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;20200603: 修正從【4】到【9】式的公式錯誤（感謝 俊嘉 細心的揪出公式的錯誤）&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="剖析深度學習"></category></entry><entry><title>剖析深度學習 (3)：MLE、MAP差在哪？談機器學習裡的兩大統計觀點</title><link href="https://ycc.idv.tw/deep-dl_3.html" rel="alternate"></link><published>2020-03-07T12:00:00+08:00</published><updated>2020-03-07T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-03-07:/deep-dl_3.html</id><summary type="html">&lt;p&gt;本講主要探討統計的兩大學派（頻率學派和貝氏學派）對於機器如何學習的觀點。頻率學派主張Maximum Likelihood Estimation (MLE)，會提到這等同於最小化data與model之間的Cross Entropy或KL Divergence。而貝氏學派則主張Maximum A Posterior (MAP) ，會提到這會等同於極大化Likelihood並同時考慮Regularization Term，我們也可以在本講看到L1和L2 Regularation Term是怎麼被導出的。&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;深度學習發展至今已經有相當多好用的套件，使得進入的門檻大大的降低，因此如果想要快速的實作一些深度學習或機器學習，通常是幾行程式碼可以解決的事。但是，如果想要將深度學習或機器學習當作一份工作，深入了解它背後的原理和數學是必要的，才有可能因地制宜的靈活運用，YC準備在這一系列當中帶大家深入剖析深度學習。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本講主要探討統計的兩大學派（頻率學派和貝氏學派）對於機器如何學習的觀點。頻率學派主張Maximum Likelihood Estimation (MLE)，會提到這等同於最小化data與model之間的Cross Entropy或KL Divergence。而貝氏學派則主張Maximum A Posterior (MAP) ，會提到這會等同於極大化Likelihood並同時考慮Regularization Term，我們也可以在本講看到L1和L2 Regularation Term是怎麼被導出的。&lt;/p&gt;
&lt;h3&gt;條件機率&lt;/h3&gt;
&lt;p&gt;因為本講會牽涉到許多條件機率的計算，所以把條件機率常用的公式先列下來，讓大家溫習一下。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;邊際機率（marginal probability）
    &lt;div class="math"&gt;$$
    p(X=x_i)=\sum_j p(X=x_i, Y=y_j)  \ \ ↪︎【1】
    $$&lt;/div&gt;
(如下圖所示)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;條件機率（conditional probability）
  &lt;div class="math"&gt;$$
  p(X=x_i, Y=y_j)=p(Y=y_j\mid X=x_i)p(X=x_i)  \ \ ↪︎【2】
  $$&lt;/div&gt;
(如下圖所示)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;條件機率的鏈鎖法則
  &lt;div class="math"&gt;$$
  p(a,b,c)=p(a\mid b,c)\cdot p(b,c)=p(a\mid b,c)\cdot p(b\mid c)\cdot p(c)  \ \ ↪︎【3】
  $$&lt;/div&gt;
(因為 &lt;span class="math"&gt;\(a\cap b\cap c=a\cap (b\cap c)\)&lt;/span&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;獨立性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;if X and Y are independent, &lt;span class="math"&gt;\(p(X=x_i,Y=y_j)=p(X=x_i)\cdot p(Y=y_j)\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;if X and Y are independent, 
    &lt;span class="math"&gt;\(p(X=x_i,Y=y_j\mid Z=z_k)=p(X=x_i\mid Z=z_k)\cdot p(Y=y_j\mid Z=z_k)  \ \ ↪︎【4】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/DeepDL/IMG_conditional_probability.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
碰到條件機率，心中應該要有這張圖，如果公式忘記了也可以輕易的推出來
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;頻率學派 v.s. 貝氏學派&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;頻率學派和貝氏學派為統計上面重要的兩大觀點，透徹的了解這兩個學派的觀點，可以幫助你在做機器學習或深度學習時選擇模型有所幫助。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;舉個例子，如果你在做詞性標記（POS tagging）的任務，你採用隱馬爾科夫模型 (HMM)，那麼你採用的是類似貝氏學派的觀點；但如果你採用 Conditional Random Field (CRF)，那麼你就是採用類似頻率學派的觀點。&lt;/p&gt;
&lt;p&gt;再舉個平易近人一點的例子，Loss Function中的Regularization Term其實可以看作是從先驗機率 (Prior Probability) 來的，這也有貝氏學派的味道，待會會再仔細的介紹。&lt;/p&gt;
&lt;p&gt;簡單講，&lt;strong&gt;頻率學派相信世界的本質是穩定的&lt;/strong&gt;，所有現象背後都有一個穩定的母群體，所以我們只要透過來自母群體大量隨機且可重複實驗的事件，就可以透過&lt;strong&gt;期望值&lt;/strong&gt;估算各種統計量，這就是頻率學派的認知：不需要太多其他的人為假設，只需要單純從母群體抽樣即可。&lt;/p&gt;
&lt;p&gt;但是貝氏學派不這麼認為，他們認為至少有一些現象是不穩定的，例如：北極的冰是否會在本世紀溶解殆盡，這種問題不存在穩定的母群體讓你可以在短時間內「大量」的量測，因為系統不斷的在演進，因此我們需要有一個具有演進特性的統計模型，那就是貝氏機率，&lt;strong&gt;貝氏機率的精髓是演進，手中先握著一個先驗機率 (Prior Probability) ，再透過不斷觀察新的證據來更新手上的機率&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在機器學習上，&lt;strong&gt;頻率學派的優點是無額外的假設&lt;/strong&gt;，相反的貝氏學派需要假設先驗機率 (Prior Probability) ，錯誤的先驗機率可能會去誤導模型，讓它反而忽視甚至曲解數據帶來的信息。另外，&lt;strong&gt;貝氏學派的優點也正是因為它有先驗機率，所以在資料筆數不多的情況下較不容易出錯&lt;/strong&gt;，如果你使用頻率學派的觀點在小樣本上，由於統計量不足可能導致你的估計也不準確。&lt;/p&gt;
&lt;p&gt;所以說，先驗機率的使用是把雙面刃，水可載舟亦可覆舟。&lt;/p&gt;
&lt;h3&gt;Maximum Likelihood Estimation (MLE)&lt;/h3&gt;
&lt;p&gt;首先我們來看頻率學派，也就是傳統學派，究竟用什麼什麼觀點讓機器學習的，我們從頭到腳來理一遍。&lt;/p&gt;
&lt;p&gt;給定一個Dataset &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; ，並且決定好Model的Hyperparameters &lt;span class="math"&gt;\(m\)&lt;/span&gt;，此時我只需要尋找Model權重參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; ，就可以決定好一個Model。&lt;/p&gt;
&lt;p&gt;那怎麼決定 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 呢？頻率學派認為只要先決定好Model (決定&lt;span class="math"&gt;\(m\)&lt;/span&gt;和&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;)，再算算看這個Model產生Dataset &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 的機率，這就是Likelihood，我們希望這個Likelihood可以越高越好，也就是Maximum Likelihood。化成數學式子： ⚠️
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ p(\mathcal{D}\mid m,\theta)  \ \ ↪︎【5】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(p(\mathcal{D}\mid m,\theta)\)&lt;/span&gt; 稱為Likelihood。&lt;/p&gt;
&lt;p&gt;而Dataset &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 理當有多筆資料：
&lt;/p&gt;
&lt;div class="math"&gt;$$
=argmax_\theta\ p_{\mathcal{D}_1,...,\mathcal{D}_N}(d_1,...,d_N\mid m,\theta)  \ \ ↪︎【6】
$$&lt;/div&gt;
&lt;p&gt;
假設從Dataset &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 抽樣資料的過程每一筆是Independent的，則
&lt;/p&gt;
&lt;div class="math"&gt;$$
=argmax_\theta\ \prod_{i}\ p_{\mathcal{D}_i}(d_i\mid m,\theta)  \ \ ↪︎【7】
$$&lt;/div&gt;
&lt;p&gt;
再假設從Dataset &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 抽樣資料的過程是從同一個分布來的 (identically distributed) ，則
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \prod_{i}\ p(d_i\mid m,\theta)  \ \ ↪︎【8】
$$&lt;/div&gt;
&lt;p&gt;
接下來，為了方便計算而取 &lt;span class="math"&gt;\(\operatorname{ln}\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(d_i\mid m,\theta)  \ \ ↪︎【9】
$$&lt;/div&gt;
&lt;p&gt;
再來我們&lt;strong&gt;考慮監督式學習的情況&lt;/strong&gt;，每一筆數據 &lt;span class="math"&gt;\(d_i\)&lt;/span&gt; 是由一對輸入與輸出所組成 &lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt; ：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(x_i,y_i\mid m,\theta)  \ \ ↪︎【10】
$$&lt;/div&gt;
&lt;p&gt;
因為 &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; 是depend on &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 的，所以使用【3】式將 &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 往後搬，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
=argmax_\theta\ \sum_{i}\ \operatorname{ln}[p(y_i\mid x_i,m,\theta)p(x_i\mid m,\theta)]  \ \ ↪︎【11】
$$&lt;/div&gt;
&lt;p&gt;
 &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; 與 model參數 &lt;span class="math"&gt;\(m\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 應該是互相獨立的，
&lt;/p&gt;
&lt;div class="math"&gt;$$
=argmax_\theta\ \sum_{i}\ \operatorname{ln}[p(y_i\mid x_i,m,\theta)p(x_i)]  \ \ ↪︎【12】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)+\operatorname{ln}p(x_i)  \ \ ↪︎【13】
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(p(x_i)\)&lt;/span&gt; 與&lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 無關，在優化過程可忽略，得： ⚠️
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)  \ \ ↪︎【14】
$$&lt;/div&gt;
&lt;p&gt;
上面這個式子是可以透過實驗來找到 &lt;span class="math"&gt;\(\theta_{MLE}\)&lt;/span&gt; 的，其中 &lt;span class="math"&gt;\(p(y_i\mid x_i,m,\theta)\)&lt;/span&gt; 表示在Model固定的情況下，估計輸入&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;會得到&lt;span class="math"&gt;\(y_i\)&lt;/span&gt;的機率，我們可以透過Grandient Descent的方法調整參數來最大化這一項，詳細怎麼做我會在&lt;a href="https://www.ycc.idv.tw/deep-dl_4.html"&gt;下一講&lt;/a&gt;交代清楚。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;還沒有結束喔！大家有沒有覺得【14】式怪眼熟的，這很像是我們在&lt;a href="https://www.ycc.idv.tw/deep-dl_2.html"&gt;上一講&lt;/a&gt;討論的東西。&lt;/p&gt;
&lt;p&gt;我們把【14】式乘上負號，再除上資料筆數 &lt;span class="math"&gt;\(N\)&lt;/span&gt;，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmin_\theta\ \frac{1}{N}\sum_{i}-\operatorname{ln}p(y_i\mid x_i,m,\theta)  \ \ ↪︎【15】
$$&lt;/div&gt;
&lt;p&gt;
有沒有看出來！沒有的話，我們繼續導下去，&lt;span class="math"&gt;\(\frac{1}{N}\sum_i\)&lt;/span&gt; 其實就是代表對data的採樣並平均，將它表示成為期望值
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmin_\theta\ E_{x\sim p_{data}}[-\operatorname{ln}p_{model}(y_i\mid x_i,m,\theta)]＝argmin_\theta\ H(p_{data},p_{model})  \ \ ↪︎【16】
$$&lt;/div&gt;
&lt;p&gt;
&lt;strong&gt;所以說Maximum Likelihood就是在最小化Data和Model的Cross Entropy&lt;/strong&gt;，當&lt;span class="math"&gt;\(p_{data}=p_{model}\)&lt;/span&gt;時有最小的Cross Entropy，這也間接證明了Maximum Likelihood的最優解就是&lt;span class="math"&gt;\(p_{data}=p_{model}\)&lt;/span&gt;，也就是model分布等同於母體分布。&lt;/p&gt;
&lt;p&gt;當然，&lt;strong&gt;你也可以說Maximum Likelihood就是在最小化KL Divergence&lt;/strong&gt;，KL Divergence：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(p_{data}\| p_{model})=E_{x\sim p_{data}}[\operatorname{ln}p_{data}(x)-\operatorname{ln}p_{model}(x)]  \ \ ↪︎【17】
$$&lt;/div&gt;
&lt;p&gt;
但是因為 &lt;span class="math"&gt;\(\operatorname{ln}p_{data}(x)\)&lt;/span&gt; 與 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 無關，所以計算還是只剩下第二項的Cross Entropy。&lt;/p&gt;
&lt;p&gt;特別注意，&lt;strong&gt;剛剛的所有推論並沒有提到我們的問題是Regression問題還是Classification問題，再次強調Cross Entropy並不專屬於Sigmoid或Softmax&lt;/strong&gt;，甚至&lt;a href="https://www.ycc.idv.tw/deep-dl_4.html"&gt;下一講&lt;/a&gt;我還會提到Mean Squared Error是源於Data分布與Normal Distribution的Cross Entropy。&lt;/p&gt;
&lt;h3&gt;Maximum A Posterior (MAP)&lt;/h3&gt;
&lt;p&gt;再來我們聊聊貝氏學派，我們好奇頻率學派究竟有什麼不足？MLE有什麼不足？為什麼我們需要一個新的觀點。&lt;/p&gt;
&lt;p&gt;最重要的一點是&lt;strong&gt;MLE認為所有 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 出現的機率是均等的&lt;/strong&gt;，我們優化的目標是 &lt;span class="math"&gt;\(p(\mathcal{D}\mid m,\theta)\)&lt;/span&gt; [from 【5】]，式子中是直接「給定」一組 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; ，所以這個 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 怎麼來的、 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 出現的機率，這些都不在MLE的考量當中。&lt;/p&gt;
&lt;p&gt;我們知道這樣並不好，假設以下有兩組 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 都可以得到一樣的Likelihood，依照你學機器學習和深度學習的經驗，你會喜歡哪組？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一組： &lt;span class="math"&gt;\(\theta_1=0.5,\ \theta_2=0.1,\ \theta_3=-0.1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;第二組： &lt;span class="math"&gt;\(\theta_1=1000.0,\ \theta_2=12.5,\ \theta_3=-500.0\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以前的所學告訴我們第一組比較好，&lt;strong&gt;因為 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 接近 0，這樣的參數會讓我的輸入和輸出不會差異太大，Model會比較穩定，比較不會Overfitting，普遍作法是將Loss加入Regularization Term來壓低 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 的大小&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那麼這個Regularization Term究竟怎麼來的？&lt;/p&gt;
&lt;p&gt;一開始學機器學習或深度學習時，你應該會覺得貿然的加上Regularization Term怪怪的，而今天你可以從這裡得到更好的解釋。神奇的是！當你在優化MAP時假設了參數 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 分布的同時，Regularization Term會被自然的推導出來，讓我們繼續看下去。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;透過【5】式 &lt;span class="math"&gt;\(p(\mathcal{D},m,\theta)\)&lt;/span&gt; 可以列出以下關係式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p((\mathcal{D}\cap m), \theta)=p(\mathcal{D},m\mid \theta)p(\theta)=p(\theta\mid \mathcal{D},m)p(\mathcal{D},m)  \ \ ↪︎【18】
$$&lt;/div&gt;
&lt;p&gt;
稍作移項可得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\theta\mid \mathcal{D},m)=\frac{p(\mathcal{D},m\mid \theta)p(\theta)}{p(\mathcal{D},m)}  \ \ ↪︎【19】
$$&lt;/div&gt;
&lt;p&gt;
因為 &lt;span class="math"&gt;\(m\)&lt;/span&gt; 是給定的，我們想要將它向後移，使用【1】、【2】、【3】去轉換右式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{p(\mathcal{D},m\mid \theta)p(\theta)}{p(\mathcal{D},m)}=\frac{p(\mathcal{D}\mid m,\theta)[p(m\mid \theta)p(\theta)]}{p(\mathcal{D\mid m})p(m)}＝\frac{p(\mathcal{D}\mid m,\theta)}{p(\mathcal{D}\mid m)}\frac{p(m,\theta)}{p(m)}=\frac{p(\mathcal{D}\mid m,\theta)p(\theta\mid m)}{p(\mathcal{D}\mid m)}  \ \ ↪︎【20】
$$&lt;/div&gt;
&lt;p&gt;
於是我們得到了大名鼎鼎的貝氏定理： ⚠️
&lt;/p&gt;
&lt;div class="math"&gt;$$
p(\theta\mid \mathcal{D},m)=\frac{p(\mathcal{D}\mid m,\theta)p(\theta\mid m)}{p(\mathcal{D}\mid m)}  \ \ ↪︎【21】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(p(\mathcal{D}\mid m,\theta)\)&lt;/span&gt; 就是Likelihood&lt;/strong&gt;，Maximum這一項就是MLE，不用再特別討論。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 稱為先驗機率 (Prior Probability)&lt;/strong&gt;，它所代表的意義正是描述 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 這個參數在給定 &lt;span class="math"&gt;\(m\)&lt;/span&gt; 之後出現的機率有多大，這一項是先於經驗的，這裡的經驗指的是「這一次的實驗」，而這個 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 是與本次實驗 &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 無關的。因此這個 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 是需要人為給定的，你可以自己假設分布，例如：假設為有最少假設的Normal Distribution，或者是從過去的歷史紀錄去統計出 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 也行。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(p(\mathcal{D}\mid m)\)&lt;/span&gt; 稱為資料機率（probability of data）&lt;/strong&gt;，通常數據與模型的Hyperparameters應該是相互獨立的，所以其實可以簡寫成  &lt;span class="math"&gt;\(p(\mathcal{D})\)&lt;/span&gt; ，這一項只需要統計一下Dataset的分布即可得到。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(p(\theta\mid \mathcal{D},m)\)&lt;/span&gt; 稱為後驗機率 (Posterior Probability)&lt;/strong&gt;，它所代表的意義是給定數據 &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; 和 Hyperparameters &lt;span class="math"&gt;\(m\)&lt;/span&gt; 之後，會出現 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 的機率，有注意到嗎？我們在【21】中同時連結了先驗機率和後驗機率，這代表的是手上原先有一個 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 分布（先驗機率），經過觀察數據後，我重新的去更新這個 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 分布（後驗機率），這充分的傳達了貝氏學派的演進概念。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;後驗機率也是我們準備要最大化的目標，所以此方法才稱為Maximum A Posterior (MAP) 。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最大化後驗機率MAP是直接問貝氏定理什麼樣的 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 在給定條件下出現機率最大？而最大化Likelihood則是間接的希望找出一組 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 讓Model能產生Data的機率變高，兩者的優化邏輯是不一樣的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;從 【21】式出發：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ p(\theta\mid \mathcal{D},m)=argmax_\theta\ \frac{p(\mathcal{D}\mid m,\theta)p(\theta\mid m)}{p(\mathcal{D}\mid m)}  \ \ ↪︎【22】
$$&lt;/div&gt;
&lt;p&gt;
然後如【6】、【7】、【8】式一樣假設取樣是i.i.d. (independent and identically distributed)，得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ [\prod_{i}\ \frac{p(d_i\mid m,\theta)}{p(d_i\mid m)}]p(\theta\mid m)  \ \ ↪︎【23】
$$&lt;/div&gt;
&lt;p&gt;
接下來，為了方便計算如【9】式取 &lt;span class="math"&gt;\(\operatorname{ln}\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ \{\sum_{i}\ [\operatorname{ln}p(d_i\mid m,\theta)-\operatorname{ln}p(d_i\mid m)]\}+\operatorname{ln}p(\theta\mid m)  \ \ ↪︎【24】
$$&lt;/div&gt;
&lt;p&gt;
其中 &lt;span class="math"&gt;\(-\operatorname{ln}p(d_i\mid m)\)&lt;/span&gt; 與 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 無關，可忽略：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ \{\sum_{i}\ \operatorname{ln}p(d_i\mid m,\theta)\}+\operatorname{ln}p(\theta\mid m)  \ \ ↪︎【25】
$$&lt;/div&gt;
&lt;p&gt;
第一項其實就是跟MLE一模模一樣樣，所以直接套用【9】到【14】式的推論，得：⚠️
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ \{\sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)\}+\operatorname{ln}p(\theta\mid m)  \ \ ↪︎【26】
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;觀察 【26】式，非常清楚的我們在優化第一項就如同優化MLE，&lt;strong&gt;但是MAP比MLE多了 &lt;span class="math"&gt;\(\operatorname{ln}p(\theta\mid m)\)&lt;/span&gt; &lt;/strong&gt;，我們就針對這一項來討論，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假設 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 為一個Uniform Distribution，也就是說所有的 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 出現機率均等，則 &lt;span class="math"&gt;\(p(\theta\mid m)=const.\)&lt;/span&gt;，這麼一來這一項在【26】式可以直接槓掉，因為它與 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 無關，此時 &lt;span class="math"&gt;\(\theta_{MAP}=\theta_{MLE}\)&lt;/span&gt; 。所以說在貝氏學派的觀點下，MLE只是MAP的一個特例，MLE只是假設 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 出現機率均等的MAP，&lt;a href="https://www.ycc.idv.tw/deep-dl_2.html"&gt;上一講有提過&lt;/a&gt;Uniform Distribution為所有分布當中Entropy最大的，也就是不確定程度最大的，也就是人為假設幾乎為零的分布，確實符合頻率學派的觀點：不需要太多人為假設。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是基於剛剛的推論 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 的均等並不是我們想要的，我們希望 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 可以多出現在接近 &lt;span class="math"&gt;\(\theta=0\)&lt;/span&gt; 的附近，&lt;strong&gt;因此我們希望  &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 的分布具有 有限Variance且平均值接近0 (&lt;span class="math"&gt;\(E_{p(\theta\mid m)}[x]=0\)&lt;/span&gt;)&lt;/strong&gt;。此時的第一首選就是Normal Distribution，因為我們在&lt;a href="https://www.ycc.idv.tw/deep-dl_1.html"&gt;第一講&lt;/a&gt;中說過：Normal Distribution是具有限Variance分布中具有最少人為假設的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假設 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 為一個Normal Distribution且平均值為0，則
  &lt;div class="math"&gt;$$
  p(\theta\mid m)=\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{\theta^2}{2\sigma^2}}\}  \ \ ↪︎【27】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
  \operatorname{ln}p(\theta\mid m)=\operatorname{ln}(\frac{1}{\sqrt{2\pi}\sigma})-\frac{\theta^2}{2\sigma^2}  \ \ ↪︎【28】
  $$&lt;/div&gt;
&lt;p&gt;代入【26】，並且取負號，得
  &lt;/p&gt;
&lt;div class="math"&gt;$$
  \theta_{MAP}=argmin_\theta\ \sum_{i}-\operatorname{ln}p(y_i\mid x_i,m,\theta)+\frac{1}{2\sigma^2}\theta^2  \ \ ↪︎【29】
  $$&lt;/div&gt;
&lt;p&gt;
  哇！ L2 Regularization Term &lt;span class="math"&gt;\(\frac{1}{2\sigma^2}\theta^2\)&lt;/span&gt; 就在這不經意間被導出了，所以&lt;strong&gt;未來看到L2 Regularization Term就要知道它隱藏了參數分布呈現Normal Distribution的假設&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;可以預期的，&lt;strong&gt;當假設不同的參數分布 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 就會得到不同型式的Regularization Term&lt;/strong&gt;，如果我假設Laplace Distribution則會得到L1 Regularization Term，來看一下。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;假設 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 為一個Laplace Distribution且平均值為0，則
  &lt;div class="math"&gt;$$
  p(\theta\mid m)=\frac{1}{2b}exp\{{-\frac{|\theta|}{b}}\}  \ \ ↪︎【30】
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="math"&gt;$$
  \operatorname{ln}p(\theta\mid m)=\operatorname{ln}(\frac{1}{2b})-\frac{|\theta|}{b}  \ \ ↪︎【31】
  $$&lt;/div&gt;
&lt;p&gt;代入【26】，並且取負號，得
  &lt;/p&gt;
&lt;div class="math"&gt;$$
  \theta_{MAP}=argmin_\theta\ \sum_{i}-\operatorname{ln}p(y_i\mid x_i,m,\theta)+\frac{1}{b}|\theta|   \ \ ↪︎【32】
  $$&lt;/div&gt;
&lt;p&gt;
  第二項就是L1 Regularization Term。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/DeepDL/Gaussian-distribution-and-Laplace-distribution.ppm.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  Courtesy &lt;a href="https://www.researchgate.net/figure/Gaussian-distribution-and-Laplace-distribution_fig7_321825093"&gt;Youngjoo Kim&lt;/a&gt;
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;這一講我們清楚的了解了頻率學派和貝氏學派各自的觀點，並且從兩者觀點出發去探討機器學習問題。&lt;/p&gt;
&lt;p&gt;頻率學派使用Maximum Likelihood Estimation (MLE) 來優化，優化關係式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MLE}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)
$$&lt;/div&gt;
&lt;p&gt;
此項經過轉換會等同於最小化Data與Model之間的Cross Entropy，或等同於最小化Data與Model之間的KL Divergence，與&lt;a href="https://www.ycc.idv.tw/deep-dl_2.html"&gt;上一講的資訊理論&lt;/a&gt;完美契合。&lt;/p&gt;
&lt;p&gt;貝氏學派則使用Maximum A Posterior (MAP) 來優化，優化關係式如下：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\theta_{MAP}=argmax_\theta\ \sum_{i}\ \operatorname{ln}p(y_i\mid x_i,m,\theta)+\operatorname{ln}p(\theta\mid m)
$$&lt;/div&gt;
&lt;p&gt;
除了第一項與MLE一樣之外，此時我們還需考慮了參數可能的分布，當參數分布是均等時，MAP和MLE是等價的。但是我們希望 &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; 可以接近0，所以一般會去假設 &lt;span class="math"&gt;\(p(\theta\mid m)\)&lt;/span&gt; 為一個Variance有限且平均值為0的分布，如果選擇使用Normal Distribution，則會得到L2 Regularization Term；如果選擇用Laplace Distribution，則會得到L1 Regularization Term。&lt;/p&gt;
&lt;p&gt;本講已經給出了兩個觀點的機率優化式，但是要怎麼變換成擬合問題呢？這需要一大篇幅來介紹，我們會在&lt;a href="https://www.ycc.idv.tw/deep-dl_4.html"&gt;下一講&lt;/a&gt;來仔細討論這個問題，敬請期待囉！&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.deeplearningbook.org"&gt;Ian Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Christopher Bishop. Pattern Recognition and Machine Learning. 2006.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/32480810"&gt;聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/"&gt;MLE vs MAP: the connection between Maximum Likelihood and Maximum A Posteriori Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.metaflow.fr/ml-notes-why-the-log-likelihood-24f7b6c40f83"&gt;Morgan. ML notes: Why the log-likelihood?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;[此文章為原創文章，轉載前請註明文章來源]&lt;/em&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="剖析深度學習"></category></entry><entry><title>剖析深度學習 (2)：你知道Cross Entropy和KL Divergence代表什麼意義嗎？談機器學習裡的資訊理論</title><link href="https://ycc.idv.tw/deep-dl_2.html" rel="alternate"></link><published>2020-02-25T12:00:00+08:00</published><updated>2020-02-25T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-25:/deep-dl_2.html</id><summary type="html">&lt;p&gt;在深度學習裡面，尤其是分類問題，常常會用到Cross Entropy，教學上通常會從Maximum Likelihood推導而來，但是Cross Entropy其實具有更廣義的涵義，甚至不限於分類問題使用。還有學習過程也經常會出現KL Divergence這樣既熟悉又陌生的東西，甚至到了GAN會用到更多種類的Divergence，例如：JS Divergence。這全部都與資訊理論息息相關，這一講讓我們來搞清楚Entropy、Cross Entropy、KL Divergence和f-Divergence到底具有什麼涵義。&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;深度學習發展至今已經有相當多好用的套件，使得進入的門檻大大的降低，因此如果想要快速的實作一些深度學習或機器學習，通常是幾行程式碼可以解決的事。但是，如果想要將深度學習或機器學習當作一份工作，深入了解它背後的原理和數學是必要的，才有可能因地制宜的靈活運用，YC準備在這一系列當中帶大家深入剖析深度學習。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.ycc.idv.tw/deep-dl_1.html"&gt;在上一講當中&lt;/a&gt;，我鉅細靡遺的介紹了Normal Distribution。其中我有稍微的提到Entropy的概念，並且說在未來會有一講專門來談機器學習裡面會用到的資訊理論，而那個未來就是現在！&lt;/p&gt;
&lt;p&gt;在深度學習裡面，尤其是分類問題，常常會用到Cross Entropy，教學上通常會從Maximum Likelihood推導而來，但是Cross Entropy其實具有更廣義的涵義，甚至不限於分類問題使用。&lt;/p&gt;
&lt;p&gt;還有學習過程也經常會出現KL Divergence這樣既熟悉又陌生的東西，甚至到了GAN會用到更多種類的Divergence，例如：JS Divergence。&lt;/p&gt;
&lt;p&gt;這全部都與資訊理論息息相關，這一講讓我們來搞清楚Entropy、Cross Entropy、KL Divergence和f-Divergence到底具有什麼涵義。&lt;/p&gt;
&lt;p&gt;這一切都要先從Entropy開始講起。&lt;/p&gt;
&lt;h3&gt;資訊熵（Information Entropy）&lt;/h3&gt;
&lt;p&gt;資訊理論是應用數學的一個分支，主要是對訊號中存在的資訊多寡做量化。最初研究目的是為了數據傳輸的編碼，探討要怎麼編碼資料傳輸才有效率。&lt;/p&gt;
&lt;p&gt;資訊理論背後的直覺是，越是不容易發生的事件帶給我們的資訊量越大，資訊量的大小可以看作是事件給我們的驚訝程度。舉個例子，「今天早上太陽升起」這樣幾乎永遠都是對的事件，能帶給我們的資訊量可以說是零（你不用告訴我，我也知道）；相反的「今天早上有日蝕」的事件則含有相對多的資訊量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(事件的資訊量 \propto 事件的不確定程度\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有了這樣的洞見，我們怎麼把它化成數學呢？我們可以定義事件的Self-Information為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
I(x)=-log_2p(x)  \ \ ↪︎【1】
$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(log_2\)&lt;/span&gt;以2為底，則代表所採用的單位為 bits (比特)；&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;代表的是事件的出現機率，此式子符合以下四個特性&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(0&amp;lt;p(x)\leq 1\)&lt;/span&gt;時，則&lt;span class="math"&gt;\(I(x)&amp;gt;0\)&lt;/span&gt;，這意味著資訊量必為正，如果遇到雜訊干擾才有可能扣掉一些資訊量&lt;/li&gt;
&lt;li&gt;當事件永遠是對的時，則&lt;span class="math"&gt;\(p(x)=1\)&lt;/span&gt;，相應的 &lt;span class="math"&gt;\(I(x)=0\)&lt;/span&gt;，代表資訊量為零&lt;/li&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;越小，則&lt;span class="math"&gt;\(I(x)\)&lt;/span&gt;越大，意味著事件出現的機率越小，它所攜帶的資訊量越大&lt;/li&gt;
&lt;li&gt;若&lt;span class="math"&gt;\(p(x)=p_1(x)\times p_2(x)\)&lt;/span&gt;則代表兩個獨立事件發生的機率是相乘的關係，此時資訊量應該是相加的關係，關係式&lt;span class="math"&gt;\(I(x)=I_1(x)+I_2(x)\)&lt;/span&gt;正表示這樣的關係&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Self-Information只處理單一結果，我們更想要關注整個系統，Shannon Entropy可以量化整個機率分布中不確定性的程度：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(x)=E_{x\sim p}[I(x)]=-E_{x\sim p}[log_2p(x)]  \ \ ↪︎【2】
$$&lt;/div&gt;
&lt;p&gt;
上式所傳達的意思是：Shannon Entropy即是評估Self-Information的期望值。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/DeepDL/Entropy_flip_2_coins.jpg"&gt;&lt;/p&gt;
&lt;p&gt;為了進一步的了解Shannon Entropy的內涵，我們來舉個例子。假設你的系統是離散的，可以進一步表示成為
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(x)=-E_{x\sim p}[log_2p(x)]=-\sum_i p_ilog_2 p_i  \ \ ↪︎【3】
$$&lt;/div&gt;
&lt;p&gt;
假設你要描述的事件有三個獨立事件A、B和C，A事件出現機率為&lt;span class="math"&gt;\(1/2\)&lt;/span&gt;，B事件出現機率為&lt;span class="math"&gt;\(1/4\)&lt;/span&gt;，C事件出現機率為&lt;span class="math"&gt;\(1/4\)&lt;/span&gt;，代入式【3】求系統的Shannon Entropy：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H=-\frac{1}{2}log_2(\frac{1}{2})-\frac{1}{4}log_2(\frac{1}{4})-\frac{1}{4}log_2(\frac{1}{4})=0.5+0.5+0.5=1.5  \ \ ↪︎【4】
$$&lt;/div&gt;
&lt;p&gt;
這代表什麼意義呢？這代表今天如果你設計一個系統得當的話，你只需要用到平均1.5個bits來傳輸。怎麼說呢？我們逐步拆解式【4】帶給我們的觀念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A事件的&lt;span class="math"&gt;\(I=1\)&lt;/span&gt;，代表用1個bit去傳輸A事件，即&lt;span class="math"&gt;\(bits_A=1\)&lt;/span&gt;，可能編碼為&lt;code&gt;0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;B事件的&lt;span class="math"&gt;\(I=2\)&lt;/span&gt;，代表用2個bits去傳輸B事件，即&lt;span class="math"&gt;\(bits_B=2\)&lt;/span&gt;，可能編碼為&lt;code&gt;10&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;C事件的&lt;span class="math"&gt;\(I=2\)&lt;/span&gt;，代表用2個bits去傳輸C事件，即&lt;span class="math"&gt;\(bits_C=2\)&lt;/span&gt;，可能編碼為&lt;code&gt;11&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你會發現經常出現的事件A就用較少的bits來傳輸，而較不常發生的事件B就用比較多的bits來傳輸，如此一來傳輸會更有效率。假設有200個事件要傳輸，依照機率分布，其中應該有100件屬於A事件，有50件屬於B事件，有50件屬於C事件，依照上述的編碼方式，我們傳輸這200個事件所需要的預期平均bits數就是1.5。
&lt;/p&gt;
&lt;div class="math"&gt;$$
\frac{n_A\times bits_A+n_B\times bits_B+n_C\times bits_C}{n_A+n_B+n_C}=\frac{100\times 1+50\times 2+50\times 2}{200}=1.5  \ \ ↪︎【5】
$$&lt;/div&gt;
&lt;p&gt;
而理論告訴我們這 &lt;span class="math"&gt;\(1.5\)&lt;/span&gt; 其實就是最小的預期位元，你無法找到比這個更小的，也就是說：你無法找到比這個更有效率的編碼系統。理論如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Noiseless Coding Theorm (Shannon, 1948):&lt;/p&gt;
&lt;p&gt;The entropy is a lower bound on the number of bits needed to transmit the state of a random variable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;來做個小結論，仔細回想剛剛的過程你會更能了解公式隱藏的意義。如果你仔細的去理解剛剛我舉的例子，你會發現： &lt;strong&gt;&lt;span class="math"&gt;\(I(x)\)&lt;/span&gt;，或者Entropy中的&lt;span class="math"&gt;\(-log\)&lt;/span&gt; ，所扮演的角色是「編碼」，決定需要用幾個bits來傳輸事件，而Entropy的意義就是這套「編碼」運用到系統的bits期望值，並且Shannon的理論告訴我們&lt;span class="math"&gt;\(-log\ p(x)\)&lt;/span&gt;已經是最有效率的「編碼」，它可以得到最小的bits期望值，所以Entropy是bits期望值的下界&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;在物理上或是機器學習上，我們常使用自然對數&lt;span class="math"&gt;\(e\)&lt;/span&gt;當作底，所以Entropy為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p)=E_{x\sim p}[-\operatorname{ln}p(x)]  \ \ ↪︎【6】
$$&lt;/div&gt;
&lt;p&gt;
其實就只是把度量的單位從比特(bits)換成奈特(nats)，只是「編碼」的單位改變而已，其意義都跟上述的一樣。只是如果使用nats當單位在計算上會方便許多，因為許多的分布都可以表示成以&lt;span class="math"&gt;\(e\)&lt;/span&gt;為底的指數，例如：Normal Distribution。&lt;/p&gt;
&lt;p&gt;還記得&lt;a href="https://www.ycc.idv.tw/deep-dl_1.html"&gt;上一講中&lt;/a&gt;我請大家記住的【2】到【4】式嗎？現在套用到這裡的【6】，可得：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;連續情境下， &lt;span class="math"&gt;\(H=\int -p(x)\operatorname{ln}p(x)dx  \ \ ↪︎【7】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;離散情境下， &lt;span class="math"&gt;\(H=\sum_i -p_i\operatorname{ln}p_i  \ \ ↪︎【8】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;實驗情境下，&lt;span class="math"&gt;\(H=\sum_k -(\frac{n_k}{N})\operatorname{ln}(\frac{n_k}{N})  \ \ ↪︎【9】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;最後，來看看什麼分布的Entropy最大，在給定 &lt;span class="math"&gt;\(\int p(x)dx=1\)&lt;/span&gt; 的情況下試圖找到一個 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 可以使 Entropy &lt;span class="math"&gt;\(H\)&lt;/span&gt; 最大，引入&lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;Lagrange Multiplier&lt;/a&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$
L=\int^{\infty}_{-\infty}-\operatorname{ln}p(x)\cdot p(x)dx+\lambda (\int^{\infty}_{-\infty}p(x)dx-1)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\int^{\infty}_{-\infty}[\lambda p(x)-p(x)\operatorname{ln}(p(x))]dx-\lambda  \ \ ↪︎【11】
$$&lt;/div&gt;
&lt;p&gt;接下來對【11】微分求極值
&lt;/p&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial L}{\partial p(x)}|_{p^*(x)}=\int^{\infty}_{-\infty}[\lambda-\operatorname{ln}(p^*(x))-1]dx  \ \ ↪︎【12】
$$&lt;/div&gt;
&lt;p&gt;
所以
&lt;/p&gt;
&lt;div class="math"&gt;$$
p^*(x)=exp\{\lambda-1\}  \ \ ↪︎【13】
$$&lt;/div&gt;
&lt;p&gt;此時的分布是一個與 &lt;span class="math"&gt;\(x\)&lt;/span&gt; 無關的常數，所以 &lt;span class="math"&gt;\(p^*(x)\)&lt;/span&gt; 是一個Uniform Distribution。所以&lt;strong&gt;平均分配會使得系統得到最大的Entropy，也就是Uniform Distribution是隨機性最大的分布，也是資訊量最大的分布&lt;/strong&gt; ，這與我們剛剛的討論是自恰的。（注意：&lt;a href="https://www.ycc.idv.tw/deep-dl_1.html#anchor2"&gt;在有限Variance的情況下是Normal Distribution有最大Entropy&lt;/a&gt;）&lt;/p&gt;
&lt;h3&gt;Cross Entropy&lt;/h3&gt;
&lt;p&gt;接下來聊聊學過機器學習和深度學習都知道的Cross Entropy，在分類問題當中Cross Entropy被定義成：
&lt;/p&gt;
&lt;div class="math"&gt;$$
Cross\ Entropy=-y\ \operatorname{ln}(q)-(1-y)\operatorname{ln}(1-q)  \ \ ↪︎【14】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(y_i=0,1\)&lt;/span&gt;為data的labels，&lt;span class="math"&gt;\(q_i\)&lt;/span&gt;為模型預測的輸出值。&lt;/p&gt;
&lt;p&gt;但它其實有更一般的定義：
&lt;/p&gt;
&lt;div class="math"&gt;$$
Cross\ Entropy:\ H(p, q)=E_{x\sim p}[-\operatorname{ln}q(x)]  \ \ ↪︎【15】
$$&lt;/div&gt;
&lt;p&gt;一般&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;代表的是目標分布，也就是想要學習的未知分布；而&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;則代表是模型的輸出分布。&lt;/p&gt;
&lt;p&gt;我們剛剛說過Entropy是bits或nats期望值的下界，所以：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p,q)=E_{x\sim p}[-\operatorname{ln}q(x)]\geq E_{x\sim p}[-\operatorname{ln}p(x)]=H(p,p)=H(p)  \ \ ↪︎【16】
$$&lt;/div&gt;
&lt;p&gt;
所以當我們試圖減少Cross Entropy時，其實就是試圖調整&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;使其接近&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;，因為當&lt;span class="math"&gt;\(q(x)=p(x)\)&lt;/span&gt;時，&lt;span class="math"&gt;\(H(p,q)=H(p)\)&lt;/span&gt;有最小的Cross Entropy。&lt;/p&gt;
&lt;p&gt;延續剛剛「編碼」的概念套用在Cross Entropy，今天我雖然知道 &lt;span class="math"&gt;\(-\operatorname{ln}p(x)\)&lt;/span&gt;是最好的「編碼」，但是我不知道 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 長什麼樣子，所以退一步我們使用模型的 &lt;span class="math"&gt;\(q(x)\)&lt;/span&gt; 來做「編碼」，&lt;strong&gt;Cross Entropy的意義就是利用 &lt;span class="math"&gt;\(-\operatorname{ln}q(x)\)&lt;/span&gt; 這套「編碼」去算系統的nats期望值，並且想辦法改善編碼方法來降低Cross Entropy&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下來我想要帶大家從式【15】到【14】導一遍式子。&lt;/p&gt;
&lt;p&gt;因為我們知道目標分布是Binary的離散系統，所以可以把【15】寫成：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p,q)=-p_{positive}\cdot \operatorname{ln}(q_{positive})-p_{negative}\cdot \operatorname{ln}(q_{negative})  \ \ ↪︎【17】
$$&lt;/div&gt;
&lt;p&gt;
因為是Binary Classification的問題，只有兩種states其機率相合為1，因此：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p,q)=-p_{positive}\cdot \operatorname{ln}(q_{positive})-(1-p_{positive})\cdot \operatorname{ln}(1-q_{positive})  \ \ ↪︎【18】
$$&lt;/div&gt;
&lt;p&gt;
接下來，&lt;span class="math"&gt;\(p\)&lt;/span&gt; 是目標分布，當label為positive (&lt;span class="math"&gt;\(y=1\)&lt;/span&gt;) 則&lt;span class="math"&gt;\(p_{positive}=1\)&lt;/span&gt;，當label為 (&lt;span class="math"&gt;\(y=0\)&lt;/span&gt;) negative則&lt;span class="math"&gt;\(p_{positive}=0\)&lt;/span&gt;；&lt;span class="math"&gt;\(q\)&lt;/span&gt; 是模型預測，其目標是預測positive的可能機率，所以最後寫成：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(p,q)=-y\cdot \operatorname{ln}(q)-(1-y)\cdot \operatorname{ln}(1-q)
$$&lt;/div&gt;
&lt;p&gt;
就跟【14】式一模一樣了，&lt;strong&gt;這裡注意一點，在推導的過程當中我都不需要去假設模型的長相，我不需要假設模型為Sigmoid，推出Cross Entropy的過程是和模型的選擇無關的，所以千萬不要認為選擇使用Cross Entropy是因為Sigmoid的緣故。進一步說Cross Entropy其實可以用在各種問題（或分布），包括：Regression問題，我們會在&lt;a href="https://www.ycc.idv.tw/deep-dl_3.html"&gt;接下來的文章&lt;/a&gt;裡讓大家真正了解這一點。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;KL Divergence (Kullback-Leibler Divergence)&lt;/h3&gt;
&lt;p&gt;KL Divergence對碰過深度學習一段時間的大家應該是一個既熟悉又陌生的東西吧！我們就在這邊把它搞懂吧！&lt;/p&gt;
&lt;p&gt;如果有兩個獨立的機率分布&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;同時對應到同一個隨機變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;，也就是它們所在的空間是一樣的，則可以使用KL Divergence來測量這兩個分布的差異程度：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(p||q)=-E_{x\sim p}[\operatorname{ln}q(x)-\operatorname{ln}p(x)]=-E_{x\sim p}[\operatorname{ln}\frac{q(x)}{p(x)}]  \ \ ↪︎【19】
$$&lt;/div&gt;
&lt;p&gt;這個式子不好懂，沒關係！我們稍微代換一下式子：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(p||q)=E_{x\sim p}[-\operatorname{ln}q(x)]-E_{x\sim p}[-\operatorname{ln}p(x)]=H(p,q)-H(p)  \ \ ↪︎【20】
$$&lt;/div&gt;
&lt;p&gt;
有看出來了嗎？&lt;strong&gt;KL Divergence其實就是Cross Entropy扣掉目標分布的Entropy，更深層的說，KL Divergence表示的是目前的編碼方法最多還可以下降多少nats期望值&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;雖然你可以將KL Divergence視作距離，但是嚴格來說它不是，因為KL Divergence不具有對稱性：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(p||q)\neq D_{KL}(q||p)  \ \ ↪︎【21】
$$&lt;/div&gt;
&lt;p&gt;
其中：
&lt;/p&gt;
&lt;div class="math"&gt;$$
D_{KL}(q||p)＝-E_{x\sim q}[\operatorname{ln}p(x)-\operatorname{ln}q(x)]＝H(q,p)-H(q)  \ \ ↪︎【22】
$$&lt;/div&gt;
&lt;p&gt;
【21】式意味著從&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;到&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;所降低的nats期望值與從&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;到&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;所降低的nats期望值不相等。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/DeepDL/KL-Gauss-Example.png"&gt;&lt;/p&gt;
&lt;h3&gt;f-Divergence&lt;/h3&gt;
&lt;p&gt;Divergence其實有多個形式，定義如下：
當有兩個獨立的機率分布&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;同時對應到同一個隨機變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;，
&lt;/p&gt;
&lt;div class="math"&gt;$$
f-divergence:\ D_f(p\|q)=E_{x\sim q}[f(\frac{p(x)}{q(x)})]=\int q(x)f(\frac{p(x)}{q(x)})dx
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(f(.)\)&lt;/span&gt; 只需要遵守兩條規則就可以：&lt;span class="math"&gt;\(f(.)\)&lt;/span&gt; 是Convex的且&lt;span class="math"&gt;\(f(1)=0\)&lt;/span&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(f(u)=u\cdot \operatorname{ln}u\)&lt;/span&gt;，則 
  &lt;div class="math"&gt;$$
  D_f(p\|q)=\int q(x)(\frac{p(x)}{q(x)})\operatorname{ln}(\frac{p(x)}{q(x)})dx=\int p(x)\operatorname{ln}(\frac{p(x)}{q(x)})dx=D_{KL}(p\|q)
  $$&lt;/div&gt;
  ，為KL Divergence&lt;/li&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(f(u)=-\operatorname{ln}u\)&lt;/span&gt;，則 
  &lt;div class="math"&gt;$$
  D_f(p\|q)=-\int q(x)\operatorname{ln}(\frac{p(x)}{q(x)})dx=\int q(x)\operatorname{ln}(\frac{q(x)}{p(x)})dx=D_{KL}(q\|p)
  $$&lt;/div&gt;
  ，為Reverse KL Divergence&lt;/li&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(f(u)=(u-1)^2\)&lt;/span&gt;，則 
  &lt;div class="math"&gt;$$
  D_f(p\|q)=-\int q(x)(\frac{p(x)}{q(x)}-1)^2dx=\int \frac{(p(x)-q(x))^2}{q(x)}dx
  $$&lt;/div&gt;
  ，為Chi Square Divergence&lt;/li&gt;
&lt;li&gt;當&lt;span class="math"&gt;\(f(u)=-(u+1)\operatorname{ln}\frac{u+1}{2}+u\operatorname{ln}u\)&lt;/span&gt;，則 
  &lt;div class="math"&gt;$$
  D_f(p\|q)=\frac{1}{2}[D_{KL}(p\|\frac{p+q}{2})+D_{KL}(q\|\frac{p+q}{2})]
  $$&lt;/div&gt;
  ，為 JS Divergence (用在GAN)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;f-divergence在意義上代表：我在model的分布 &lt;span class="math"&gt;\(q(x)\)&lt;/span&gt; 上估計 &lt;span class="math"&gt;\(f(\frac{p(x)}{q(x)})\)&lt;/span&gt; ，&lt;span class="math"&gt;\(f(.)\)&lt;/span&gt;是用來評估分布間距離的函數，當&lt;span class="math"&gt;\(p(x)=q(x)\)&lt;/span&gt;時距離為0 — &lt;span class="math"&gt;\(f(\frac{p(x)}{q(x)})=0\)&lt;/span&gt;，當&lt;span class="math"&gt;\(p(x)\neq q(x)\)&lt;/span&gt;時距離為正 — &lt;span class="math"&gt;\(f(\frac{p(x)}{q(x)})&amp;gt; 0\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3&gt;結論&lt;/h3&gt;
&lt;p&gt;機器學習和深度學習充分的借用了資訊理論裡對資訊量的衡量技術，包括：Entropy、Cross Entropy和KL Divergence。因為機器學習中充斥著統計分布，而這些衡量方法可以幫助我們度量種種分布的資訊量，有了這把尺我們才可以進行各類優化來讓機器學習。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Entropy:  &lt;span class="math"&gt;\(H(p)=E_{x\sim p}[-\operatorname{ln}p(x)]\)&lt;/span&gt; ，為最小可得的nats期望值&lt;/li&gt;
&lt;li&gt;Cross Entropy: &lt;span class="math"&gt;\(H(p, q)=E_{x\sim p}[-\operatorname{ln}q(x)]\)&lt;/span&gt; ，利用&lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;來編碼可得的nats期望值&lt;/li&gt;
&lt;li&gt;KL Divergence: &lt;span class="math"&gt;\(D_{KL}(p\|q)=H(p,q)-H(p)\)&lt;/span&gt; ，表示的是目前的編碼方法最多還可以下降多少nats期望值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其實，這一講的內容可以說是為了接下來幾講而生，&lt;a href="https://www.ycc.idv.tw/deep-dl_3.html"&gt;下一講&lt;/a&gt;我們會開始套用資訊理論的這些概念，帶大家重新了解優化和擬合是怎麼一回事，敬請期待！&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.deeplearningbook.org"&gt;Ian Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Christopher Bishop. Pattern Recognition and Machine Learning. 2006.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)"&gt;Wiki: Entropy_(information_theory)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Cross_entropy"&gt;Wiki: Cross Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence"&gt;Wiki: Kullback–Leibler_divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://hackmd.io/@sXG2cRDpRbONCsrtz8jfqg/ry-0k0PwH"&gt;從計算機編碼的角度看Entropy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=av1bqilLsyQ&amp;amp;list=PLJV_el3uVTsMq6JEFPW35BCiOQTsoqwNw&amp;amp;index=6&amp;amp;t=0s"&gt;李宏毅, GAN Lecture 5 (2018): General Framework&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;[此文章為原創文章，轉載前請註明文章來源]&lt;/em&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="剖析深度學習"></category></entry><entry><title>剖析深度學習 (1)：為什麼Normal Distribution這麼好用？</title><link href="https://ycc.idv.tw/deep-dl_1.html" rel="alternate"></link><published>2020-02-18T12:00:00+08:00</published><updated>2020-02-18T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2020-02-18:/deep-dl_1.html</id><summary type="html">&lt;p&gt;如果你已經學了好一陣子的機器學習或深度學習，應該對於Normal Distribution不陌生，但是你真的懂Normal Distribution嗎？本講會詳細的探討Normal Distribution，並且引入中央極限定理（Central Limit Theorm）來解釋為何自然界的隨機誤差大都呈現Normal Distribution，再來介紹Entropy，並且利用Entropy揭示Normal Distribution具有最少先驗知識（Prior Knowledge）的特性。&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;深度學習發展至今已經有相當多好用的套件，使得進入的門檻大大的降低，因此如果想要快速的實作一些深度學習或機器學習，通常是幾行程式碼可以解決的事。但是，如果想要將深度學習或機器學習當作一份工作，深入了解它背後的原理和數學是必要的，才有可能因地制宜的靈活運用，YC準備在這一系列當中帶大家深入剖析深度學習。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先第一講，我們來聊一個最常見的分布—正態分布（Normal Distribution），也稱為高斯分布（Gaussian Distribution）。&lt;/p&gt;
&lt;p&gt;如果你已經學了好一陣子的機器學習或深度學習，應該對於Normal Distribution不陌生，但是你真的懂Normal Distribution嗎？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;為什麼Normal Distribution通常作為雜訊的分布？&lt;/li&gt;
&lt;li&gt;為何在DL（deep learning），參數的初始化要用Normal Distribution？&lt;/li&gt;
&lt;li&gt;為何在Bayesian公式裡常常會使用Normal Distribution當作Prior Probability？&lt;/li&gt;
&lt;li&gt;在使用GAN（generative adversarial network）時，為什麼給予的輸入要假設Normal Distribution？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你不知道為什麼使用Normal Distribution，你用起來不會怕嗎？這一講我想要回答的是：為什麼Normal Distribution這麼好用？甚至已經到了無腦用的程度，我會從統計學和資訊理論來回答這個問題。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/DeepDL/best_gaussian.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;small&gt;
  Courtesy &lt;a href="https://www.facebook.com/nas.mooty"&gt;Nas Mouti&lt;/a&gt;
&lt;/small&gt;&lt;/center&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;認識Normal Distribution&lt;/h3&gt;
&lt;p&gt;首先來看看Normal Distribution的數學表示式
&lt;/p&gt;
&lt;div class="math"&gt;$$
p_{normal}(x)=\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\}  \ \ ↪︎【1】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(\mu\)&lt;/span&gt; 剛好是Normal Distribution的 Mean（平均值），&lt;span class="math"&gt;\(\sigma^2\)&lt;/span&gt; 剛好是Normal Distribution的 Variance（方差），來證明一下吧！&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;證明之前，先來了解什麼是「期望值」，期望值指的是在相同場景下隨機試驗多次，所有那些可能狀態的平均結果。期望值 &lt;span class="math"&gt;\(E[g(x)]\)&lt;/span&gt; 跟兩件事有關：試驗的物理量 &lt;span class="math"&gt;\(g(x)\)&lt;/span&gt; 和試驗的出現機率 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;連續形式寫成：&lt;/p&gt;
&lt;div class="math"&gt;$$
E[g(x)]\equiv\int^{\infty}_{-\infty}g(x)\cdot p(x)dx  \ \ ↪︎【2】
$$&lt;/div&gt;
&lt;p&gt;
另外期望值也有離散的形式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
E[g]\equiv \sum_{i}g_i\cdot p_i  \ \ ↪︎【3】
$$&lt;/div&gt;
&lt;p&gt;上面兩個式子有個前提是已知機率分布 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 或 &lt;span class="math"&gt;\(p_i\)&lt;/span&gt; 的情況下才能使用，如果今天我們不知道機率分布，只能使用實驗的方法求近似的期望值，此時&lt;span class="math"&gt;\(p_i\)&lt;/span&gt;可以用採樣來取代，則變換式【3】為以下公式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
E[g]= \frac{1}{N}\sum_{i=1}^{N}g_i  \ \ ↪︎【4】
$$&lt;/div&gt;
&lt;p&gt;
請大家牢記上面三個式子，在機器學習中會反覆使用到。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;有了期望值的概念，我們開始來看Mean和Variance的定義&lt;/p&gt;
&lt;p&gt;Mean的定義為
&lt;/p&gt;
&lt;div class="math"&gt;$$
E[x]\equiv\int^{\infty}_{-\infty}x\cdot p(x)dx  \ \ ↪︎【5】
$$&lt;/div&gt;
&lt;p&gt;
也就是求物理量 &lt;span class="math"&gt;\(x\)&lt;/span&gt; 的期望值。&lt;/p&gt;
&lt;p&gt;Variance的定義為
&lt;/p&gt;
&lt;div class="math"&gt;$$
Var[x]\equiv E[(x-E[x])^2]  \ \ ↪︎【6】
$$&lt;/div&gt;
&lt;p&gt;
將上式化約可得
&lt;/p&gt;
&lt;div class="math"&gt;$$
Var[x]=E[x^2]-E[x]^2  \ \ ↪︎【7】
$$&lt;/div&gt;
&lt;p&gt;
其中：&lt;span class="math"&gt;\(E[x^2]\equiv\int^{\infty}_{-\infty}x^2p(x)dx\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;接下來只要把式【1】的Normal Distribution代入就可以得到它的Mean和Variance。&lt;/p&gt;
&lt;p&gt;在這之前，我們先來看一個重要的積分式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int^{\infty}_{-\infty}exp\{-a(x+b)^2\}dx=\sqrt{\frac{\pi}{a}}  \ \ ↪︎【8】
$$&lt;/div&gt;
&lt;p&gt;上述式子證明稍嫌複雜，有興趣的詳見&lt;a href="https://zh.wikipedia.org/wiki/高斯积分"&gt;維基百科的證明&lt;/a&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;先看看Normal Distribution是不是機率總和為1&lt;/p&gt;
&lt;p&gt;將式【1】做積分
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int^{\infty}_{-\infty}p_{normal}(x)dx=\frac{1}{\sqrt{2\pi}\sigma}\int^{\infty}_{-\infty}exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\}dx
$$&lt;/div&gt;
&lt;p&gt;
接下來令&lt;span class="math"&gt;\(a=\frac{1}{2\sigma^2}\)&lt;/span&gt;、&lt;span class="math"&gt;\(b=-\mu\)&lt;/span&gt;，此時可以套用式【8】，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
\int^{\infty}_{-\infty}p_{normal}(x)dx=\frac{1}{\sqrt{2\pi}\sigma}\sqrt{\frac{\pi}{\frac{1}{2\sigma^2}}}=1  \ \ ↪︎【9】
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;再來求其Mean
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{x\sim normal}[x]=\int^{\infty}_{-\infty}x\cdot p_{normal}(x)dx=\frac{1}{\sqrt{2\pi}\sigma}\int^{\infty}_{-\infty}x\cdot exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\}dx
$$&lt;/div&gt;
&lt;p&gt;
令&lt;span class="math"&gt;\(s=x-\mu\)&lt;/span&gt; 代入上式，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\frac{1}{\sqrt{2\pi}\sigma}\int^{\infty}_{-\infty}(s+\mu)\cdot exp\{{-\frac{1}{2\sigma^2}s^2}\}ds
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\frac{1}{\sqrt{2\pi}\sigma}[\int^{\infty}_{-\infty}s\cdot exp\{{-\frac{1}{2\sigma^2}s^2}\}ds+\int^{\infty}_{-\infty}\mu\cdot exp\{{-\frac{1}{2\sigma^2}s^2}\}ds]
$$&lt;/div&gt;
&lt;p&gt;上式的第一項必為0，因為&lt;span class="math"&gt;\(s\)&lt;/span&gt;對原點為奇對稱，而&lt;span class="math"&gt;\(exp\{{-\frac{1}{2\sigma^2}s^2}\}\)&lt;/span&gt;對原點為偶對稱，所以&lt;span class="math"&gt;\(s\cdot exp\{{-\frac{1}{2\sigma^2}s^2}\}\)&lt;/span&gt;為奇對稱，積分後會相互抵銷為0。接下來把第二項的&lt;span class="math"&gt;\(s\)&lt;/span&gt;還原回去，得&lt;/p&gt;
&lt;div class="math"&gt;$$
=\mu\int^{\infty}_{-\infty} \frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\}dx=\mu \int^{\infty}_{-\infty}p_{normal}(x)dx
$$&lt;/div&gt;
&lt;p&gt;將式【9】代入，最後得到
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{x\sim normal}[x]=\mu  \ \ ↪︎【10】
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;最後來算一下Variance
&lt;/p&gt;
&lt;div class="math"&gt;$$
Var_{x\sim normal}[x]=E_{x\sim normal}[x^2]-E_{x\sim normal}[x]^2  \ \ ↪︎【11】
$$&lt;/div&gt;
&lt;p&gt;
第一項
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{x\sim normal}[x^2]=\int^{\infty}_{-\infty}x^2p_{normal}(x)dx=\frac{1}{\sqrt{2\pi}\sigma}\int^{\infty}_{-\infty}x^2\cdot exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\}dx
$$&lt;/div&gt;
&lt;p&gt;
令&lt;span class="math"&gt;\(a=\frac{1}{2\sigma^2}\)&lt;/span&gt;、&lt;span class="math"&gt;\(s=x -\mu\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\sqrt{\frac{a}{\pi}}\int^{\infty}_{-\infty}(s+\mu)^2\cdot exp\{{-a \cdot s^2}\}ds
$$&lt;/div&gt;
&lt;p&gt;
展開
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\sqrt{\frac{a}{\pi}}[\int^{\infty}_{-\infty}s^2\cdot exp\{{-a \cdot s^2}\}ds+\int^{\infty}_{-\infty}2s\mu\cdot exp\{{-a \cdot s^2}\}ds+\int^{\infty}_{-\infty}\mu^2\cdot exp\{{-a \cdot s^2}\}ds]
$$&lt;/div&gt;
&lt;p&gt;
第二項的積分裡面是奇函數，所以第二項積分完的結果是0。第三項把&lt;span class="math"&gt;\(\mu^2\)&lt;/span&gt;提出去，積分的部分其實就是式【9】。得
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{x\sim normal}[x^2]=\sqrt{\frac{a}{\pi}}\int^{\infty}_{-\infty}s^2\cdot exp\{{-a \cdot s^2}\}ds+\mu^2
$$&lt;/div&gt;
&lt;p&gt;
接下來有點tricky，上式的第一項可看成一個微分形式
&lt;/p&gt;
&lt;div class="math"&gt;$$
=\sqrt{\frac{a}{\pi}}\frac{-\partial}{\partial a}(\int^{\infty}_{-\infty} exp\{{-a \cdot s^2}\}ds)+\mu^2=\sqrt{\frac{a}{\pi}}\frac{-\partial}{\partial a}(\sqrt{\frac{\pi}{a}})+\mu^2=\frac{1}{2a}+\mu^2
$$&lt;/div&gt;
&lt;p&gt;
所以
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{x\sim normal}[x^2]=\sigma^2+\mu^2  \ \ ↪︎【12】
$$&lt;/div&gt;
&lt;p&gt;
將【12】和【10】代入【11】，可得
&lt;/p&gt;
&lt;div class="math"&gt;$$
Var_{x\sim normal}[x]=\sigma^2+\mu^2-(\mu)^2=\sigma^2  \ \ ↪︎【13】
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;所以未來當你看到Normal Distribution的公式時，應該能夠馬上看出他的Mean和Variance。&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/DeepDL/IMG_gaussian_distribution.png"&gt;&lt;/p&gt;
&lt;h3&gt;隨機誤差大都呈現Normal Distribution&lt;/h3&gt;
&lt;p&gt;雖然說並非所有的隨機分布都是Normal Distribution。例如有：適用於二元系統的Bernoulli Distribution；適用於計數系統的Poisson Distribution；適用於時間間隔的Gamma Distribution；...等等。&lt;/p&gt;
&lt;p&gt;但是大多數情況下，沒有特別的理由，隨機誤差會遵循Normal Distribution。&lt;/p&gt;
&lt;p&gt;接下來我要試著用中央極限定理來解釋這個現象。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;先從中央極限定理（Central Limit Theorm）開始講起&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Central Limit Theorm:&lt;/p&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(\{x_1,x_2,...,x_n\}\)&lt;/span&gt; be a random sample of size &lt;span class="math"&gt;\(n\)&lt;/span&gt; — that is, a sequence of independent and identically distributed (i.i.d.) random variables drawn from a distribution of expected value &lt;span class="math"&gt;\(E[x_i]=\mu\)&lt;/span&gt; and variance &lt;span class="math"&gt;\(Var[x_i]=\sigma^2&amp;lt;\infty\)&lt;/span&gt;. Suppose we are interested in the sample average: &lt;span class="math"&gt;\(S_n=(x_1+x_2+...+x_n)/n\)&lt;/span&gt; , Then as &lt;span class="math"&gt;\(n\rightarrow \infty\)&lt;/span&gt; , &lt;span class="math"&gt;\(S_n\)&lt;/span&gt; follows normal distribution &lt;span class="math"&gt;\(p_{normal}(\mu,(\frac{\sigma}{\sqrt{n}})^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也就是說，今天我們從一個任意分布 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 當中採樣 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 筆，這&lt;span class="math"&gt;\(n\)&lt;/span&gt;筆採樣的過程符合不互相影響彼此（independent）且都從同一分布而來（identically distributed），即 i.i.d.。&lt;/p&gt;
&lt;p&gt;而如果我們已知這個任意分布 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 的Mean &lt;span class="math"&gt;\(E_{x\sim p(x)}[x]=\mu\)&lt;/span&gt; 和 Variance &lt;span class="math"&gt;\(Var_{x\sim p(x)}[x]=\sigma^2&amp;lt;\infty\)&lt;/span&gt;，注意：&lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 不一定需要是Normal Distribution才能算Mean和Variance。&lt;/p&gt;
&lt;p&gt;我們關注這&lt;span class="math"&gt;\(n\)&lt;/span&gt;筆採樣的平均值，計作&lt;span class="math"&gt;\(S_n\)&lt;/span&gt;，統計學告訴我們：
&lt;/p&gt;
&lt;div class="math"&gt;$$
E[S_n]=\mu  \ \ ↪︎【14】
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
Var[S_n]=\frac{\sigma^2}{n}  \ \ ↪︎【15】
$$&lt;/div&gt;
&lt;p&gt;當&lt;span class="math"&gt;\(n=1\)&lt;/span&gt;時，&lt;span class="math"&gt;\(S_n\)&lt;/span&gt;的分布其實就是 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 的分布，當然Mean和Variance會和原分布 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 一模一樣。&lt;/p&gt;
&lt;p&gt;中央極限定理告訴我們如果今天採樣數量 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 增加到一定的量，&lt;span class="math"&gt;\(S_n\)&lt;/span&gt;的分布會趨近於Normal Distribution，也就是說隨著 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 的增加，&lt;span class="math"&gt;\(S_n\)&lt;/span&gt; 的分布會從 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 變成接近 &lt;span class="math"&gt;\(p_{normal}(\mu,(\frac{\sigma}{\sqrt{n}})^2)\)&lt;/span&gt; 分布。&lt;/p&gt;
&lt;p&gt;眼見為憑，接下來我要透過&lt;a href="https://seeing-theory.brown.edu/probability-distributions/index.html"&gt;Seeing-Theory&lt;/a&gt;這個網站來Demo一下中央極限定理，&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/DeepDL/clt_demo.gif"&gt;&lt;/p&gt;
&lt;p&gt;給定一個採樣分布（黃色），每次採樣 &lt;span class="math"&gt;\(n=15\)&lt;/span&gt; 作平均並打點記下來，經過多次的操作就可以得到累積分布圖（紅色），而因為 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 夠大，所以這個累積分布圖會逼近於Normal Distribution。&lt;/p&gt;
&lt;p&gt;再來看看採樣平均的累積分布怎麼隨著 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 增加而改變&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://www.ycc.idv.tw/media/DeepDL/clt_demo_2.gif"&gt;&lt;/p&gt;
&lt;p&gt;觀察上面的動圖，會發現 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 越大，Variance越來越小，而且分布狀況也越接近Normal Distribution。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;好！講了這麼多，那這跟隨機誤差有什麼關係呢？&lt;/p&gt;
&lt;p&gt;中央極限定理告訴我們只要從一個固定的採樣分布當中作夠多的樣本平均，其分布會接近Normal Distribution。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;而自然界的巨觀現象往往是源自於微觀現象的累積，我們量測的物理量常常來自於多個微小貢獻疊加而成，而不管這些微小貢獻本身的分布狀況如何，其巨觀的物理量因為中央極限定理而成為Normal Distribution，這也是為什麼「隨機誤差大都呈現Normal Distribution」的原因。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;舉例，電壓就是反應電荷疊加的物理量，用普通方法我們是很難量到單一電荷的，所以我們能量到的已經是疊加過後的結果，也因此電壓的隨機分布才呈現Normal Distribution。&lt;/p&gt;
&lt;p&gt;所以，如果今天你沒有特別的理由，假設Normal Distribution往往是最接近真實的，這是第一個理由能讓你無腦使用Normal Distribution，還有第二個理由我們接下去討論。&lt;/p&gt;
&lt;h3&gt;Normal Distribution是所有機率分布當中假設最少的&lt;/h3&gt;
&lt;p&gt;首先來看一段從&lt;a href="https://www.deeplearningbook.org"&gt;Goodfellow的書&lt;/a&gt;中的一段話，這段話清楚的告訴我們選擇用Normal Distribution的理由&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;First, many distributions we wish to model are truly close to being normal distributions. The central limit theorem shows that the sum of many independent random variables is approximately normally distributed. This means that in practice, many complicated systems can be modeled successfully as normally distributed noise, even if the system can be decomposed into parts with more structured behavior.&lt;/p&gt;
&lt;p&gt;Second, out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers. We can thus think of the normal distribution as being the one that inserts the least amount of prior knowledge into a model. &lt;/p&gt;
&lt;p&gt;-- from: Deep Learning 3.9.3&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上述的第一段就是剛剛我們討論的那些，而我們接下去要討論的就是第二段的內容。&lt;/p&gt;
&lt;p&gt;總結一下Goodfellow在第二段說的內容：&lt;/p&gt;
&lt;p&gt;在所有有相同Variance的分布當中，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normal Distribution是隨機性最大的分布&lt;/li&gt;
&lt;li&gt;Normal Distribution是最少先驗知識（Prior Knowledge）假設的&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;要討論這個問題，我們必須先了解一些資訊理論。&lt;/p&gt;
&lt;p&gt;在資訊理論當中，我們常常使用Entropy（熵）來衡量隨機性，Entropy的定義為
&lt;/p&gt;
&lt;div class="math"&gt;$$
H\equiv E[-ln\ p(x)]  \ \ ↪︎【16】
$$&lt;/div&gt;
&lt;p&gt;
因為篇幅的緣故，Entropy的完整介紹會在&lt;a href="https://www.ycc.idv.tw/deep-dl_2.html"&gt;接下來的文章中介紹&lt;/a&gt;，請大家先把這個定義背起來。&lt;/p&gt;
&lt;p&gt;透過式【2】可以將Entropy寫成連續形式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H_{x\sim p(x)}=E[-ln\ p(x)]=- \int^{\infty}_{-\infty}ln\ p(x)\cdot p(x)dx  \ \ ↪︎【17】
$$&lt;/div&gt;
&lt;hr&gt;
&lt;p&gt;接下來將Normal Distribution 【1】式代入【17】
&lt;/p&gt;
&lt;div class="math"&gt;$$
H_{x\sim normal}=- \int^{\infty}_{-\infty}p_{normal}(x)\cdot ln\ p_{normal}(x) dx
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=- \int^{\infty}_{-\infty}\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\} \cdot [ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^2}(x-\mu)^2]dx
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=ln(\sqrt{2\pi}\sigma)+ \frac{1}{2\sigma^2} \int^{\infty}_{-\infty}\frac{1}{\sqrt{2\pi}\sigma}exp\{{-\frac{1}{2\sigma^2}(x-\mu)^2}\} \cdot (x-\mu)^2dx
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=ln(\sqrt{2\pi}\sigma)+ \frac{1}{2\sigma^2} E_{x\sim normal}[(x-\mu)^2]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=ln(\sqrt{2\pi}\sigma)+ \frac{1}{2\sigma^2} Var_{x\sim normal}[x]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
H_{x\sim normal}=ln(\sqrt{2\pi}\sigma)+\frac{1}{2}=\frac{1}{2}ln(2\pi e\sigma^2)  \ \ ↪︎【18】
$$&lt;/div&gt;
&lt;p&gt;我們因此得到了Normal Distribution 的Entropy，而這個Entropy是所有有相同Variance的分布當中最大的。緊接著來證明這件事。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;回到式【17】，我們可以列出一個有限制條件的優化問題：&lt;/p&gt;
&lt;p&gt;在給定：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\int p(x)dx=1  \ \ ↪︎【19】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(E[x]=\mu  \ \ ↪︎【20】\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(Var[x]=\sigma^2  \ \ ↪︎【21】\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;的情況下試圖找到一個 &lt;span class="math"&gt;\(p(x)\)&lt;/span&gt; 可以使 Entropy &lt;span class="math"&gt;\(H\)&lt;/span&gt; 最大：
&lt;/p&gt;
&lt;div class="math"&gt;$$
p^*(x) = argmax_{p(x)}\ H_{x\sim p(x)}=argmin_{p(x)}\int^{\infty}_{-\infty}ln\ p(x)\cdot p(x)dx  \ \ ↪︎【22】
$$&lt;/div&gt;
&lt;p&gt;
引入&lt;a href="https://en.wikipedia.org/wiki/Lagrange_multiplier"&gt;Lagrange Multiplier&lt;/a&gt;結合【19】,【20】,【21】,【22】：
&lt;/p&gt;
&lt;div class="math"&gt;$$
L=\int^{\infty}_{-\infty}ln\ p(x)\cdot p(x)dx-\lambda_1 (\int^{\infty}_{-\infty}p(x)dx-1)-\lambda_2(\int^{\infty}_{-\infty}x\cdot p(x)dx-\mu)-\lambda_3(\int^{\infty}_{-\infty}(x-\mu)^2\cdot p(x)dx-\sigma^2)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=\int^{\infty}_{-\infty}[-\lambda_1 p(x)-\lambda_2 p(x)x-\lambda_3 p(x)(x-\mu)^2+p(x)ln(p(x))]dx+\lambda_1 +\mu\lambda_2+\sigma^2\lambda_3  \ \ ↪︎【23】
$$&lt;/div&gt;
&lt;p&gt;接下來對【23】微分求極值
&lt;/p&gt;
&lt;div class="math"&gt;$$
0=\frac{\partial L}{\partial p(x)}|_{p^*(x)}=\int^{\infty}_{-\infty}[-\lambda_1-\lambda_2 x-\lambda_3 (x-\mu)^2+ln(p^*(x))+1]dx  \ \ ↪︎【24】
$$&lt;/div&gt;
&lt;p&gt;
所以
&lt;/p&gt;
&lt;div class="math"&gt;$$
p^*(x)=exp\{\lambda_1+\lambda_2 x+\lambda_3 (x-\mu)^2-1\}  \ \ ↪︎【25】
$$&lt;/div&gt;
&lt;p&gt;上面還有三個未知變數 &lt;span class="math"&gt;\(\lambda_1\)&lt;/span&gt;, &lt;span class="math"&gt;\(\lambda_2\)&lt;/span&gt;, &lt;span class="math"&gt;\(\lambda_3\)&lt;/span&gt; ，這些變數必須滿足Constraints，所以將【25】代入 【19】,【20】,【21】得三個方程求解三個變數，可得：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\lambda_1=1-ln(\sqrt{2\pi}\sigma);\ \lambda_2=0;\ \lambda_3=-\frac{1}{2\sigma^2}  \ \ ↪︎【26】
$$&lt;/div&gt;
&lt;p&gt;
最後將【26】回代【25】就會得到剛剛好是Normal Distribution，🥳&lt;/p&gt;
&lt;p&gt;因此這邊我們證明了：&lt;strong&gt;在給定Mean和Variance下，Normal Distribution為所有分布當中Entropy最大的。這也同時意味著，Normal Distribution是隨機性最大的，Normal Distribution是額外假設最少的。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Back to the Question&lt;/h3&gt;
&lt;p&gt;這一講也走到尾聲了，接下來我們已經有能力回答一開始問的問題，在回答問題之前我們複習一下剛剛學到了什麼。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中央極限定理告訴我們：如果今天採樣數量 &lt;span class="math"&gt;\(n\)&lt;/span&gt; 增加到一定的量，&lt;span class="math"&gt;\(S_n\)&lt;/span&gt;的分布會趨近於Normal Distribution&lt;/li&gt;
&lt;li&gt;自然界的巨觀現象往往是源自於微觀現象的累積，我們量測的物理量常常來自於多個微小貢獻疊加而成，而不管這些微小貢獻本身的分布狀況如何，其巨觀的物理量因為中央極限定理而成為Normal Distribution，這也是為什麼「隨機誤差大都呈現Normal Distribution」的原因&lt;/li&gt;
&lt;li&gt;Entropy是衡量隨機性的指標，定義為：&lt;span class="math"&gt;\(H\equiv E[-ln\ p(x)]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;在給定Mean和Variance下，Normal Distribution為所有分布當中Entropy最大的。這也同時意味著，Normal Distribution是隨機性最大的，Normal Distribution是額外假設最少的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此我們有兩個原因去使用Normal Distribution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果沒有特別理由，請假設隨機誤差為Normal Distribution，因為自然界的隨機誤差大都呈現Normal Distribution&lt;/li&gt;
&lt;li&gt;如果想要人為假設一個分布，請優先選擇Normal Distribution，因為它是包含最少先驗知識（Prior Knowledge）的分布&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最後來逐一回答剛開始的問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q：為什麼Normal Distribution通常作為雜訊的分布？&lt;ul&gt;
&lt;li&gt;A：因為自然界的隨機誤差大都呈現Normal Distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：為何在DL（deep learning），參數的初始化要用Normal Distribution？&lt;ul&gt;
&lt;li&gt;A：因為它是包含最少先驗知識（Prior Knowledge）的分布&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：為何在Bayesian公式裡常常會使用Normal Distribution當作Prior Probability？&lt;ul&gt;
&lt;li&gt;A：因為它是包含最少先驗知識（Prior Knowledge）的分布&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Q：在使用GAN（generative adversarial network）時，為什麼給予的輸入要假設Normal Distribution？&lt;ul&gt;
&lt;li&gt;A：因為它是包含最少先驗知識（Prior Knowledge）的分布&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://seeing-theory.brown.edu/probability-distributions/index.html"&gt;Seeing Theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.deeplearningbook.org"&gt;Ian Goodfellow and Yoshua Bengio and Aaron Courville. Deep Learning. 2016.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Christopher Bishop. Pattern Recognition and Machine Learning. 2006.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;[此文章為原創文章，轉載前請註明文章來源]&lt;/em&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="剖析深度學習"></category></entry><entry><title>[Paper] Wide &amp; Deep Learning for Recommender Systems</title><link href="https://ycc.idv.tw/wide-and-deep-learning.html" rel="alternate"></link><published>2019-06-01T12:00:00+08:00</published><updated>2019-06-01T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2019-06-01:/wide-and-deep-learning.html</id><summary type="html">&lt;p&gt;以往認為deep learning有辦法完全取代feature engineering，Google在2016年寫下的這篇paper，指出在數據相對稀疏（sparse）的情況下feature engineering仍然有其重要性&lt;/p&gt;</summary><content type="html">&lt;h3&gt;結論寫在前面&lt;/h3&gt;
&lt;p&gt;以往認為deep learning有辦法完全取代feature engineering，Google在2016年寫下的這篇paper，指出在數據相對稀疏（sparse）的情況下feature engineering仍然有其重要性，此篇paper使用聯合訓練（jointly train）deep和wide結構的方法，得到比純粹deep或純粹wide的效果還好的成果，這裡的wide就是我們一般所說的feature engineering。&lt;/p&gt;
&lt;h3&gt;Memorization and Generalization&lt;/h3&gt;
&lt;p&gt;Memorization指的就是wide learning，此部分的產生需要較多的人為參與，也就是feature engineering，通常我們會依照人為的認知、或是統計數據、或是反覆實驗的結果，將一些有關聯性的變數進行cross-product以讓model得以「記住」這些相關性，通常這種基於memorization的結構會比較具有解釋性。&lt;/p&gt;
&lt;p&gt;Generalization指的就是deep Learning，此部份與wide learning相反，結構本身因為它的深度結構，等效於多次的non-linear transformation，也因此model自身在學習的過程就會將隱藏的feature組合給找出來，所以不用太多的人為參與，所以它會比較「一般化」。&lt;/p&gt;
&lt;p&gt;近年來deep learning大行其道，所以人們往往認為已經沒有必要再去做feature engineering了，只需要設定好深度結構，機器自動會去學出我們人類已知的feature組合，甚至學出隱藏的feature組合，但這篇paper指出這樣的想法是錯誤的，Wide結構有其重要性。&lt;/p&gt;
&lt;h3&gt;推薦問題&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Overview of the recommender system" src="http://www.ycc.idv.tw/media/Papers/WideAndDeepLearning-001.png"&gt;&lt;/p&gt;
&lt;p&gt;這篇paper想要解決的是Google Play上的app推薦問題，如上圖所示，我們透過Learner從Logs中學習出一個Model，接下來使用這個Model為app打上推薦的分數，再進行排序，不過因為我們要評分的app超過一百萬個以上，在幾十 miliseconds 的限制推薦時間之下，根本是來不及的，所以這邊需要先進行Retrieval，我們並不是把所有的app都評過一次分數，而是Retrieval事先挑選出一些app再進行評分，挑選方式是使用其他的machine learning方法或是人為規則限定。&lt;/p&gt;
&lt;h3&gt;精神&lt;/h3&gt;
&lt;p&gt;基於memorization的推薦系統，通常比較容易推薦出過去曾經被使用者作用過的app；相反的，基於generalization的推薦系統，它的多樣性（diversity ）會更好一點，更可能去推薦一些不曾使用過或很少被使用過的app，以下詳細解釋。&lt;/p&gt;
&lt;p&gt;對於大型推薦系統而言，我們經常使用wide learning搭配logistic regression，因為它簡單、可擴充和可解釋。舉個cross-product的例子： &lt;code&gt;AND(user_installed_app=netflex, impression_app=pandora)&lt;/code&gt; ，這個feature組合就相當有可解釋性，它指的是同時滿足使用者曾經安裝過netflex和接下來會顯示pandora，相當直觀。但也因為這種feature組合是基於過去資料，造成model難以學出過去沒出現過的組合，所以多樣性會較差。&lt;/p&gt;
&lt;p&gt;另一方面，embedding-base models，例如：factorization machines或deep neural networks，能利用低維度的embedding vector去學出更一般的行為，讓過去沒出現過的組合更有可能出線，進而增加多樣性。其實說穿了，embedding的概念就是降維，當今天維度降低、輸入參數變少，就會迫使系統去學更一般、更重要的規則。&lt;/p&gt;
&lt;p&gt;不過embedding-base models如果遇到稀疏（sparse）的情形就會很慘，因為資料稀疏，一般化的規則出現次數不怎麼多，所以容易過度一般化（over-generalize），然後就學出完全不相關的東西。&lt;/p&gt;
&lt;p&gt;綜上所述，在稀疏情況下的推薦系統，最好要同時考慮memorization和generalization，作者這裡是使用聯合訓練（jointly train），也就是將兩種系統綁在一起優化，這和ensemble models不一樣，ensemble models是不同model各自訓練再結合，這樣做的缺點是需要更多的model參數。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Apps recommendation pipeline overview" src="http://www.ycc.idv.tw/media/Papers/WideAndDeepLearning-002.png"&gt;&lt;/p&gt;
&lt;h3&gt;Wide &amp;amp; Deep Learning: 資料處理&lt;/h3&gt;
&lt;p&gt;如上圖所示，我們採用User Data和App Impression Data，接下來利用Vocabulary Generator把categorical feature轉成整數ID，categorical feature的labels不是0就是1，另外continuous features則會進行normalization，將labels映射到 [0, 1] 之間，normalization的方法是採用分位數（quantiles）方法，將feature的數據分成 &lt;span class="math"&gt;\(n_q\)&lt;/span&gt; 位數，每個級距內的數值映射到 &lt;span class="math"&gt;\((i-1)/(n_q-1)\)&lt;/span&gt; ，&lt;span class="math"&gt;\(i\)&lt;/span&gt; 指的是它的分位數落在哪裡。&lt;/p&gt;
&lt;h3&gt;Wide &amp;amp; Deep Learning: 模型訓練&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Wide &amp;amp; Deep model structure for apps recommendation" src="http://www.ycc.idv.tw/media/Papers/WideAndDeepLearning-003.png"&gt;&lt;/p&gt;
&lt;p&gt;上圖是model structure，左側是Deep的部分，右側是Wide的部分。&lt;/p&gt;
&lt;p&gt;Wide &amp;amp; Deep Learning是採用back-propagation來優化「評分」，而Wide的部分，使用FTRL (Follow-the-regularized-leader) algorithm搭配L1 regularization當作optimizer來優化，這種算法會使得model weights更稀疏，詳細解釋未來需要整整一篇來介紹；Deep的部分，則是使用AdaGrad當作optimizer來優化。&lt;/p&gt;
&lt;p&gt;來更仔細的看Deep部份的結構，從下而上看起，每個categorical features都會經過一個降維轉化，轉化成32維的embedding vector，降維迫使系統學習更一般的規則。接下來把所有的feature包含continuous feature和categorical features連結在一起，成為Concatenated Embeddings，這大概有1200維左右。這個Concatenated Embeddings接下來會餵進去三層的ReLU，就完成了。&lt;/p&gt;
&lt;h3&gt;實驗結果&lt;/h3&gt;
&lt;p&gt;Offline實驗的結果，純粹Wide當作控制組它的AUC為&lt;span class="math"&gt;\(0.726\)&lt;/span&gt;，純粹Deep的AUC為&lt;span class="math"&gt;\(0.722\)&lt;/span&gt;，稍差於控制組，Wide &amp;amp; Deep的AUC為&lt;span class="math"&gt;\(0.728\)&lt;/span&gt;，稍好於控制組。Online AB分流實驗差異比較顯著，與控制組（純粹Wide）相比，純粹Deep增加了&lt;span class="math"&gt;\(2.9\%\)&lt;/span&gt; 的獲取率，Wide &amp;amp; Deep增加了 &lt;span class="math"&gt;\(3.9\%\)&lt;/span&gt; 的獲取率，所以確實Wide &amp;amp; Deep是效果最好的。&lt;/p&gt;
&lt;p&gt;眼尖的讀者應該發現一個奇怪的地方，純粹Deep在offline明明比控制組差，為何在online會比控制組好，可能的原因是offline實驗數據是固定的，所以當我增加更多的多樣性，對於offline是沒什麼幫助的，但是對於online而言多樣性可能造成用戶更多的獲取，從這裡我們也看到多樣性的重要性。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Papers"></category></entry><entry><title>實作Tensorflow (6)：Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)</title><link href="https://ycc.idv.tw/tensorflow-tutorial_6.html" rel="alternate"></link><published>2017-11-25T12:00:00+08:00</published><updated>2017-11-25T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-25:/tensorflow-tutorial_6.html</id><summary type="html">&lt;p&gt;概論RNN / 梯度消失與梯度爆炸 / Long Short-Term Memory (LSTM) / 使用LSTM實作文章產生器&lt;/p&gt;</summary><content type="html">&lt;p&gt;如果我們想要處理的問題是具有時序性的，該怎麼辦呢？本章將會介紹有時序性的Neurel Network。&lt;/p&gt;
&lt;p&gt;本單元程式碼LSTM部分可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/06_LSTM.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;h3&gt;概論RNN&lt;/h3&gt;
&lt;p&gt;當我們想使得Neurel Network具有時序性，我們的Neurel Network就必須有記憶的功能，然後在我不斷的輸入新資訊時，也能同時保有歷史資訊的影響，最簡單的作法就是將Output的結果保留，等到新資訊進來時，將新的資訊和舊的Output一起考量來訓練Neurel Network。&lt;/p&gt;
&lt;p&gt;&lt;img alt="unrolling" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.010.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;這種將舊有資訊保留的Neurel Network統稱為Recurrent Neural Networks (RNN)，這種不斷回饋的網路可以攤開來處理，如上圖，如果我有5筆數據，拿訓練一個RNN 5個回合並做了5次更新，其實就等效於攤開來一次處理5筆數據並做1次更新，這樣的手法叫做Unrolling，我們實作上會使用Unrolling的手法來增加計算效率。&lt;/p&gt;
&lt;p&gt;&lt;img alt="RNN" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.011.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;接下來來看RNN內部怎麼實現的，上圖是最簡單的RNN形式，我們將上一回產生的Output和這一回的Input一起評估出這一回的Output，詳細式子如下：&lt;/p&gt;
&lt;div class="math"&gt;$$
o_{new}=tanh(i \times W_i + o \times W_o + B)
$$&lt;/div&gt;
&lt;p&gt;如此一來RNN就具有時序性了，舊的歷史資料將可以被「記憶」起來，你可以把RNN的「記憶」看成是「短期記憶」，因為它只會記得上一回的Output而已。&lt;/p&gt;
&lt;h3&gt;梯度消失與梯度爆炸&lt;/h3&gt;
&lt;p&gt;但這種形式的RNN在實作上會遇到很大的問題，還記得第二章當中，我們有講過像是tanh這類有飽和區的函數，會造成梯度消失的問題，而我們如果使用Unrolling的觀點來看RNN，將會發現這是一個超級深的網路，Backpapagation必須一路通到t0的RNN，想當然爾，有些梯度將會消失，部分權重就更新不到了，那有一些聰明的讀者一定會想到，那就使用Relu就好啦！不過其實還有一個重要的因素造成梯度消失，同時也造成梯度爆炸。&lt;/p&gt;
&lt;p&gt;注意喔！雖然我們使用Unrolling的觀點，把網路看成是一個Deep網路的連接，但是和之前DNN不同之處，這些RNN彼此間是共享同一組權重的，這會造成梯度消失和梯度爆炸兩個問題，在RNN的結構裡頭，一個權重會隨著時間不斷的加強影響一個單一特徵，因為不同時間之下的RNN Cell共用同一個權重，這麼一來若是權重大於1，影響將會隨時間放大到梯度爆炸，若是權重小於1，影響將會隨時間縮小到梯度消失，就像是蝴蝶效應一般，微小的差異因為回饋的機制，而不合理的放大或是消失，因此RNN的Error Surface將會崎嶇不平，這會造成我們無法穩定的找到最佳解，難以收斂。這才是RNN難以使用的重要原因，把Activation Function換成Relu不會解決問題，文獻上反而告訴我們會變更差。&lt;/p&gt;
&lt;p&gt;解決梯度爆炸有一個聽起來很廢但廣為人們使用的方法，叫做Gradient Clipping，也就是只要在更新過程梯度超過一個值，我就切掉讓梯度維持在這個上限，這樣就不會爆炸啦，待會會講到的LSTM只能夠解決梯度消失問題，但不能解決梯度爆炸問題，因此我們還是需要Gradient Clipping方法的幫忙。&lt;/p&gt;
&lt;p&gt;在Tensorflow怎麼做到Gradient Clipping呢？作法是這樣的，以往我們使用&lt;code&gt;optimizer.minimize(loss)&lt;/code&gt;來進行更新，事實上我們可以把這一步驟拆成兩部分，第一部分計算所有參數的梯度，第二部分使用這些梯度進行更新。因此我們可以從中作梗，把gradients偷天換日一番，一開始使用&lt;code&gt;optimizer.compute_gradients(loss)&lt;/code&gt;來計算出個別的梯度，然後使用&lt;code&gt;tf.clip_by_global_norm(gradients, clip_norm)&lt;/code&gt;來切梯度，最後再使用&lt;code&gt;optimizer.apply_gradients&lt;/code&gt;把新的梯度餵入進行更新。&lt;/p&gt;
&lt;h3&gt;Long Short-Term Memory (LSTM)&lt;/h3&gt;
&lt;p&gt;LSTM是現今RNN的主流，它可以解決梯度消失的問題，我們先來看看結構，先預告一下，LSTM是迄今為止這系列課程當中看過最複雜的Neurel Network。&lt;/p&gt;
&lt;p&gt;&lt;img alt="LSTM" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.012.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;最一開始和RNN一樣，Input會和上一回的Output一起評估一個「短期記憶」，&lt;/p&gt;
&lt;div class="math"&gt;$$
f_m = tanh (i \times W_{mi} + o \times W_{mo} + B_m)
$$&lt;/div&gt;
&lt;p&gt;但接下來不同於RNN直接輸出，LSTM做了一個類似於轉換成「長期記憶」的機制，「長期記憶」在這裡稱為State，State的狀態由三道門所控制，Input Gate負責控管哪些「短期記憶」可以進到「長期記憶」，Forget Gate負責調配哪一些「長期記憶」需要被遺忘，Output Gate則負責去決定需要從「長期記憶」中輸出怎樣的內容，先不要管這些Gate怎麼來，我們可以把這樣的記憶機制寫成以下的式子，假設State為&lt;span class="math"&gt;\(f_{state}\)&lt;/span&gt;、Input Gate為&lt;span class="math"&gt;\(G_i\)&lt;/span&gt;、Forget Gate為&lt;span class="math"&gt;\(G_f\)&lt;/span&gt;和Output Gate為&lt;span class="math"&gt;\(G_o\)&lt;/span&gt;。&lt;/p&gt;
&lt;div class="math"&gt;$$
f_{state,new} = G_i \times f_m + G_f \times f_{state}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
o_{new} = G_o \times tanh(f_{state,new})
$$&lt;/div&gt;
&lt;p&gt;如果我們要使得上面中Gates的部分具有開關的功能的話，我們會希望Gates可以是0到1的值，0代表全關，1代表全開，sigmoid正可以幫我們做到這件事，那哪些因素會決定Gates的關閉與否呢？不妨考慮所有可能的因素，也就是所有輸入這個Cell的資訊都考慮進去，但上一回的State必須被剔除於外，因為上一回的State來決定下一個State的操作是不合理的，因此我們就可以寫下所有Gates的表示式了。&lt;/p&gt;
&lt;div class="math"&gt;$$
G_i = Sigmoid (i \times W_{ii} + o \times W_{io} + B_i)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
G_f = Sigmoid (i \times W_{fi} + o \times W_{fo} + B_f)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
G_o = Sigmoid(i \times W_{oi} + o \times W_{oo} + B_o)
$$&lt;/div&gt;
&lt;p&gt;這就是LSTM，「長期記憶」的出現可以解決掉梯度消失的問題，RNN只有「短期記憶」，所以一旦認為一個特徵不重要，經過幾回連乘，這個特徵的梯度就會消失殆盡，但是LSTM保留State，並且使用「加」的方法更新State，所以有一些重要的State得以留下來持續影響著Output，解決了梯度消失的問題。但是，不幸的LSTM還是免不了梯度爆炸，為什麼呢？如果一個特徵真的很重要，State會記住，Input也會強調，所以幾輪下來還是有可能出現爆炸的情況，這時候我們就需要Gradient Clipping的幫忙。&lt;/p&gt;
&lt;h3&gt;使用LSTM實作文章產生器&lt;/h3&gt;
&lt;p&gt;接下來我們來實作LSTM，目標是做一個文章產生器，我們希望機器可以不斷的根據前文猜測下一個「字母」(Letters)應該要下什麼，如此一來我只要給個開頭字母，LSTM就可以幫我腦補成一篇文章。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;string&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;zipfile&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;urllib.request&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;urlretrieve&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ascii_lowercase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# [a-z] + &amp;#39; &amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;FIRST_LETTER_ASCII&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;ord&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ascii_lowercase&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maybe_download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_bytes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Download a file if not present, and make sure it&amp;#39;s the right size.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urlretrieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;statinfo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;statinfo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;st_size&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;expected_bytes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Found and verified &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;statinfo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;st_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Failed to verify &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;. Can you get to it with a browser?&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;zipfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ZipFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;namelist&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;char2id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;char&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ascii_lowercase&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;ord&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;FIRST_LETTER_ASCII&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;char&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Unexpected character: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;char&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;id2char&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictid&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;dictid&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictid&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;FIRST_LETTER_ASCII&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;


&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Downloading text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;maybe_download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;http://mattmahoney.net/dc/text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;./text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;31344016&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;=====&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Data size &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; letters&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;=====&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;valid_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="n"&gt;valid_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;valid_size&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;train_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;valid_size&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;train_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Train Dataset: size:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;letters,&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;  first 64:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_text&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Validation Dataset: size:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;letters,&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;  first 64:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_text&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Downloading text8.zip&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Found and verified ./text8.zip&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=====&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Data size 100000000 letters&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=====&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Train Dataset&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;99999000 letters,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;first 64&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ons anarchists advocate social relations based upon voluntary as&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Validation Dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;1000 letters,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;first 64&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;anarchism originated as a term of abuse first used against earl&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;上面操作我們建制完成了字母庫，接下來就可以產生我們訓練所需要的Batch Data，所以我們來看看究竟要產生怎樣格式的資料。&lt;/p&gt;
&lt;p&gt;&lt;img alt="LSTM Implement" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.013.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;如上圖所示，有點小複雜，假設我要設計一個LSTM Model，它的Unrolling Number為3，Batch Size為2，然後遇到的字串是"abcde fghij klmno pqrst"，接下來就開始產生每個Round要用的Data，產生的結果如上圖所示，你會發現產生的Data第0軸表示的是考慮unrolling需要取樣的資料，總共應該會有(Unrolling Number+1)筆，如上圖例，共有4筆，3筆當作輸入而3筆當作Labels，中間有2筆重疊使用，另外還有一點，我們會保留最後一筆Data當作下一個回合的第一筆，這是為了不浪費使用每一個字母前後的組合。而第1軸則是餵入單一LSTM需要的資料，我們一次可以餵多組不相干的字母進去，如上圖例，Batch Size=2所以餵2個字母進去，那這些不相干的字母在取樣的時候，我們會盡量讓它平均分配在文字庫，才能確保彼此之間不相干，以增加LSTM的訓練效率和效果。&lt;/p&gt;
&lt;p&gt;因此，先產生Batch Data吧！&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;characters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Turn a 1-hot encoding or a probability distribution over the possible&lt;/span&gt;
&lt;span class="sd"&gt;    characters back into its (most likely) character representation.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;id2char&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;batches2string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Convert a sequence of batches back into their (most likely) string&lt;/span&gt;
&lt;span class="sd"&gt;    representation.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;characters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rnn_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;text_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;### initialization&lt;/span&gt;
    &lt;span class="n"&gt;segment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;
    &lt;span class="n"&gt;cursors&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;offset&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;segment&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;offset&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

    &lt;span class="n"&gt;batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;batch_initial&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;cursor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;id_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;char2id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cursor&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;batch_initial&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;id_&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;

        &lt;span class="c1"&gt;# move cursor&lt;/span&gt;
        &lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt;

    &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_initial&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;### generate loop&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;cursor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="n"&gt;id_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;char2id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cursor&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;id_&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;

                &lt;span class="c1"&gt;# move cursor&lt;/span&gt;
                &lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cursors&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;text_size&lt;/span&gt;
            &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;batches&lt;/span&gt;  &lt;span class="c1"&gt;# [last batch of previous batches] + [unrollings]&lt;/span&gt;


&lt;span class="c1"&gt;# demonstrate generator&lt;/span&gt;
&lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;
&lt;span class="n"&gt;num_unrollings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

&lt;span class="n"&gt;train_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rnn_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;valid_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rnn_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;*** train_batches:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches2string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_batches&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches2string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_batches&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;*** valid_batches:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches2string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_batches&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches2string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_batches&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;*** train_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ons&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;anarchi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;when&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;milita&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lleria&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;arch&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;abbeys&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;and&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;married&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;urr&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;hel&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;and&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ric&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;and&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;litur&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ay&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;opened&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;f&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tion&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;from&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;migration&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;new&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;york&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ot&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;he&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;boeing&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;listed&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;wi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;eber&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;has&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;pr&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;o&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;be&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;made&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;yer&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;who&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;rec&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ore&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;signifi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;fierce&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;cr&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;two&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;six&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ei&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;aristotle&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ity&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;can&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;be&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;and&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;intrac&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tion&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dy&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;to&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;pass&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;f&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;certain&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;d&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;at&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;it&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;will&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;convince&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ent&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;told&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;hi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ampaign&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;and&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rver&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;side&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ious&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;texts&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;o&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;capitaliz&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;duplicate&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;gh&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ann&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;es&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;d&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ine&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;january&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ross&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;zero&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;cal&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;theorie&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ast&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;instanc&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;dimensiona&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;most&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;holy&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;t&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;support&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;u&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;is&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;still&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;oscillati&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;o&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;eight&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;sub&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;italy&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;la&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;s&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;tower&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;klahoma&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;pre&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;erprise&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;lin&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ws&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;becomes&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;et&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;in&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;a&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;naz&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;fabian&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;etchy&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;to&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;re&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;sharman&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ne&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ised&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;empero&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ting&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;in&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;pol&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;d&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;neo&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;latin&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;th&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;risky&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ri&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;encyclopedi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;fense&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;duating&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;fro&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;treet&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;grid&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ations&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;more&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;appeal&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;d&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;si&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;have&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;mad&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ists&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;advoca&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ary&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;governm&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;hes&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;nationa&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;d&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;monasteri&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;raca&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;prince&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;chard&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;baer&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rgical&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;lang&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;for&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;passeng&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;nationa&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;took&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;place&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ther&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;well&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;k&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seven&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;six&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ith&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;a&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;gloss&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;robably&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;bee&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;to&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;recogniz&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ceived&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;icant&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;than&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ritic&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;th&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ight&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;in&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;sig&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;s&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;uncaused&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;lost&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;as&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;in&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;cellular&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ic&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;size&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;him&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;a&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;stic&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;drugs&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;confu&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;take&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;to&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;co&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;priest&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;im&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;to&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;name&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;d&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;barred&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;at&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;standard&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;fo&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;such&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;as&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;es&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ze&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;on&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;g&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;or&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;d&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;hiver&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;one&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;y&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;eight&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;mar&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;lead&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ch&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;es&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;classica&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ce&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;non&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;al&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;analysis&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mormons&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;bel&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;t&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;or&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;at&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;lea&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;disagreed&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ing&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;system&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;btypes&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;base&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anguages&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;th&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;r&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;commissio&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ess&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;one&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;nin&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;nux&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;suse&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;li&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;first&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;zi&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;concentr&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;society&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ne&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;elatively&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;s&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;etworks&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;sha&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;or&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;hirohito&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;litical&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ini&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;n&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;most&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;of&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;t&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;iskerdoo&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;ri&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ic&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;overview&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;air&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;compone&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;om&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;acnm&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;acc&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;centerline&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;e&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;than&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;any&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;devotional&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;de&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;such&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;dev&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;*** valid_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;&lt;/span&gt;&lt;span class="nv"&gt; &lt;/span&gt;&lt;span class="s"&gt;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;an&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;定義一下待會會用到的函數。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample_distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Sample one element from a distribution assumed to be an array of normalized&lt;/span&gt;
&lt;span class="sd"&gt;    probabilities.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distribution&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Turn a (column) prediction into 1-hot encoded samples.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_distribution&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;logprob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Log-probability of the true labels in a predicted batch.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;1e-10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1e-10&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;multiply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;開始建制LSTM Model。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;LSTM&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_unrollings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unrollings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_unrollings&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_memory&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saved&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new grap&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unrollings&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unrollings&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;  &lt;span class="c1"&gt;# labels are inputs shifted by one time step.&lt;/span&gt;


            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;n_batch&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;

            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdagradOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# gradient clipping&lt;/span&gt;

            &lt;span class="c1"&gt;# output gradients one by one&lt;/span&gt;
            &lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compute_gradients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;clip_by_global_norm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# clip gradient&lt;/span&gt;
            &lt;span class="c1"&gt;# apply clipped gradients&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_gradients&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gradients&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Sampling and validation eval: batch 1, no unrolling.&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

            &lt;span class="n"&gt;saved_sample_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="n"&gt;saved_sample_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_sample_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;     &lt;span class="c1"&gt;# reset sample state operator&lt;/span&gt;
                &lt;span class="n"&gt;saved_sample_output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
                &lt;span class="n"&gt;saved_sample_state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;

            &lt;span class="n"&gt;sample_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lstm_cell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;saved_sample_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;saved_sample_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;control_dependencies&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;saved_sample_output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_output&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                          &lt;span class="n"&gt;saved_sample_state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_state&lt;/span&gt;&lt;span class="p"&gt;)]):&lt;/span&gt;
                &lt;span class="c1"&gt;# use tf.control_dependencies to make sure &amp;#39;saving&amp;#39; before &amp;#39;prediction&amp;#39;&lt;/span&gt;

                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xw_plus_b&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;lstm_cell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&amp;quot;Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf&lt;/span&gt;
&lt;span class="sd"&gt;        Note that in this formulation, we omit the various connections between the&lt;/span&gt;
&lt;span class="sd"&gt;        previous state and the gates.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="c1"&gt;## Build Input Gate&lt;/span&gt;
        &lt;span class="n"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;im&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;ib&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;input_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;input_gate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;im&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ib&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;## Build Forget Gate&lt;/span&gt;
        &lt;span class="n"&gt;fx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;forget_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;fm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;forget_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;fb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;forget_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;forget_gate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;## Memory&lt;/span&gt;
        &lt;span class="n"&gt;cx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;memory_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;cm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;memory_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;cb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;memory&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;update&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cb&lt;/span&gt;
        &lt;span class="c1"&gt;## Update State&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;forget_gate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;input_gate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;## Build Output Gate&lt;/span&gt;
        &lt;span class="n"&gt;ox&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;output_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;om&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;output_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;ob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;output_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;output_gate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ox&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;om&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ob&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;## Ouput&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_gate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_batch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saved&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;input_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;input_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;forget_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;forget_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;output_gate_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;output_gate_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;memory_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;memory_o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;

            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;input_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;forget_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;output_gate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;memory&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
              &lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="c1"&gt;# Variables saving state across unrollings.&lt;/span&gt;
        &lt;span class="n"&gt;saved_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;saved_state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;n_batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="c1"&gt;# Unrolled LSTM loop.&lt;/span&gt;
        &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;saved_output&lt;/span&gt;
        &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;saved_state&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;input_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lstm_cell&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# State saving across unrollings.&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;control_dependencies&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;saved_output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                      &lt;span class="n"&gt;saved_state&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)]):&lt;/span&gt;
            &lt;span class="c1"&gt;# use tf.control_dependencies to make sure &amp;#39;saving&amp;#39; before &amp;#39;calculating loss&amp;#39;&lt;/span&gt;

            &lt;span class="c1"&gt;# Classifier&lt;/span&gt;
            &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xw_plus_b&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;classifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_unrollings&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;perplexity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;sum_logprob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
        &lt;span class="n"&gt;sample_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_sample_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sample_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;newshape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                &lt;span class="n"&gt;sample_label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;newshape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sample_input&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
                &lt;span class="n"&gt;sum_logprob&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;logprob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sample_label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;perplexity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sum_logprob&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sample_size&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;perplexity&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;feed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;id2char&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;LETTER_SIZE&lt;/span&gt;&lt;span class="p"&gt;)]])&lt;/span&gt;
        &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;characters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_sample_state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_prediction&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;feed&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
            &lt;span class="n"&gt;feed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;sentence&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;characters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feed&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sentence&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# build training batch generator&lt;/span&gt;
&lt;span class="n"&gt;batch_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rnn_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# build validation data&lt;/span&gt;
&lt;span class="n"&gt;valid_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rnn_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;valid_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_batches&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_size&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# build LSTM model&lt;/span&gt;
&lt;span class="n"&gt;model_LSTM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LSTM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_unrollings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_unrollings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_memory&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_train_batch&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# initial model&lt;/span&gt;
&lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# online training&lt;/span&gt;
&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;
&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;
&lt;span class="n"&gt;valid_freq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_generator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;

    &lt;span class="n"&gt;train_perplexity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;perplexity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;s loss = &lt;/span&gt;&lt;span class="si"&gt;%6.4f&lt;/span&gt;&lt;span class="s1"&gt;, perplexity = &lt;/span&gt;&lt;span class="si"&gt;%6.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;
           &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_perplexity&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;valid_freq&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;=============== Validation ===============&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;validation perplexity = &lt;/span&gt;&lt;span class="si"&gt;%6.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;perplexity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Generate From &lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;a&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;:  &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Generate From &lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;h&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;:  &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Generate From &lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;m&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;:  &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;==========================================&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 1/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;66s loss = 1.8249, perplexity = 5.6840&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 2/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.5348, perplexity = 5.7269&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 3/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.4754, perplexity = 5.7866&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 4/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.4412, perplexity = 5.3462&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 5/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.4246, perplexity = 5.8845&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.7260&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ah plays agrestiom scattery at an experiments the a&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ht number om one nine six three kg aid rosta franci&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;m within v like opens and solepolity ledania as was&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 6/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.4094, perplexity = 6.0429&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 7/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.3954, perplexity = 5.6133&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 8/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3905, perplexity = 5.4791&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 9/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3675, perplexity = 5.7168&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 10/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3861, perplexity = 5.3937&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.5992&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ands their hypenman sam diversion passes to rouke t&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;hash pryess the setuluply see include the grophistr&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;merhouses tourism in vertic or influence carbon min&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 11/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.3782, perplexity = 5.5835&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 12/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3802, perplexity = 6.0567&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 13/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3723, perplexity = 6.0672&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 14/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3729, perplexity = 6.4365&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 15/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3682, perplexity = 6.2878&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.7153&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ate at decade a july uses mobe on the john press to&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;htell yullandi is u one five it naval railandly eng&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ment theory president and much three sinit in harde&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 16/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;65s loss = 1.3647, perplexity = 5.5579&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 17/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3691, perplexity = 5.3885&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 18/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.3535, perplexity = 6.4797&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 19/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3637, perplexity = 5.8126&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 20/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;62s loss = 1.3567, perplexity = 5.9839&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.6210&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ate treaty jack a golderazogon develoged civilized&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;hyene is ricpstowed dark preferent crurts annivaril&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;mer centine all level end of a character of tracks&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 21/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;65s loss = 1.3584, perplexity = 6.0557&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 22/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3535, perplexity = 7.0777&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 23/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3700, perplexity = 5.7674&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 24/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3609, perplexity = 6.1226&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 25/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.3663, perplexity = 6.2711&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.6048&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;an vary palest in some live halleten converting to&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;heper could use that the l bidging the five zero th&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;mer yort can the real forexanded or rather then for&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 26/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;66s loss = 1.3551, perplexity = 6.1640&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 27/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;65s loss = 1.3586, perplexity = 6.3620&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 28/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;65s loss = 1.3744, perplexity = 5.5748&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 29/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;64s loss = 1.3634, perplexity = 6.0498&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;Epoch 30/30&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;63s loss = 1.3671, perplexity = 6.2313&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=============== Validation ===============&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;validation perplexity = 3.4751&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;an one brivistrial empir thorodox to an of one city&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;h&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ho wing two he wonders marding where never boat lit&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Generate From &amp;#39;m&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;mptemeignt linerical premore logical boldving on ch&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;==========================================&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;最後來產生一篇以"t"為開頭的1000字文章吧！&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_LSTM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;generate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;t&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_generate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;th the oppose asia college on all of indirect i suicide upse angence and including khazool cashle with jeremp of the case hasway was catiline tribui s law can be wounds to free from an eventually locations university colid for admirum syn semition goths display the might the official up it alder stowinity name like or day elenth names and lesk external links a loons for have the genione e elevang cress leven isbn effects on cultural leave to oldincil he hokerzon blacklomen with the known resolvement of literated by college founded to families in ak urke player jain of highling fake state a first o al reason into the son then mmpt one nine three three npunt university unexal and currently amnyanipation behavion from ber and ii variety of the gupife number topan has one three zero z capital prime genary brown one nine five nine so universities country recipient the vegetables bether form the distinct de plus out as a first a johnson quicky s remain which an death to anti in panibus series&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;看得出來LSTM想表達什麼嗎，哈哈！&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb"&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>實作Tensorflow (5)：Word2Vec</title><link href="https://ycc.idv.tw/tensorflow-tutorial_5.html" rel="alternate"></link><published>2017-11-19T12:00:00+08:00</published><updated>2017-11-19T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-19:/tensorflow-tutorial_5.html</id><summary type="html">&lt;p&gt;Word2Vec觀念解析 / Word2Vec的架構 / Word2Vec的兩種常用方法：Skip-Gram和CBOW / 準備文本語料庫 / 實作Skip-Gram / 實作CBOW (Continuous Bag of Words)&lt;/p&gt;</summary><content type="html">&lt;p&gt;機器有辦法自行從文本中觀察出詞彙間的相似度嗎？是可以的，word2vec是"word to vector"的縮寫，代表的正是將每個字轉換成向量，而一旦兩個字的向量越是靠近，就代表它的相似度越高，我們究竟要如何得到這些向量呢？方法簡單但出奇有效，文章的最後會向大家呈現它的精彩的結果。&lt;/p&gt;
&lt;p&gt;本單元程式碼Skip-Gram Word2Vec部分可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/05_1_word2vec_SkipGram.py"&gt;Github&lt;/a&gt;下載，CBOW Word2Vec部分可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/05_2_word2vec_CBOW.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;h3&gt;Word2Vec觀念解析&lt;/h3&gt;
&lt;p&gt;Word2Vec的形式和Autoencoder有點像，一樣是從高維度的空間轉換到低維度的空間，再轉換回去原本的維度，只是這一次轉回去的東西不再是原本一模一樣的東西了。&lt;/p&gt;
&lt;p&gt;Word2Vec的Input和Output這次變成是上下文的文字組合，舉個例子，"by the way"這個用法如果多次被機器看過的話，機器是有辦法去學習到這樣的規律的，此時"by"與"the"和"way"便會產生一個上下文的關聯性，為了將這樣的關聯性建立起來，我們希望當我輸入"by"時，機器有辦法預測並輸出"the"或"way"，這代表在機器內部它已經學習到了上下文的關聯性。&lt;/p&gt;
&lt;p&gt;那如果今天這個機器也同時看到很多次的"on the way"這種用法，所以當我輸入"on"時，機器要有辦法預測並輸出"the"或"way"，但是我們不希望"on"和"by"兩個詞在學習時是分開學習的，我們希望機器可以因為"by the way"和"on the way"的結構很相似，所以有辦法抓出"on"和"by"是彼此相似的結論。&lt;/p&gt;
&lt;p&gt;如何做到呢？答案就是限縮這個上下文的關聯性的儲存維度，如果我的字彙量有1000個，這1000個字彙彼此有上下文的關聯性，最完整表示上下文關聯性的方法就是設置一個1000x1000或者更大的表格，把所有字彙間的上下文關聯性全部存起來，但我們不想要這麼做，我要求機器用更小的表格來儲存上下文的關聯性，此時機器被迫將一些詞彙使用同樣的表格位置，同樣的轉換。一旦限縮了上下文關聯性的儲存維度，"on the way"和"by the way"中的"on"和"by"就會被迫分為同一類，因此我們成功的建立了字詞間的相似性關係。&lt;/p&gt;
&lt;h3&gt;Word2Vec的架構&lt;/h3&gt;
&lt;p&gt;&lt;img alt="word2vec" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.008.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;實作上如上圖所示，我們輸入一個字詞，譬如"cat"，通常會將他轉成One-hot encoding表示，但要注意喔！文本的字彙量是非常龐大的，所以當我們使用One-hot encoding表示時，將會出現一個非常長但Sparse的向量，相同的輸出層也同樣是一個很長的One-hot encoding，它的維度會和輸入層一樣大，因為我們要分析的字彙在輸入和輸出是一樣多的。&lt;/p&gt;
&lt;p&gt;然後，和Autoencoder使用一樣的手法，中間的Hidden Layer放置低維度、少神經元的一層，但不同於Autoencoder，Word2Vec所有的轉換都是線性的，沒有非線性的Activation Function夾在其中，為什麼呢？因為我們的輸入是Sparse的而且只有0和1的差別，所以每一條通路就變成只有導通或不導通的差別，Activation Function有加等於沒加，使用線性就足夠了。&lt;/p&gt;
&lt;p&gt;這個中間的Hidden Layer被稱為Embedding Matrix，它做了一個線性的Dimension Reduction，將原本高維度的One-hot encoding降低成低維度，然後再透過一個線性模型轉換回去原本的維度。假設字彙的數量有N個，所以輸入矩陣X是一個1xN的矩陣，輸出的矩陣同樣也是1xN的矩陣，當我先做一個線性的Dimension Reduction，將維度降到d維，此時Embedding Matrix會是一個Nxd的矩陣V，然後再由線性模型轉換回去原本的維度，這個轉換矩陣W是一個Nxd矩陣，因此綜合上述，可用一個簡潔的表示式表示：&lt;span class="math"&gt;\(Y=W^T VX\)&lt;/span&gt;，我們的目標就是找出這個W和V矩陣的每個元素。&lt;/p&gt;
&lt;p&gt;你會想說線性模型很簡單啊！就是仿照Autoencoder的作法，然後把Activation Function拿掉不就了事了，並且因為輸出是One-hot Encoding所以最後套用Softmax，那不就輕鬆完成！但是真正的大魔王就出在字彙量，字彙量一旦很大，事情就變得不可收拾了，而且字彙量是一定小不得的，那怎麼辦？&lt;/p&gt;
&lt;p&gt;在Dimension Reduction我們可以採取一個快速的方法，因為除了我要表示的字的位置是1以外其他都是0，所以其他都可以不看，我們就直接看是在第幾個位置上是1，然後再到Embedding Matrix上找到相應的行直接取出就是答案了，這樣查詢的動作，在Tensorflow中可以使用&lt;code&gt;tf.nn.embedding_lookup&lt;/code&gt;來辦到。&lt;/p&gt;
&lt;p&gt;再接下來最後的Cross-Entropy Loss計算也非常龐大，因為有幾個字彙就需要累加幾組數字，我們有一招偷吃步的方法叫做「Sampled Softmax」，作法是這樣的，我們不去計算全部詞彙的Cross-Entropy，而是選擇幾組詞彙來評估Cross-Entropy，在選擇上我們會隨機挑選一些Labels和預測結果差異度很大的詞彙(稱為Negative Examples)來算Cross-Entropy，我們在Tensorflow可以使用&lt;code&gt;tf.nn.sampled_softmax_loss&lt;/code&gt;來辦到「Sampled Softmax」。&lt;/p&gt;
&lt;p&gt;我們先不管輸入和輸出究竟怎麼取得，如果我們成功的建立了輸入和輸出的上下文關係，此時中間的Embedding空間正是精華的所在，經過剛剛推論，我們預期在這個空間當中，相似的詞彙會彼此靠近，我們評估兩個向量的相似性可以使用Cosine來評估，當兩向量的夾角越小代表它們越是相似，待會的實作當中我們將會利用Cosine來建立Similarity的大小，藉此來找到前幾個和它很靠近的詞彙。&lt;/p&gt;
&lt;p&gt;另外，經研究指出這個Embedding空間的效果不只是可以算出詞彙間的相似性，還可以顯示詞彙間的比較關係，例如：北京之於中國，等同於台北之於台灣，這樣的比較關係也顯示在這個Embedding空間裡頭，所以在這空間裡會有以下的向量關係式：&lt;span class="math"&gt;\(V_{北京} - V_{中國}+V_{台灣}=V_{台北}\)&lt;/span&gt;，是不是很神奇啊！&lt;/p&gt;
&lt;h3&gt;Word2Vec的兩種常用方法：Skip-Gram和CBOW&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Skip-Gram和CBOW" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.009.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;剛剛一直在講的是中間的結構應該怎麼建立，現在來看看我們可以輸入和輸出哪些詞彙來建立起上下文的關係，有兩種常用的類別：Skip-Gram和CBOW。&lt;/p&gt;
&lt;p&gt;Skip-Gram如上圖所示，當我輸入一個&lt;span class="math"&gt;\(word(t)\)&lt;/span&gt;時，我希望它能輸出它的前文和後文，這是相當直覺的建立上下文的方法，所以如果我希望用前一個字和後一個字來訓練我的Word2Vec，我就會有兩組數據：&lt;span class="math"&gt;\((w(t),w(t-1))\)&lt;/span&gt;和&lt;span class="math"&gt;\((w(t),w(t+1))\)&lt;/span&gt;，相當好理解。&lt;/p&gt;
&lt;p&gt;而CBOW(Continuous Bag of Words)使用另外一種方法來建立上下文關係，它將一排字挖掉中間一個字，然後希望由上下文的關係有辦法猜出中間那個字，就像是填空題，此時輸入層就變成會有多於1個字，那該怎麼處理，答案是轉換到Embedding空間後再相加平均，因為是線性轉換，所以直接線性累加就可以了。&lt;/p&gt;
&lt;h3&gt;準備文本語料庫&lt;/h3&gt;
&lt;p&gt;先帶入一些待會會用到的函式庫，並且決定我們要取用多少&lt;code&gt;VOCABULARY_SIZE&lt;/code&gt;個詞彙量來做訓練。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;zipfile&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;math&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;urllib.request&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;urlretrieve&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;

&lt;span class="n"&gt;VOCABULARY_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;接下來下載Dataset，並做一些前處理。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;maybe_download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected_bytes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Download a file if not present, and make sure it&amp;#39;s the right size.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urlretrieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;statinfo&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;statinfo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;st_size&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;expected_bytes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Found and verified &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;statinfo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;st_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;Failed to verify &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;. Can you get to it with a browser?&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Extract the first file enclosed in a zip file as a list of words&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;zipfile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ZipFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;namelist&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;vocabulary_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VOCABULARY_SIZE&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;UNK&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;collections&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Counter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;most_common&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocabulary_size&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;unk_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;  &lt;span class="c1"&gt;# dictionary[&amp;#39;UNK&amp;#39;]&lt;/span&gt;
            &lt;span class="n"&gt;unk_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unk_count&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unk_count&lt;/span&gt;
    &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;


&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Downloading text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;maybe_download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;http://mattmahoney.net/dc/text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;./text8.zip&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;31344016&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;=====&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Data size &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;First 10 words: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;=====&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;build_dataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                &lt;span class="n"&gt;vocabulary_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VOCABULARY_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;  &lt;span class="c1"&gt;# Hint to reduce memory.&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Most common words (+UNK)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Sample data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Downloading text8.zip&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Found and verified ./text8.zip&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=====&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Data size 17005207&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;First 10 words&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;=====&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Most common words (+UNK) [[&amp;#39;UNK&amp;#39;, 189230], (&amp;#39;the&amp;#39;, 1061396), (&amp;#39;of&amp;#39;, 593677), (&amp;#39;and&amp;#39;, 416629), (&amp;#39;one&amp;#39;, 411764)]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;我們取用&lt;code&gt;VOCABULARY_SIZE = 100000&lt;/code&gt;，也是說我們將文本中的詞彙按出現次數的多寡來排列，取前面&lt;code&gt;VOCABULARY_SIZE&lt;/code&gt;個保留，其餘詞彙皆歸類到「UNK Token」裡頭，UNK代表UNKnown的縮寫。&lt;/p&gt;
&lt;p&gt;我們文本的字詞數量總共有17005207個字，開頭前十個字的句子是'anarchism originated as a term of abuse first used against'。所有的這17005207個字會依照&lt;code&gt;dictionary&lt;/code&gt;給予每個字Index，而文本會被表示為一個由整數所構成的List，這會放在&lt;code&gt;data&lt;/code&gt;裡頭，而這個Index也就直接當作One-hot Encoding中代表這個詞彙的維度位置。當我想要把Index轉換回去我們看得懂的字的時候，就需要&lt;code&gt;reverse_dictionary&lt;/code&gt;的幫忙，有了這些，我們的語料庫就已經建立完成了。&lt;/p&gt;
&lt;h3&gt;實作Skip-Gram&lt;/h3&gt;
&lt;p&gt;有了語料庫，我們就可以產生出我想要的輸入和輸出，在Skip-Gram方法，如果我的輸入是&lt;code&gt;target word&lt;/code&gt;，我會先從&lt;code&gt;target word&lt;/code&gt;向前、向後看出去&lt;code&gt;skip_window&lt;/code&gt;的大小，所以可以選擇當作輸出的字有&lt;code&gt;skip_window*2&lt;/code&gt;個，接下來我從這&lt;code&gt;skip_window*2&lt;/code&gt;個中選擇&lt;code&gt;num_skips&lt;/code&gt;個當作輸出，所以一個&lt;code&gt;target word&lt;/code&gt;會產生&lt;code&gt;num_skips&lt;/code&gt;筆數據，如果我一個batch需要&lt;code&gt;batch_size&lt;/code&gt;筆數據，我就必須有&lt;code&gt;batch_size//num_skips&lt;/code&gt;個&lt;code&gt;target word&lt;/code&gt;，依照這樣的規則下面建立一個Generator來掃描文本，並輸出要訓練使用的Batch Data。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;skip_gram_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;num_skips&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;num_skips&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt;

    &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;span&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# [ skip_window target skip_window ]&lt;/span&gt;
    &lt;span class="n"&gt;buffer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collections&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;span&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# initialization&lt;/span&gt;
    &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;span&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# generate&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt;  &lt;span class="c1"&gt;# target label at the center of the buffer&lt;/span&gt;
        &lt;span class="n"&gt;targets_to_avoid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;targets_to_avoid&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;span&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;targets_to_avoid&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

        &lt;span class="c1"&gt;# Recycle&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

        &lt;span class="c1"&gt;# scan data&lt;/span&gt;
        &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Enough num to output&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;


&lt;span class="c1"&gt;# demonstrate generator&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;di&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;di&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)]:&lt;/span&gt;
    &lt;span class="n"&gt;batch_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;skip_gram_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_generator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;with num_skips = &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt; and skip_window = &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;:&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;    batch:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bi&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;bi&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;    labels:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;li&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;li&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;with num_skips = 2 and skip_window = 1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;with num_skips = 4 and skip_window = 2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SkipGram&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_embedding&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new grap&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# normalize embeddings&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                          &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdagradOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# similarity&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                               &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_similarity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_uniform&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                                  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                             &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                                 &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="c1"&gt;# Look up embeddings for inputs.&lt;/span&gt;
        &lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Compute the softmax loss, using a sample of the negative labels each time.&lt;/span&gt;
        &lt;span class="n"&gt;num_softmax_sampled&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;

        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                 &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sampled_softmax_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                            &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                            &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;num_sampled&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_softmax_sampled&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;nearest_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;similarity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_similarity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
        &lt;span class="n"&gt;X_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;valid_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;nearests&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;valid_word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# select highest similarity word&lt;/span&gt;
            &lt;span class="n"&gt;nearest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;similarity&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;nearest&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;以上就是我建立的Model，這裡我採取&lt;code&gt;online_fit&lt;/code&gt;的方法，不同於之前的&lt;code&gt;fit&lt;/code&gt;，&lt;code&gt;online_fit&lt;/code&gt;可以不用事先將所有Data一次餵進去，而是可以陸續的餵入Data，所以我會從上面的Generator陸續產生Batch Data並餵入Model裡來做訓練。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# build skip-gram batch generator&lt;/span&gt;
&lt;span class="n"&gt;batch_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;skip_gram_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_skips&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;skip_window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# build skip-gram model&lt;/span&gt;
&lt;span class="n"&gt;model_SkipGram&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SkipGram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VOCABULARY_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# initial model&lt;/span&gt;
&lt;span class="n"&gt;model_SkipGram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# online training&lt;/span&gt;
&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;
&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_generator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_SkipGram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;s loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch 1/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    4.2150&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 2/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.7561&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 3/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.6276&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 4/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.5098&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 5/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.5123&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 6/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.5000&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 7/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.5155&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 8/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.3983&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 9/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.4418&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.4118&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 11/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3993&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 12/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.4074&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 13/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.3243&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 14/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.3448&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 15/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3607&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 16/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3408&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 17/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3705&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 18/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3894&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 19/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3536&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 20/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3123&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 21/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3046&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 22/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3117&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 23/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3023&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 24/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2623&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 25/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.3197&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 26/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2833&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 27/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2456&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 28/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2272&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 29/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2663&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 30/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2274&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 31/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2335&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 32/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.3003&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 33/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.2507&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 34/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2486&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 35/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2382&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 36/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2687&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 37/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2145&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 38/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2437&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 39/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2171&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 40/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.0492&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 41/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.9380&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 42/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1556&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 43/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1804&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 44/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;16s loss =    3.2800&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 45/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1366&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 46/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2190&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 47/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2381&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 48/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2419&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 49/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.0127&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 50/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1232&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;我們來看看效果如何，我們使用Embedding Vectors彼此間的Cosine來定義出字詞間的相關性，並且列出8個最為靠近的字詞。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;valid_words_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;210&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;239&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;392&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;396&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nearests&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_SkipGram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nearest_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_words_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Nearest to &lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;two&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;three&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;four&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;five&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;eight&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;six&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;one&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seven&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;zero&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;that&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;which&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;however&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;thus&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;what&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sepulchres&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dancewriting&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tatars&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;resent&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;his&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;her&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;their&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;your&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;my&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;its&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;our&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;othniel&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;personal&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;were&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;are&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;was&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;have&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;remain&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;junkanoo&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;those&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;include&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;had&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;all&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;both&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;various&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;many&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;several&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;every&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;these&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;some&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;obtaining&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;area&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;areas&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;region&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;territory&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;location&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;xylophone&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;stadium&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;city&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;island&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;east&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;west&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;south&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;southeast&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;north&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;eastern&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;southwest&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;central&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mainland&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;himself&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;him&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;themselves&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;herself&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;them&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;itself&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;wignacourt&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;majored&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mankiewicz&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;white&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;green&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;yellow&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dark&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;papyri&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;kemal&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;結果相當驚人，與'two'靠近的真的都是數字類型的文字，與'that'靠近的都是文法功能性的詞彙，與'his'靠近的都是所有格代名詞，與'were'靠近的是be動詞，與'all'最靠近的是'both'，與'east'靠近的都是一些代表方向的詞彙，與'white'靠近的都是一些顏色的詞彙，真的是太神奇了！&lt;/p&gt;
&lt;p&gt;接下來直接來觀察Embedding空間，以下使用t-SNE來圖像化Embedding空間。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pylab&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.manifold&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;More labels than embeddings&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# in inches&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
        &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;xytext&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;textcoords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;offset points&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="n"&gt;ha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;right&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;va&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bottom&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;visualization_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;
&lt;span class="c1"&gt;# transform embeddings to 2D by t-SNE&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_SkipGram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;visualization_words&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;tsne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;perplexity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pca&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;exact&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;two_d_embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tsne&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# list labels&lt;/span&gt;
&lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;model_SkipGram&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;visualization_words&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="c1"&gt;# plot&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;two_d_embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/05_output_13_0.png"&gt;&lt;/p&gt;
&lt;p&gt;如此一來你將可以簡單的看出，哪些詞彙彼此相似而靠近。&lt;/p&gt;
&lt;h3&gt;實作CBOW (Continuous Bag of Words)&lt;/h3&gt;
&lt;p&gt;接著看CBOW的方法，如果我預期輸出的字是&lt;code&gt;target word&lt;/code&gt;，從&lt;code&gt;target word&lt;/code&gt;向前向後看出去&lt;code&gt;context_window&lt;/code&gt;的大小，看到的字都當作我的輸入，所以我輸入的字總共需要&lt;code&gt;context_window*2&lt;/code&gt;個，一個&lt;code&gt;target word&lt;/code&gt;只會產生一筆數據，如果我一個batch需要&lt;code&gt;batch_size&lt;/code&gt;筆數據，我就必須有&lt;code&gt;batch_size&lt;/code&gt;個&lt;code&gt;target word&lt;/code&gt;，依照這樣的規則下面建立一個Generator來掃描文本，並輸出要訓練使用的Batch Data。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;cbow_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;span&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;context_window&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# [ context_window target context_window ]&lt;/span&gt;
    &lt;span class="n"&gt;num_bow&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;span&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_bow&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;buffer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;collections&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deque&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maxlen&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;span&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# initialization&lt;/span&gt;
    &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;span&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# generate&lt;/span&gt;
    &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;context_window&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;bow&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;bow&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bow&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;
        &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

        &lt;span class="c1"&gt;# Recycle&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

        &lt;span class="c1"&gt;# scan data&lt;/span&gt;
        &lt;span class="n"&gt;buffer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Enough num to output&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# demonstrate generator&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;di&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;di&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;context_window&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;batch_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cbow_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_generator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;with context_window = &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;:&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;batch:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;show_batch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
        &lt;span class="n"&gt;tmp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
            &lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
        &lt;span class="n"&gt;show_batch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;show_batch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;labels:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;li&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;li&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;with context_window = 1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="nt"&gt;with context_window = 2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;anarchism&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;originated&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;early&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;],&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;early&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;working&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;as&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;term&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;of&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;abuse&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;first&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;used&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;against&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_embedding&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;context_window&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new grap&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their predictions and loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# normalize embeddings&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                          &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;norm&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdagradOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

            &lt;span class="c1"&gt;# similarity&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                               &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_similarity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_uniform&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                                  &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                            &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                                &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="c1"&gt;# Look up embeddings for inputs.&lt;/span&gt;
        &lt;span class="n"&gt;embed_bow&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_lookup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;embeddings&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embed_bow&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Compute the softmax loss, using a sample of the negative labels each time.&lt;/span&gt;
        &lt;span class="n"&gt;num_softmax_sampled&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;

        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                 &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sampled_softmax_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                            &lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                            &lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;num_sampled&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_softmax_sampled&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="n"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;nearest_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;similarity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_similarity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
        &lt;span class="n"&gt;X_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

        &lt;span class="n"&gt;valid_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;nearests&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_size&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;valid_word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_word&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# select highest similarity word&lt;/span&gt;
            &lt;span class="n"&gt;nearest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;similarity&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;nearest&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_dataset&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normalized_embeddings&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;context_window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="c1"&gt;# build CBOW batch generator&lt;/span&gt;
&lt;span class="n"&gt;batch_generator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cbow_batch_generator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# build CBOW model&lt;/span&gt;
&lt;span class="n"&gt;model_CBOW&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CBOW&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_vocabulary&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VOCABULARY_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_embedding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;context_window&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# initialize model&lt;/span&gt;
&lt;span class="n"&gt;model_CBOW&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# online training&lt;/span&gt;
&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;
&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5000&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_generator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_CBOW&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;online_fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
    &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_batchs_in_epoch&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;s loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;avg_loss&lt;/span&gt; &lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch 1/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.8700&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 2/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.2961&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 3/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1988&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 4/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.1201&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 5/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.0734&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 6/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    3.0239&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 7/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.9378&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 8/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.9549&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 9/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.9651&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.9028&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 11/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.8770&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 12/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.8298&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 13/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.8437&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 14/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7681&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 15/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7823&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 16/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7867&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 17/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7540&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 18/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7567&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 19/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7340&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 20/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.6212&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 21/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5187&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 22/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7150&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 23/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.6647&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 24/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.7381&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 25/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5337&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 26/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.6587&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 27/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.6648&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 28/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5963&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 29/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5418&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 30/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.6041&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 31/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5535&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 32/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5928&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 33/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5535&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 34/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5233&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 35/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5658&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 36/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5966&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 37/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5422&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 38/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5673&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 39/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5142&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 40/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5175&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 41/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4909&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 42/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4872&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 43/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5513&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 44/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4917&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 45/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5198&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 46/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.5007&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 47/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.2530&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 48/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4154&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 49/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4927&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 50/50&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    2.4948&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;valid_words_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;210&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;239&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;392&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;396&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nearests&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_CBOW&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nearest_words&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;valid_words_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_nearest&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Nearest to &lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="se"&gt;\&amp;#39;&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;nearests&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;two&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;three&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;four&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;five&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;six&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;seven&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;eight&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;nine&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;zero&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;that&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;which&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;what&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;furthermore&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;however&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;talmudic&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;endress&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tonight&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;how&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;his&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;her&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;their&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;my&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;your&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;its&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;our&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;the&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;photographs&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;were&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;are&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;have&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;include&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;contain&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;was&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;vigorous&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;tend&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;substituting&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;all&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;various&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;both&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;many&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;every&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;shamed&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;everyone&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;those&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;wiccan&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;area&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;areas&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;region&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;regions&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;taipan&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;northeast&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;boundaries&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;hattin&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;surface&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;east&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;west&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;southeast&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;south&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;northwest&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;southwest&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;eastern&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;northeast&amp;#39;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;north&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;himself&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;him&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;themselves&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;herself&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;itself&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;them&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;donal&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;activex&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;carnaval&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Nearest to &amp;#39;white&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;:&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;black&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;morel&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;green&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;bluish&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dead&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;lessig&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pylab&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.manifold&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;More labels than embeddings&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# in inches&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;embeddings&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
        &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;blue&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;xytext&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;textcoords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;offset points&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="n"&gt;ha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;right&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;va&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;bottom&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pylab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;visualization_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;800&lt;/span&gt;
&lt;span class="c1"&gt;# transform embeddings to 2D by t-SNE&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_CBOW&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;embedding_matrix&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;visualization_words&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
&lt;span class="n"&gt;tsne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;perplexity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pca&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;exact&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;two_d_embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tsne&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# list labels&lt;/span&gt;
&lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;model_CBOW&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reverse_dictionary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;visualization_words&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="c1"&gt;# plot&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;two_d_embed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/05_output_20_0.png"&gt;&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>實作Tensorflow (4)：Autoencoder</title><link href="https://ycc.idv.tw/tensorflow-tutorial_4.html" rel="alternate"></link><published>2017-11-18T12:00:00+08:00</published><updated>2017-11-18T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-18:/tensorflow-tutorial_4.html</id><summary type="html">&lt;p&gt;Autoencoder觀念解析 / Autoencoder程式碼 / 測試Autoencoder / 壓縮碼Code與視覺化 / 去雜訊(De-noise) Autoencoder&lt;/p&gt;</summary><content type="html">&lt;p&gt;Autoencoder是一個Neurel Network重要的工具，我個人認為它還漂亮的呈現Neurel Network的強大。&lt;/p&gt;
&lt;p&gt;本單元程式碼Autoencoder部分可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/04_1_Autoencoder_on_MNIST.py"&gt;Github&lt;/a&gt;下載，De-noise Autoencoder部分可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/04_2_DenoiseAutoencoder_on_MNIST.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;h3&gt;Autoencoder觀念解析&lt;/h3&gt;
&lt;p&gt;在「機器學習技法」的系列文章，我也&lt;a href="http://www.ycc.idv.tw/YCNote/post/35"&gt;曾經介紹過Autoencoder&lt;/a&gt;，可以搭配這篇服用。&lt;/p&gt;
&lt;p&gt;Autoencoder概念很簡單，就是做資訊的壓縮，概念是這樣的，當我在一層當中使用神經元愈多，可以儲存的資訊量也就愈多，相反的神經元越少，可以儲存的資訊量越少，如果我要使用Neurel Network作資料壓縮的話，我希望的是可以使用比原本更少的資訊量來儲存，如果原本是一張MNIST的圖，有28x28=784個Pixels，所以可以想知，如果我要作壓縮就要使得壓縮後的神經元可以比784個更少。&lt;/p&gt;
&lt;p&gt;但是什麼都不做我們就可以平白無故的做到壓縮？當然不行，我們還得從資料中找到一些規律，套用這些規律把多餘的東西去除，留下精髓，我們才可以把資料作壓縮，所以在實作上我們會建立一個神經元由大到小的Neurel Network，逐步的轉換，逐步的壓縮資訊。&lt;/p&gt;
&lt;p&gt;那麼壓縮的目的是為了什麼？當然是有辦法還原回去原本狀態，這樣的壓縮才是有意義的，例如：將文檔打包成RAR，檔案大小會變小，但如果實際要再使用這個檔案，那就必須先做解壓縮，然後還原回去原本的檔案，這裡的還原率必須是百分之一百的，Autoencoder一樣的有一個機制可以還原，在實作上我們會建立一個神經元由小到大的Neurel Network，逐步的還原回去原本的狀態。&lt;/p&gt;
&lt;p&gt;因此一個Autoencoder的圖像就出現了，我們需要有一組「Encoder」來逐步的壓縮，最後留下非常精簡的「Embedding Code」，而這組「Embedding Code」可以再經由「Decoder」還原回去原本的樣子，那我們怎麼讓他自己產生「Encoder」和「Decoder」呢？把原本的Input當作Output的目標答案去訓練Neurel Network就可以了，這就是Autoencoder巧妙的地方。&lt;/p&gt;
&lt;p&gt;不管是「Encoder」還是「Decoder」他們的權重是可以調整的，所以如果你將Encoder+Decoder的結構建立好並搭配Input當作Output的目標答案，它在Training的過程，Autoencoder會試著找出最好的權重來使得資訊可以盡量完整還原回去，所以Autoencoder可以自行找出了Encoder和Decoder。&lt;/p&gt;
&lt;p&gt;Encoder的效果等同於做Dimension Reduction，Encoder轉換原本數據到一個新的空間，這個空間可以比原本Features描述的空間更能精簡的描述這群數據，而中間這層Layer的數值Embedding Code就是新空間裡頭的座標，有些時候我們會用這個新空間來判斷每筆Data之間彼此的接近程度。&lt;/p&gt;
&lt;p&gt;&lt;img alt="autoencoder" src="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/img/TensorflowTutorial.007.jpeg?raw=true"&gt;&lt;/p&gt;
&lt;h3&gt;Autoencoder程式碼&lt;/h3&gt;
&lt;p&gt;實現Autoencoder和之前DNN並沒有太大的差異，只有兩點要特別提醒一下。&lt;/p&gt;
&lt;p&gt;第一點，以下我會特別把&lt;code&gt;encoder&lt;/code&gt;額外的在&lt;code&gt;structure&lt;/code&gt;裡頭輸出出來，並且增加新的函數&lt;code&gt;encode&lt;/code&gt;，讓使用者可以使用Train好的Encoder來做Encode。&lt;/p&gt;
&lt;p&gt;第二點，以下的Regularizer不是採用單純的L2 Regularizer，我將會使用Weight-Elimination L2 Regularizer，這個Regularizer的好處是會使得權重接近Sparse，也就是說權重會留下比較多的0，這有一個好處，就是每個神經元彼此之間的依賴減少了，因為內積(評估相依性)時有0的那個維度將不會有所貢獻。&lt;/p&gt;
&lt;p&gt;Weight-Elimination L2 Regularizer有這樣的效果原因是這樣的，L2 Regularizer在抑制W的方法是，如果&lt;span class="math"&gt;\(W\)&lt;/span&gt;的分量大的話就抑制多一點，如果分量小就抑制少一點（因為&lt;span class="math"&gt;\(W^2\)&lt;/span&gt;微分為一次），所以最後會留下很多不為0的微小分量，不夠Sparse，這樣的Regularization顯然不夠好，L1 Regularizer可以解決這個問題（因為在大部分位置微分為常數），但不幸的是它無法微分，沒辦法作Backpropagation，所以就有了L2 Regularizer的衍生版本，&lt;/p&gt;
&lt;p&gt;Weight-elimination L2 regularizer: 
&lt;/p&gt;
&lt;div class="math"&gt;$$
\sum_{jk} (W_{jk} (ℓ))^2 / [1+ (W_{jk} (ℓ) )^2]
$$&lt;/div&gt;
&lt;p&gt;這麼一來不管W大或小，它受到抑制的值大小接近的 (Weight-elimination L2 regularizer微分為 &lt;span class="math"&gt;\(-1\)&lt;/span&gt;次方)，因此就可以使得部分&lt;span class="math"&gt;\(W\)&lt;/span&gt;可以為&lt;span class="math"&gt;\(0\)&lt;/span&gt;，達成Sparse的目的。&lt;/p&gt;
&lt;p&gt;那為什麼我要特別在Autoencoder講究Sparse特性呢？原因是我們現在正在做的事是Dimension Reduction，做這件事就好像是替原本空間找出新的軸，而這個軸的數量比原本空間軸的數量來得小，達到Dimension Reduction的效果，所以我們會希望這個新的軸彼此間可以不要太多的依賴，什麼是不依賴呢？直角座標就是最不依賴的座標系，X軸和Y軸內積為0，這樣的軸展開的效率是最好的，所以我們希望在做Regularization的同時可以減少新軸的彼此間的依賴性。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Config the matplotlib backend as plotting inline in IPython&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Autoencoder&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new grap&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_targets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their predictions and loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                                               &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                               &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_targets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                               &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# regularization loss&lt;/span&gt;
            &lt;span class="c1"&gt;# weight elimination L2 regularizer&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt; \
                &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;

            &lt;span class="c1"&gt;# total loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularizer&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_targets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                                                          &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                          &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_targets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                          &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularizer&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

            &lt;span class="n"&gt;n_encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_encoder&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_encoder&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_encoder&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;n_decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reversed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_decoder&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_decoder&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_decoder&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="n"&gt;activation&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;

        &lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
                &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;encoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))],&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;encode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))],&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;decoder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt;
                &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))],&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;decode&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))],&lt;/span&gt;
            &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;targets&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="n"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

            &lt;span class="c1"&gt;# mini-batch gradient descent&lt;/span&gt;
            &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
            &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;index_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_size&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

                &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt;
                             &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_targets&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]}&lt;/span&gt;
                &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

                &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;     &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# evaluate at the end of this epoch&lt;/span&gt;
            &lt;span class="n"&gt;msg_valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;val_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;msg_valid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, val_loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] &lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;s loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt; &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                   &lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;msg_valid&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_encoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_targets&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;測試Autoencoder&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MNIST_data/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;
&lt;span class="n"&gt;valid_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Autoencoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model_1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img_original&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_original&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;15s loss =    0.0332 , val_loss =    0.0327&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0303 , val_loss =    0.0299&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0288 , val_loss =    0.0285&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  4/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0278 , val_loss =    0.0275&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  5/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0272 , val_loss =    0.0271&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  6/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0267 , val_loss =    0.0268&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  7/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0263 , val_loss =    0.0264&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  8/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0259 , val_loss =    0.0262&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  9/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0256 , val_loss =    0.0259&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0254 , val_loss =    0.0257&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 11/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0252 , val_loss =    0.0257&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 12/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0250 , val_loss =    0.0256&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 13/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0248 , val_loss =    0.0255&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 14/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0247 , val_loss =    0.0254&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 15/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0245 , val_loss =    0.0253&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 16/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0244 , val_loss =    0.0253&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 17/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0242 , val_loss =    0.0251&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 18/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0243 , val_loss =    0.0252&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 19/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0240 , val_loss =    0.0251&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 20/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;14s loss =    0.0239 , val_loss =    0.0250&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_loss =    0.0256&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA2oAAACNCAYAAADGgomsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXmcVXX5x9/nzsa+yRKggJCSgKJCiituIaC4ZWmWZi4tZrll+UszKy3LvTINU3PLpdwwNdwXFDVRURQSBRQBQWUZEGZgZs7vjzuf7zlz5zLrveeee3jerxevGe7c5Tz3+Z7v8qye7/sYhmEYhmEYhmEY8SFV6AswDMMwDMMwDMMwGmIHNcMwDMMwDMMwjJhhBzXDMAzDMAzDMIyYYQc1wzAMwzAMwzCMmGEHNcMwDMMwDMMwjJhhBzXDMAzDMAzDMIyYYQc1wzAMwzAMwzCMmNGug5rneRM9z/uf53nveZ53Xq4uKk4kXUaTr/hJuoxJlw+SL2PS5YPky2jyFT9JlzHp8kHyZUy6fG3C9/02/QNKgPeBoUA5MBsY0db3i+O/pMto8hX/v6TLmHT5tgQZky7fliCjyVf8/5IuY9Ll2xJkTLp8bf1XutkTXPPsBrzn+/4CAM/z7gIOB97Z3As8z/Pb8XmF5P36nz+nCRmTLh8UrYzvh35PonxgOnQUqXxg84wj6TImXT4oWhltnglRpDKaDutJunxQ1DIC4Pu+19xz2hP6OBBYHPr/R/WPNcDzvO96nveq53mvtuOz4kIjGZMuHyROxqTLZzosfmyeKX5Mh8VP0uUzHRY/Ns9sAbTHo9YifN+fCkyF4j/5ZiPp8kHyZUy6fJB8GU2+4ifpMiZdPki+jEmXD5Ivo8lX/GwJMoZpj0dtCbBN6P9b1z+WZJIuY9Llg+TLZzosfpKuw6TLB8mXMenyQfLlMx0WP0nXYdLlaxHt8aj9F9jO87xtSX+RxwLH5eSq4kveZfzJT34CQMeOHQHYaaedOProoxs857rrrmPmzJkA3Hbbbbn8+C1Bh9MKfQF5xnRY/CRdh0mXD5IvY9LlA5tnckJFRQUAL7zwAgC77LILDz30EABHHHFEvj/edFjcJF2+FtHmg5rv+zWe550OTCddqeUm3/ffztmVxZN7Ei5j0uUj6fJhOkwCSddh0uWD5MuYdPlsnkkASZeP5Osw6fK1CK++JGY0H1bksaTNVWdpj3x33303QCPv2eZ4//10UZyDDjoIgA8//LCtH+1oSfWZKHW4/fbbAzBv3jwAzjjjDAD+9Kc/tfk986nDzdG5c2cuu+wyAL73ve8BMGvWLL72ta8B8MEHH+Tss+Kmw3xQCB1GyZYuHyRfxkLI17NnTwAGDRrU6G+ag8466yzmzJkDwLvvvgvA7NmzGz3fdJhb+fbee28AZs6cyfDhwwE49NBDATjkkEN4+OGHGzz/xRdfBGDGjBlt/sx861CetKuuugqA7373u+5vv/rVrwD4zW9+09a3bxFxvA9zyZYuH+RPxosuugiAX/7ylwA888wz7L///jn/nHxXfTQMwzAMwzAMwzDyQN6rPhrNc/fdd2/WkzZv3jymT58OwNChQwGYMmUKw4YNA+Cb3/wmAL/73e8iuNJo2WWXXQCoq6sD4KOPPirk5bSZ/v37c+qppwKBLGPGjHEW02uvvbZg19ZWdt11VwDuu+8+AIYMGdKq10+YMIG5c+cCsHjx4maeHV+mTJkCwLRp6VSI008/HYDrr7+e2tragl3X5ujbty8A99xzDxBY5qdOncqiRYva9J7du3cHYN999+U///kPAJs2bWrnlRq54JBDDuGwww4DYL/99gPgi1/8YqPnyXs2ePBg5wkRJSUl+b3ILZBu3boBcMcddwBwwAEHALBhwwbKy8sB6NKli3v+Pvvs0+D1GzZsAGD9+vX84Ac/AOBf//pXfi+6lfz4xz8GAk/aU089BcCFF17ISy+9VLDrMtpHz5492XnnnQGYNGkSAOeee67b22gcykt/xRVXsHz58gJcafsYP358g//vt99+bg595plnIr0WO6gVkLFjxwJw5JFHusfefjsdjqvF9dNPP2XdunUAbgJ/6aWXGD16NABbbbVVZNcbNZoMPv/8cwDuv//+Ql5Oq+nTpw8At9xyS4GvJPccfPDBAI02dS1lypQpnHTSSQAce+yxObuuKNlqq634y1/+0uCxP//5zwDcdNNNbjMVF3r27OnmFx2utIC25ZCm95g1axaQHu9jxowB4L333mvv5eYMbYp/97vfMWrUKCAIGU/SgXLYsGH88Ic/BHCGoY4dO+J5zUbWuDBzIxp+//vfA+mDdJiOHTs6A9Ynn3wCQGVlpfu7dKnXdezYkRtvvBEIDttvvvlmHq+85XzhC19o8P8nnngCwA5pRUZZWRkA55xzDgA//OEP6d+/f4Pn1NXVoTSqr371qw3+1rt3b7fWFxM6lGV7LOqDmoU+GoZhGIZhGIZhxIxYetQUBiir4NKlS6mqqgKCUIGPP/4YiJfltrXIKuF5nrN0y1OxbNmyRs+XRWPEiBHuscwk46QwatQoF0aW4xYEeUchHyo9vNtuu2V93r777gtAKpW2lyhp/7nnnsv3JbaZ0tL0lDF58uR2vc+sWbM4++yzgXSxFQg8p8XCvvvuy9Zbb93gsTvvvBPAzVdxoHfv3kA6xLpXr14AzhP4ox/9qM3ve8EFFwCw7bbbAulCOXGajxUWfskllwCwzTZB20952T777LPoLyxPbL311q7gUktRoSatP8WAQjc1ro888khn6Vb41fXXXw+kS8LHaUwCjBw5slGqg8L6TzjhBHe9q1evBnARNRCsFRdeeCGQvgc1llX04JRTTmHVqlV5lKBldO3aFQi81vKoJZ2dd97ZFUnROplKpRqFBp5//vlAeq+nIhVPPvkkQKyiMVQA7eKLL97sc5599lm3n8nkhBNOKEqPWjZUYCRqzKNmGIZhGIZhGIYRM2JZnn/BggVA0wUK1q5dC7TNEijr1R/+8AcAXn311Ra9Ll+lUAcPHuzkWbly5WafJ4+L8iwgyLV4+umn2/LRDYhTyeWjjz7aFTyQtenZZ59t9/tGUc5WRSRkQctG2MImlHx7zDHHuLyf1pJvHX7lK18B4NFHHwWCe+jnP/95q97nrLPOci0L5FlWTkZzFLokcbiBq3KyhCyo+n7aQq7lmzBhQqNrUv5IS7/zTEaOHMlbb70FBLmjJ554opvHmiLfY1Reztdffx0I8njDa53aoZx++ulNzrltJV9jtHfv3s5rpgbCKuAybtw4HnnkESDwTnfu3JnHHnsMwJXdf/nll4H09yPLfWu92VGvFVrzTj/9dI466igg8Kg1RU1NDf/73/+AoJT9GWecwcaNG5t9bb50OG7cOFfER2NSURitLSz129/+lp/85CdAEO0wZcqUFkXa5FOHAwYMcEWiJGtmQZQoiGKtUA6Xik/cfPPNjXK4PM8jc699++23A2lPvzzC3/72txv8rTnyKd/IkSOBoAhMtnoI5513HgDXXHMNv/71r4F0YZFMNDZbSyH3pNnORi3J923D5zT7prEMfVTI40477QTA3Llz2WGHHYCg2pwG9rhx49yEEA5tETU1NUCwIQnfQOo91tKDWr5oro+WBn444VuLrX4mjZ/+9Kfueym0flqKNkkKT2mKzz77zIW0DB48GAhCyF555ZVYVlobNWqUC+1TH7/f/va3bXqvww8/PGfXFTU77rgjQINDmuaZ9hzQco0qPIaTu08++WSgfQc0aBjGpINaSw5pUaCNq8I8s3HMMccAMHHiRBcaqf6MLdnER43Cgx977DFXSCpchArSRRq0Pqo4zKBBg5xhsinDUdzQ2q/iKNKXwvwAlixZAsDzzz/PwoULgfS6AUGBm912282NAxlRZs+e7UIjC0G4AJMKTbW18u/Pf/5z991o/TjqqKMKnhKhsOj2MG7cOKDhvk7GahVOiQO652QwgSB1Rekb69evd3/Tei/jyJ/+9Cc352RLeSkEI0eOdJXEZRDRweWDDz5wxe5U+Kaurs6F42o9UCXk3r17uwI3uq+LAfX6U0gxBKGPUYdAWuijYRiGYRiGYRhGzIilR00JlfoJDa0VkC41DenETVnPvvzlLzd6LyX1ywIzd+5cZ2GTVyDOHHrooc6lrPL8K1as4P/+7/+AhpaaJKBw17FjxzqdFUORifHjxzN8+HAgsFxns2DLkvvYY4+xZs0aIOiho+RiwPXGue666/J30a3kggsucJb9iRMnAg0T3VuC7r3x48cXlYU/TGb5YcCFl8WJK664AoBvfetbQNrL8M9//rNd76nwpX79+vH3v/8daHmYThQMHjyY73znOw0ekzV3+fLlLlRcdO/e3XngMgtVxQHN+f/4xz8AGD16tPNiZyvOkNlmQVEjxcRf//pX5y3MDG988sknXcitwq3DhXv23HNPIJg/b7rpJtfmRa0orr32Wu69916g7Z7l9qBCE5CbiBj1Wf3+978PBJ6oQhJuO6D2Ac2htU6v1R6vY8eO7jlqVXDVVVc1+B4LgaIL5DkSTz75pNufvfbaa41eN2DAAAAefPBBAHr06OHSAMJ73kKy6667Oj0oQkhev7/85S9ZU45UNOaVV14BcOvDOeec46JQpk6dCgS99eJM2JNWaMyjZhiGYRiGYRiGETNi6VFrCSo/Gy6i0ZQ1Qlbwnj17OoucEsrjzNixY51VVdx99905KawRR8Ld4Ath7Wwt8gDeddddm01u/+CDD5wFV3HPYU+ocvFkZerTp48r0tGhQwcg3Ui5UM15VUp68uTJrnR0W/MG5TWsq6tzTSNVhrpYCJchlpUx7A2NC8opkOdy6dKlrc6/kjVb3ovTTjvNvXccSy7vvPPOriz4888/DwRzSocOHfjGN74BBPIMGzbMFVaRhXvSpElA04WdoqBLly7OMn/ooYcC8Omnn3L55ZcDyYmm0Byn/LJTTjnFJe1rDZC35bLLLmsywkIFD5Tje9FFF7loHOUGFYqhQ4cCaY+Koim0F2kPKvYgj1oh6dSpE5AuHqEcQnlWwqi4hPK77r//fncfyoMj3T/xxBPueYMGDQLSa+Wtt94KNJ/jny9+8YtfAIHXV3mBZ599dpMtIVQYZ5dddnGPZUaMFZpJkyY1Wj+0XitSozlUaGTSpElO5rFjx+b4SrcMzKNmGIZhGIZhGIYRM4rWo9ZSVPlMDV5TqZTL+Sq0xbQpHnjgASAorQ04C1IuKirFFcUyQ1D6Pc7IMpjNmyav57HHHsunn3662feQRVBVlq688kpnmdR3MG3atILlVH7ta18D0tZS3UetRZ5HNSGura11DTQL5SlsLcp/0U8I8iffeOONglxTazjkkENcLp28mE3lQI4fP75Bdd0watoaNyoqKpwl+Kqrrmrwt6qqKm6++WYgGNPyckDgoYpL1ccjjjjCWaWVa7bPPvs4b0xS0BhTdWPP85w3RpEwynvJRklJiasMqDVSFXiV56T3BbjtttsK4sVXrujQoUNdhIVK1yeFU045BUjnsCofKZMBAwa46JHwXmbp0qVAWj8Q7NlUtRSCfLDJkye7Ct6F8KjdcMMNbg7RGqB7dXPeNJXxl5dc4/HZZ5+NTYSUPNK77bZbo79JL63ltttu4/e//327rmtLJ/EHNZX37dOnD5AOmVRflTiiyUebwYqKCrfJ18a2tQUcigFtBFUI4PXXX+fxxx8v5CW1GYUFKjSsqUNaGC1C3/zmN7MWxoma7t27Aw036W0tbqKFWQfauXPn5qT3X5Rk00mcir1kcs011wBBH8IBAwa4sE1tElRmORvZev+ox2Vr++ZFhUIbIShKIKNXmGwhOC+99BIQn/k1bBBQT7jwpjUpKExR/SchaHex++67A0H49Ze+9CX3HPWA22GHHVz7Hs21/fr1a/Q5KiZy8cUXF8Q4dOyxxwKwZs0ad28mjXA43/z587M+54ILLuB73/seEIRnP/XUU5x11llA071xN/eeUTN27Fh37Zov3nnnnc0+v6yszBU/UUEmvV6OgzigljPhHsYKIc9FywcZTvr37x+bVgTFgIU+GoZhGIZhGIZhxIzEetT22msvIHBHiyOOOII5c+YU4pJahEIiwl3gVf66GNoJtBWVzVb59v/85z8Nyi7HnXCTa1mBW4u8HKlUqlHT7Isuuojjjz++7RfYBtSYdeDAgQCu2XVbGDZsWIP/x/ke3ByZXpjVq1fH2qOmtiVqMrrzzju7tgoKM1PCvhrvhrnttttcg1mhUK24zkV33nmn8xLKAyovzI477ujKvsuyu3r1avf7qaeeCgQhPk1ZyKNAXiQI2mH88pe/dEVPiiHctiWoGIY87AcddJArGvHHP/4RoIFnV543eeLCZHrS6urqXAPeH//4x0DhmwrPmzePGTNmFPQa8oVKz2dj++23B4Lm5ZAOIQQ444wzWhVy/Nprr2UtfR835Jk67bTTOPvssxv8TeMwTvexPGphVKZeBfzag0KUR40aVfD7sC1E3ehamEfNMAzDMAzDMAwjZiTWozZ58mQgSOBU6f6ZM2cW7JqaQlZglaEVzzzzTKwa7+WL0aNHA4HlNK7FCjJRSeRcNG+eMmUKkI7zz2yaXQhLztq1a4HA4rfTTjs5j2dLC/GomE/YOwAUnUV577335rjjjmvw2Jo1a4oiZyjcykRei5/97GfNvm7o0KHOy6sxoObQceWJJ55wxTZUmEiesbBXRs2if/jDH/Lvf/8bgO222w4IPC+FLnfep08fd//Lu33hhRe6AgzXX389EOTWDRo0yBUyCOf5qDGv1r64jVnlmsnb2aNHDxcJo8iYzz77DEgXVdF3oTUjW+EDMXXqVJdPWag2IJ07dwaCvUiSUWsMzRthfvSjHwFp/aqBuxqTt/b9N23aVNCiP++8846bXxT9pDzSMMrJHjBgQKN8X+1J49SeRkXMwvrLRaETRQjlYp+0JZLIg1rHjh1dqIhuZh124lhhbquttnKLSeZk/sYbb8QmuT1ffOELX3AJtir0onCVuKPDVVtQgZsRI0YA2Qs0KDStEONWGyiFuX31q191CcVXXnnlZl+nnilDhw51oR+Zi1SxTdhbbbVVo3DUYi1201IuvPBCpzcd7OLe23DlypV8/etfBwJjj4riAPzpT38CAnmqqqq47777gCBM/uCDDwbS4bqFDPG8/PLLG4VLQbDpUU87/WwO6U79kFTcIm6sXr26UcpCNlThMXxQk3FJ39vf//73BkVKCoHGo8K/W1pcqqVkFgRSIZZCoPkic76HoFCa7/vu95aikMqTTz4ZwN2zheKUU06hW7duQOAUCFeszuSwww7jhBNOAIJKpjK0xAmFi2fTX3vQep/r991SsNBHwzAMwzAMwzCMmJFIj9q5557rysSq43uc+5Wcc845jUp/q6T0lhD2eOKJJ7oQuUcffbTAVxMd559/PhC0kAizaNEiAL797W8DQR+lQqAx6HmeK3neVGERWYx938/aXw7Slu5iIhy6qVCVv/71r4W6nLyi/kAnnHCC81Ao9KwYUFijdKaQ1dWrV3PhhRcCNChUpLLZKvEuD8WFF17o7r9CcN5553H33XcDuFCx0tJSl5Cf6eFtDnnw9b1ccMEFruVLMfHTn/4UyO4RVLhqewofFRNjxozh0EMPbfBYXFtnqCT/Xnvt5UJa1VNs6tSpTc4x8qCp1+EVV1yRz0ttlg0bNrhoGvUBDBebUuix9jPXXnutu+/effddIL4FmfKJosOKaT2JA+ZRMwzDMAzDMAzDiBmJ8qjJ2v+LX/yCyspKIF7NBDdHtjyE008/HYhP89V8MnjwYPd7LkrAFgOPPPIIw4cP3+zfVQAhDkU35s2bB6RzLXbeeWcAvvjFL272+eFCMCr7/s1vfrPBc5T/Fne23nprgAaFRFSMQY3Nk8akSZPc7yq0UQylsDORZ00/N4fGorxX8qjtv//+rS6ek0tqa2vdGFNpc4ADDzwQCPKZVWgoW0P2bKhQQLZS3HHnlFNOccVUSkuD7Ys8GIXOXYoK6e7ss8+mR48eALzwwgsATJ8+PfLrUQ5ZU7ln8qLsuuuuTJs2DQi82RMnTnSeQXnx9f8LLrjARUjJA6wCOnFAOZ/6mY3vf//7Lj/rv//9LxD/fN9codw8COaqYlhPpE95TCG4/qiLuzXrUfM8bxvP8572PO8dz/Pe9jzvjPrHe3me97jnefPrf/bM/+Ua+cR0WPyYDoufpOsw6fJtCZgO40tLCzYUqw5V1dUoXh22lKTL11Ja4lGrAc7xff81z/O6ArM8z3scOBF40vf9Sz3POw84D2i+5nMeUHlUNccsKSnhkUceAeJleWkNsuRurtqfJiv9XdbVcIUzWdpOPvlk1qxZw6BBg6iqquLSSy/llFNO4eWXX+a5554Lf0ZBdBiOsX/ooYei/vh2EW5SLcIeCUjH32c2Ak2lUk1WPmxHNcm86lBl2lvapHPBggVZHx81alRRNL3ec889gYb6Vf5oHinYXArB+P3888/zlQtSUPk2xz333AMEHrVjjjnGRTbEKTJDZb2FvNxf/vKXXcW/m2++GUg3FD7zzDMBGrWXaCeR6lCVHa+44gq6dOnS4G/r1q1zuWnV1dVRXVKLUb6xPEXtQU2+1SbjmGOOYcmSJQ0ea0WVy5zpcOnSpQDMnz8fSEfJHHDAAUCQy6v8smXLljnvr9b+uXPnuv2K5hxVeFy/fr3zpMkDVyyo6jEE0VFXX311Lj8ip/ehqq3+5z//cfnlN910EwAnnXRSm96zd+/eznvYhkqXsVwroqbZg5rv+8uAZfW/r/U8by4wEDgc2K/+abcAz1CAL7SkpMQVDNl2222BdJLmL37xi6gvJae8+eabTf79n//8JxB0t+/Xrx+Qnrib4+mnn2bfffflX//6Fz/4wQ/cARc4ggh1uPfeewPp8vzFynXXXQfAH/7wB/eYwsXCB7Fsh7LNHdTaWbY3Uh02hw6ymX11iuGQBoERCIIiKddcc02+P7YgOtRmV3PJihUr8hWiEqsxKnQ/6l4+/PDDXSGdu+66CwgKAcSJxx57DIBLLrnEhQOeeuqpQDpEORy6E6ad/dQi1aEMV+qjBWlDAqQP1gr7iyPqXagDVbdu3dwmuCWl+nfaaSfXgkF9VsOFK771rW8B8PLLL7f20nKuQx2uHn74YVe2XqGYaumiPQvA7rvvDqSLiuh3rRVq1XP++ecXTbueTML7UBmhczyn5lSHMsCee+65ruCXikv9+c9/Blp+/TfccAOQXk+0Xw0XcWohBVkr9ttvv83Om4WgVcVEPM8bAuwCvAz0qz/EAXwM9MvplRl5YdGiRbzxxhvsvvvuLF++vMHCh+kwCZgOi5+k6zDp8m0JmA6LH9Nh8ZN0HSZdvhbR4mIinud1Ae4FzvR9vzJsIfd93/c8L2tgtOd53wW+294L3RzDhg1rlBR99tlnR1b6NBfyPfLIIxx++OGteo2sHNlQ+EvYYzNt2jSqqqr46U9/yrhx4/jtb39LVVVVZhhNpDo88sgjgbRX9PXXXwfgueeey/XHNEt75FPy+rnnnuvKX7cUhQPMnTsXgO9+N30JYYtjGyjIfbg5mmqAmkvyJZ+aH0PQIiGCHIlGX1YU+pNHTbpSc3MIPBk9e6ZTBtrZLiJWYzQTWZUvvPBCLrvsMgB++9vfAnD88ce3uRBOvuTT/HHPPfe45spi//33d78rJE56bUlT6SaIRIcadyrJH+aOO+4Ami7ikGtyId8OO+zgooBaMtePGzeugWcfAk/ctGnTXHGKNpBzHcpLO3HiROdJ3GOPPYAgCqj+M9IXkGVdUNiumtLnupR7FPPMyJEjgaC5NeStyEte1ooXXnjBtQRRyPT48eOB5j1qmnO0v1uxYkV7QscLslbErS1Wiw5qnueVkT6k3eH7vsoqLfc8r7/v+8s8z+sPrMj2Wt/3pwJT698ncW3Ji0W+mpoarrjiCnbYYQdXQaxTp05s3Lgx/DTTYfHLZzosfvka6TDp8kFxyNgeg0MxyNcKilaH7SFh8pkOi18+WyuKX8Zmafag5qVNHzcCc33fvzL0p2nAt4FL638+mJcr3Awq6a74fEh7NSDIESoWjjrqKGcxVFGQMLLOZMs/U6KnEpYB7r33XiAoq55J2AKZUQ4/Eh126tQJwMWwQ1DSvRWJ0LHggw8+ANLNV4844ggAzjjjjBa99pJLLgHSzTBzSKT3YXN06NChwf+LpSy/7sNhw4a5xxRfv7kCPzkkFjqsra11bRXOOussICiD3s5G0LGQrzluvfVW16T3qKOOwvd9OnTo0JY8i7yie+rMM890hTaUw9S3b1+3Ntx2221AzkpL51WHkkNtSsLrovK3VSSlWDj//POBdLl55Zq1FEXHqE2E8r0uvfTS9lxS3nS4bNkyxo0bBwT7FrV0OfXUU/nb3/4GNDR83HjjjcDm9y3FhPQrj7Dv+/maN/KiwwULFrj8OjUnl5epT58+jZqqb7/99q5AzFVXXQXQoDiM7uM2EOlaoby0bPlp+++/f6Te+zBecxZCz/P2Bp4H3gIUS/dz0nlq9wCDgA+Ar/u+32SzmVyefLXJVWd7CCpD5au/ke/7XlN/T8DJfqsodKhF99lnnwXSrnG511UZKl9EocOJEycCQSjjlClTXN+YqVOn6nPc5NXOMLJMItFhS/n444+BoOeRqna1pyBHFDpUdTVtKE488URuvfVWoN2HlJbQpA7zpT+F/O244476HLeR0iZK+lu8eHF7PipWY7QpBg0aBKQNYTNmzGDfffdt0esKvVYcf/zxQDps7le/+hWQnmdzSF51qMqbDz6Y3qeF9ynqI6fQunyRLx0OGDDAhT6OGjWq2effcMMNLjWgnYWmMima+7CtFOo+lGFLodNvv/02o0ePzsdH5X2tUG88jb3x48ezcOHCBo/9+te/bhSeK4fJOeec055UpEjHqA5oTz/9tJs3890zrbkxCi2r+jgD2NwbHdjaizLiS3M3hBF/TIfFT9J1WKzy7b333txxxx2NmrdviRSrDo0A02Hxk3QdJl2+ltKsRy2nH5aDk69KuqtPWrininnU2kdLTvZJlzHp8kG0MqqEwSffAAAgAElEQVQksUJ1cmEFj1KH6n938cUXM2vWLCDnoaqNKNQY1dyqxO/nnnvOtZ9QiHRGTmubiNsYbQkKsd9jjz1cGfGmwnlsnmmfjLNnzwYC76647LLLXJGJfGM6TL6M+ZJPHlCN3/POO4/LL788558TpXzq0Tt8+HAXFqmem+F+m0q9UdERFbdrCzZG07SqPL9hGIZhGIZhGIaRf1pcnj8u7LPPPkBDT5riX9X53TCMeKBGtcXK0qVLATjppJMKfCX5Z8aMGQAccMABBb6S+HH00UcDaU+PiiK0I0HeaIZevXoBQRl35dddffXVBbsmw2gpmhsyPcLFjNrSvPLKK0W/rhcb5lEzDMMwDMMwDMOIGUXnUctk9uzZrgqUStcahmEYRq6orKwEYNttty3wlWwZKKdVP1VttCUNog2j0Kiqp9q7tKMpuWEUXzGRQmLJxcmXMenyQfJlNPnijY3R5MsHyZcx6fJB8mU0+eKNjdE0FvpoGIZhGIZhGIYRM6IOffwU+Lz+Z9zpTcPrHNyC1yRdPki+jEmXD2Ad8L/cX07OyZQPTIeQfPkg+TImXT4oHhltntk8tlbEB5tnspN4GSMNfQTwPO9V3/fHRvqhbaCt15l0+dr72igxHeb2dVFjYzT3r4sa02HuXxc1Sddh0uUDG6P5em2UmA7z89ooaet1WuijYRiGYRiGYRhGzLCDmmEYhmEYhmEYRswoxEFtagE+sy209TqTLl97XxslpsPcvi5qbIzm/nVRYzrM/euiJuk6TLp8YGM0X6+NEtNhfl4bJW26zshz1AzDMAzDMAzDMIymsdBHwzAMwzAMwzCMmGEHNcMwDMMwDMMwjJgR2UHN87yJnuf9z/O89zzPOy+qz20Oz/O28Tzvac/z3vE8723P886of/wiz/OWeJ73Rv2/yS14r9jJmHT5IHcyJl2++tckWsaky1f/mkTLmHT56l8TOxmTLh/YGDUdNnifRMtX/5pEy5h0+Ry+7+f9H1ACvA8MBcqB2cCIKD67BdfWH9i1/veuwLvACOAi4CfFLmPS5cuVjEmXb0uQMenybQkyJl2+OMuYdPlyJWPS5dsSZEy6fFuCjEmXL/yvXR61VpxmdwPe831/ge/7G4G7gMPb89m5wvf9Zb7vv1b/+1pgLjBQfy92GZMuHzQr4/CEy2c6DChW+UyHAcUqX9HrMOnygY1R06GjWOUzHQYUq3ytps0HNc/zSoBrgUmkT4rf8DxvxGaePhBYHPr/R7TjovOF53lDgF2Al+sfOh14EHgT2JMilzHp8kEjGT3gRMAHZgDfSph8YDoMU4zygekwTDHKBwnTYdLlAxujmA6LTT4wHYYpRvkATvc8703P827yPK9nS96jPR612J5m24LneV2Ae4Ezfd+vBK4Dvgk8Q9pteSlFLGPS5YOsMr4MPEXakLAEqCJZ8pkOiwzToekw7iRdPrAxiukw9pgOE6vDYcDOwDLgiha9UTtiMI8G/hb6//HAnzfz3D2A6aRPycX8rzkZC319+ZYvCTpMunymw8JfX751WOjra++/xabDgl9fvuVLgg6TLp/psPDXl28dFvr62vsv8WtFS85bpeQZz/O+C3wX2DHfn1UIQvIlli1Ih4mUD5Iv4xYkXxL4KNuDpsPiZwvSYSLlg/zJ6Hme+1lXV9fgsfqNdyQkXYcJm2e2yLUik/aEPi4Btgn9f+v6xxrg+/5U3/fHAke247PiQiMZJV+9jMXOlqDDpMtnOix+kj7PPI3psNixeab4yasOy8vLKS8vZ9y4cYwbN44JEyYwYcIETj75ZPbcc0/23HNP+vbtS9++fUml8tYpaovTYcLmmS1hrWiW9twd/wW28zxvW8/zyoFjgWmbe7Lv+4+047PiQpMyJoAtQYdJl890WPwkfZ45BNNhsWPzTPFjOix+kj7PbAlrRbO0OfTR9/0az/NOJx0jWgLc5Pv+2zm7snhyTy5lLC1Nf/2pVIq99toLgEmTJgEwfPhwADZu3MgzzzwDwKxZswCYP38+69atA6C6ujpXlwM5li+OFEI+hXcAlJSUAFBWVkZtbS0AmzZt0rXl4uNMh8VP0nWYdPkgRjJmhpfJe6HwszYSG/nyRSHkS6VSDUIEhXQlHcZ9rUilUvTq1QuAbbfdFoADDjgAgLVr1/Lpp58C8O677wL5C31M+hgl+fdh0uVrEe3KUas/zW4RJ1oA3/cvKfQ15JOky7clYDosfpKuw6TLB8mXMenybQmYDoufpOsw6fK1lLwXEzEa4nke3bp1A2CrrbYC4MADD+Swww4DYPvttwegoqICSHtgZImqqqoC4K233uL+++8H4OabbwbSnrckICtiaWkpAwYMAGD8+PEALF6cbpkxa9YsKisrC3OBLUBeM/3s0aMHe+yxBwBf+tKXABg6dCirVq0C4NVXXwXg5ZfTbTY+/vhj52WLG7LGh629kNaXxmA266hep5/yJkNgLS62Mex5nruHp0yZAsCSJelw+hdffNF5vYuBTH2KTF02lUvSTg9Nu9jc9YeJsmBBnNB3o/lIOqypqcm1h8ZoJZoHO3XqBMBOO+3E3nvvDcCIEUH7qI8+StdUmD9/PgCvvfYaAJ988gmfffYZQJPzb9SUlpYyatQoAE488UQg2NO8/PLLLFu2DID169dv9j00brPNOYpGMfJPpoe3Y8eObg+rcQuwYcMGAFauXAk0HI9xGJOtoUOHDhxyyCEAbLfddgA899xzvPXWWwBubY9KrrxlcBqGYRiGYRiGYRhtwzxqERO2LgwdOhSAvfbai0GDBgHp3CUIrNMVFRXOMlFeXg7A/vvv76xtr7/+OgCvvPJKRBLkF303paWlHH300QB85StfAeCOO+4AAs9T3JDFWnrq06cPkPa2nHTSSUAQr19SUuIsUGPGjAECy82NN97IJ598AsTDOipKS0vp378/EIzdzz//3P190aJFAKxevRpIWz0zrfl6/aBBg1i7di0Aa9asAdJW45qamjxLkTtSqRTf+c53APjJT34CBJbu1157LXYetVQq5cZmpte3tLTUeexlwfZ9v5Fnt0uXLkB6nuratSsQWMVXrlzpcmajGLee5zmPRNj6nunZ0//r6uoa5WmF/x6ne62tZHqsS0tLG8kXll2PyUNRjNbvYsPzPLfO9+3bF0iv6QBf//rX3dzaoUMH9xrdV/KeKcrkpZde4oknngCC+TcOc2ifPn1cJNAOO+wA4PLSVqxYwcKFCwEazC+ZXnHNTdAwAgPS47SQHvym8DyvUQSJ53nuvsqWbxjHe07X3rFjRwD69esHwMEHH8wRRxwBBB611atX8/HHHwPB/uzRRx8F0l7fHOfh5w2NwYEDBzpP8Lhx4wDYb7/9OOOMMwB4//33gejutdgc1MIDO5vLe3OLafj5TS242W6KQg0aDXxNPs8//7zbtGpAK8xv9erVLgTwmGOOAdITmN5Dg+i///0vEP8boTmky379+rHnnnsCOPnnzp0L5LyASs7QwqJD949//GMAjjjiCJdYrQU6/PuOO6ZbgXzhC18A0iGxF154IdB0aEhUSCedOnVyobkad7q+OXPmsGDBAqDhGNTveo+ePXsC6Y2JFu6nn3463yLkhV69evGNb3wDSIe3AnTu3BmIx2Ypc9NeVlbmwo+0wPbu3RtIGxc0f2ou+vzzz918pAOaDtpf+tKX3OZexqLKyspI7s3w4T8znK+8vJwhQ4YAwcZCxoSqqio35hR2XF1dHdv5pLWkUimnX60PXbp0aTQOunfv7v4vI4kO6WvWrHHPV3i59FzIjbGuqUuXLm5+1bzZvXt3J5PuvxUrVgDpg43WDR0OKisrC7JOhsP6FTKtImKaR/r168fSpUuBhuFVkl9y7rTTTkB6bZQs2hh/9NFHBdOVxli/fv2cAVL36PLlywF49tln3f2nedL3/UYhdnqvDh06NAq5j0PYbuZ9pXuue/fuzui62267AWk9aa2UEfbtt992PzUvyXhb6H1cKpVy99LgwYMBOOGEEwA4+uijXeijDH8QzCGSWXuEG264wa0RcU9vCBdeGjhwIBAYTLbZZptGjpSosNBHwzAMwzAMwzCMmBEbj1q48MLIkSOB4CRfXl7uTrU6tetnSUmJCzHTaTgcciWLsCyHc+bMcdYLJT2uW7cuUuuMPvell14C0if2//znP0BgcZB8vu+78Igvf/nLQPp7kSUqjiFyuaBHjx6u8MY777wDwP/+9z8gnonEqVTKecROPvlkILCSdu3a1VnfwiX59bus4NLzQQcdxLRp6dYhM2bMAOJRqKFXr17OyynP2htvvAHAhx9+6CzA2TzbklXeja5duzprXI7KhUfODjvs4MaoZJV3MA5hj5mhRJ7nOavv1ltvDQQesrKyMmfl1py5YsUK522S1VietUGDBjmZVeAgKi+iPre8vNx5BjWGhg4d6ubJL37xi0DgxZ05cybvvfceEEQsrFq1ys3HcS3g0xyy8nbu3NnNQfKU9uvXz62d8uqPHZvug1teXu6K3yiUZ9GiRc4b9eGHHwJByFq+0XgtLy93Mmmc6ueYMWPYfffdgaCFTZcuXdyY0Popr9SKFStcWKHG6QsvvMC8efOAwLMaxfqpMdqpUyfnSfvBD34ApK31APPmzeP2228H4IMPPgDSMun+UxjhwQcfDKRD6fV9SG9r1qxx+52okd7Gjh3rrlXr24svvgik9ZAtFC4z+kL/D0ehhOejQu4DwmHXCgHXfnXChAkuXUPrQ3l5uZNZ4avPP/+8e7/Zs2cDwfgtVESGvvuKigo3byq8/+tf/zqQ9hjq+jTOqqurnT40J2uvUFdXx8UXXwwERXHivtZr/w3B+OvZs6fzJJpHzTAMwzAMwzAMYwun4B41xcHq9D58+HAX46qCGcOHD3cnWVkclBNSUlLiLFXh+NFwDgIEnqenn37aeSlkAdi4cWOkeQqyrIRzQcIJ/BCc2EtLS50lVF6Mjh07uvwBWauSgr6HnXfe2Y2JOXPmAA0LV8SNVCrlYvJV1lXeh5KSEmehkSW3urq6Ud5PuOStLK1vvvkmEBToKATydg8YMMDJqHGq+6qqqso9L5tnQs8Pe84zcxGKBclw+umnO2+F5o8bbrgBiEeOWqaFuqyszOW4yLMmD4zv+846qrmzqqqq0Vwlunfv7uZP/a22tjZSz351dbW7ZyTH+PHjXaK7vEryrgwZMsRZrrUGvPXWWw2uH+Jv7RVa75SzddRRR7m5RPkVHTt2dAVuNG41VsvLy93z5WFcv36988xojYmsBHUo50feUJWqlwdq9OjRLoJGelq+fHmDAkZh+vfv7/KF5OHp1q2by6eNgrCnENJesGOPPRYIPC4qBHL77bfzzDPPAMF6V1JS4nSt+1H7nx49ejB69Ggg0OHChQtd/lPUXmJd15FHHulyRPVdP/nkk0B6jGUbU5mFp7QulJSUuLVS6+imTZvc91OI6IWSkhLnSVOu5KmnngrAPvvs465dOpkzZ45bK1QsZtiwYUB6DKhdgbxthVo/NM46dOjgisFoPyPdbtq0ye1jFBW2atUqt6Zoj6DaAmPGjOFrX/saAFOnTgUarydxIRytobVS30nXrl2d7uQdjoqC75AyJ7GOHTs616kWoP79+7uBH04+hfQGNjMccs2aNe4m1mKtQXPggQe6v2lyLBThSluZcoUnrcmTJwNBeITv+y4MMCnVHiWvJrOTTjrJ/a6NRhzDO7W52GqrrfjWt74FNKzsCOmJ7dlnnwVwIa7Lli1j1113BRqGSEL6XtDGRBuW1157rWAbSBlT9thjD7chVoK+NnPr169vsrJTODQN0kYHjX9tHLNV7IsTGqPaKE6YMME9psONwugKjed5bvzpPurWrZubZ7VB1Ob1k08+cWGs0ummTZsaGY60sR85cqQ7pEfdU0bU1dW55HvdJ3vttZdbYFWFTDpavHix6+2kcRze2Gs9iEMBn6aQDqU7Vcft37+/Cy1S8n737t0bGXsUYt2hQwc352hjv3r1aqfXKBL/w+G52hANHz7chVlps6hrTqVSTkaFji1cuNCNQa2R+jlmzBj3uwwRw4YNc3NaFGjuk+FxwoQJrmqjuPPOOwF4+OGHGxkkS0tLnUFWOlQI7+DBg50Od955ZyA9znU4iuqgJhllZN91113dnu3xxx8HgjUj2yHE8zw3rrVnUwrMwIEDnf5lRFi2bJn7DiRrFGtHuKiN1vnTTz8dCIwJK1ascIVdpk+f7h6TnnS/KkR75MiR7uCjfV24SmSUSGd9+vRxvX1l9NH3+8orr3D99dcDwTisqqpyB3Nd93777Qek9/T77LMPAA899BCQnmPjmMISNh5LX+G9uOaeqLHQR8MwDMMwDMMwjJhRMI9auMwqBN6wRYsWOcuTLHobN2504StKcJZ1LBwSFva6ySsnq+Npp50GpL10e+yxBxBYscKW40KQrY+Gvp9tt92W448/HgiSND///HMXwqOSw8WO5JXFfscdd3QWcYUpxdGjJivgAQcc4CyameGos2fPdsm0St6vra11CfzyeBx33HFAOsRAXht53WbPnh25t0k6kYVz++23d94GJbqHvRBNWch0byrxfejQoY08MpnFL+KGdH344YcDaQ+VxuStt94KBOWV44CuVz87dOjgijLIsxkuCKI5VjKEx1umNXj48OFuDGgODpfYjupeVbiN5g3P85x3SBZ8hRNVVla6cHqN1b333tt5MHRvxplUKuXuR8mikKQXXnjBtWmRDtetW+c8pNKn7tkuXbq4NUX34MaNG9140fqbT136vu8+T16TI4880oVbyfMpS/b999/foICRrl1eeY2DAw88EEjPy+FeT5AOd41CNiH5FII5ZcoU95jCHO+++26gYWGzcNuhcIGf8HWHUyNUXGXEiBFuTZHu840+T+tVjx493Hxy3333AdlDFMO9unbZZRcAF8op73fv3r1doQ7t+xYuXMg///lPgEahuvlEUQpbbbWVWwd03Qpz/Otf/+qKSoWLvSl8U4/JA1VXV+fet1AtBzLTEEaMGOFaQMjTrfn+lltuYdasWUAwR5SWlrr2C4899hgQzDdf+cpXXHScSvYvXrw4VmtlJuH+k+GzivakUWMeNcMwDMMwDMMwjJhRMI9aZjndcPEPnbRfeOEFIJ33IYtZZnxz2OobtsjLQiFrmrx0nTt3du+VWU68kGTm6slK+sc//tFZv/Wc1atXO0tcUpq1SjbFbXfq1IknnngCiGfiqSyBit+ePHmyy4OULBrHl19+Oe+++y4QjN+SkhKXQCxdKo67Z8+ezrIlK2z37t0jK5MtdA2ygvXo0cNZNOV9kDWzqqoqq1dY35M8MioO0Lt3b9d2IY6x6pmkUinnvTnqqKOAtHzyMN51111AfLy+4Rw15Wz17NnTjSfNh9Lf/PnznaVX1vtw3oj0pmiEmpoal0+h7yBbZEC+0fynOXzBggVubGp8af545513XO5WuPG65lp5wuPYLkLX1KFDBxclontKCf0zZsxwc0Q2j1E2a31mfndFRUWkTWlTqZTzNBx55JFAuj2J5lJFjKhdyT333OPyQMPtePQeikSYOHEikPaw6bvT3PXSSy+5cZ9vwvIpZ2ebbbZx3p/rrrsOCLy+dXV1jSIL6urq3FjUT+l5/vz5Lk9IRS0GDBjQyHuTz/sylUq5saiy9L7vu9xyebbD87x0Im/gmDFjOPTQQ4Eg106enNLSUud9Ul5YVVWViw7QfS7PeD7WE+lEa8D222/v8vHkOXzkkUcAePDBB91YC3s+pRPlssmD/Mknn2y2GE5UhMvyQzpvTk3ZNTdInwsWLHDzrnRUXV3dKB9S89KoUaOcd07j47nnnit4jYhshL1n4RZZolBFXsyjZhiGYRiGYRiGETMKXvVRFqKwBVAWWv1t06ZNjaybzeWz6O+KbZYledOmTTz11FNA4PEotBU8lUo5y7Ws4GeeeSaQLjedWbp/zpw5PPfcc+61EC/rb1uQ/LKE1tbW8sADD7jf44b0JOv2mDFjnAxhPUE6vyxc4RPS1r/MpuWy/Id1KauWcgCiRNZO5cIMHTrUeQblDQznHWgsirBHRnLofqytrXUW0Mxm4HHE8zz3fUgGwH0f8i7FBc/znLVTHqORI0e6ZseKNND1r1u3roHXBtLWUnkolOMrT9Sbb77p5iCN40LkUMrqqbyldevWuTEZ9hZCwzxKyTh//nz3PUm/8m7EAelElu6+ffs6Hci6r3zldevWuXUvvD5mVu7U/2tra51lXM/ftGlTg2av+aa0tNRVZVSFx0GDBjmvmTxqquwXLtWucdetWzf3HqeccgoQNNstLS116/yNN94IpMduVGPV8zznDVLuVadOnVypeulO4zLbvibsqc5W9fqtt94CYNKkSUB6HGdWrMu3R025gZobKysrXVXOzFykcGl7VWs98MAD3fUrkkM53O+8844rZa+8NUUJhH/XfZtPj5que/fdd3fjUJ6m+++/H0jrRONL+4SePXu6vEl51NRaZOXKlS6irFBrYLg1BqTXfP0u76/kXLx4sduva69eXV3dYA6BwOs7Z86cBq22IN0qRXmncWhlI6S3DRs2OC+n8uvC92HUudgFP6gJDdDa2toWfQlN/c3zPLcQf+973wOCnlbLly93PRDicrjxfd8tukqiVnhVePOr7+Xiiy9utPEvZjzPcxOgDmqVlZWuZG3cZCwpKXHjS6Ga/fv3bxTyqFL8q1evdvrSJig8Ieu99DNcmleLfNQTeElJiStaoMWxS5cuLpk2nESsn5m90rp27eo2wQpnCR84tQmLc1JxuDRvZunvuro6V4Y56n5FLUFzhw5b48aNcwulFkdtcsaNG+f0IFm6d+/uQlZUJEA6nj17tivmoHk0PFdFMbeGi5cojGbQoEFuTMpIEA7rUbiU5tuKigo3RrVGNEfmZjqf81PmBrFHjx7uQClZ9P8+ffq4Q6rmi06dOjm9ZraO8H2/kXElqkNatt5+upaVK1e6wgQyYoVl1aZWet56663d/HLQQQc1+Fttba0rfKB7tbKyMrLNYXl5uZvDdQ99/vnn/Pvf/wYah/VnO2BDcL9m7o2qqqoa6Boazt1RFGiqqKhwm1m1PVi8eLE7hGbOD+H+fZqPxowZ06D8OwQFmlavXs2QIUOAoFdZ165d3biXwS+fc07mfTh69Ghn5ND1htuBZLYaOPTQQ/nxj38MBPerDjkzZ850h5ZC7XUyW8+MGDHCjSsZSVQgpbKyslEqUraxqpDQ6upqN85VHGjQoEFu3o3TQS1MOLRaRGkACWOhj4ZhGIZhGIZhGDEjNh61MO09pXqe50J8ZBHWqXj69OnupF9oT03Y3Sx3uDxKmYUpAO69914g3Zw0yoTvXNBUA0fP81yJWyXYTp8+PVYhSGE8z3PhR2oc3LlzZyefQgVkya2trXVeimzFXxQCocIkvu8766Asc1F7nSoqKpyMsgp26tTJeR2kJ43TdevWOW+Z/ta9e3f3XSisRe/16aefusIO4QIr2axYUREeo5kWs7KyMteYXJbrDRs2tDo8N0pLnK5TFtxw24dweXpIl5iWl0neiHXr1jlPhsaowgn//e9/OytptiIVUXidfN93Fvxw+KJCzFSmP+wlUsEFFQKoqalxIS56L4VvLVu2rFFofnMeilzLmRkWXFVV5YonKPxY82SvXr1cmKu8vv369XOvleVfHqlVq1a5e08/o7rvJFcqlXJhnQrDXbt2baNiKPKYde7c2elLc5Hv+64Qk3QovS1dujRra5R8Ey53rjGna5s/f77zNmWGnobnoGxjKTOMta6uzt3n+k49z2tUWjwfhMNytQ5onfroo48aeboUadGtWze3RigkrqamxjVm/8c//gEEqQMQrI36zLKyMrem6mc+9apr15jr3LmzWx/lEdY11tbWupDOKVOmAOmIBRWF0/ehVhqPPvpog4JMhUSezr59+7rvU0VBFLVQXV2d1ZMmJJ/mpxdeeIHJkycDQQj3gAED3HoTx4ia0tJSt/cS1dXVbm2MGvOoGYZhGIZhGIZhxIxYetSyIYtGU1aTcC7CSSedBAQ5Pkps/Mtf/hJpsnRTyDrUqVMnZ5lQzpMsxLW1tfzhD38A4Pe//z2QjsnOtGQUc1GR0tJSzjrrLCDQ7zPPPBNbr2E4Bl0eilQq5b575VfIQrxx48asY06WOJWlVvyz7/vu+fIKRPVdyGJUUVHhvF+y+paXl7viKbKoyXLYtWtXZzGWZfHDDz90eTH77rsvENzHK1ascGWlZRGtq6sruEUxsxiDfnbr1s3lv4gVK1Y4Hbf0uvMtX/i6ZaWVDhYsWOB0pHEra3zHjh2d7jX21q5d64rASEcqkz5z5sys5YujTrZWTlY4z0leblmw5V0aNWqUWw8k1+rVq917aGy//PLLQLqEtPJOZB2GwCqc6b357LPPcmrVD3tD9Blr1qxx37ss3JobunTp0igHb+jQoe4xebxVFGD9+vVOP/oOor7/SktLXR6aCsLMmTPH5bIIzRUdOnRwc6/G7pgxYxrMnRCs9xdccAGvvvoqEMxjUbaRKCsra1TmfNmyZc6rmVngJdtjTXn6w4U8NB4/+ugj9/5RyFlaWuruCX3vpaWlbg+jvUm4ZZLu13BjbnnUNK7laenXr58rDqO1ZcmSJS7HOdOznw8ycznXrVvn5pcTTzwRgAkTJgBpnUsnGqOlpaVOft1r//rXv4B0I+lCF9LSuFKuYVlZmZtX5PkLF99ryXctmebOneui2PS9dO3aNdZ71dLSUrdGal2srq52Xvmor70oDmqe57XogKabady4ca4nhzYYd955J5Cuzlbo5MXMLvDdunVz1aq0qdBzXnzxRVetKpwontnXQ274NWvWNKjEtznCydZRTObhxP/MRadHjx6uP5P6j0ybNq3gm/bN4ft+o1CI8GKqIigK/chWta/03oIAABg9SURBVLSiosJVIlXYpxYt3/ddcvHjjz8ORHdQC3/nGkeqmjdo0CC30ZVsWqxqampcOIve4/3333ebKn1PWrznzZvnNr/hMIpC6zxbyCOkQ1yV0K6/zZw5030PcSF8/bo2hRCtXLnS9foR2kjU1dW58EYd8MaOHetCI7WZvu2229z/m5qTo9Bj+J4Lh/AtXrwYCAwf22+/PZCuNKawRt1rNTU17t5U+LnG+G677cazzz4LBFU933//fReupU2HDgAvvviiO1DkSj6h77qurs7pVT81F9XV1bkCL5ovdthhB7cuHnfccUBgQKqrq2vU7ykqwhXitPnRfBOuzplZoAhoVCVyp512cn+XPNdeey0ADz30kAu/jlLGcBEijSfNJdDY4JwZ0hh+j2zXrfW7R48erreYnvfqq6+6A1MUMpeUlLjvWEahvn37usN2uEiW0HVp/qmqqnL3jg60ev0xxxzjjJn63h555BEXOp/ZSzHXhPefCrt96aWX3PVpnlFhlPCaoB5ygwcPdvLL+KP+qdkM71Gja9M63bFjRzfnay+i77elh0rJVF1d7caFDnvr1q2L5UFN38MOO+zg9CqWL1/uxnnUWOijYRiGYRiGYRhGzCgKj1pz1gadgmXRP+uss9xpWNY6JaiuXbu24NYLocTv4447jv322w8IikfImnHmmWc2smhAILOsi7Ja9ezZ0/UfCYcmZXods4VV5JvMz9E1HXLIIU5uJbwvXrw4NnrKJFwcRDqEQD79TWMwXBQlnFB98MEHAzhPjf5WU1PjSuKqR06UfX+ECkfIchm2pGlMykpcUlLi5JZHY9GiRS4MLbNYQdiils2aXAiyefQk31lnneX0I6vg1KlTCx6ykkm4EIHuf+lx7dq1DdopQDDf1NbWuu9fXvopU6a4cCr1TJNnadOmTU0WPYgCz/MaJbV/+OGHjdp6KPSvd+/eLvJA3tz//ve/ziKvwlOylA8ZMsR5JhTa1bNnT1esRJ5KPT+z1Hp7KS0tdfO6LN3l5eWNQr2kw4qKCncPKvTt448/dtcrz4u+g40bNxZMdxpr4ZBweXI3bdq02aiXsrIyF4kgD2H37t2dF0P9yW6//Xag4XofpazhzwwXS4K0vsLeNcjuPct2vZnROLvuuitjxowBgu/vtddei8RTGg6zljdUclVUVLiCPdqP6F4qKytzetc616FDBzd2pV95TI877jgXKin9Pvroo26PlO/WKOHiXppLp02b5uRSyLSuceXKle7awiG4+r40b2idrKmpic1eR3NJXV2d88rrp8Zva6+1Q4cOzqusteXjjz+OpHVEawn3X87cK8+fP79gejKPmmEYhmEYhmEYRsxo1qPmed42wK1AP8AHpvq+f43neb2Au4EhwCLg677vr8rfpW72+pwVQCV6d9ttN2fh+ec//wngShoX2moPwaldScZHH320s9jK+qYY+0WLFjWyGKVSKWdhVY6QqKysdJ5FWXNkoZRlqKyszBVuyPDS9YxSh5L5qKOOcl4K5cDEsWRrGFkEsyXYqhy6cmNkhYPAmr3ffvtxzDHHAIGVSWzcuJEnnniiwWtbOm7bq0N9zvr1653XQcUH5s2b5yzXspyGm6zKiqjXVVdXu7Gr7yZc4EJW37h5paBh/iTgcighSHZ//fXX8zKftEeH2Szy4flDlvhMD3tNTY37XdbPiRMnOn2rDYFyDdqjs1zOM5mloCsrK51HTd5AFWjyPM9Z5KXDuXPnurlU+WvS9cKFC12T5PXr17Np0yaWLVvGQw89hOd5dOrUiR49erBkyRKqq6tzPhYqKiqct05zSlVVlbNwZ5ZgT6VSToeSqWfPnq48vP4mT397PBG5mmfCOXcim9Va1vz+/ftz5plnAsH86vu+ywX61a9+BeBy9Qo9t1RXV7s5XN9///793dgMR1E0RaYnTd7fH/3oR67AhorgvP3226xbt64lkUg5uQ83bdrk9hryEm2zzTbOo6breOGFF4D095Apd0VFhfMM7r777kCQM+r7vou0+c1vfkN1dTVvvvlmpHs5jSON1SVLlrj1S9+79jO+77t9gSJK+vXr5zzhKsiUi5L8udKhvkt54sMRFloDW/t9a7zvvvvuLn9P+/IFCxZQWVkZ2RhtKRqXinKCQPfhKK+WFDfMJS3xqNUA5/i+PwIYB/zQ87wRwHnAk77vbwc8Wf9/I6aUl5fTqVMnOnbs6MKWstx4psPix3RY/CRdh0Ur34ABAxgzZgw77bQTa9asYePGjWzatKlBsYsthKLVYdLxPI+SkpJG4ZVZKEodep5HWVkZqVQqluFzEVO0OkzyGM01zXrUfN9fBiyr/32t53lzgYHA4cB+9U+7BXgG+FlerrIJUqmUsyidd15ap6Wlpa5J3/XXXw8UrvxwNnQNOo1v2LDBWWBkLd11110BOOigg1yVIA3q7t27Oy/MIYccAgSWrE6dOrkqfTNnzgTSpbhVrWjt2rV8+OGHbNq0iY0bN1JRURFuwnwEEepQVt8hQ4Y4GdUkOg6ez83h+777PlVBbfvtt3f6kWdX/+/atauzVI0fPx6AQw89lGHDhgENq7ZB2tov66Msjq3IJWyXDsOVnXTPyEMBjashahzW1NQ4a6Mshdkar+o9V65cmffcgvYgOeVd6tatm5NFY7Ql1VXbSE7vw3BlxKbyAuUdPfDAA4F0zq9yJB977DEgZxbEnMhXV1fn7h2NpbVr17pxKO+DPBrTp09386yqd4U3erIcy1teUlLivE+qSJdKpdz71dTUsHTp0rxVES4pKXGVOJVfVlJS4ryBmoNkuR44cKBrqaFojZNOOsnl6KltiMptt7NNTc7GaGYV4DDSodb4Sy65xDXP1d8WLFjAr3/9ayA+kTPh+0vfu9b2zp07O2+RPFFaH1KpVKPvI5VKOVnHjh0LwBVXXAGk86M0HuX1Xrx4cUvHZLt0GM43lqddrRAGDRrEqFGjgKBZuUrs6/uAwLvbp08fN9fqp+R66aWX+N3vfgfgSvhnq6QcBWGZM6tNhnWuuVRrfK9evdw6qu8obnMpBPfPokWL3BwiPWreaG4vEq6cCHDaaae5+1f7moULF7ZU/kj3pEJRaRDswXSPQgHamLTmyZ7nDQF2AV4G+tUf4gA+Jh0aGTllZWVceeWVQFAede3atdx8880ABet70BRSsq7t4YcfdhsIJdJqQh41apQL7QyXC9UmUZsShfcMGDDAhaBpQqysrHSJry+++CIffvghffr0cZ8fIlId6ubt0KGDmyB0YIs7mqSvuuoqIO3eHzBgABBseBVCNWrUKLfQ6nBaUVHRKFlVsl999dVuUs8sU90CcqLDmpoaN0G1NQy1pKTEfScKC1FhlbfeeisWRpPNocVGB+tUKuV0rvDcPM4pOb0Pw4ahphZHFa6YMmWKe9306dOBYEOZI5lzJl+4nxSkZdS41cZYhoPmCPebAxg5cqTrKyTDn0LG9S9siMj1eK6urnbf+8iRIwEYPXq0C3vTeNTcsmHDBveYNkkdOnRw68BDDz0EBCXD27lRzPlake3701z67W9/G4BJkya5uURr4LXXXusMCXFa5yE9BrW2KRxz22235Wtf+xoQFOeRISBs/JGcffr0cev7z3/+cwA3r65atcrtddTKpRXzdU50WFVVxQcffADA888/D6Tl1typXps6tGy33XZOznCbIBlbdK9p7nnggQec8Tnfpfhbiu/7TbbM0Vz6jW98A0jrUj1RFaYdp7lU16Lx+NZbb7kCd5pvZPB59913sxoCNAcr3PPcc88F0nsjGZXuvfdeIL03beF8WZBzRTgdRdfZtWvXRnu2qGhxMRHP87oA9wJn+r7foJmAn77qrFfued53Pc971fO8V9t1pTGlmOSrqqpi+vTp7LXXXo0aONZjOix+TIfFTyMdJl0+KB4ZVcmvtaFXxSJfCylqHbaVhMlnOix+bK3YAmiRR83zvDLSh7Q7fN+/r/7h5Z7n9fd9f5nnef2BFdle6/v+VGBq/fvk7Biqg8aBBx7IvvvuCwQuyttuu801E2xneEeztEc+ecNuueUWZ91UmIDcztkOVCoIArhQF1FdXe28FnLVrlixglWrVnHLLbcwbNgwBg0a5L6fUNgjRKRDyaSm5J7n5crK2ybaIp+uU5bQV155xYXkZJbU1s9MZMWSxfvqq68G0h7WdozbSO/DpigpKXHNgWUdVshZSUnJ5owFbSJfY1SNViGwdqs1QR6tao10mG/9eZ7nLN/y6q9du9Y1fM5xmGrOxmguLeu6pxVpsNtuuzkviO7p1atXZy333pKx0Fr5qqurnZddeli/fn2jxuSa5+vq6lwSvCzjvu87T6EKVOWoSXte5xnNF5MmTQLg1FNPBdIhyJobNV/+9a9/jWzdaK18NTU1rhy7CpudccYZzkOmEEaFUy9atMh5ERV+tcsuuzgPqSz9WjPuvvtupk6dCgTRCq2Yl3Kiw3AqgIp+bNy40Y1LjUnNK+Xl5Y2K3qxZs4aFCxemL6o+5FGFfJYuXer2KbnQcxRzqUKVtZ/bsGGDS0VpyhPXBnKyVmjMaH1+4IEHnBdfsnz1q18F0mNOUVthb5OKwfzgBz8Agiivmpoa/va3vwHwr3/9C2i072yVfPWfmxcdSp5wqoPOFbncr7SWZj/ZS5sMbwTm+r5/ZehP04Bv1//+beDB3F+ekQt83+fee++lT58+zo0NWQee6bD4MR0WP0nXYVHKF67sakUMilOHRgNMh8VPUeqwFcaEopQv17TEo7YXcDzwlud5b9Q/9nPgUuAez/NOBj4Avp6fS2yIFkiV5r300ktdqds33khf3i233NIW61LkhJso3n333UBgSVQM+9ChQ12Omg5WVVVVTj4VEZGFbc6cOS6eW5bUuXPnsmjRIlKplLPqbOZ7uTSX8m0OWaeVpLphwwaXIC9rU5z1FkaWpV//+tdOd1/5yleAQJdhJFdVVZVLKpbuZXH97LPP2uMpiESHLSFsCJA8suZ369Yt1ptd6U55lLW1tc6TFm5gnici12EqlXI5JZL9k08+cfNLjnNCYjNGw+jelEW/srLSeVE7d+7cwAocxfxUV1fnPlNevpUrV7r5M9xsHtJWbY1N/e355593OXfhNiE5IG86LCsrc9EJF198MZAuaQ/p713z5mWXXQbkP2qmPYS9Tffdlw5GGjhwoPPUK+JAa2FdXZ3zmkmv1dXVbn2fPXs2gPOiPfLII+1pbp1zHWqeeO2111xbF60D2seEH5NePc9zY1wejLD3rNA5aa2hpKTEFXnTvbpixQpef/11IOfjNac61P7rlVdecXmfEydOBALv9o477uhaDWjvPWDAALcnV5SX8mv/9Kc/ccMNNwC4sdoKIl0rlDMZrgeh72ThwoVubMaumIjv+zOAze2oDszt5TSPXOXf+c53gLQ7XYPm/vvvB9LhA4Xun9Iaamtr3SR1+eWXA8FE3LdvXze4NYF369bNhSJpQdbNv2DBgkZhSjU1NS7ZvakB5vv+ylzJ1BRagHSQ/PTTT10yciHdy21B3+fbb7/NaaedBqT74gEcdthhQLqKlRYpJXrPmDHD9VNRJSSN4/ZMAlHpsKVo7GrDr7G5bNmyWC++ujaFvvXs2dOFJ+W7x18hdNitWzd3UNNitXDhwrzoKG5jNBPNpYsWLXIVXLVYr1q1Km9VHrMRrtgJaUOHrk96UqhcdXW1e0zXu27dOvd7LtfEfOhQhpvBgwdzyimnAIGhRHz66af87GfpInCaL+OOvnetcZdeeqkzKk+YMAEIqjz36dPHHc610X3nnXdcYQ2FwGpe2rBhQ5vXi6juQ80h2Q4oKnIWLsYTZwNeS+jcubMrBifdz5w5My+F7XKtw3Bhs3vuuQcIZNBBbeLEie6AJl35vu+MWuoBqz3sU0895YwVrR2rUa8VknX+/PmukJTW+w8//LBgY7O4dsWGYRiGYRiGYRhbAK0qz19IFI4zevRoAPbff38gbTGUdeof//gH0D4rU6HQSV4/ZX3KcbhKLJDFUG0VBg4c6BKJiy30UdTW1jqv0TXXXAPg3P1du3Z18sijtGbNmkgt84XA932XSP/gg+lQc1kV33vvvVwnVecUWc4uvTQdeTFkyBAXmhqnnoztRXL26tXLWUlVwOLxxx+PIswzdoSL/Eh+PdarV6+CzMnZWixoLsnmqchW8CTuKGLkyCOPdLnU8hBqzbjzzjtdP6dikg2CtX358uWuxYfmFN17JSUlbl6UXuvq6pyus/U/TAKtLcwTZ7bddltXmEnemEWLFrnfi0G+2tpaF2arsFuFUB9wwAGuQIwiuubPn+8ihNSGQGt9G8IdC4b2ZP/3f//n2gwoGmrevHmtKYKSU8yjZhiGYRiGYRiGETOKwqOWSqVcUuaOO+4IBCf5JUuW5Kspq5EnZDFUaXv9TAoaf/K8hJuYbklUV1czY8YMIGhiKqqqqmJ9n8r6qbkl6axevdqVb5d1f9asWbEu1JBvNm7c6Lxn8jKq5HgcyPSapVKporDWN8eKFStcDpbkUQGR3//+97luFVEQNPepxUJLm7Ib8UW596WlpS66RnvSxYsXF8wb01Z076lBtxqr62cS0X25bNkyJ3ccMI+aYRiGYRiGYRhGzPCitMC1tTFdSUkJvXr1AoJSoXvssQeQrgJ5++23A+nqMkDecn9832+y5EtUjYTzRXPyQfJlTLp8kHwZTb54U0xjVJEbrfXimA5bL6Mq/m699dau1LciaZSHvmTJksi8hqbD5MuYS/k0fnv16uXym8IVZFXlOsdVH22MbgkyFsNBrf61QLBwKsGvvLzcJSvmuyS/3RTJlzHp8kHyZTT54o2N0eTLB8mXMenyQfJlNPnijY3RNBb6aBiGYRiGYRiGETOiLibyKfB5/c9WIc+fClFEUNq7Nw2vc3ALXtNm+QpAW+SD5MuYdPkA1gH/y/3l5JxM+cB0CMmXD5IvY9Llg+KR0eaZzWNrRXyweSY7iZcx0tBHAM/zXvV9f2ykH9oG2nqdSZevva+NEtNhbl8XNTZGc/+6qDEd5v51UZN0HSZdPrAxmq/XRonpMD+vjZK2XqeFPhqGYRiGYRiGYcQMO6gZhmEYhmEYhmHEjEIc1KYW4DPbQluvM+nytfe1UWI6zO3rosbGaO5fFzWmw9y/LmqSrsOkywc2RvP12igxHebntVHSpuuMPEfNMAzDMAzDMAzDaBoLfTQMwzAMwzAMw4gZkR3UPM+b6Hne/zzPe8/zvPOi+tzm8DxvG8/znvY87x3P8972PO+M+scv8jxvied5b9T/m9yC94qdjEmXD3InY9Llq39NomVMunz1r0m0jEmXr/41sZMx6fKBjVHTYYP3SbR89a9JtIxJl8/h+37e/wElwPvAUKAcmA2MiOKzW3Bt/YFd63/vCrwLjAAuAn5S7DImXb5cyZh0+bYEGZMu35YgY9Lli7OMSZcvVzImXb4tQcaky7clyJh0+cL/ovKo7Qa85/v+At/3NwJ3AYdH9NlN4vv+Mt/3X6v/fS0wFxjYhreKpYxJlw9yJmPS5YPky5h0+SD5MiZdPoipjEmXD2yMtoKky5h0+SD5MiZdPkdUB7WBwOLQ/z+iHRedLzzPGwLsArxc/9Dpnue96XneTZ7n9Wzm5bGXMenyQbtkTLp8kHwZky4fJF/GpMsHRSBj0uUDG6PNvDzpMiZdPki+jEmXz2HFROrxPK8LcC9wpu/7lcB1wDBgZ2AZcEUBL6/dJF0+SL6MSZcPki9j0uWD5Mto8hW3fJB8GZMuHyRfxqTLB8mXMVfyRXVQWwJsE/r/1vWPxQLP88pIf5l3+L5/H4Dv+8t936/1fb8OuIG0m7UpYitj0uWDnMiYdPkg+TImXT5IvoxJlw9iLGPS5QMbo5gOIfnyQfJlTLp8jqgOav8FtvM8b1vP88qBY4FpEX12k3ie5wE3AnN9378y9Hj/0NOOBOY081axlDHp8kHOZEy6fJB8GZMuHyRfxqTLBzGVMenygY3RekyHyZcPki9j0uUL8KOrgjKZdOWT94Hzo/rcFlzX3oAPvAm8Uf9vMnAb8Fb949OA/sUoY9Lly6WMSZdvS5Ax6fJtCTImXb64yph0+WyMmg63JPm2BBmTLp/+efVvahiGYRiGYRiGYcQEKyZiGIZhGIZhGIYRM+ygZhiGYRiGYRiGETPsoGYYhmEYhmEYhhEz7KBmGIZhGIZhGIYRM+ygZhiGYRiGYRiGETPsoGYYhmEYhmEYhhEz7KBmGIZhGIZhGIYRM+ygZhiGYRiGYRiGETP+HzStIx8o+YLIAAAAAElFTkSuQmCC"&gt;&lt;/p&gt;
&lt;p&gt;上面圖中上排是進去Autoencoder之前的圖片，下排是經過Autoencoder後的圖片，效果是不是很驚人！大致都有辦法還原回去原圖。但是仍有幾張圖還原的不是很好，做個Regularization看看能不能解決這個問題。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Autoencoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0005&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model_2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img_original&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_original&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;18s loss =    0.0320 , val_loss =    0.0316&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0296 , val_loss =    0.0294&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0285 , val_loss =    0.0283&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  4/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0277 , val_loss =    0.0277&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  5/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0271 , val_loss =    0.0272&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  6/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0266 , val_loss =    0.0268&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  7/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0262 , val_loss =    0.0265&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  8/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0260 , val_loss =    0.0264&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  9/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0257 , val_loss =    0.0261&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0256 , val_loss =    0.0260&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 11/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0255 , val_loss =    0.0260&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 12/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0251 , val_loss =    0.0257&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 13/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0249 , val_loss =    0.0256&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 14/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0248 , val_loss =    0.0256&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 15/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;18s loss =    0.0247 , val_loss =    0.0255&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 16/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0245 , val_loss =    0.0253&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 17/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0243 , val_loss =    0.0252&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 18/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0242 , val_loss =    0.0252&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 19/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0242 , val_loss =    0.0252&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 20/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0240 , val_loss =    0.0251&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_loss =    0.0256&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,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"&gt;&lt;/p&gt;
&lt;p&gt;似乎看起來是有效果的，數字還原的比較清晰。&lt;/p&gt;
&lt;h3&gt;壓縮碼Code與視覺化&lt;/h3&gt;
&lt;p&gt;剛剛提到在Autoencoder前半段是一個Encoder，所以我們可以利用這個Encoder來做壓縮，會得到一個Code，在上面的這個例子，這個Code總共有4個值，因為中間層有4個神經元，可以把這個Code看成Dimension Reduction的結果，原本一張圖代表的是28x28=784個維度下的一個點，現在經過轉換後變成是4個維度下的一個點，而我們會直覺的認為同樣一群的數字圖形應該會有較高的相似度，所以在4個維度之下，同樣的數字圖片應該會彼此靠近的比較近，甚至聚成一團。&lt;/p&gt;
&lt;p&gt;我想要驗證一下這件事，我們需要先圖像化，不過卻卡在維度太高的問題，人類無法想像高於3個維度以上的空間，也沒辦法將它視覺化，這個時候我們需要再做一次的Dimension Reduction，將維度降到低於3才可以視覺化，那一般手法是使用PCA來做這件事，有關於PCA我之前已經介紹過，請參考&lt;a href="http://www.ycc.idv.tw/YCNote/post/35"&gt;這篇&lt;/a&gt;，如此一來就可以在4個維度中切一個重要的截面來視覺化這些數據。不過記得喔！4個維度才是真正可以表示這群資料，做PCA只是為了畫圖而做的粗略轉換而已。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# get code&lt;/span&gt;
&lt;span class="n"&gt;encode&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# PCA 2D visualization&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.decomposition&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt;
&lt;span class="n"&gt;pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PCA&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# plot&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;colorbar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/04_output_9_0.png"&gt;&lt;/p&gt;
&lt;p&gt;上面我以不同顏色當作不同的數字圖形，我們可以看到同樣的數字圖形會彼此聚成一團，所以的確同樣的數字的族群會被歸類到具有相似的特性，因此在code裏頭距離是彼此靠近的，還記得一開始我們沒加Regularization時。Model會把5看成是6，在這張圖你就會到原因，因為5號藍綠色和6號黃色靠的很近，很容易誤判。&lt;/p&gt;
&lt;p&gt;這張圖同時揭露了Autoencoder的一個強大特性，注意喔!我們一開始Train這個Autoencoder的時候是沒有給它看任何Labels的，但他卻可以在壓縮資訊的同時找出規律，這個規律可以想成是我們人類在辨認每個不同數字的方法，所以Autoencoder可以在沒有Labels的情況下做歸納和學習，因此Autoencoder常常會被用在Unsupervised Learning (非監督式學習)。&lt;/p&gt;
&lt;p&gt;另外介紹一種也是很流行的方法叫做t-SNE (讀作"tee-snee") ，這裡不多著墨這個方法的原理，但是它卻是目前2D Visualization最流行的作法，PCA只用線性的方式去做座標轉換，也就是從一個橫切面去看數據，這樣粗略的轉換並不能讓我們在視覺化時看出資料和資料間彼此的距離，尤其是從高維度轉換過來，經常會失真，而t-SNE是針對數據和數據間的距離去做轉換，最後被攤成2維時正是顯示數據點的距離關係，更能描述群聚的現象。&lt;/p&gt;
&lt;p&gt;來看看t-SNE做起來效果如何。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# get code&lt;/span&gt;
&lt;span class="n"&gt;encode&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# TSNE 2D visualization&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.manifold&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;
&lt;span class="n"&gt;tsne&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TSNE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_embedded&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tsne&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# plot&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_embedded&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X_embedded&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;colorbar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/04_output_11_0.png"&gt;&lt;/p&gt;
&lt;h3&gt;去雜訊(De-noise) Autoencoder&lt;/h3&gt;
&lt;p&gt;我們巧妙的利用一下Autoencoder，我們將原本Autoencoder的前面加了一道人工雜訊的流程，但是最終又要讓Autoencoder試著去還原出原來沒有加入雜訊的資訊，這麼一來我們將可以找到一個Autoencoder是可以自行消除雜訊的，把這個Denoising Autoencoder加到正常Neural Network的前面，那這個Neural Network就擁有了抑制雜訊的功用，所以可以當作一種Regularization的方法。&lt;/p&gt;
&lt;p&gt;先將圖片加上雜訊。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;add_noise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;noise_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;
    &lt;span class="n"&gt;noisy_ndarr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ndarr&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;noise_factor&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;noisy_ndarr&lt;/span&gt;

&lt;span class="n"&gt;noisy_train_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add_noise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;noisy_valid_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add_noise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;noisy_test_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;add_noise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noisy_train_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,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"&gt;&lt;/p&gt;
&lt;p&gt;圖片現在看起來非常的髒。&lt;/p&gt;
&lt;p&gt;用這些髒圖片當作Input，正常圖當作Output的目標，我們就可以自然而然的Train出可以消除雜訊的Autoencoder。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;denoise_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Autoencoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0003&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;denoise_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;noisy_train_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noisy_valid_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noisy_test_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img_original&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_original&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img_noisy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noisy_test_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_noisy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;denoise_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;noisy_test_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;18s loss =    0.0393 , val_loss =    0.0389&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0356 , val_loss =    0.0355&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0341 , val_loss =    0.0341&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  4/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0327 , val_loss =    0.0329&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  5/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0320 , val_loss =    0.0324&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  6/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0314 , val_loss =    0.0320&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  7/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0309 , val_loss =    0.0316&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  8/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;18s loss =    0.0307 , val_loss =    0.0315&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  9/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0302 , val_loss =    0.0312&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0299 , val_loss =    0.0309&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 11/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0298 , val_loss =    0.0310&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 12/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0296 , val_loss =    0.0308&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 13/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0293 , val_loss =    0.0307&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 14/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0291 , val_loss =    0.0306&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 15/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0290 , val_loss =    0.0305&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 16/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0287 , val_loss =    0.0304&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 17/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0287 , val_loss =    0.0304&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 18/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0287 , val_loss =    0.0304&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 19/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0286 , val_loss =    0.0304&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 20/20&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;17s loss =    0.0285 , val_loss =    0.0303&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_loss =    0.0309&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,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"&gt;&lt;/p&gt;
&lt;p&gt;上面圖片第一排為原圖，第二排是加完雜訊後的結果，第三排是經過Autoencoder後的圖，傑克真的是太神奇啦！所有的雜訊都被消除掉了，特別注意，這裡我的Regularization下的特別重，原因是雜訊增多了，也更容易Overfitting，所以要下更多的Regularization才能抑制它。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>實作Tensorflow (3)：Build First Convolutional Neurel Network (CNN)</title><link href="https://ycc.idv.tw/tensorflow-tutorial_3.html" rel="alternate"></link><published>2017-11-12T12:00:00+08:00</published><updated>2017-11-12T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-12:/tensorflow-tutorial_3.html</id><summary type="html">&lt;p&gt;影像有什麼特性 / DNN用在影像上的侷限 / Convolutional Neurel Network (CNN) / Convolution Layer / Pooling Layer / 最簡單的CNN架構：LeNet5 / 圖像化&lt;/p&gt;</summary><content type="html">&lt;p&gt;這一章我們終於要討論到影像辨識的重頭戲啦！通常，處理影像類別我們會用Convolutional Neurel Network，聽起來很難很厲害，不過只要了解背後概念你就知道為什麼要這麼做了，讓我們看下去。&lt;/p&gt;
&lt;p&gt;本單元程式碼可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/03_CNN_classification_on_MNIST.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;h3&gt;影像有什麼特性&lt;/h3&gt;
&lt;p&gt;來想一下，影像具備了什麼特性？&lt;/p&gt;
&lt;p&gt;(1) 局域性：通常物件只在一個局域的範圍裡有效，而與太遠的距離無關，譬如我要找一張圖的鳥嘴，鳥嘴的呈現在一張圖當中只會出現在一個小範圍內，所以其實只需要評估這小範圍就可以判斷這是不是鳥嘴了。&lt;/p&gt;
&lt;p&gt;(2) 平移性：通常一張圖任意平移並不影響它的意義，一隻鳥不管是放在圖片的左上角還是右下角，牠都是一隻鳥。&lt;/p&gt;
&lt;p&gt;(3) 縮放性：通常一張圖我把它等比例的放大縮小是不影響它的意義的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="影像特性" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.004.jpeg"&gt;&lt;/p&gt;
&lt;h3&gt;DNN用在影像上的侷限&lt;/h3&gt;
&lt;p&gt;我們剛剛看過了影像具有的三種特性：「局域性」、「平移性」和「縮放性」，那我們就拿這三種特性來檢驗上一回的DNN Classification。&lt;/p&gt;
&lt;p&gt;DNN有「局域性」嗎？沒有，因為我們把圖片攤平處理，原本應該是相鄰的關係就被打壞了，事實上DNN的結構會造成每個Input都會同時影響下一層的「每個」神經元，所以相不相鄰根本沒關係，因為每個Pixels的影響是全域的。&lt;/p&gt;
&lt;p&gt;DNN有「平移性」嗎？沒有，沒有局域性就沒有平移性。&lt;/p&gt;
&lt;p&gt;DNN有「縮放性」嗎？我們沒有一層試著去縮放，而且圖片已經被攤平了，難以做到縮放的效果。&lt;/p&gt;
&lt;p&gt;所以其實使用DNN並不能好好的詮釋影像。&lt;/p&gt;
&lt;h3&gt;Convolutional Neurel Network (CNN)&lt;/h3&gt;
&lt;p&gt;我們需要引入新的架構來處理影像，讓它可以擁有以上三種特性，Convolutional Neurel Network (CNN)此時就登場了，CNN有兩大新要素：Convolution Layer和Pooling Layer，Convolution Layer為我們的Model添加了局域性和平移性，而Pooling Layer則讓Model擁有縮放的特性。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Convolution Layer和Pooling Layer" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.005.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Convolution Layer是由Filters所構成的，Filters可以想像是一張小圖，以上面的例子，Filter是一個鳥嘴的小圖，這張小圖要怎麼去過濾原圖呢？答案是使用Convolution(卷積)，把小圖疊到大圖的任意位置，接下來將大圖小圖對到的相應元素相乘起來再加總一起，然後在另外一張表格中填入這個加總值，接下來移動Filter，重複的動作再做一次，如此循環並將值陸續填入表格中，這表格最後就會像是另外一張圖一樣，而這張圖可以繼續串另外的Neurel Network，這就是Convolution Layer的計算方法。&lt;/p&gt;
&lt;p&gt;舉個例子，假設我今天有矩陣A：&lt;/p&gt;
&lt;p&gt;[[1, 2, 3, 4],&lt;br/&gt;
 [4, 5, 6, 7],&lt;br/&gt;
 [7, 8, 9,10],&lt;br/&gt;
 [1, 3, 5, 7]]&lt;/p&gt;
&lt;p&gt;然後再有一個Filter：&lt;/p&gt;
&lt;p&gt;[[1, 0, 0],&lt;br/&gt;
 [0, 1, 0],&lt;br/&gt;
 [0, 0, 0]]&lt;/p&gt;
&lt;p&gt;使用Filter對A做Convolution得到B為：&lt;/p&gt;
&lt;p&gt;[[6, 8 ],&lt;br/&gt;
 [12,14]]&lt;/p&gt;
&lt;p&gt;原本4x4的矩陣做了Convolution後變成了2x2的矩陣，原因在於邊界限制了Filter的移動，那如果我想要讓Convolution玩的矩陣維持在4x4，怎麼做？我們可以在邊緣的地方鋪上0就可以達到這樣的效果，來試試看，先將矩陣A擴張成矩陣C：&lt;/p&gt;
&lt;p&gt;[[0, 0, 0, 0, 0, 0],&lt;br/&gt;
 [0, 1, 2, 3, 4, 0],&lt;br/&gt;
 [0, 4, 5, 6, 7, 0],&lt;br/&gt;
 [0, 7, 8, 9,10, 0],&lt;br/&gt;
 [0, 1, 3, 5, 7, 0],&lt;br/&gt;
 [0, 0, 0, 0, 0, 0]]&lt;/p&gt;
&lt;p&gt;再使用Filter對C做Convolution會得到D為：&lt;/p&gt;
&lt;p&gt;[[1, 2, 3, 4],&lt;br/&gt;
 [4, 6, 8,10],&lt;br/&gt;
 [7,12,14,16],&lt;br/&gt;
 [1,10,13,16]]&lt;/p&gt;
&lt;p&gt;而此時D就是一個4x4的矩陣。&lt;/p&gt;
&lt;p&gt;Convolution的過程造成怎麼樣的效果呢？當Filter是一個鳥嘴的小圖，一旦遇到與鳥嘴相似的局部，此時加總的值會是一個大的值，如果是一個和鳥嘴無關的局部此時的值會是一個小的值，所以這個Filter具有將特定特徵篩選出來的能力，符合特徵分數高不符合則分數低，因此局部的特徵變得是有意義的，此時我的Model就具有局域性，而且藉由Filter的平移掃視，這個特徵就具有可平移的特性。&lt;/p&gt;
&lt;p&gt;Convolution Layer不同於Fully-connected Layer有兩點，第一，每個Pixels間有相對的距離關係，擁有上下左右的關係才有辦法構成一張圖，第二，除了有距離上的關係以外，它還能在有限範圍內抓出一種特徵模式，所以我們將可以使用影像的語言來做特徵轉換和抓取特徵。&lt;/p&gt;
&lt;p&gt;實際情況下，Filter上的Weights是會自行調整的，Model Fitting的時候，Model會根據數據自行訓練出Filter，也就是說機器可以自行學習出圖片的特徵。通常這樣的Filters會有好幾個，讓機器可以有更多維度可以學習。&lt;/p&gt;
&lt;p&gt;接下來來看Pooling Layer如何讓Model擁有檢視縮放的特性？&lt;/p&gt;
&lt;p&gt;先來看在影像上如何做到放大縮小，以Pixels的觀點來看，最簡單的放大方法是，在每個既有的Pixels附近增加一些與它們相似的新Pixel，這樣做就像是將原本的小圖直接拉成大圖，畫質雖然很差，不過這是最簡單的放大方法。那麼縮小就相反操作，把一群附近的Pixels濃縮成一個Pixel當作代表，就可以達到縮小圖片的效果。&lt;/p&gt;
&lt;p&gt;所以回到Pooling Layer的討論，Pooling做的事情就是在縮小圖片，例如我使用2x2來做Pooling，它會在原圖上以2x2來掃描，所以會有四個元素被檢視，然後從這四個值當中產生一個代表值，把原本2x2的Pixels減少成這個1x1的代表值，如果是Max Pooling就是從四個中選最大的那個，如果是Average Pooling則是平均四個值得到平均值，通常如果是2x2的Pooling我們會以每2格一跳的方式掃視，如果是3x3則會以每3格一跳，以此類推。&lt;/p&gt;
&lt;h3&gt;Convolution Layer&lt;/h3&gt;
&lt;p&gt;來看一下Tensorflow如何實作Convolution Layer。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Config the matplotlib backend as plotting inline in IPython&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;([[[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]]],&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# shape of img: [batch, in_height, in_width, in_channels]&lt;/span&gt;

    &lt;span class="n"&gt;filter_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;([[[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]]],&lt;/span&gt;
                                 &lt;span class="p"&gt;[[[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]]],&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# shape of filter: [filter_height, filter_width, in_channels, out_channels]&lt;/span&gt;

    &lt;span class="n"&gt;conv_strides&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;padding_method&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;

    &lt;span class="n"&gt;conv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;filter_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
        &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conv_strides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;conv_strides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
        &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;padding_method&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Shape of conv:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Conv:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Shape of conv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;(1, 4, 4, 1)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Conv&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[[[[&lt;/span&gt;&lt;span class="nv"&gt;14.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;2.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;3.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;3.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;2.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;14.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;3.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;6.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;6.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;1.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;2.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;1.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;首先一開始是圖片&lt;code&gt;img&lt;/code&gt;的部分，Rank為4，每個維度分別為&lt;code&gt;[batch, in_height, in_width, in_channels]&lt;/code&gt;，in_channels的部分一般是RGB，這邊我採用和MNIST相同的灰階表示，所以in_channels只有1個維度。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;filter_&lt;/code&gt;的部分Rank為4，每個維度分別為&lt;code&gt;[filter_height, filter_width, in_channels, out_channels]&lt;/code&gt;，當如果我想要使用多個filters的時候，我的out_channels就不只1而已，如果有RGB的話，in_channels則會是3。&lt;/p&gt;
&lt;p&gt;接下來來看一下&lt;code&gt;tf.nn.conv2d&lt;/code&gt;裡頭的參數&lt;code&gt;strides&lt;/code&gt;，這可能會讓人感到困惑，它的設定值是&lt;code&gt;[1, conv_strides[0], conv_strides[1], 1]&lt;/code&gt;，我特別把第二、三項額外用&lt;code&gt;conv_strides&lt;/code&gt;來表示，因為這兩個值才是真正代表在圖片上平移每步的距離，那第一項和最後一項的1代表什麼意義呢？是這樣的，Tensorflow是站在維度的角度看平移這件事情，這四個維度分別表示&lt;code&gt;[batch, in_height, in_width, in_channels]&lt;/code&gt;的移動量，所以一般情況下只有&lt;code&gt;in_height&lt;/code&gt;和&lt;code&gt;in_width&lt;/code&gt;需要指定平移的距離。&lt;/p&gt;
&lt;p&gt;最後一個參數就是&lt;code&gt;padding&lt;/code&gt;，有兩種可以選擇，分別為&lt;code&gt;VALID&lt;/code&gt;和&lt;code&gt;SAME&lt;/code&gt;，&lt;code&gt;VALID&lt;/code&gt;指的就是沒有額外鋪上0的邊界的情形，&lt;code&gt;SAME&lt;/code&gt;則是額外鋪上0的邊界，並且使得輸出的維度和輸入一樣。&lt;/p&gt;
&lt;h3&gt;Pooling Layer&lt;/h3&gt;
&lt;p&gt;接下來來看Tensorflow如何實作Pooling Layer。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;([[[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;
                       &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]]],&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# shape of img: [batch, in_height, in_width, in_channels]&lt;/span&gt;

    &lt;span class="n"&gt;pooling&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Shape of pooling:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pooling&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Pooling:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pooling&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Shape of pooling&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;(1, 2, 2, 1)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Pooling&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[[[[&lt;/span&gt;&lt;span class="nv"&gt;3.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;1.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;

&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="nv"&gt;1.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;3.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;tf.nn.max_pool&lt;/code&gt;的參數中&lt;code&gt;ksize&lt;/code&gt;代表kernel size，也就是要Pooling的大小，一樣依照Input layer的Rank去配置，還有&lt;code&gt;strides&lt;/code&gt;決定平移的方法。&lt;/p&gt;
&lt;h3&gt;最簡單的CNN架構：LeNet5&lt;/h3&gt;
&lt;p&gt;&lt;img alt="LeNet5" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.006.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;接下來我們就真正的來實作一下CNN網路，和之前兩個單元一樣，我們拿MNIST的分類問題來當作題目。&lt;/p&gt;
&lt;p&gt;本單元介紹的是最簡單的CNN Classification的方法—LeNet5，流程如上圖所示。&lt;/p&gt;
&lt;p&gt;(1) conv1+relu+pooling2：一開始使用Convolution抓取圖片中的特徵，並且加入Activation Function使得Model具有非線性因子，通常在這種非常深的網路，我們會採用Relu，它的好處是不會出現「梯度消失」(Vanishing Gradient)的問題，如果你使用像是tanh或sigmoid這類在飽和區梯度接近0的函數，則就很有可能在深網路的情形下，造成一開始的幾個Layers梯度太小的問題，也就是前面幾層我們無法訓練到，而Relu在Turn-on的情況下是線性的，不會有飽和的問題，也就不會出現「梯度消失」。做完Convolution後，我們已經對於這個圖片有一點認識了，所以減少一些Pixels來減少一些計算量，所以加入Pooling Layer，注意喔！Pooling Layer結束之後，不需要再做一次Activation，因為Pooling只是用來平均前面的結果而已。&lt;/p&gt;
&lt;p&gt;(2) conv3+relu+pooling4：做第二次的圖片特徵抽取。&lt;/p&gt;
&lt;p&gt;(3) fatten5：將抽取完的特徵完全打平，為了接下來的Fully-connected Network做準備。&lt;/p&gt;
&lt;p&gt;(4) fc6+fc7+fc8+softmax：這個部分就和之前DNN Classification做的事情一樣，全盤考慮每一個擷取來的特徵，並且非線性的轉換成最後可以分為相應的10種類別。&lt;/p&gt;
&lt;p&gt;接下來我們看一下程式碼要怎麼寫？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;CNNLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape_picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape_picture&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;shape_picture&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new grap&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_pictures&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                 &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape_picture&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                               &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their predictions and loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pictures&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_pictures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# regularization loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l2_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt; \
                &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;

            &lt;span class="c1"&gt;# total loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_pictures&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                               &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape_picture&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                             &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pictures&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_pictures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                 &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pictures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="c1"&gt;## LeNet5 Architecture(http://yann.lecun.com/exdb/lenet/)&lt;/span&gt;
        &lt;span class="c1"&gt;# input:(batch,28,28,1) =&amp;gt; conv1[5x5,6] =&amp;gt; (batch,24,24,6)&lt;/span&gt;
        &lt;span class="c1"&gt;# pool2 =&amp;gt; (batch,12,12,6) =&amp;gt; conv2[5x5,16] =&amp;gt; (batch,8,8,16)&lt;/span&gt;
        &lt;span class="c1"&gt;# pool4 =&amp;gt; fatten5 =&amp;gt; (batch,4x4x16) =&amp;gt; fc6 =&amp;gt; (batch,120)&lt;/span&gt;
        &lt;span class="c1"&gt;# (batch,120) =&amp;gt; fc7 =&amp;gt; (batch,84)&lt;/span&gt;
        &lt;span class="c1"&gt;# (batch,84) =&amp;gt; fc8 =&amp;gt; (batch,10) =&amp;gt; softmax&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                         &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                         &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc6&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                       &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc7&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;84&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                       &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;84&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                                       &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc6&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;120&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc7&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;84&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_conv_2d_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pictures&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                               &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_conv_2d_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;pool4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                               &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;fatten5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_flatten_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;fatten5&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fatten5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="n"&gt;fc6&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fatten5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc6&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc6&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                   &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;fc6&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="n"&gt;fc7&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc7&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc7&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                   &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                 &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_conv_2d_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                          &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                          &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
              &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                           &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_flatten_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_shape&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
            &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;# mini-batch gradient descent&lt;/span&gt;
            &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
            &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;index_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_size&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

                &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_pictures&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                &lt;span class="p"&gt;}&lt;/span&gt;
                &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                        &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

                &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%.4f&lt;/span&gt;&lt;span class="s1"&gt;     &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# evaluate at the end of this epoch&lt;/span&gt;
            &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;val_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;val_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, val_loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, val_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;test_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_pictures&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_pictures&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MNIST_data/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;
&lt;span class="n"&gt;valid_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CNNLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;shape_picture&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.07&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;valid_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;test_img&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0894, acc = 97.21%, val_loss =   0.0872, val_acc = 97.34%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0532, acc = 98.41%, val_loss =   0.0589, val_acc = 98.30%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0567, acc = 98.27%, val_loss =   0.0578, val_acc = 98.18%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  4/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0384, acc = 98.85%, val_loss =   0.0475, val_acc = 98.56%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  5/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0307, acc = 99.10%, val_loss =   0.0431, val_acc = 98.82%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  6/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0299, acc = 99.09%, val_loss =   0.0388, val_acc = 98.88%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  7/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0280, acc = 99.17%, val_loss =   0.0403, val_acc = 98.86%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  8/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0233, acc = 99.31%, val_loss =   0.0372, val_acc = 99.02%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  9/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0225, acc = 99.32%, val_loss =   0.0356, val_acc = 99.02%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.0234, acc = 99.27%, val_loss =   0.0411, val_acc = 98.84%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_acc = 98.89%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;太棒了！我們的預測效果如果跟之前的結果比較，在Epoch=3已經達到98.5%了，在Epoch=10更是高達99%！&lt;/p&gt;
&lt;h3&gt;圖像化&lt;/h3&gt;
&lt;p&gt;有這麼好的成果，我們不妨就拉進去看，裡面每個Filters究竟是長什麼樣子的？&lt;/p&gt;
&lt;p&gt;先來看第一層Convolution的Filters。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAagAAABTCAYAAADKkJOuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0ABxQREfkSBxQREfkSBxQREfnS/wHRnw01r1VfLAAAAABJRU5ErkJggg==%0A"&gt;&lt;/p&gt;
&lt;p&gt;再來看第二層Convolution的Filters。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAkkAAAKvCAYAAAB+nVurAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A0N/LBB+YAAAAAElFTkSuQmCC%0A"&gt;&lt;/p&gt;
&lt;p&gt;單看這些Filters似乎很難看出什麼？&lt;/p&gt;
&lt;p&gt;我們試著丟幾張圖進去做Convolution，看一下在Filters的拆解之下圖片會變怎樣？&lt;/p&gt;
&lt;p&gt;以下圖片的第一行表示原圖片，第二行表示做完第一次Convolution後的結果，第三行表示做完第二次Convolution後的結果。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;picture&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:],(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv1&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv3&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6IAAADFCAYAAABO4U/4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;picture&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:],(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv1&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv3&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6IAAADFCAYAAABO4U/4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0AAAAAAElFTkSuQmCC%0A"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;picture&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pool2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;ksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;strides&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;VALID&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getConv2DLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pool2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;conv3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;eval_conv3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conv3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;picture&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv1&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eval_conv3&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="p"&gt;:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6IAAADFCAYAAABO4U/4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A"&gt;&lt;/p&gt;
&lt;p&gt;有看出什麼規律嗎？哈哈。&lt;/p&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>實作Tensorflow (2)：Build First Deep Neurel Network (DNN)</title><link href="https://ycc.idv.tw/tensorflow-tutorial_2.html" rel="alternate"></link><published>2017-11-07T12:00:00+08:00</published><updated>2017-11-07T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-11-07:/tensorflow-tutorial_2.html</id><summary type="html">&lt;p&gt;增加Hidden Layer / Activation Function的選擇 / Mini-Batch Gradient Descent / Regularization / Weight Regularization / Dropout / Optimizer的選擇 / 來看看程式怎麼寫&lt;/p&gt;</summary><content type="html">&lt;p&gt;接續著&lt;a href="http://www.ycc.idv.tw/YCNote/post/38"&gt;上一回&lt;/a&gt;，我們已經有一個單層的Neurel Network，緊接著我們來試著一步一步改造它，讓它成為我們常使用的Deep Neurel Network的形式。&lt;/p&gt;
&lt;p&gt;本單元程式碼可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/02_DNN_classification_on_MNIST.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Config the matplotlib backend as plotting inline in IPython&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;增加Hidden Layer&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們只有一層Neurel Network，也就是做完一個線性轉換後，就直接使用Softmax Layer來轉換成機率表示方式，這樣的結構並不夠powerful，我們需要把它的結構弄的又窄又深，這樣效果才會好，詳細原因請參考&lt;a href="http://www.ycc.idv.tw/YCNote/post/35"&gt;這一篇的介紹&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;因此，我們來試著加入一層Hidden Layer，來打造成兩層的Neurel Network，並在兩層之間加入Activation Function，為我的Model增加非線性因子。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# build neurel network structure and return their predictions and loss&lt;/span&gt;
    &lt;span class="c1"&gt;### Variable&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="c1"&gt;### Structure&lt;/span&gt;
    &lt;span class="c1"&gt;# layer 1&lt;/span&gt;
    &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# layer 2&lt;/span&gt;
    &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;首先在變數需要有二層的fully-connect參數，注意這些參數的大小，受到Hidden Layer的神經元數目&lt;code&gt;n_hidden&lt;/code&gt;決定。接下來開始建構整個Neurel Network的結構，&lt;code&gt;fc1&lt;/code&gt;產生一個fully-connect的結果，並且通過Activation Function再輸出，然後進到下一層，第二層直接使用&lt;code&gt;fc1&lt;/code&gt;的結果當作新的輸入，再做一次fully-connect，並且讓它通過Softmax Layer來完成最後的Logistic轉換，它的loss一樣的是使用cross-entropy來評估。&lt;/p&gt;
&lt;h3&gt;Activation Function的選擇&lt;/h3&gt;
&lt;p&gt;剛剛提到的Hidden Layer可以採用不同的Activation Function，我列幾個常使用的Activation Function給大家看看。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;tanh&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;sigmoid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;relu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tanh&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="s1"&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A/j+3uXalz1k0gQAAAABJRU5ErkJggg==%0A"&gt;&lt;/p&gt;
&lt;p&gt;這些Activation Function的使用時機可以簡單這樣說，當我們想要輸出值在+1和-1之間時使用tanh，而當我們想要一個輸出值衡為正時使用sigmoid，它可以將輸出值壓在+1和0之間。tanh比sigmoid多了兩個好處，第一，tanh的梯度變化大於sigmoid，有利於訓練效率，第二，tanh的輸出均值為0，可以避免將前層的梯度偏差帶到下一層。&lt;/p&gt;
&lt;p&gt;不過，以上的這兩種Activation Function在非常深的網路都會有一個共通問題—梯度消失，仔細看上圖，tanh和sigmoid在極大和極小的地方都會彎成平的，所以每過一次這種Activation Function，訊號就會減小一點，當我們在深網路做Backpropagation時，訊號在過程中不斷的被磨損，到了前面的幾層就已經耗損完畢，此時更新的梯度近乎0，也就是梯度消失，那麼前面的這些層就再也訓練不到了。&lt;/p&gt;
&lt;p&gt;Relu正可以解決梯度消失的問題，如上圖，在正的部分Relu是線性的，所以多少訊號進來就多少訊號出去，如此一來就不會有耗損的問題，但特別注意，因為tanh和sigmoid會將輸出值限制在一個範圍內，所以有Normalization的味道，但是Relu沒有限制，Normalization可以使我們訓練的效率提升（因為梯度的方向可以直指低點），所以Relu常常會搭配Normalization Layer一起使用，來額外做Normalization，或者是最近一篇Paper提到的一種新的Activation Function：SELU，類似於Relu但是輸出值會是Normalize過的，非常神奇，在這邊我不多論述。&lt;/p&gt;
&lt;h3&gt;Mini-Batch Gradient Descent&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們的Gradient Descent採用的是將所有的Data一次全考慮進去，評估完所有的Data在一次性的更新權重參數，這樣的作法好處是比較穩定，因為我考慮的是真正的Training Set的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;，但缺點就是計算時間長，因為要考慮所有Training Set的每筆數據，需要做大矩陣的計算，而且通常Training Set的數據量也不可以太小，這麼一來計算時間就會拉的很長。&lt;/p&gt;
&lt;p&gt;因此，有另外一種作法一次只考慮「一筆」數據，使用一筆數據來評估並更新權重參數，每一筆雖然更新結果都不怎麼準確，但是當我隨著時間看過整個Training Set後，就會有平均的效果，所以最後只要Learning Rate不要太大，最後的結果還是可以朝向最佳解的，這個手法會使得Gradient Descent具有隨機性，因此又被稱為Stochastic Gradient Descent，它所帶來的優點是計算時間變短了，我們將可以避免去涉及大矩陣的運算，但缺點是一次只評估一筆數據，將會非常的不穩定。&lt;/p&gt;
&lt;p&gt;另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Mini-Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，Mini-Batch Gradient Descent一次評估k組數據，並更新參數W，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的架構設計是非常有利於矩陣計算的，因為GPU會利用它強大的平行化將矩陣運算中每個元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Mini-Batch Gradient Descent，這個k值不要超過GPU平行計算所能容納的最大上限，這是個既有效率又更為穩定的作法，順道一提Tensorflow是可以支援GPU的計算的。&lt;/p&gt;
&lt;p&gt;實務經驗告訴我們Mini-Batch Gradient Descent雖然穩定性比Gradient Descent差，但是收斂的速度卻一點都不輸給Gradient Descent，原因就出在更新的次數，Mini-Batch Gradient Descent一次看的數據筆數比較少，所以Mini-Batch一個Epoch可以更新參數好幾次，而Gradient Descent卻只能更新一次，Mini-Batch的靈敏性，使得它的收斂速度更為快速。打個比方，就好像是兩艘船在搜尋小島，Gradient Descent像是巡洋艦，它有更好設備可以有更好的觀測能力，但是因為它的笨重造成它反應不夠靈敏，Mini-Batch Gradient Descent就像是小船一樣，雖然觀測設備沒這麼好，但是反應靈敏，卻是可以更容易率先找到小島。&lt;/p&gt;
&lt;p&gt;那我們來看看要怎麼做到Mini-Batch Gradient Descent。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="n"&gt;略&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;

    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="c1"&gt;# mini-batch gradient descent&lt;/span&gt;
        &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;index_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_size&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

            &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt;
                &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;     &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="n"&gt;略&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;在每一個&lt;code&gt;epoch&lt;/code&gt;都完整的看過數據一遍，而mini-batch gradient descent是隨機取&lt;code&gt;batch_size&lt;/code&gt;筆數據來更新權重，所以我採用這樣的作法，先依照數據的筆數&lt;code&gt;N&lt;/code&gt;列出可能的Index有哪些，然後再做一個&lt;code&gt;random.shuffle&lt;/code&gt;來做到隨機採樣，然後接下來只要簡單的從前面取&lt;code&gt;batch_size&lt;/code&gt;筆數據進行更新，直到用盡所有的index為止，就可以做到mini-batch的效果。&lt;/p&gt;
&lt;h3&gt;Regularization&lt;/h3&gt;
&lt;p&gt;當你開始加深你的DNN時，就已經在增加Model的複雜度，增加複雜度想當然爾的可以增加對於數據的描述能力，在分類問題中代表可以增加精確度，不過要特別注意Overfitting的出現，當Model越複雜越容易產生Overfitting，Overfitting的結果是有看過的數據描述的很好，但沒看過的數據預測就很差，所以在Training的過程要特別注意Validation Set的表現，如果發現Training Set的表現越來越好，但是Validation Set的表現裹足不前甚至變的更差，那就很有可能已經Overfitting了。&lt;/p&gt;
&lt;p&gt;如果你已經看到Overfitting出現了，有什麼方法可以抑制他呢？這個時候就需要Regularization的幫忙，在Neurel Network常見的Regularization有兩種：Weight Regularization和Dropout，待會會一一介紹。&lt;/p&gt;
&lt;p&gt;這邊特別注意，不要每次精確度沒有提升就怪Overfitting！如果連你的Training Set都沒辦法有好的表現，這就可能不是Overfitting，反而可能是Underfitting，這個時候不要再增加Regularization，而是試著去調整Learning Rate，或者增加模型的複雜度，加深DNN或增加神經元的數目。&lt;/p&gt;
&lt;h3&gt;Weight Regularization&lt;/h3&gt;
&lt;p&gt;我們可以藉著在Loss Function裡頭加入Weight的貢獻，來達到限制W的大小的目標，這樣做可以降低Overfitting，詳細原理請&lt;a href="http://www.ycc.idv.tw/YCNote/post/28"&gt;參考這篇的Regularization的部分&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;我們來看看應該怎麼做到L2 Regularization。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# regularization loss&lt;/span&gt;
&lt;span class="n"&gt;regularization&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l2_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;
&lt;span class="c1"&gt;# total loss&lt;/span&gt;
&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;regularization&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;在這裡我習慣將Loss對Weights的個數做平均，這有一個好處當我在調整神經元數目時，&lt;code&gt;alpha&lt;/code&gt;可以不需要大動作調整。&lt;/p&gt;
&lt;h3&gt;Dropout&lt;/h3&gt;
&lt;p&gt;Dropout是Deep Learning常用的Regularization技巧，它的作法是在訓練的時候我先隨機把部份的神經元關閉，使用較少的神經元訓練，來達到Regularization的效果，最後在「推論」的時候再使用全部的神經元，我個人覺得這有Aggregation Model的味道，分別訓練出許多的sub-model再做Aggregation以達到截長補短的效果。&lt;/p&gt;
&lt;p&gt;實作上有一些細節必須要注意，當我們關閉一些神經元時，也就是等於減少部份的貢獻量，所以我們需要依照相應比例來給予權重，以抵銷減少的部分。舉個例子，假設今天原本應該要輸出的值有十個，這十個值都是1，然後因為Dropout，變成五個1五個0的輸出，我們看到原本貢獻量因為Dropout一半而少一半，這樣並不合理，會導致我後面的Weights在更新的時候低估更新量，所以我們必須要將「沒被Dropout的部分」權重乘上一倍，才可以解決問題。因此，如果Dropout &lt;span class="math"&gt;\(r\)&lt;/span&gt;倍的神經元，權重就要乘以&lt;span class="math"&gt;\((1/r)\)&lt;/span&gt;倍，我們來看看Tensorflow怎麼做的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;S&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                           &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                           &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# 3 Data, 8 dim. Score&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Original S =&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;S_drop&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# dropout ratio = 1 - keep_prob = 0.5&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Dropout S =&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S_drop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Original S =&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[[1. 1. 1. 1. 1. 1. 1. 1.]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[3. 3. 3. 3. 3. 3. 3. 3.]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[5. 5. 5. 5. 5. 5. 5. 5.]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Dropout S =&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[[ 2.  0.  0.  2.  0.  0.  2.  0.]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[ 0.  0.  6.  0.  0.  6.  0.  6.]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;[10. 10. 10.  0. 10. 10. 10. 10.]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;我們看到因為dropout 0.5倍，所以輸出值權重乘上2倍，另外一提，Tensorflow的Dropout機制是隨機的，所以Drop out的比例會接近我們想要的比例，但不是絕對剛好。&lt;/p&gt;
&lt;p&gt;那我要怎麼把Dropout放進去我的Model呢？特別注意，我們並不希望已經Training完的Model還有Dropout這一層，所以我在&lt;code&gt;structure&lt;/code&gt;裡頭設計一個&lt;code&gt;train&lt;/code&gt;的開關，當我在Training過程就把它打開，Dropout這一層就會被加進去，「推論」的時候就關閉，保持原有的神經元數量。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

        &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="n"&gt;略&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;

        &lt;span class="c1"&gt;# layer 1&lt;/span&gt;
        &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# layer 2&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDenseLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="n"&gt;略&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Optimizer的選擇&lt;/h3&gt;
&lt;p&gt;我們可以自由的替換我們想要使用的Optimizer。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# define training operation&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AdamOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;在Tensorflow中目前有以下十種Optimizer供我們使用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;tf.train.GradientDescentOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.AdadeltaOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.AdagradOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.AdagradDAOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.MomentumOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.AdamOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.FtrlOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.ProximalGradientDescentOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.ProximalAdagradOptimizer&lt;/li&gt;
&lt;li&gt;tf.train.RMSPropOptimizer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果想要了解每個Optimizer的演算法可以參考&lt;a href="http://ruder.io/optimizing-gradient-descent/"&gt;這篇有詳細的說明&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;來看看程式怎麼寫&lt;/h3&gt;
&lt;p&gt;講了那麼多，來看看完整的程式怎麼寫？照慣例，先畫個流程圖。&lt;/p&gt;
&lt;p&gt;&lt;img alt="DNNLogisticClassification" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.003.jpeg"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DNNLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Building Graph&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their predictions and loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                         &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# regularization loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; \
                &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;l2_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt; \
                &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()])&lt;/span&gt;

            &lt;span class="c1"&gt;# total loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;

            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                 &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                 &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                                 &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_original_loss&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;regularization&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# build neurel network structure and return their predictions and loss&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="c1"&gt;# layer 1&lt;/span&gt;
        &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                   &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;fc1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keep_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# layer 2&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fc1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                 &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# fully connected layer&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;9000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;# mini-batch gradient descent&lt;/span&gt;
            &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
            &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;index_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="n"&gt;batch_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_size&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

                &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:],&lt;/span&gt;
                    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;batch_index&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                &lt;span class="p"&gt;}&lt;/span&gt;
                &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

                &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%9.4f&lt;/span&gt;&lt;span class="s1"&gt;     &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="se"&gt;\r&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# evaluate at the end of this epoch&lt;/span&gt;
            &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;[&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;] loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;val_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;val_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, val_loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, val_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;test_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                       &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MNIST_data/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;
&lt;span class="n"&gt;valid_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DNNLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_hidden&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dropout_ratio&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;batch_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/ 3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.4948, acc = 88.12%, val_loss =   0.5729, val_acc = 88.14%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/ 3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.3343, acc = 91.21%, val_loss =   0.3831, val_acc = 91.04%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/ 3&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;55000/55000&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   0.2890, acc = 92.73%, val_loss =   0.3708, val_acc = 92.06%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_acc = 91.17%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;跟上次的結果比，你會發現有長足的進步，精確率來到90幾，大家可以&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/02_DNN_classification_on_MNIST.py"&gt;下載程式碼&lt;/a&gt;，試著調整參數使得DNN Model的精確率可以更高，參數包含：
* Hidden Layer的神經元數量
* 不同的Activation Function
* 不同的Batch Size
* 調整Weight Regularization的比例
* 調整Dropout Ratio
* 選擇不同Optimizer
* 使得DNN更深&lt;/p&gt;
&lt;p&gt;調整Model是重要的工作，試著自己動手做做看，你可以讓你的Model有多準呢？&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>實作Tensorflow (1)：Simple Logistic Classification on MNIST</title><link href="https://ycc.idv.tw/tensorflow-tutorial_1.html" rel="alternate"></link><published>2017-10-23T12:00:00+08:00</published><updated>2017-10-23T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-10-23:/tensorflow-tutorial_1.html</id><summary type="html">&lt;p&gt;MNIST Dataset / Softmax / Cross-Entropy Loss / 分離數據的重要性 / Tensorflow工作流程 / Tensorflow的基本「張量」元素 / Session的操作 / 第一個Tensorflow Model&lt;/p&gt;</summary><content type="html">&lt;p&gt;初次學習Tensorflow最困難的地方莫過於不知道從何下手，已經學會很多的Deep Learning理論，但是要自己使用Tensorflow將Network建起來卻是非常困難的，這篇文章我會先簡單的介紹幾個Tensorflow的概念，最後利用這些概念建立一個簡單的分類模型。&lt;/p&gt;
&lt;p&gt;本單元程式碼可於&lt;a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/01_simple_logistic_classification_on_MNIST.py"&gt;Github&lt;/a&gt;下載。&lt;/p&gt;
&lt;p&gt;首先，先&lt;code&gt;import&lt;/code&gt;一些會用到的function&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tf&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_verbosity&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Config the matplotlib backend as plotting inline in IPython&lt;/span&gt;
&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="n"&gt;matplotlib&lt;/span&gt; &lt;span class="n"&gt;inline&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;MNIST Dataset&lt;/h3&gt;
&lt;p&gt;定義&lt;code&gt;summary&lt;/code&gt; function以便於觀察ndarray。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* shape: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* min: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* max: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* avg: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* std: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;* unique: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;ndarray是numpy的基本元素，它非常便於我們做矩陣的運算。&lt;/p&gt;
&lt;p&gt;我們使用MNIST Dataset來當作我們練習的標的，MNIST包含一包手寫數字的圖片，每張圖片大小為28x28，每一張圖片都是一個手寫的阿拉伯數字包含0到9，並且標記上它所對應的數字。我們的目標就是要利用MNIST做到手寫數字辨識。&lt;/p&gt;
&lt;p&gt;在Tensorflow你可以很簡單的得到「處理過後的」MNIST，只要利用以下程式碼，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;
&lt;span class="n"&gt;mnist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_data_sets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;MNIST_data/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;one_hot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;
&lt;span class="n"&gt;valid_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/train-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-images-idx3-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Extracting MNIST_data/t10k-labels-idx1-ubyte.gz&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;每個&lt;code&gt;train_data&lt;/code&gt;、&lt;code&gt;valid_data&lt;/code&gt;、&lt;code&gt;test_data&lt;/code&gt;都包含兩部分：圖片和標籤。&lt;/p&gt;
&lt;p&gt;我們來看一下圖片的部分，&lt;code&gt;train_data.images&lt;/code&gt;一共有55000張圖，每一張圖原本大小是28x28，不過特別注意這裡的Data已經先做過預先處理了，因此圖片已經被打平成28x28=784的一維矩陣了，另外每個Pixel的值也先做過「Normalization」了，通常會這樣處理，每個值減去128再除以128，所以你可以從以下的&lt;code&gt;summary&lt;/code&gt;中看到它的最大最小值落在0到1之間，還有這個Dataset也已經做過亂數重排了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;...&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* shape&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;(55000, 784)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* min&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* max&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;1.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* avg&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.13070042431354523&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.30815958976745605&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* unique&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0.         0.00392157 0.00784314 0.01176471 0.01568628 0.01960784&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.02352941 0.02745098 0.03137255 0.03529412 0.03921569 0.04313726&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.04705883 0.0509804  0.05490196 0.05882353 0.0627451  0.06666667&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.07058824 0.07450981 0.07843138 0.08235294 0.08627451 0.09019608&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.09411766 0.09803922 0.10196079 0.10588236 0.10980393 0.1137255&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.11764707 0.12156864 0.1254902  0.12941177 0.13333334 0.13725491&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.14117648 0.14509805 0.14901961 0.15294118 0.15686275 0.16078432&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.16470589 0.16862746 0.17254902 0.1764706  0.18039216 0.18431373&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.18823531 0.19215688 0.19607845 0.20000002 0.20392159 0.20784315&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.21176472 0.21568629 0.21960786 0.22352943 0.227451   0.23137257&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.23529413 0.2392157  0.24313727 0.24705884 0.2509804  0.25490198&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.25882354 0.2627451  0.26666668 0.27058825 0.27450982 0.2784314&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.28235295 0.28627452 0.2901961  0.29411766 0.29803923 0.3019608&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.30588236 0.30980393 0.3137255  0.31764707 0.32156864 0.3254902&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.32941177 0.33333334 0.3372549  0.34117648 0.34509805 0.34901962&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.3529412  0.35686275 0.36078432 0.3647059  0.36862746 0.37254903&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.37647063 0.3803922  0.38431376 0.38823533 0.3921569  0.39607847&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.40000004 0.4039216  0.40784317 0.41176474 0.4156863  0.41960788&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.42352945 0.427451   0.43137258 0.43529415 0.43921572 0.4431373&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.44705886 0.45098042 0.454902   0.45882356 0.46274513 0.4666667&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.47058827 0.47450984 0.4784314  0.48235297 0.48627454 0.4901961&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.49411768 0.49803925 0.5019608  0.5058824  0.50980395 0.5137255&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.5176471  0.52156866 0.5254902  0.5294118  0.53333336 0.5372549&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.5411765  0.54509807 0.54901963 0.5529412  0.5568628  0.56078434&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.5647059  0.5686275  0.57254905 0.5764706  0.5803922  0.58431375&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.5882353  0.5921569  0.59607846 0.6        0.6039216  0.60784316&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.6117647  0.6156863  0.61960787 0.62352943 0.627451   0.6313726&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.63529414 0.6392157  0.6431373  0.64705884 0.6509804  0.654902&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.65882355 0.6627451  0.6666667  0.67058825 0.6745098  0.6784314&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.68235296 0.6862745  0.6901961  0.69411767 0.69803923 0.7019608&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.7058824  0.70980394 0.7137255  0.7176471  0.72156864 0.7254902&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.7294118  0.73333335 0.7372549  0.7411765  0.74509805 0.7490196&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.75294125 0.7568628  0.7607844  0.76470596 0.7686275  0.7725491&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.77647066 0.7803922  0.7843138  0.78823537 0.79215693 0.7960785&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.8000001  0.80392164 0.8078432  0.8117648  0.81568635 0.8196079&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.8235295  0.82745105 0.8313726  0.8352942  0.83921576 0.8431373&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.8470589  0.85098046 0.854902   0.8588236  0.86274517 0.86666673&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.8705883  0.8745099  0.87843144 0.882353   0.8862746  0.89019614&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.8941177  0.8980393  0.90196085 0.9058824  0.909804   0.91372555&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.9176471  0.9215687  0.92549026 0.9294118  0.9333334  0.93725497&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.94117653 0.9450981  0.9490197  0.95294124 0.9568628  0.9607844&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.96470594 0.9686275  0.9725491  0.97647065 0.9803922  0.9843138&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;0.98823535 0.9921569  0.9960785  1.&lt;/span&gt;&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;來試著畫圖來看看，我們使用ndarray的index功能來選出第10張圖片，&lt;code&gt;train_data.images[10,:]&lt;/code&gt;表示的是選第一軸的第10個和第二軸的全部。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_fatten_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ndarr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;plot_fatten_img&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,:])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0AB4Ii/EBQhB8I6v8A+Md7QMI5IyUAAAAASUVORK5CYII=%0A"&gt;&lt;/p&gt;
&lt;p&gt;很顯而易見的，這是一個0。&lt;/p&gt;
&lt;p&gt;接下來來看標籤的部分，&lt;code&gt;train_data.labels&lt;/code&gt;不意外的一樣的也是有相應的55000筆資料，所對應的就是前面的每一張圖片，總共有10種類型:0到9，所以大小為(55000, 10)。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;[[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 1. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;...&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 0. 0. ... 0. 1. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* shape&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;(55000, 10)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* min&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* max&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;1.0&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* avg&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.1&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* std&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;0.30000000000000004&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;* unique&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0. 1.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;所以我們來看看上面那張圖片的標籤，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;1. 0. 0. 0. 0. 0. 0. 0. 0. 0.&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;看起來的確沒錯，在0的位置標示1.，而其他地方標示為0.，因此這是一個標示為0的label沒有錯，這種表示方法稱為One-Hot Encoding，它具有機率的涵義，所代表的是有100%的機會落在0的類別上。&lt;/p&gt;
&lt;h3&gt;Softmax&lt;/h3&gt;
&lt;p&gt;通常One-Hot Encoding會搭配Softmax一同服用，最後的Output結果如果是機率分布，那我也需要讓我的Neurel Network可以輸出機率分布。&lt;/p&gt;
&lt;p&gt;&lt;img alt="softmax" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.001.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;通過Softmax這一層，我們就可以將輸出轉變為以「機率」表示。&lt;/p&gt;
&lt;p&gt;我們可以來手刻一個Softmax Function，不過直接套用Tensorflow中函數的也是可以的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# avoid exp function go to too large,&lt;/span&gt;
    &lt;span class="c1"&gt;# pre-reduce before applying exp function&lt;/span&gt;
    &lt;span class="n"&gt;max_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;max_score&lt;/span&gt;

    &lt;span class="n"&gt;exp_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sum_exp_s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;exp_s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;softmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp_s&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sum_exp_s&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;softmax&lt;/span&gt;

&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;0.8360188  0.11314284 0.05083836&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Cross-Entropy Loss&lt;/h3&gt;
&lt;p&gt;一旦我們要處理機率預測的問題，就不可以使用單純的「平方誤差」，而必須使用Cross-Entropy Loss，是這樣計算的：&lt;/p&gt;
&lt;div class="math"&gt;$$
Loss_{cross-entropy} = - \sum_i y_i ln(s_i)
$$&lt;/div&gt;
&lt;p&gt;
其中，&lt;span class="math"&gt;\(y_i\)&lt;/span&gt;為目標Label，&lt;span class="math"&gt;\(s_i\)&lt;/span&gt;為經過Softmax產生的預測值。&lt;/p&gt;
&lt;p&gt;至於如果你想要了解為何需要使用Cross-Entropy Loss？這我在機器學習基石的筆記中已經有提及過，請看&lt;a href="http://www.ycc.idv.tw/YCNote/post/27"&gt;介紹Logistic Regression的部分&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;分離數據的重要性&lt;/h3&gt;
&lt;p&gt;在MNIST Dataset中，你會發現分為Training Dataset、Validation Dataset和Testing Dataset，這樣的作法在Machine Learning中是常見且必要的。&lt;/p&gt;
&lt;p&gt;流程是這樣的，我們會先使用Training Dataset來訓練Model，並且使用Validation Dataset來檢驗Model的好壞，我們會依據Validation Dataset的檢驗調整Model上的參數，試著盡可能的壓低Validation Dataset的Error，記住！在過程中所產生的所有Models都要保留下來，因為最後選擇的Model並不是Validation Dataset的Error最小的，而是要再由Testing Dataset來做最後的挑選，挑選出能使Testing Dataset的Error最小的Model。&lt;/p&gt;
&lt;p&gt;這所有的作法都是為了避免Overfitting的情況發生，也就是機器可能因為看過一筆Data，結果就把這筆Data給完整記了起來，而Data本身含有雜訊，雜訊就這樣滲透到Model裡，確實做到分離是很重要的，讓Model在測試階段時可以使用沒有看過的Data。&lt;/p&gt;
&lt;p&gt;因此，Validation Dataset的分離是為了避免讓Model在Training階段看到要驗證的資料，所以更能正確的評估Model的好壞。但這樣是不夠的，人為會根據Validation Dataset來調整Model，這樣無形之中已經將Validation Dataset的資訊間接的經由人傳給了Model，所以還是沒有徹底分離，因此在最後挑選Models時，我們會使用另外一筆從沒看過的資料Testing Dataset來做挑選，一旦挑選完就不能再去調整任何參數了。&lt;/p&gt;
&lt;h3&gt;Tensorflow工作流程&lt;/h3&gt;
&lt;p&gt;我們這一篇將會使用Tensorflow實作最簡單的單層Neurel Network，在這之前我們來看看Tensorflow是如何運作的？&lt;/p&gt;
&lt;p&gt;深度學習是由一層一層可以微分的神經元所連接而成，數學上可以表示為張量(Tensor)的表示式，我們一般講的矩陣運算是指2x2的矩陣運算，而張量(Tensor)則是拓寬到n維陣列做計算，在Machine Learning當中我們常常需要處理到相當高維度的計算，例如：有五張28x28的彩色圖的表示就必須使用到四維張量，第一維表示第幾張、第二、三維表示圖片的大小、第四維則表示RGB，如果你是物理系的學生應該也對張量不陌生，廣義相對論裡頭大量的使用四維張量運算，三維空間加一維時間。&lt;/p&gt;
&lt;p&gt;而在做Neurel Network時，我們會根據需求不同設計不同形式但合理的流程(Flow)，再使用數據來訓練我的Model。所以，這就是Tensorflow命名由來：Tensor+Flow。&lt;/p&gt;
&lt;p&gt;因此，一開始要先設計Model的結構，這在Tensorflow裡頭稱為Graph，Graph的作用是事先決定Neurel Network的結構，決定Neuron要怎麼連接？決定哪一些窗口是可以由外部置放數據的？決定哪一些變數是可以被訓練的？哪一些變數是不可以被訓練的？定義將要怎麼樣優化這個系統？...等等。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;my_graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# Initialize a new graph&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;my_graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt; &lt;span class="c1"&gt;# Create a scope to build graph&lt;/span&gt;
    &lt;span class="c1"&gt;# ...&lt;/span&gt;
    &lt;span class="c1"&gt;# detail of building graph&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Graph只是一個結構，它不具有有效的資訊，而當我們定義完成Graph之後，接下來我們需要創造一個環境叫做Session，Session會將Graph的結構複製一份，然後再放入資訊進行Training或是預測等等，因此Session是具有有效資訊的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;my_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="c1"&gt;# Copy graph into session&lt;/span&gt;
    &lt;span class="c1"&gt;# ...&lt;/span&gt;
    &lt;span class="c1"&gt;# detail of doing machine learning  &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;還有另外一種寫法也是相同作用的，我個人比較喜歡下面這種寫法。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;my_session&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;my_graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;my_session&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Tensorflow的基本「張量」元素&lt;/h3&gt;
&lt;p&gt;接下來我們就來看看有哪些構成Graph的基本元素可以使用。&lt;/p&gt;
&lt;p&gt;(1) 常數張量：&lt;/p&gt;
&lt;p&gt;一開始來看看「常數張量」，常數指的是在Model中不會改變的數值。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;(2) 變數張量：&lt;/p&gt;
&lt;p&gt;與常數截然不同的就是變數，「變數張量」是指在訓練當中可以改變的值，一般「變數張量」會用作於Machine Learning需要被訓練的參數，如果你沒有特別設定，在最佳化的過程中，Tensorflow會自動調整「變數張量」的數值來最佳化。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;因為變數通常是未知且待優化的參數，所以我們一般會使用Initalizer來設定它的初始值，&lt;code&gt;tf.truncated_normal(shape=(3,5))&lt;/code&gt;會隨機產生大小3x5的矩陣，它的值呈常態分佈但只取兩個標準差以內的數值。&lt;/p&gt;
&lt;p&gt;如果今天你想要有一個「變數張量」但是又不希望它因為最佳化而改變，這時你要特別指定&lt;code&gt;trainable&lt;/code&gt;為&lt;code&gt;False&lt;/code&gt;。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainable&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;(3) 置放張量：&lt;/p&gt;
&lt;p&gt;另外有一些張量負責擔任輸入窗口的角色，稱為Placeholder。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;因為我們在訓練之前還尚未知道Data的數量，所以這裡使用None來表示未知。&lt;code&gt;tf.placeholder&lt;/code&gt;在Graph階段是沒有數值的，必須等到Session階段才將數值給輸入進去。&lt;/p&gt;
&lt;p&gt;(4) 操作型張量：&lt;/p&gt;
&lt;p&gt;這類張量並不含有實際數值，而是一種操作，常用的「操作型張量」有兩種，第一種是作為最佳化使用，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;選擇Optimizer和最佳化的方式來定義最佳化的操作方法，上述的例子是使用learning_rate為0.5的Gradient Descent來降低loss。&lt;/p&gt;
&lt;p&gt;另外一種是初始化的操作，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;這一個步驟是必要的但常常被忽略，還記得剛剛我們定義「變數張量」時有用到Initalizer，這些Initalizer在Graph完成時還不具有數值，必須使用&lt;code&gt;init_op&lt;/code&gt;來給予數值，所以記住一定要放&lt;code&gt;init_op&lt;/code&gt;進去Graph裡頭，而且必須先定義完成所有會用到的Initalizer再來設定這個&lt;code&gt;init_op&lt;/code&gt;。&lt;/p&gt;
&lt;h3&gt;Session的操作&lt;/h3&gt;
&lt;p&gt;「張量」元素具有兩個面向：功能和數值，在Graph階段「張量」只具有功能但不具有數值，只有到了Session階段才開始有數值，那如何將這些數值取出來呢？有兩種方法，以1+1當作範例來看看，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;g1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;g1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# add x and y&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;g1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sol&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# print tensor, not their value&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Tensor(&amp;quot;Add:0&amp;quot;, shape=(), dtype=int32)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;g1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; 
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="c1"&gt;# evaluate their value&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;2&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;g1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sol&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# another way of evaluating value&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;2&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;那如果我想使用placeholder來做到x+y呢？&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;g2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;g2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sol&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# add x and y&lt;/span&gt;

&lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;g2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# if x = 2 and y = 3&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sol&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;5&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# if x = 5 and y = 7&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sol&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;12&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;因為x和y是placeholder，所以必須使用&lt;code&gt;feed_dict&lt;/code&gt;來餵入相關資訊，否則會報錯。&lt;/p&gt;
&lt;h3&gt;第一個Tensorflow Model&lt;/h3&gt;
&lt;p&gt;有了以上的認識我們就可以來建立我們第一個Model。&lt;/p&gt;
&lt;p&gt;以下我會使用物件導向的寫法，讓程式碼更有條理。&lt;/p&gt;
&lt;p&gt;Machine Learning在操作上可以整理成三個大步驟：建構(Building)、訓練(Fitting)和推論(Inference)，所以我們將會使用這三大步驟來建製我們的Model。&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;SimpleLogisticClassification&lt;/code&gt;裡頭，「建構」的動作在&lt;code&gt;__init__&lt;/code&gt;中會進行，由&lt;code&gt;build&lt;/code&gt;函式來建立Graph，其中我將Neurel Network的結構分離存於&lt;code&gt;structure&lt;/code&gt;裡。「訓練」的動作在&lt;code&gt;fit&lt;/code&gt;中進行，這裡採用傳統的Gradient Descent的方法，將所有Data全部考慮進去最佳化，未來會再介紹Batch Gradient Descent。最後，「推論」的部分在&lt;code&gt;predict&lt;/code&gt;和&lt;code&gt;evaluate&lt;/code&gt;中進行。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SimpleLogisticClassification&lt;/code&gt;將會建構一個只有一層的Neurel Network，也就是說沒有Hidden Layer，畫個圖。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Simple Logistic Classification" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.002.jpeg"&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SimpleLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# initialize new graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# building graph&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# create session by the graph&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Building Graph&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;as_default&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="c1"&gt;### Input&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;### Optimalization&lt;/span&gt;
            &lt;span class="c1"&gt;# build neurel network structure and get their predictions and loss&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# define training operation&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;minimize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Prediction&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;placeholder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                                        &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;### Initialization&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;global_variables_initializer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;structure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# build neurel network structure and return their predictions and loss&lt;/span&gt;
        &lt;span class="c1"&gt;### Variable&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;truncated_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Variable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

        &lt;span class="c1"&gt;### Structure&lt;/span&gt;
        &lt;span class="c1"&gt;# one fully connected layer&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;fc1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="c1"&gt;# predictions&lt;/span&gt;
        &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# loss: softmax cross entropy&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reduce_mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                 &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_dense_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# fully connected layer&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init_op&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Epoch &lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;: &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;# fully gradient descent&lt;/span&gt;
            &lt;span class="n"&gt;feed_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_op&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="c1"&gt;# evaluate at the end of this epoch&lt;/span&gt;
            &lt;span class="n"&gt;y_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;train_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39; loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;val_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;val_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
                &lt;span class="n"&gt;msg&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;, val_loss = &lt;/span&gt;&lt;span class="si"&gt;%8.4f&lt;/span&gt;&lt;span class="s1"&gt;, val_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;msg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;test_acc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;test_acc = &lt;/span&gt;&lt;span class="si"&gt;%3.2f%%&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_acc&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_y_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feed_dict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;_check_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;ndarray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ndarray&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleLogisticClassification&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;valid_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;Epoch  1/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   9.2515, acc = 12.81%, val_loss =   9.4888, val_acc = 11.92%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  2/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   8.2946, acc = 13.89%, val_loss =   8.5156, val_acc = 13.10%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  3/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   7.5609, acc = 15.92%, val_loss =   7.7680, val_acc = 15.02%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  4/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   6.9563, acc = 18.31%, val_loss =   7.1521, val_acc = 17.44%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  5/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   6.4402, acc = 20.94%, val_loss =   6.6249, val_acc = 19.80%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  6/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   5.9915, acc = 23.35%, val_loss =   6.1650, val_acc = 22.38%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  7/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   5.5971, acc = 25.79%, val_loss =   5.7596, val_acc = 24.98%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  8/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   5.2479, acc = 28.18%, val_loss =   5.4001, val_acc = 27.30%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch  9/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   4.9376, acc = 30.46%, val_loss =   5.0803, val_acc = 29.86%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="nt"&gt;Epoch 10/10&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;loss =   4.6608, acc = 32.71%, val_loss =   4.7947, val_acc = 32.20%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="l l-Scalar l-Scalar-Plain"&gt;test_acc = 33.58%&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="Tensorflow"></category></entry><entry><title>如何辨別機器學習模型的好壞？秒懂Confusion Matrix</title><link href="https://ycc.idv.tw/confusion-matrix.html" rel="alternate"></link><published>2017-08-04T12:00:00+08:00</published><updated>2017-08-04T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-08-04:/confusion-matrix.html</id><summary type="html">&lt;p&gt;本篇介紹包含Confusion Matrix, True Positive, False Negative, False Positive, True Negative, Type I Error, Type II Error, Prevalence, Accuracy, Precision, Recall, F1 Measure, F Measure, Sensitivity, Specificity, ROC Curve, AUC, TPR, FNR, FPR, TNR, FDR, FOR, PPV, NPV, 算數平均, 幾何平均, 調和平均&lt;/p&gt;</summary><content type="html">&lt;p&gt;有時要鑑別一個模型的好或壞，並不能簡單的看出來，所以我們需要用一些指標去判定它的好壞，也作為我們挑選模型的依據。如果你稍微查一下有哪些指標，你就會發現指標多到讓人家眼花撩亂，一堆名詞就攤在那邊，讓人無從下手。&lt;/p&gt;
&lt;p&gt;有一種分類問題常用的指標稱之為Confusion Matrix，這個命名很有趣，這個表格的確是很讓人感到很困惑啊！至少在看完這篇之前。Confusion Matrix是用於分類問題的一種常用的指標，它衍生很多不同的指標，下面這張圖我將Confusion Matrix畫出來，並把一些比較重要的衍生指標給標出來。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.001" src="http://www.ycc.idv.tw/media/mechine_learning_measure/mechine_learning_measure.001.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;我猜想，你一定看得很模糊吧！沒關係我在這篇文章中會帶大家認識這個圖裡的各個名詞。&lt;/p&gt;
&lt;p&gt;一開始我們從下面這個表格開始講起，這個表格就是所謂的Confusion Matrix，前面的True和False代表預測本身的結果是正確還是不正確的，而後面的Positive和Negative則是代表預測的方向是正向還是負向的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.002" src="http://www.ycc.idv.tw/media/mechine_learning_measure/mechine_learning_measure.002.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;舉iphone當例子，iphone具有指紋識別解鎖系統，假如iphone判定這個指紋是屬於使用者的，它就會解鎖，所以今天如果你按壓了，而iphone也順利解鎖了，那這種情形就屬於左上角的情況，稱為True Positive，也就是「正確的正向預測」，如果不幸的你按壓iphone，結果iphone認不得你的指紋，這就是左下角的情況，稱為False Negative，也就是「錯誤的負向預測」，接下來找你朋友一起來測試，正常情形下你朋友的指紋應該沒辦法讓iphone解鎖，這是右下角的情況，稱為True Negative，也就是「正確的負向預測」，如果令人意外的是你的朋友把你的手機解鎖了，那你最好改成用密碼鎖...，這種情況就是右上角的狀況，稱為False Positive，也就是「錯誤的正向預測」。&lt;/p&gt;
&lt;p&gt;從上面的描述，我們當然希望我們的模型True Positive和True Negative都可以多多出現，而False Positive和False Negative可以盡量不要出現，因此這兩種狀況就稱之為Error，又各自又命名為Type I Error和Type II Error，這兩種錯誤，錯的很不一樣，如果今天指紋辨識不是放在iphone，而是放在你家大門鎖上，那你最不希望發生哪類錯誤？當然是Type I Error，也就是False Positive，此時機器會把陌生人當成主人的開門，這是我們不想看到的，我們寧可被關在門外（Type II Error）！但如果今天這個辨別系統是用在Google廣告，Google Ad會預測一個產品的潛在客戶，並做廣告投放，這個時候反而是較不希望Type II Error發生，也就是False Negative，這叫做寧可錯殺一百個也不要放過一個潛在客戶。所以下次在訓練你的模型時想清楚你不想要Type I Error還是Type II Error （鰲拜：我全都要...），並且用一些方法來放掉另一種錯誤，來降低這個我們不希望發生的錯誤。&lt;/p&gt;
&lt;p&gt;Confusion Matrix還有衍生很多形形色色的指標，我接下來就一一的介紹。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我們把所有正確的情況，也就是True Positive和True Negative，把它加總起來除上所有情形個數，那就是Accuracy，這也是最常用的指標，但是在某些情形下這個指標會失效&lt;/strong&gt;，如果今天實際正向的例子很少，譬如有一個信用卡盜刷偵測機器人，看了一個月的信用卡紀錄，其中真正是盜刷的資料筆數是相當少的，那我只要簡單一步來設計我的模型就可以使它Accuracy達到99%以上，你猜到了嗎？那就是通通預測沒有盜刷的情況發生，所以顯然我們需要別種指標來應對這種情況。&lt;/p&gt;
&lt;p&gt;Precision（準確率）和Recall（召回率）這個時候就派上用場了，Precision和Recall同時關注的都是True Positive（都在分子），但是角度不一樣，&lt;strong&gt;Precision看的是在預測正向的情形下，實際的「精準度」是多少，而Recall則是看在實際情形為正向的狀況下，預測「能召回多少」實際正向的答案&lt;/strong&gt;。一樣的，如果是門禁系統，我們希望Precision可以很高，Recall就相較比較不重要，我們比較在意的是預測正向（開門）的答對多少，比較不在意實際正向（是主人）的答對多少。如果是廣告投放，則Recall很重要，Precision就顯得沒這麼重要了，因為此時我們比較在意的是實際正向（是潛在客戶）的答對多少，而相對比較不在意預測正向（廣告投出）答對多少。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Precision和Recall都不去考慮True Negative&lt;/strong&gt;，因為通常True Negative會是答對的Null Hypothesis，簡單講就是最無聊的正確結果。在門禁的解鎖問題就是陌生人按壓且門不開；在廣告投放的例子中就是廣告不投，結果那個人也不是潛在客戶：在信用卡盜刷的例子，機器人認為正常的刷卡紀錄，其實也正是正常的。在通常的命題之下，實際是正向的結果是比負向少的，理所當然預測正向的結果也要比負向少，所以True Negative通常是量最多的，也是最無聊的。&lt;/p&gt;
&lt;p&gt;補充：Null Hypothesis通常代表比較常見的情況，在統計上我們要驗證某種概念成立，我們通常會假設一個最普通的Null Hypothesis當作正常情況，然後嘗試著利用實驗數據去否定這個Null Hypothesis，舉例：你要證明一種藥物是有效的，那你要先假設一個Null Hypothesis，譬如說給患者吃個安慰劑（可能是一顆糖果），你的藥要有辦法和Null Hypothesis產生顯著的差異，你才能證明你的藥是有效的。所以用在機器學習的例子當中，通常會把最普通的情況當作Negative，也就是當作Null Hypothesis來看待。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.003" src="http://www.ycc.idv.tw/media/mechine_learning_measure/mechine_learning_measure.003.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;如果今天我覺得Precision和Recall都同等重要，我想要用一個指標來統合標誌它，這就是F1 Score或稱F1 Measure，它是F Measure的一個特例，當belta=1時就是F1 Measure，代表Precision和Recall都同等重要，那如果我希望多看中一點Precision，那&lt;span class="math"&gt;\(belta\)&lt;/span&gt;就可以選擇小一點，當&lt;span class="math"&gt;\(belta=0\)&lt;/span&gt;時，F Measure就是Precision；如果我希望多看中一點Recall，那belta就可以選擇大一點，當belta無限大時，F Measure就是Recall。&lt;/p&gt;
&lt;p&gt;如果你仔細看F1 Measure，你會發現它的平均方法是「調和平均」，帶大家go-through三種平均方法，你就能明白為什麼要使用調和平均了。下圖列出了三種平均方法的使用時機，我們要去了解資料或數列的特性，我們才能知道要採取哪種平均方法較為恰當，大多情況算數平均都可以使用，因為我們都假設有線性關係存在，譬如說平均距離；幾何平均常用於人口計算，因為人口增加是成比例增加的；調和平均常用於計算平均速率，在固定距離下，所花時間就是平均速率，這數據成倒數關係，&lt;strong&gt;而F1 Measure也同樣是這樣的數據特性，在固定TP的情況下，有不同的分母，所以這裡使用調和平均較為適當&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.004" src="http://www.ycc.idv.tw/media/mechine_learning_measure/mechine_learning_measure.004.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;下圖的名詞看一下有印象就好。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.005" src="http://www.ycc.idv.tw/media/mechine_learning_measure/mechine_learning_measure.005.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;最後這頁來講一下醫學上常用的指標，首先是Prevalence（盛行率），如果以人口當作所有的樣本，實際得病的患者所佔的比例就代表這個病的盛行情況。&lt;/p&gt;
&lt;p&gt;如果今天有一個診斷方法可以判定病人是否有得此病，有兩個指標可以看，那就是Sensitivity和Specificity，Sensitivity就是Recall，它代表的是診斷方法是否夠靈敏可以將真正得病的人診斷出來，其實就是真正有病症的患者有多少可以被偵測出來，而Specificity則代表實際沒病症的人有多少被檢驗正確的。兩種指標都是越高越好。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mechine_learning_measure.006" src="http://www.ycc.idv.tw/media/mechine_learning_measure/mechine_learning_measure.006.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;通常在醫學上，會通過一些閥值來斷定病人是否有得此病，而這個閥值就會影響Sensitivity和Specificity，這個不同閥值Sensitivity和Specificity的分布情況可以畫成ROC Curve，而ROC Curve底下的面積稱為AUC，AUC越大越好。&lt;/p&gt;
&lt;p&gt;想必這個時候你再回去看第一張圖就更加了解了，有了這些指標，我們就多一把尺來評斷我們的分類模型究竟是做的好還是不好。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category></entry><entry><title>機器學習技法 學習筆記 (7)：Radial Basis Function Network與Matrix Factorization</title><link href="https://ycc.idv.tw/ml-course-techniques_7.html" rel="alternate"></link><published>2017-04-22T12:00:00+08:00</published><updated>2017-04-22T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-04-22:/ml-course-techniques_7.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋Radial Basis Function (RBF) Network、K-Means、One-Hot Encoding和Matrix Factorization&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Radial Basis Function (RBF) Network&lt;/h3&gt;
&lt;p&gt;回顧一下Gaussian Kernel SVM，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W = 𝚺_{n=sv}  α_n y_n Z_n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(G_{SVM}\)&lt;/span&gt;   &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= sign[WZ+b]\)&lt;/span&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= sign{[𝚺_{n=sv} α_n y_n K(X_n,X)]+b}\)&lt;/span&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(⇒ G_{SVM} = sign{[𝚺_{n=sv} α_n y_n exp(-γ |X-X_n|^2)]+b}\)&lt;/span&gt; &lt;br/&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;看到這個式子你想到了什麼？有沒有融會貫通的感覺，你同樣的可以把上面的式子看成是Aggregation，又或者是Network。&lt;/p&gt;
&lt;p&gt;先來定義一下RBF Function， 其實就是Gaussian Function，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RBF Function: &lt;span class="math"&gt;\(RBF(X,X_n)=exp(-γ|X-X_n|^2)\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以我們可以仿造SVM的形式來造一個Network，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(G=Output{[𝚺_m β_m RBF(X,μ_m)]+b}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;當&lt;span class="math"&gt;\(Output\)&lt;/span&gt;為&lt;span class="math"&gt;\(sign\)&lt;/span&gt; Function、&lt;span class="math"&gt;\(β_m\)&lt;/span&gt;為&lt;span class="math"&gt;\(α_n y_n\)&lt;/span&gt;就回到特例SVM了。&lt;/p&gt;
&lt;p&gt;我們來細看這個式子傳遞的概念，RBF Network的第一層是先產生&lt;span class="math"&gt;\(M\)&lt;/span&gt;組&lt;span class="math"&gt;\(RBF(X,μ_m)\)&lt;/span&gt;，意味著以這&lt;span class="math"&gt;\(M\)&lt;/span&gt;個位置&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;當作中心點來評估各個&lt;span class="math"&gt;\(X\)&lt;/span&gt;與它的相似程度，RBF是有評估相似度的味道，越接近&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;的點，RBF越大，並隨著與&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;距離變大，RBF的值也快速遞減，所以這&lt;span class="math"&gt;\(M\)&lt;/span&gt;個&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;是有象徵性的，越接近它你越受它的影響。&lt;/p&gt;
&lt;p&gt;決定了每一筆數據各是受哪些&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;影響，接下來第二層是由這M個代表性的位置來進行投票決定最後的結果，這意味的不同的地方對&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;最後結果也有不同的影響力，舉個例子，假設在SVM裡頭，某個&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;如果它的&lt;span class="math"&gt;\(y_m =+1\)&lt;/span&gt;，那它對最後的影響就會是正的；那如果某個&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;的&lt;span class="math"&gt;\(y_m=-1\)&lt;/span&gt;，那它對最後的影響就會是負的，所以一個點進來，先評估一下它和象徵性的幾個點&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;的距離，如果相鄰幾點都是正的，這個點最後的結果就會是正的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="RBF Network" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_05.png"&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RBF Network在歷史上是Neural Network的一個分支，不過從上面的介紹你就會發現，它們的結構是有差異的，演算法也就不一樣。&lt;/p&gt;
&lt;p&gt;通常最佳化RBF Network做法是這樣的，我們會先用一些方法將&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;決定，如果&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;很懶惰的就直接使用所有的Training Data，總共有&lt;span class="math"&gt;\(N\)&lt;/span&gt;個，這&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;就叫做&lt;strong&gt;「Full RBF Network」&lt;/strong&gt;。&lt;strong&gt;我們也可以使用一些歸納的演算法找出代表資料群體的幾個象徵性的中心點，例如待會會介紹的K-Means的方法&lt;/strong&gt;，找出k個&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;再做計算，這樣的RBF Network稱為&lt;strong&gt;「k Nearest Neighbor RBF Network」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;找到了&lt;span class="math"&gt;\(μ_m\)&lt;/span&gt;就已經決定了所有的RBF Function，接下來就可以線性組合這些RBF Function，我們可以使用Regression的方法來求取&lt;span class="math"&gt;\(β_m\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;而如果你使用「Full RBF Network」，你會發現做完Regression後&lt;span class="math"&gt;\(E_{in}=0\)&lt;/span&gt;，這是典型的Overfitting，那這時你可能就要採用有Regularization的Regression啦！譬如說Ridge Regression之類的。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;K-Means&lt;/h3&gt;
&lt;p&gt;&lt;img alt="K-Means" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_06.png"&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;接下來來看怎麼用K-Means找到代表資料群體的幾個象徵性的中心點。&lt;/p&gt;
&lt;p&gt;首先，先決定要有幾個「中心點」，這裡假設我要有&lt;span class="math"&gt;\(k\)&lt;/span&gt;個好了，接下來先隨機給這些「中心點」一個初始的位置，接下來根據數據的靠近程度開始歸類，如果一筆數據比較所有的「中心點」後發現離「中心點」A是最近的話，那這筆數據就歸「中心點」A了，就用這樣的規則把所有數據都做分類。&lt;/p&gt;
&lt;p&gt;分完類後，接下來平均每一個資料群體裡的數據座標找出新的代表這個群體的「中心點」，然後又拿這個新的「中心點」根據數據的靠近程度再歸類一次，如此循環多次，直到收斂為止。這樣的話，這&lt;span class="math"&gt;\(k\)&lt;/span&gt;個「中心點」收斂後會各自佔據四方，並且代表某個群體的中心點。我們就可以找到代表性的&lt;span class="math"&gt;\(k\)&lt;/span&gt;個點，並拿這些點做「k Nearest Neighbor RBF Network」。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;One-Hot Encoding&lt;/h3&gt;
&lt;p&gt;討論這麼久的ML，我們還沒有討論過假設遇到「類別」要怎麼處理！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;通常遇到類別的狀況，我們還是需要把它轉換成數值或向量來處理，常見的方法叫做One-Hot Encoding。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;舉個例子，如果要描述血型應該要怎麼做？我們可是無法拿字串下去Regression的啊～此時就需要One-Hot Encoding，假設血型有A, B, AB, O四種，我們可以這樣設定，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(A = [1, 0, 0, 0]^T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(B = [0, 1, 0, 0]^T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(AB = [0, 0, 1, 0]^T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(O = [0, 0, 0, 1]^T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;就是這麼簡單，這個動作就叫做One-Hot Encoding。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Matrix Factorization&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;那如果今天我的Input和Output都是類別，而我們想要讓機器自己去找到匹配Input和Output的機制，解決這個問題的方法稱之為Matrix Factorization。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Matrix Factorization精神上有點像是Autoencoder，Autoencoder找出隱含在Data裡的特性，而Matrix Factorization則是找出隱含的匹配關係。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;舉個例子，如果Netflix有了一堆用戶和他們曾看過的電影的資料，我們想要從中抽取出用戶與他愛看的電影之間的關係，所以這不單單只是匹配而已，單純匹配就只需要硬碟就做的到了，我們要做的是找出匹配的規律，並且用更少、更精簡的方式表示這個匹配關係，舉個例子，有可能有部分用戶會被歸納到愛看恐怖片的，並且同時這些客戶會被連結到具有恐怖元素的電影，我們預期Matrix Factorization會有自行歸納整理的能力。&lt;/p&gt;
&lt;p&gt;可以仿造Autoencoder來設計Matrix Factorization，而你會發現Activation Function只要使用線性就已經足夠了，因為對於One-Hot Encoding的類別來說，只有一條通道是有效的，這已經具有開關的味道了，所以我們不用在Activation Function上面再弄一道開關，所以採用Linear就足夠了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Matrix Factorization" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_07.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;因為是線性模型的緣故，我們可以很簡單的使用矩陣來描述，&lt;/p&gt;
&lt;p&gt;Hypothesis: &lt;span class="math"&gt;\(h(X) = W^TVX\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;而如果是某一用戶，則&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(h(X_n) = W^TV_n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;對某個用戶而言與他匹配的電影是一個向量，上面紀錄了他看過的電影，假設我再指定一部電影&lt;span class="math"&gt;\(m\)&lt;/span&gt;，此時&lt;span class="math"&gt;\(W_m^T V_n\)&lt;/span&gt;就代表這個用戶有沒有看過這部電影。&lt;/p&gt;
&lt;p&gt;用這個方法來想問題，假設今天你把用戶和電影填成一個大的表格，或是矩陣，有交集的部分就打個勾，這個矩陣的每個元素表示成&lt;span class="math"&gt;\(r_{nm}\)&lt;/span&gt;，有打勾的部分&lt;span class="math"&gt;\(r_{nm}=1\)&lt;/span&gt;，沒打勾的部分&lt;span class="math"&gt;\(r_{nm}=0\)&lt;/span&gt;，那我們做的轉換W和V最終就是為了讓&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W_m^T V_n ≈r_{nm}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;為了評估匹配的好壞，我們定義Error Function為&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(E_{in}(\{W_m\},\{V_n\}) = (1/𝚺_m |D_m|)×𝚺_{n,m} (r_{nm}-W_{m}^TV_n)^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;最佳化Matrix Factorization有兩個演算法，一個是Alternating Least Squares，另外一個是SGD。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Alternating Least Squares for Matrix Factorization&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Alternating Least Squares for Matrix Factorization" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_08.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第一個方法是利用Linear Regression交互的優化&lt;span class="math"&gt;\(W_m\)&lt;/span&gt;和&lt;span class="math"&gt;\(V_n\)&lt;/span&gt;，我們的目標是使得&lt;span class="math"&gt;\(W_m^T V_n =r_{nm}\)&lt;/span&gt;，這式子可以用兩個角度看，如果固定&lt;span class="math"&gt;\(W_m\)&lt;/span&gt;，優化&lt;span class="math"&gt;\(V_n\)&lt;/span&gt;，那就是線性擬合&lt;span class="math"&gt;\(\{V_n, r_{nm}\}\)&lt;/span&gt;的問題；那如果固定&lt;span class="math"&gt;\(V_n\)&lt;/span&gt;，優化&lt;span class="math"&gt;\(W_m\)&lt;/span&gt;，這就是線性擬合&lt;span class="math"&gt;\(\{W_{m}, r_{nm}\}\)&lt;/span&gt;的問題。&lt;strong&gt;因此，交替優化&lt;span class="math"&gt;\(W_m\)&lt;/span&gt;和&lt;span class="math"&gt;\(V_n\)&lt;/span&gt;就可以使得&lt;span class="math"&gt;\(W_m^T V_n\)&lt;/span&gt;越來越接近&lt;span class="math"&gt;\(r_{nm}\)&lt;/span&gt;了&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;SGD for Matrix Factorization&lt;/h3&gt;
&lt;p&gt;&lt;img alt="SGD for Matrix Factorization" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_09.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第二個方法則是老招—Gradient Descent，這裡採用隨機的版本SGD，所以過程中我們會隨意的從&lt;span class="math"&gt;\((n,m)\)&lt;/span&gt;中挑點，然後根據Error Measure&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{in}(\{W_m\},\{V_n\}) = (1/𝚺_m |D_m|) \times 𝚺_{n,m} (r_{nm}-W_{m}^T V_n )^2
$$&lt;/div&gt;
&lt;p&gt;
我們就可以得到更新&lt;span class="math"&gt;\(W_m\)&lt;/span&gt;和&lt;span class="math"&gt;\(V_n\)&lt;/span&gt;的方法，詳細的方法見上圖所示。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;目前，SGD方法是處理大型Matrix Factorization最流行的作法。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;本篇介紹類似Neural Network的兩種Network結構，分別為Radial Basis Function (RBF) Network和Matrix Factorization。&lt;/p&gt;
&lt;p&gt;在做RBF Network時，我們先找出幾個代表的中心，並評估一筆資料與這些中心的距離，再來再考慮不同中心對於答案的貢獻，加總起來可以預測這筆資料的答案，我們可以使用K-Means的方法來找出k點代表性的中心點來做RBF Network。&lt;/p&gt;
&lt;p&gt;Matrix Factorization和Autoencoder有點類似，Autoencoder目標在於找出隱含在Data裡的特性，而Matrix Factorization則是找出隱含的匹配關係，並且介紹了兩種Matrix Factorization的演算法：Alternating Least Squares和SGD方法。&lt;/p&gt;
&lt;p&gt;這系列的介紹文章，到這裡算是走到尾聲了，最後跟大家推薦一下老師的最後一堂課的投影片：&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/216_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/216_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;這個投影片裡頭林軒田教授用心的彙整了一整個學期的內容，很值得一看。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)</title><link href="https://ycc.idv.tw/ml-course-techniques_6.html" rel="alternate"></link><published>2017-04-17T12:00:00+08:00</published><updated>2017-04-17T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-04-17:/ml-course-techniques_6.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋神經網路(Neural Network, NN)、深度學習(Deep Learning, DL)、反向傳播算法(Backpropagation, BP)、Weight-elimination Regularizer、Early Stop、Autoencoder、Principal Component Analysis (PCA)&lt;/p&gt;</summary><content type="html">&lt;h3&gt;神經網路(Neural Network)&lt;/h3&gt;
&lt;p&gt;最後一個主題，我們要來講第三種「特徵轉換」— Extraction Models，其實就是現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，包括下圍棋的AlphaGo、Tesla的自動駕駛都是採用這一類的Machine Learning。&lt;/p&gt;
&lt;p&gt;Extraction Models的基本款就是廣為人知的「神經網路」(Neural Network)，它的特色是使用神經元來做非線性的特徵轉換，那如果具有多層神經元，就是做了多次的非線性特徵轉換，這就是所謂的「深度學習」(Deep Learning)。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Neural Network" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.016.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;上圖左側就是具有一層神經元的Neural Network，首先我們有一組特徵&lt;span class="math"&gt;\(X\)&lt;/span&gt;，通常我們會加入一個維度&lt;span class="math"&gt;\(X_{0}=1\)&lt;/span&gt;，這是為了可以讓結構變得更好看，未來可以與&lt;span class="math"&gt;\(W_{0}\)&lt;/span&gt;相乘產生常數項。使用&lt;span class="math"&gt;\(W\)&lt;/span&gt;來給予特徵&lt;span class="math"&gt;\(X\)&lt;/span&gt;權重，最後總和的結果稱之為Score，&lt;span class="math"&gt;\(s = W_{0}X_{0}+𝚺_{i=1}W_{i}X_{i} = 𝚺_{i=0}W_{i}X_{i}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;這個Score會被輸入到一個Activation Function裡頭，&lt;strong&gt;Activation Function的用意就是開關&lt;/strong&gt;，當Score大於某個閥值，就打通線路讓這條路的貢獻可以繼續向後傳遞；當Score小於某個閥值，就關閉線路，所以Activation Function可以是Binary Function，但在實際操作之下不會使用像Binary Function這類不可以微分的Activation Function，所以我們會找具有相似特性但又可以微分的函數，例如&lt;span class="math"&gt;\(tanh\)&lt;/span&gt;或者是&lt;span class="math"&gt;\(ReLU\)&lt;/span&gt;這類比較接近開關效果的函數，經過Activation Function轉換後的輸出表示成&lt;span class="math"&gt;\(g_{t} = σ(𝚺_{i}W_{i}X_{i})\)&lt;/span&gt;，這個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;就稱為神經元、&lt;span class="math"&gt;\(σ\)&lt;/span&gt;為Activation Function、&lt;span class="math"&gt;\(𝚺_{i} W_{i}X_{i}\)&lt;/span&gt;是Score。&lt;/p&gt;
&lt;p&gt;如果我們有多組權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;就能產生多組神經元&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，然後最後把&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;做線性組合並使用Output Function &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt;來衡量出最後的答案，Output Function可以是Linear Classification的Binary Function &lt;span class="math"&gt;\(h(x)=sign(x)\)&lt;/span&gt;，不過一樣的問題，它不可以微分，通常不會被使用，常見的是使用Linear Regression &lt;span class="math"&gt;\(h(x)=x\)&lt;/span&gt;，或者Logistic Regression &lt;span class="math"&gt;\(h(x)=Θ(x)\)&lt;/span&gt;來當作Output Function，最後的結果可以表示成 &lt;span class="math"&gt;\(y=h(𝚺_{t}α_{t}g_{t})\)&lt;/span&gt;，看到這個式子有沒有覺得很熟悉，它就像我們上一回講的Aggregation，將特徵X使用特徵轉換轉成使用&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;表示，再來組合這些&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;成為最後的Model，所以單層的Neural Network就使用到了Aggregation，它繼承了Aggregation的優點。&lt;/p&gt;
&lt;p&gt;有了這個Model的形式了，我們可以使用Gradient Descent的手法來做最佳化，這也就是為什麼要讓操作過程當中所使用的函數都可以微分的原因。Gradient Descent在Neural Network的領域裡面發展出一套方法稱為Backpropagation，我們待會會介紹。&lt;strong&gt;因此實現Backpropagation，我只要餵Data進去，Model就會去尋找可以描述這組Data的特徵轉換&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，這就好像是可以從Data中萃取出隱含的Feature一樣，所以這類的Models才會被統稱為Extraction Models&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;深度學習(Deep Learning)&lt;/h3&gt;
&lt;p&gt;剛剛我們介紹了最基本款的Neural Network，那如果這個Neural Network有好幾層，我還會稱它為Deep Learning，所以基本上Deep Neural Network和Deep Learning是指同一件事，那為什麼會有兩個名字呢？其實是有歷史典故的。&lt;/p&gt;
&lt;p&gt;Neural Network的歷史相當悠久，早在1958年就有人提出以Perceptron當作Activation Function的單層Neural Network，大家也知道一層的Neural Network是不Powerful的，所以在1969年，就有人寫了論文叫做「perceptron has limitation」，從那時起Neural Network的方法就很少人研究了。&lt;/p&gt;
&lt;p&gt;直到1980年代，有人開始使用多層的Neural Network，並在1989年，Yann LeCun博士等人就已經將反向傳播演算法(Backpropagation, BP)應用於Neural Network，當時Neural Network的架構已經和現在的Deep Learning很接近了，不過礙於當時的硬體設備計算力不足，Neural Network無法發揮功效，並且緊接的&lt;strong&gt;有人在1989年證明了只要使用一層Neural Network就可以代表任意函數，那為何還要Deep呢？&lt;/strong&gt;所以Deep Neural Network這方法就徹底黑掉了。&lt;/p&gt;
&lt;p&gt;一直到了最近，&lt;strong&gt;G. E. Hinton博士為了讓Deep Neural Network起死回生，重新給了它一個新名字「Deep Learning」&lt;/strong&gt;，再加上他在2006年提出的RBM初始化方法，這是一個非常複雜的方法，所以在學術界就造成了一股流行，雖然後來被證明RBM是沒有用的，不過卻因為很多人參與研究Deep Learning的關係，也找出了解決Deep Learning痛處的方法，&lt;strong&gt;2009年開始有人發現使用GPU可以大大的加速Deep Learning&lt;/strong&gt;，從這一刻起，Deep Learning就開始流行起來，直到去年的2016年3月，圍棋程式Alpha GO運用Deep Learning技術以4:1擊敗世界頂尖棋手李世乭，Deep Learning正式掀起了AI的狂潮。&lt;/p&gt;
&lt;p&gt;聽完這個故事我們知道改名字的重要性XDD，不過大家是否還有看到什麼關鍵，「使用一層Neural Network就可以代表任意函數，那為何還要Deep呢？」這句話，這不就否定了我們今天做的事情了嗎？的確，使用一層的Neural Network就可以形成任意函數，而且完全可以用一層的神經元來表示任何多層的神經元，數學上是行得通的，但重點是參數量。Deep Learning的學習方法和人有點類似，我們在學習一個艱深的理論時，會先單元式的針對幾個簡單的概念學習，然後在整合這些概念去理解更高層次的問題，Deep Learning透過多層結構學習，雖然第一層的神經元沒有很多，能學到的也只是簡單的概念而已，不過第二層再重組這些簡單概念，第三層再用更高層次的方法看問題，所以同樣的問題使用一層Neural Network可能需要很多神經元才有辦法描述，但是Deep Learning卻可以使用更少的神經元做到一樣的效果，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;同樣表示的數學轉換過程，雖然單層和多層都是做得到相同轉換的，但是多層所用的參數量是比單層來得少的，依照VC Generalization Bound理論 (請參考：&lt;a href="https://www.ycc.idv.tw/ml-course-foundations_2.html"&gt;機器學習基石 學習筆記 (2)：為什麼機器可以學習?&lt;/a&gt;) 告訴我們可調控的參數量代表模型的複雜度，所以多層的NN比單層的有個優勢是在做到同樣的數學轉換的情況下更不容易Overfitting。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;因此，Deep Learning中每一層當中做了Aggregation，在增加模型複雜度的同時，也因為平均的效果而做到截長補短，這具有Regularization的效果，並且在採用多層且瘦的結構也同時因為「模組化」而做到降低參數使用量，來減少模型複雜度，這就不難想像Deep Learning為何如此強大。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;反向傳播算法(Backpropagation, BP)&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Neural Network" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.017.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;我們接下來就來看一下Deep Learning的演算法—反向傳播法，我們來看要怎麼從Gradient Descent來推出這個算法。&lt;/p&gt;
&lt;p&gt;看一下上面的圖，我畫出了具有&lt;span class="math"&gt;\(L\)&lt;/span&gt;層深的Deep Learning，每一層都有一個權重&lt;span class="math"&gt;\(W_{ij}^{(ℓ)}\)&lt;/span&gt;，因此我們可以估計出每一層的Score &lt;span class="math"&gt;\(s_{j}^{(ℓ)}= 𝚺_{i} W_{ij}^{(ℓ)}X_{i}^{(ℓ-1)}\)&lt;/span&gt;，把Score &lt;span class="math"&gt;\(s_{j}^{(ℓ)}\)&lt;/span&gt;通過Activation Function，就可以得到下一層的Input，如此不斷的疊上去，直到最後一層L為Output Layer，Output最後的結果&lt;span class="math"&gt;\(y\)&lt;/span&gt;，這裡我使用Linear Function來當作Output Function，這就是Deep Learning最簡單的架構。&lt;/p&gt;
&lt;p&gt;而我們需要Training的就是這些權重&lt;span class="math"&gt;\(W_{ij}^{(ℓ)}\)&lt;/span&gt;，我們如何一步一步的更新&lt;span class="math"&gt;\(W_{ij}^{(ℓ)}\)&lt;/span&gt;，使得它可以Fit數據呢？回想一下Gradient Descent的流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定義出Error函數&lt;/li&gt;
&lt;li&gt;Error函數讓我們可以去評估&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;算出它的梯度&lt;span class="math"&gt;\(∇E_{in}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;朝著&lt;span class="math"&gt;\(∇E_{in}\)&lt;/span&gt;的反方向更新參數W，而每次只跨出&lt;span class="math"&gt;\(η\)&lt;/span&gt;大小的一步&lt;/li&gt;
&lt;li&gt;反覆的計算新參數&lt;span class="math"&gt;\(W\)&lt;/span&gt;的梯度，並一再的更新參數&lt;span class="math"&gt;\(W\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;假設使用平方誤差的話，Error函數在這邊就是&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(L = (1/2) (y-\overline{y})^{2}\)&lt;/span&gt;，&lt;/p&gt;
&lt;p&gt;因此我們的更新公式可以表示成&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W_{ij}^{(ℓ)} ←  W_{ij}^{(ℓ)}-η×∂L/∂W_{ij}^{(ℓ)}\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;那我們要怎麼解這個式子呢？關鍵就在&lt;span class="math"&gt;\(∂L/∂W_{ij}^{(ℓ)}\)&lt;/span&gt;這項要怎麼計算，這一項在Output Layer (&lt;span class="math"&gt;\(ℓ=L\)&lt;/span&gt;)是很好計算的，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(∂L/∂W_{ij}^{(L)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= \frac{∂L}{∂s_{j}^{(L)}} \frac{∂s_{j}^{(L)}}{{∂W_{ij}^{(L)}}}\)&lt;/span&gt;  (連鎖率)&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= {δ_{j}^{(L)}}×{X_{i}^{(L-1)}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上式當中我們使用了微分的連鎖率，並且令&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(δ_{j}^{(L)} = ∂L/∂s_{j}^{(L)}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(δ_{j}^{(L)}\)&lt;/span&gt;這一項被稱為Backward Pass Term，而&lt;span class="math"&gt;\(X_{i}^{(L-1)}\)&lt;/span&gt;這項被稱為Forward Pass Term，所以&lt;span class="math"&gt;\(L\)&lt;/span&gt;層權重的更新取決於Forward Pass Term和Backward Pass Term相乘&lt;span class="math"&gt;\(δ_{j}^{(L)}×X_{i}^{(L-1)}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;我們先來看一下&lt;span class="math"&gt;\(L\)&lt;/span&gt;層的Forward Pass Term要怎麼計算，&lt;span class="math"&gt;\(X_{i}^{(L-1)}\)&lt;/span&gt;這項是很容易求的，我們只要讓數據一路從&lt;span class="math"&gt;\(0\)&lt;/span&gt;層傳遞上來就可以自然而然的得到&lt;span class="math"&gt;\(X_{i}^{(L-1)}\)&lt;/span&gt;的值，所以我們會稱&lt;span class="math"&gt;\(X_{i}^{(L-1)}\)&lt;/span&gt;這一項為Forward Pass Term，因為我們必須要往前傳遞才可以得到這個值。&lt;/p&gt;
&lt;p&gt;再來看一下&lt;span class="math"&gt;\(L\)&lt;/span&gt;層的Backward Pass Term要怎麼計算，&lt;span class="math"&gt;\(δ_{j}^{(L)}\)&lt;/span&gt;一樣是很容易求得的，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(δ_{j}^{(L)} = ∂L/∂s_{j}^{(L)} = ∂[(1/2) (y-\overline{y})^{2}]/∂y = (y-\overline{y})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;你會發現這一項的計算需要得到誤差的資訊，而誤差資訊要等到Forward的動作做完才有辦法得到，所以資訊的傳遞方向是從尾巴一路回到頭，是一個Backword的動作。&lt;/p&gt;
&lt;p&gt;因此，最後一層也是Output Layer的更新公式如下：&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W_{ij}^{(L)} ←  W_{ij}^{(L)}-η×δ_{j}^{(L)}×X_{i}^{(L-1)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;權重的更新取決於Input和Error的影響，需要考慮Forward Pass Term和Backward Pass Term。&lt;/p&gt;
&lt;p&gt;那除了Output這一層以外的權重應該怎麼更新？來看一下&lt;span class="math"&gt;\((ℓ)\)&lt;/span&gt;層，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(∂L/∂W_{ij}^{(ℓ)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= \frac{∂L}{∂s_{j}^{(ℓ)}}\frac{∂s_{j}^{(ℓ)}}{∂W_{ij}^{(ℓ)}}\)&lt;/span&gt; (連鎖率)&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= δ_{j}^{(ℓ)}×X_{i}^{(ℓ-1)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;一樣是Forward Pass Term和Backword Pass Term相乘，不過&lt;span class="math"&gt;\(δ_{j}^{(ℓ)}\)&lt;/span&gt;這一項的計算有點技巧性，來看一下，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(δ_{j}^{(ℓ)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= ∂L/∂s_{j}^{(ℓ)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= 𝚺_{k} \frac{∂L}{∂s_{k}^{(ℓ+1)}}\frac{∂s_{k}^{(ℓ+1)}}{∂X_{jk}^{(ℓ)}}\frac{∂X_{jk}^{(ℓ)}}{∂s_{j}^{(ℓ)}}\)&lt;/span&gt; (連鎖率)&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(= 𝚺_{k} {δ_{k}^{(ℓ+1)}}×{W_{jk}^{(ℓ)}}×{σ'(s_{j}^{(ℓ)})}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(W_{jk}^{(ℓ)}\)&lt;/span&gt;和&lt;span class="math"&gt;\(σ'(s_{j}^{(ℓ)})\)&lt;/span&gt;都是Forward之後就會得到的資訊，而&lt;span class="math"&gt;\(δ_{k}^{(ℓ+1)}\)&lt;/span&gt; 而是需要Backward才可以得到，我們已經知道&lt;span class="math"&gt;\(δ_{j}^{(ℓ=L)}\)&lt;/span&gt;的值，就可以從&lt;span class="math"&gt;\(δ_{j}^{(ℓ=L)}\)&lt;/span&gt;開始利用上面的公式，一路Backward把所有的&lt;span class="math"&gt;\(δ_{j}\)&lt;/span&gt;都找齊。好！那現在我們已經找到了更新所有Weights的方法了。&lt;/p&gt;
&lt;p&gt;看一下上圖中的最下面的Flow，一開始我們Forward，把所有&lt;span class="math"&gt;\(X\)&lt;/span&gt;和&lt;span class="math"&gt;\(s\)&lt;/span&gt;都得到，到了Output Layer，我們得到了&lt;span class="math"&gt;\(δ_{j}^{(ℓ=L)}\)&lt;/span&gt;，再Backward回去找出所有的&lt;span class="math"&gt;\(δ\)&lt;/span&gt;，接下來就可以用Forward Pass Term和Backword Pass Term來Update所有的&lt;span class="math"&gt;\(W\)&lt;/span&gt;了。&lt;/p&gt;
&lt;p&gt;總結一下，反向傳播算法(Backpropagation, BP)更新權重的方法為&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(W_{ij}^{(ℓ)} ←  W_{ij}^{(ℓ)}-η×δ_{j}^{(ℓ)}×X_{i}^{(ℓ-1)}\)&lt;/span&gt;  &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If output layer (&lt;span class="math"&gt;\(ℓ=L\)&lt;/span&gt;), &lt;span class="math"&gt;\(δ_{j}^{(ℓ=L)}=(y-ŷ)\)&lt;/span&gt;  &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If other layer, &lt;span class="math"&gt;\(δ_{j}^{(ℓ)}= σ'(s_{j}^{(ℓ)}) × 𝚺_{k} δ_{k}^{(ℓ+1)}×W_{jk}^{(ℓ)}\)&lt;/span&gt;  &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(δ_{j}^{(ℓ)}\)&lt;/span&gt;為Backword Pass Term；&lt;span class="math"&gt;\(X_{i}^{(ℓ-1)}\)&lt;/span&gt;為Forward Pass Term。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Regularization in Deep Learning&lt;/h3&gt;
&lt;p&gt;那麼使用Deep Learning的時候，我們要怎麼避免Overfitting呢？有五個方法。&lt;/p&gt;
&lt;p&gt;第一個方法，就是我們剛剛提過的&lt;strong&gt;「設計Deep Neural Network的結構」&lt;/strong&gt;，藉由限縮一層當中的神經元來達到一種限制，做到Regularization。&lt;/p&gt;
&lt;p&gt;第二個方法是&lt;strong&gt;「限制W的大小」&lt;/strong&gt;，和標準Regularization作一樣的事情，我們將&lt;span class="math"&gt;\(W\)&lt;/span&gt;的大小加進去Cost裡頭做Fitting，例如使用L2 Regularizer &lt;span class="math"&gt;\(Ω(W)=𝚺(W_{jk}^{(ℓ)})^{2}\)&lt;/span&gt;，但這樣使用有一個問題就是&lt;span class="math"&gt;\(W\)&lt;/span&gt;並不是Sparse的，L2 Regularizer在抑制&lt;span class="math"&gt;\(W\)&lt;/span&gt;的方法是，如果W的分量大的話就抑制多一點，如果分量小就抑制少一點（因為&lt;span class="math"&gt;\(W^{2}\)&lt;/span&gt;微分為1次），所以最後會留下很多很小的分量，造成計算量大大增加，尤其像是Deep Learing這麼龐大的Model，這樣的Regularization顯然不夠好，L1 Regularizer顯然可以解決這個問題（因為在大部分位置微分為常數），但不幸的是它無法微分，所以就有了L2 Regularizer的衍生版本，&lt;/p&gt;
&lt;p&gt;Weight-elimination L2 regularizer: &lt;span class="math"&gt;\(𝚺\frac{(W_{jk}^{(ℓ)})^{2}}{1+(W_{jk}^{(ℓ)})^{2}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;這麼一來不管&lt;span class="math"&gt;\(W\)&lt;/span&gt;大或小，它受到抑制的值大小是接近的 (因為Weight-elimination L2 regularizer微分為 &lt;span class="math"&gt;\(-1\)&lt;/span&gt;次方)，因此就可以使得部分&lt;span class="math"&gt;\(W\)&lt;/span&gt;可以為&lt;span class="math"&gt;\(0\)&lt;/span&gt;，大大便利於我們做計算。&lt;/p&gt;
&lt;p&gt;第三種方法是最常使用的&lt;strong&gt;「Early Stopping」&lt;/strong&gt;，所謂的Early Stopping就是，在做Backpropagation的過程去觀察Validation Data的Error有沒有脫離Training Data的Error太多，如果開始出現差異，我們就立刻停止計算，這樣就可以確保Model裡的參數沒有使得Model產生Overfitting，是一個很直接的作法。&lt;/p&gt;
&lt;p&gt;第四種方法是&lt;strong&gt;「Drop-out」&lt;/strong&gt;，在Deep Learing Fitting的過程中，隨機的關閉部分神經元，藉由這樣的作法使得Fitting的過程使用較少的神經元，並且使得結構是瘦長狀的，來達到Regularization。&lt;/p&gt;
&lt;p&gt;第五種方法是接下來會用更大篇幅介紹的&lt;strong&gt;「Denoising Autoencoder」&lt;/strong&gt;，在Deep Neural Network前面加入這樣的結構有助於抑制雜訊。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Autoencoder&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Regularization in Deep Learning" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.018.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Neural Network針對不同需要發展出很多不同的型態，包括CNN, RNN，還有接下來要介紹的Autoencoder，&lt;strong&gt;Autoencoder是一種可以將資料重要資訊保留下來的Neural Network&lt;/strong&gt;，效果有點像是資料壓縮，在做資料壓縮時，會有一個稱為Encoder的方法可以將資料壓縮，那當然還要有另外一個方法將它還原回去，這方法稱為Decoder，壓縮的過程就是用更精簡的方式保存了資料。&lt;strong&gt;Autoencoder同樣的有Encoder和Decoder，不過它不像資料壓縮一樣可以百分之一百還原，不過特別之處是Autoencoder會試著從Data中自己學習出Encoder和Decoder，並盡量讓資料在壓縮完了可以還原回去原始數據&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;見上圖中Basic Autoencoder的部分，透過兩層的轉換，我們試著讓Input &lt;span class="math"&gt;\(X\)&lt;/span&gt;可以完整還原回去，通常中間這一層會使用比較少的神經元，因為我們想要將資訊做壓縮，所以第一層的部分就是一個Encoder，而第二層則是Decoder，他們由權重&lt;span class="math"&gt;\(W_{jk}^{(ℓ)}\)&lt;/span&gt;決定，而在Training的過程，Autoencoder會試著找出最好的&lt;span class="math"&gt;\(W_{jk}^{(ℓ)}\)&lt;/span&gt;來使得資訊可以盡量完整還原回去，這也代表Autoencoder可以自行找出了Encoder和Decoder。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Encoder這一段就是在做一個Demension Reduction&lt;/strong&gt;，Encoder轉換原本數據到一個新的空間，這個空間可以比原本Features描述的空間更能精準的描述這群數據，而中間這層Layer的數值就是新空間裡頭的座標，有些時候我們會用這個新空間來判斷每筆Data之間彼此的接近程度。&lt;/p&gt;
&lt;p&gt;我們也可以讓Encoder和Decoder可以設計的更複雜一點，所以你同樣的可以使用多層結構，稱之為Deep Autoencoder。另外，也有人使用Autoencoder的方法來Pre-train Deep Neural Network的各個權重。&lt;/p&gt;
&lt;p&gt;緊接著介紹兩種特殊的例子，第一個是Linear Autoencoder，我們把所有的Activation Function改成線性的，這個方法可以等效於待會要講的Principal Component Analysis (PCA)的方法，PCA是一個全然線性的方法，所以它的效力會比Autoencoder差一點。&lt;/p&gt;
&lt;p&gt;第二個是剛剛提到的Denoising Autoencoder，我們在原本Autoencoder的前面加了一道增加人工雜訊的流程，但是又要讓Autoencoder試著去還原出原來沒有加入雜訊的資訊，這麼一來&lt;strong&gt;我們將可以找到一個Autoencoder是可以消除雜訊的&lt;/strong&gt;，把這個Denoising Autoencoder加到正常Neural Network的前面，那這個Neural Network就擁有了抑制雜訊的功用，所以可以當作一種Regularization的方法。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Principal Component Analysis (PCA)&lt;/h3&gt;
&lt;p&gt;最後來講一下Principal Component Analysis (PCA)，它不太算是Deep Learning的範疇，不過它是一個傳統且重要的Dimension Reduction的方法，我們就來看一下。&lt;/p&gt;
&lt;p&gt;&lt;img alt="PCA" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.019.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;PCA的演算法是這樣的，第一步先求出資料Features的平均值，並且將各個Features減掉平均值，令為&lt;span class="math"&gt;\(ζ\)&lt;/span&gt;，第二步求出由&lt;span class="math"&gt;\(ζ^{T}ζ\)&lt;/span&gt;產生的矩陣的Eigenvalue和Eigenvector，第三步，從這些Eigenvalue和Eigenvector中挑選前面&lt;span class="math"&gt;\(k\)&lt;/span&gt;個，並組成轉換矩陣&lt;span class="math"&gt;\(W\)&lt;/span&gt;，而最終PCA的轉換就是&lt;span class="math"&gt;\(Φ(x)=W^{T}(X-mean(X))\)&lt;/span&gt;，這個轉換做的就是Dimension Reduction，將數據降維到&lt;span class="math"&gt;\(k\)&lt;/span&gt;維。&lt;/p&gt;
&lt;p&gt;PCA做的事是這樣的，每一個Eigenvector代表新空間裡頭的一個軸，而Eigenvalue代表站在這個軸上看資料的離散程度，當然我們如果可以描述每筆資料越分離，就代表這樣的描述方法越好，所以Eigenvalue越大的Eigenvector越是重要，&lt;strong&gt;所以取前面&lt;span class="math"&gt;\(k\)&lt;/span&gt;個Eigenvector的用意是在降低維度的過程，還可以盡量的保持對數據的描述力，而且Eigenvector彼此是正交的，也就是說在新空間裡頭的每個軸是彼此垂直，彼此沒有Dependent的軸是最精簡的，所以PCA所做的Dimension Reduction一定是線性模型中最好、最有效率的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;另外，剛剛有提到的Linear Autoencode幾乎是等效於PCA，大家可以看上圖中的描述，這裡不多贅述，不過不同的是，Linear Autoencoder並沒有限制新空間軸必須是正交的特性，所以它的效率一定會比PCA來的差。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;這一篇當中，我們介紹了Neural Network，並且探討多層Neural Network—Deep Neural Network，也等同於Deep Learning，並且說明為什麼需要「Deep」，然後介紹Deep Learning最重要的演算法—反向傳播算法，接著介紹五種常用的Regularization的方法：設計Deep Neural Network的結構、限制W的大小、Early Stopping、Drop-out和Denoising Autoencoder。&lt;/p&gt;
&lt;p&gt;介紹完以上內容，我們就已經對於Deep Learning的全貌有了一些認識了，緊接著來看Deep Learning的特殊例子—Autoencoder，Autoencoder可以用來做Dimension Reduction，那既然提到了Dimension Reduction，那就不得不在講一下重要的線性方法PCA。&lt;/p&gt;
&lt;p&gt;那在下一回，我們會繼續探討Neural Network還有哪些特殊的分支。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (5)：Boost Aggregation Models</title><link href="https://ycc.idv.tw/ml-course-techniques_5.html" rel="alternate"></link><published>2017-04-02T12:00:00+08:00</published><updated>2017-04-02T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-04-02:/ml-course-techniques_5.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋AdaBoost (Adaptive Boost)、Gradient Boost、AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)。&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Boost的精髓&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們介紹的Aggregation Models都屬於沒有Boost的，不管是Bagging或Decision Tree都沒有要試著在Training的過程中改善Model，&lt;strong&gt;而這篇將要提到的Boost方法，則是在產生每個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;時試圖讓Model整體更完善，更能發揮Aggregation Models中截長補短中的「補短」的效果，也就是說&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;可以彼此互補不足之處&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那實際上我們應該怎麼做才能實踐Boost呢？其實方法的道理早就透漏在上一回中的Bagging和Decision Tree裡頭了，不管是Bagging和Decision Tree都是使用變換Data來做到變異度，在這個方法下Model的架構可以本身是不變的，這帶來相當的便利性，而今天我們要講的Boost也同樣的利用「變換Data」來做到變異度，但不同的是Boost的過程中「變換Data」這件事是有目標性的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Boost方法在「變換Data」時會試著去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善，這就是Boost的主要精髓。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;AdaBoost (Adaptive Boost) for Classification&lt;/h3&gt;
&lt;p&gt;剛剛上一段的最後我已經揭露了Boost的真正精髓，拿這樣的概念來做分類問題，就是我們接下來要談的AdaBoost，全名稱為Adaptive Boost。&lt;/p&gt;
&lt;p&gt;在分類問題中我們怎麼做到「凸顯原先做錯的Data」？簡單的想法是這樣的，我們可以減少原本已經是正確分類的Data的數量，然後增加原本錯誤分類的Data的數量，&lt;strong&gt;增減Data的數量其實是等效於改變每筆Data的權重&lt;/strong&gt;，假如我們給每筆資料權重，要做的事是拉低正確分類Data的權重，而且拉高錯誤分類Data的權重。&lt;/p&gt;
&lt;p&gt;那我們應該要提升權重或降低權重到什麼程度才是OK的呢？換個方式思考，我們為什麼要去調整權重？目的其實是要去凸顯原先做錯的部分，降低原本做對的部分，也就是想&lt;strong&gt;藉由調整每筆Data的多寡或權重來做到「弭平原先的預測性」，最好可以讓原本的預測方法看起來是隨機分布&lt;/strong&gt;，也就是「錯誤率＝正確率」，讓它像是擲銅板一樣，沒有什麼預測能力。&lt;/p&gt;
&lt;p&gt;&lt;img alt="AdaBoost" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.012.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;有了概念之後，我們來看實際應該要怎麼做？見上圖說明，首先我們需要先將Data權重&lt;span class="math"&gt;\(u^{(1)}\)&lt;/span&gt;先初始化，接下來就可以開始找&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;了，我們使用任意一個分類問題的Model搭配上Data的權重，求得一組&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，接下來計算這組&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的&lt;strong&gt;「錯誤率」&lt;span class="math"&gt;\(ε_{t}\)&lt;/span&gt;&lt;/strong&gt;，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(ε_{t}= 𝚺_{n} u_{n}^{(t)} ⟦y_{n}≠g_{t}(x_{n})⟧ / 𝚺_{n} u_{n}^{(t)}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有注意到考慮「錯誤率」&lt;span class="math"&gt;\(ε_{t}\)&lt;/span&gt;的時候必須要評估&lt;span class="math"&gt;\(u_{n}^{(t)}\)&lt;/span&gt;，要記得會有Data權重是為了表示增加或減少原本的Data的數量，所以依照每筆Data的出現機會不同，會有不同的權重，也就會有對「錯誤率」不同的貢獻程度。&lt;/p&gt;
&lt;p&gt;那為了待會要對權重重新分配，我們先定義了&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;，在未來我會將錯誤的Data的權重乘上&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;，即&lt;span class="math"&gt;\(u_{n}^{(t+1)}=u_{n}^{(t)}×β_{t}\)&lt;/span&gt;，並且把正確的Data權重除以&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;，即&lt;span class="math"&gt;\(u_{n}^{(t+1)}=u_{n}^{(t)}/β_{t}\)&lt;/span&gt;，&lt;strong&gt;而期望的結果是重新分配的Dataset在&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的預測下可以表現的像隨機的一樣，於是乎下一次使用這組Dataset訓練出來的&lt;span class="math"&gt;\(g_{t+1}\)&lt;/span&gt;將會彌補&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的不足&lt;/strong&gt;，根據這樣的原則我們來推一下&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(𝚺_{n} u_{n}^{(t+1)} ⟦y_{n}≠g_{t}(x_{n})⟧ / 𝚺_{n} u_{n}^{(t+1)}=1/2\)&lt;/span&gt; (預測能力像隨機分布)&lt;/p&gt;
&lt;p&gt;⇒  &lt;span class="math"&gt;\(𝚺_{n} u_{n}^{(t+1)} ⟦y_{n}≠g_{t}(x_{n})⟧ = 𝚺_{n} u_{n}^{(t+1)} ⟦y_{n}=g_{t}(x_{n})⟧\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;⇒  &lt;span class="math"&gt;\(𝚺_{n} (u_{n}^{(t)}×β_{t})  ⟦y_{n}≠g_{t}(x_{n})⟧ = 𝚺_{n} (u_{n}^{(t)}/β_{t}) ⟦y_{n}=g_{t}(x_{n})⟧\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;⇒  &lt;span class="math"&gt;\(β_{t}^{2} = \frac{𝚺_{n} u_{n}^{(t)} ⟦y_{n}=g_{t}(x_{n})⟧ }{𝚺_{n} u_{n}^{(t)}  ⟦y_{n}≠g_{t}(x_{n})⟧}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;⇒  &lt;span class="math"&gt;\(β_{t}^{2} = \frac{𝚺_{n} u_{n}^{(t)} ⟦y_{n}=g_{t}(x_{n})⟧ /  𝚺_{n} u_{n}^{(t)}}{ 𝚺_{n} u_{n}^{(t)}  ⟦y_{n}≠g_{t}(x_{n})⟧ / 𝚺_{n} u_{n}^{(t)}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;⇒  &lt;span class="math"&gt;\(β_{t}^{2} = \frac{1-ε_{t}}{ε_{t}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;⇒  &lt;strong&gt;&lt;span class="math"&gt;\(β_{t} = \sqrt{\frac{1-ε_{t}}{ε_{t}}}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以我們就可以利用這個&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;來更新我的Data權重，並且在多次迭代後，得到很多個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;。而將來我們會把所有的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;做線性組合，而我們希望&lt;strong&gt;「錯誤率」越低的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;可以有更高的貢獻度&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;&lt;/strong&gt;，所以使用&lt;span class="math"&gt;\(β_{t}\)&lt;/span&gt;緊接著計算「&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的權重」&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;，定義為&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(α_{t} = ln(β_t)\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以當一個百分之一百可以完全預測的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;出現時，它的&lt;span class="math"&gt;\(ε_{t}=0\)&lt;/span&gt;，此時它的&lt;span class="math"&gt;\(β_{t} →∞\)&lt;/span&gt;，同時&lt;span class="math"&gt;\(α_{t} →∞\)&lt;/span&gt;，所以這樣的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;會有完全的貢獻。&lt;/p&gt;
&lt;p&gt;如果一個預測效果很差的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;出現，它的&lt;span class="math"&gt;\(ε_{t}=1/2\)&lt;/span&gt;，此時它的&lt;span class="math"&gt;\(β_{t}=1\)&lt;/span&gt;，同時&lt;span class="math"&gt;\(α_{t}=0\)&lt;/span&gt;，所以這樣的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;並沒有任何參考價值。&lt;/p&gt;
&lt;p&gt;那如果出現一個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;它的&lt;span class="math"&gt;\(ε_{t} &amp;gt; 1/2\)&lt;/span&gt;，那這樣的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;並不能說它沒有用處，反而是一個很好的反指標，我們只需要反著看就好了，當&lt;span class="math"&gt;\(ε_{t} &amp;gt; 1/2\)&lt;/span&gt;時，&lt;span class="math"&gt;\(β_{t} &amp;lt; 1\)&lt;/span&gt;，所以&lt;span class="math"&gt;\(α_{t} &amp;lt; 0\)&lt;/span&gt;，這樣的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;具有逆向的貢獻。&lt;/p&gt;
&lt;p&gt;最後只要把這些訓練好的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;乘上各自的&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;再加總起來，我們就完成了AdaBoost啦！&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Gradient Boost for Regression&lt;/h3&gt;
&lt;p&gt;剛剛我們講了AdaBoost，是個很神奇的方法，當我們做錯了，沒關係！從哪裡跌倒就從哪裡站起來，利用這種精神我們就可以做到Boost的效果，但美中不足的是上面的方法只能用在「分類問題」上，那如果我也想在「Regression問題」也做到Boost呢？這就是接下來要講的GradientBoost的方法。&lt;/p&gt;
&lt;p&gt;在課程中林軒田教授是從AdaBoost出發經過推導後，得到一個很像是Gradient Decent的式子，接下來將式子一般化成為可以使用任意Error Measure的形式，我稍微列一下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;GradientBoost: &lt;span class="math"&gt;\(min_{η}\ min_{h}\ (1/N) 𝚺_{n} Error[𝚺_{τ=1}^{τ=t-1} α_{τ} g_{τ}(x_{n}) + η h(x_{n}), y_{n}]\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我們這邊會考慮Error為平方誤差&lt;span class="math"&gt;\((s-y)^{2}\)&lt;/span&gt;的結果，詳細的推導這邊就不多加討論，可以到影片中學習，這裡我想要從我觀察出來的觀點，概念性的來看這個GradientBoost的方法。&lt;/p&gt;
&lt;p&gt;「從哪裡跌倒就從哪裡站起來」就是Boost的精神，所以今天你有一個Regression問題沒做好，&lt;strong&gt;留下了餘數Residual，怎麼辦？那我就把這個餘數當作另外一個Regression問題來做它&lt;/strong&gt;，再把這個結果附到先前的那個就好啦！如果第一次Regression後的Model是&lt;span class="math"&gt;\(g_{1}(x)\)&lt;/span&gt;，那剩下的沒做好的餘數就應該是&lt;span class="math"&gt;\(y(x)-g_{1}(x)\)&lt;/span&gt;，我們拿這個餘數下去在做一次Regression得到另外一個Model &lt;span class="math"&gt;\(g_{2}(x)\)&lt;/span&gt;，此時合併這兩個結果的餘數就變成了&lt;span class="math"&gt;\(y(x)-g_{1}(x)-g_{2}(x)\)&lt;/span&gt;，就可以使用這個餘數繼續做下去，最後組合所有的&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;就會得到一個更好的Model。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gradient Boost" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.013.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;依循這樣的概念我們來看GradientBoost作法，如上圖，一開始我們先初始化每一筆Data的預測值&lt;span class="math"&gt;\(s_{n}\)&lt;/span&gt;為0，再接下來開始產生&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，我們先把Data的 &lt;span class="math"&gt;\(y_{n}\)&lt;/span&gt; 減去每一筆Data當前的預測值&lt;span class="math"&gt;\(s_{n}\)&lt;/span&gt;，就會產生餘數&lt;span class="math"&gt;\((y_{n}-s_{n})\)&lt;/span&gt;，當然，在一開始&lt;span class="math"&gt;\(s_{n}=0\)&lt;/span&gt;，所以&lt;span class="math"&gt;\(y_{n}-s_{n}=y_{n}\)&lt;/span&gt;，等於是對原問題求解。&lt;/p&gt;
&lt;p&gt;接下來因為最後我們要線性組合&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;，所以需要決定&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;前面的係數&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;，也就是貢獻度，這個&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;的決定方式是去求解一個One-Variable-Linear-Regression (單變數線性迴歸)，目的是&lt;strong&gt;去縮放&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;使得它更接近剛剛的餘數&lt;span class="math"&gt;\((y_{n}-s_{n})\)&lt;/span&gt;，而找到這個縮放值就是&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;&lt;/strong&gt;。所以每一次&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;的產生都是為了可以把G(x)描述的更好，最後&lt;span class="math"&gt;\(G(x)=𝚺_{t} α_{t}g_{t}(x)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;看到這裡有人一定會認為One-Variable-Linear-Regression求&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;這一步是多餘的，因為在一開始做&lt;span class="math"&gt;\(\{x_{n},y_{n}-s_{n}\}\)&lt;/span&gt;的Regression中我們已經最佳化過&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;，那為什麼還要把&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;乘上&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;再做同樣的事呢？&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;一定是1的啊！就像我一開始舉的例子一樣啊！其實問題就出在於你把&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;理所當然的看成是線性模型，你才會覺得這一步是多餘的，如果&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;不是線性的，求&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;就很重要的，因為你要使用線性組合來組出&lt;span class="math"&gt;\(G(x)\)&lt;/span&gt;，但是你的&lt;span class="math"&gt;\(g_{t}(x)\)&lt;/span&gt;不是線性的，所以你只好在外面再用線性模型來包裝一遍。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)&lt;/h3&gt;
&lt;p&gt;&lt;img alt="AdaBoosted and GrandientBoosted DTree" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.014.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;和Random Forest一樣，我們也可以將AdaBoost和GradientBoost套用到Decision Tree上面，&lt;strong&gt;如果是處理分類問題就使用AdaBoosted Decision Tree；那如果是處理Regression問題可以使用Gradient Boosted Decision Tree&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;但要特別注意的是，這邊的Decision Tree都必須是弱的，也就是Pruning過後的樹，如果直接使用完全長成的樹，你會發現在AdaBoosted Decision Tree中，因為&lt;span class="math"&gt;\(ε_{t}=0\)&lt;/span&gt;所以&lt;span class="math"&gt;\(α_{t}→∞\)&lt;/span&gt;；在Gradient Boosted Decision Tree中，&lt;span class="math"&gt;\(y_{n}-s_{n}→0\)&lt;/span&gt;，因為錯誤出現的太少了，所以造成我們不能真正使用到Boost的效果，也就失去做Boost的意義了，&lt;strong&gt;因此在做AdaBoosted Decision Tree或Gradient Boosted Decision Tree時要使用「弱」一點的Decision Tree&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;這一篇當中，我們完整提了Boost的方法，Boost的精神就是從哪裡跌倒就從哪裡站起來，使用變換Data權重的手法去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善。&lt;/p&gt;
&lt;p&gt;我們提了兩種Boost的方法，如果是處理分類問題就使用AdaBoost；如果是處理Regression問題可以使用GradientBoost，而且這兩種方法都可以和Decision Tree做結合。&lt;/p&gt;
&lt;p&gt;以上兩回，我們已經完成了Aggregation Models了，接下來的下一回將要探討的就是現今很流行的類神經網路和深度學習等等。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (4)：Basic Aggregation Models</title><link href="https://ycc.idv.tw/ml-course-techniques_4.html" rel="alternate"></link><published>2017-03-29T12:00:00+08:00</published><updated>2017-03-29T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-03-29:/ml-course-techniques_4.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋Blending、Bagging、Decision Tree和Random Forest&lt;/p&gt;</summary><content type="html">&lt;h3&gt;綜觀Aggregation Models&lt;/h3&gt;
&lt;p&gt;如果今天我有很多支的Model，我有辦法融合他們得到更好的效果嗎？&lt;/p&gt;
&lt;p&gt;這就是Aggregation Models的精髓，Aggregation Models藉由類似於投票的方法綜合各個子Models的結果得到效果更好的Model。換個角度看，你可以把整個體系看成一個新的Model，而原本這些子Models當作轉換過後的新Features，&lt;strong&gt;所以Aggregation Model裡頭做了「特徵轉換」，這個特徵轉換產生出許多有預測答案能力的Features，稱為Predictive Features，然後再綜合它們得到最後的Model&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Aggregation Models" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.007.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Aggregation Models可以分成兩大類，第一種的作法比較簡單，先Train出一個一個獨立的Predictive Features，然後在綜合它們，&lt;strong&gt;「集合」的動作是發生在得到Train好的Predictive Feature之後，這叫做「Blending Models」&lt;/strong&gt;；第二種作法則是，&lt;strong&gt;「集合」的動作和Training同步進行，這叫做「Aggregation-Learning Models」&lt;/strong&gt;，Aggregation-Learning Models有一個特殊的例子叫做Boost，翻開字典查Boost的意思是「促進」，在這邊的意義是&lt;strong&gt;假設在Training過程所產生的Predictive Feature朝著改善Model的方向前進就叫做Boost&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;從「集合」的方法上也可以進一步細分三種類型，有票票等值的&lt;strong&gt;「Uniform Aggregation Type」&lt;/strong&gt;，有給予Predictive Features不同權重的&lt;strong&gt;「Linear Aggregation Type」&lt;/strong&gt;，甚至還可以用條件或任意Model來分配Predictive Features，這叫做&lt;strong&gt;「Non-linear Aggregation Type」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以兩種類型、三種Aggregation Type，交互產生各類的Aggregation Models。有Blending的三種Aggregation Type，Aggregation-Learning的Uniform Type—Bagging，再加上Aggregation-Learning的Linear Type兩種—AdaBoost和GradientBoost，這兩種也亦是Boost的方法，AdaBoost負責處理Classification的問題，而GradientBoost則負責處理Regression的問題，最後介紹Aggregation-Learning的Non-Linear Type—Decision Tree。然後接著，使用Decision Tree結合其他方法再進一步的產生Random Forest、AdaBoost Decision Tree和GradientBoost Decision Tree。&lt;/p&gt;
&lt;p&gt;我將會分兩篇來介紹Aggregation Models，一篇介紹沒Boost的部分，就是今天這一篇，另外一篇則是來專攻有Boost的部分。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Blending&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Blending是泛指在Training結束之後得到幾個Predictive Features，然後再對這些Predictive Features做集合的方法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Blending" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.008.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;如上圖，基本流程是這樣的，一開始先把Data切成一部分拿來Training，另外一部分拿來Validation，這部份很重要，因為我們待會要利用Validation的Error來決定每筆Predictive Feature對Model的貢獻分配比重；接下來使用不同的方法來產生不同的Predictive Features &lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，來源可能是不同的Model形式、不同的參數變化、不同的隨機情形等等；有了各類的&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;之後，我們就可以選擇使用怎樣的方式來結合它們，如果是Uniform Combination，就直接平均所有&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;就可以了，那如果是Linear Combination，想當然爾就是使用線性模型來結合，那如果是Non-Linear Combination，你可以使用任意Model來描述也行；決定好結合方式了，也就同時決定了「特徵轉換」的方法，接下來出動Validation Data，使用這個「特徵轉換」來轉化Validation Data並且做Fitting，最後我們會找到一組解最佳的參數來確定結合的方法，如果是Uniform Combination是不需要這一步的，基本上你得到&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;就直接平均就得到結果了，而Linear Combination則是需要去找出&lt;span class="math"&gt;\(α_{t}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;在數學上可以證明Aggregation的效果會比單一一個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的描述的結果還好&lt;/strong&gt;，這很像是在做投票選舉，不同方法可能帶有不一樣的偏見，但是綜合所有意見之後可以找到共識，這個共識是具有較少偏見的，你可以想像偏見就像是Overfitting，&lt;strong&gt;所以Aggregation是具有像Regularizaiton一般抑制Overfitting的效果的&lt;/strong&gt;，但有些時候特別的看法不一定是偏見，也許這一個方法可以看出其他方法看不出來的規律，此時這個部分也不會被完全忽略掉，&lt;strong&gt;所以Aggregation也可以同時擁有像Feature Transform一樣的複雜度。因此Aggregation的方法可以同時增加Model複雜度又同時防止它Overfitting，這個效果是我們以前沒看過的，所以我們會說Aggregation具有截長補短的效果&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Bagging&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Bagging" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.009.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bagging是一種利用變換原本Data來造出不同&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;的簡單方法&lt;/strong&gt;，Bagging的全名稱為Bootstrap Aggregation，其中&lt;strong&gt;Bootstrap指的是「重新取樣原有Data產生新的Data，取樣的過程是均勻且可以重複取樣的」&lt;/strong&gt;，使用Bootstrap我們就可以從一組Data中生出多組Dataset，然後就可以使用這些Dataset來產生多組&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，最後再Uniform Combination這些&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，就完成了Bagging。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Decision Tree（決策樹）&lt;/h3&gt;
&lt;p&gt;接下來談Decision Tree這個重要的概念，Decision Tree其實就像是一個多層次的分類，每一次的分類會根據某一個Feature來當作依據判斷它應該繼續往哪一條路走，然後繼續使用可能是另外一個Feature來繼續細分下去。舉個例子好了，假設今天有一個自由式摔跤重量63公斤的女選手Ms. D要參加奧運，所以得透過奧運的分級制度分級，一開始可能根據比賽模式這個Feature下去分類，我查了一下有自由式和古典式兩種，所以Ms. D會被歸類到自由式，再來根據性別這個Feature下去分類，Ms. D是女選手所以分到女選手這一類，再繼續可能會根據體重來細分，體重在奧運分級共有8級，Ms. D可能就被分到62公斤级的那類，這樣的分類精神就是Decision Tree。&lt;/p&gt;
&lt;p&gt;所以，Decision Tree的優點是結果所提供的結構非常容易讓人了解，另外在演算法部分也很容易實現，而且因為具有以條件篩選的結構，所以其實很容易可以做到多類別分類。但是Decision Tree也有一些為人詬病的缺點，Decision Tree整體理論是缺乏基礎的，存在很多是前人的巧思，很多作法都是使用起來感覺效果不錯就延續下去了，目前並不了解背後的原因，也因此沒有一個代表性的演算法存在。&lt;/p&gt;
&lt;p&gt;在講Decision Tree操作方法之前應該要先來講一下Decision Stump，Decision Stump做的事其實就是上述中提到的對某個Feature做切分的這件事，&lt;strong&gt;可以想知Decision Stump是一個預測效果很差的Model，而Aggregation這些Decision Stump形成Decision Tree卻有很好的效果&lt;/strong&gt;，這就是Aggregation的威力。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Decision Tree" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.010.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;見上圖，我們來看一下Decision Tree的流程，Decision Tree最為人所知的演算法是C&amp;amp;RT，C&amp;amp;RT是一整套的套件，我們今天只是提到它整套套件中的一種特例。Decision Tree產生的函式是這樣的，一開始先判斷進來的這筆資料還能不能繼續分支下去，在三個情況下，我們沒辦法繼續分支下去：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;數據&lt;span class="math"&gt;\(Ɗ\)&lt;/span&gt;只剩一筆數據。&lt;/li&gt;
&lt;li&gt;這群數據&lt;span class="math"&gt;\(Ɗ\)&lt;/span&gt;已經最佳化了，我們會說它的Impurity=0，這個時候我們不知道要從哪裡再切一刀。&lt;/li&gt;
&lt;li&gt;這群數據&lt;span class="math"&gt;\(Ɗ\)&lt;/span&gt;的Feature &lt;span class="math"&gt;\(X_{n}\)&lt;/span&gt;都完全相同。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;當無法再繼續分支下去時，會回傳一個&lt;span class="math"&gt;\(g_{t}(x)=constant\)&lt;/span&gt;，這個常數是一個可以使得這個群體內&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小的數值，在分類問題中這個常數是&lt;span class="math"&gt;\(\{y_{n}\}\)&lt;/span&gt;中佔多數的類別，在Regression問題中這個常數是&lt;span class="math"&gt;\(\{y_{n}\}\)&lt;/span&gt;的平均值。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;大家應該會有點驚訝，Decision Tree也有辦法做Regression？其實是可以的，我們只要讓群裡頭的數字作平均當代表，這們一來要處理實數問題也是可以做到的，不過我們會預期處理Regression問題時會切的比Classification問題來的細和多層。&lt;/p&gt;
&lt;p&gt;那接下來來看假如還可以繼續分支下去應該要怎麼做，這邊假設我們只切一刀分為兩個區塊&lt;span class="math"&gt;\(C=2\)&lt;/span&gt;，我們該根據怎樣的條件來切呢？我們剛剛其實有稍微提到，那就是Impurity，我們&lt;strong&gt;可以根據Impurity Function來衡量「一群資料的不相似程度」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;分類問題的Impurity Function有以下兩種：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(Impurity(Ɗ) = (1/N) 𝚺_{n} ⟦y_{n}≠y^*⟧\)&lt;/span&gt;，其中&lt;span class="math"&gt;\(y^*\)&lt;/span&gt;是&lt;span class="math"&gt;\(Ɗ\)&lt;/span&gt;中佔多數的類別，這個衡量方法就直接的去數出錯誤答案的比例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gini Index: &lt;span class="math"&gt;\(Impurity(Ɗ) = 1 - 𝚺_{k} [ 𝚺_{n}⟦y_{n}=k⟧  / N ]^{2}\)&lt;/span&gt;&lt;/strong&gt;，Gini Index是最為流行的作法，它不同於上一個作法，它是在評估所有的類別後才去計算Impurity，其中 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 代表類別。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而Regression問題有以下方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(Impurity(Ɗ) = (1/N) 𝚺_{n} ( y_{n} - \overline{y} )^{2}\)&lt;/span&gt;&lt;/strong&gt;，其中&lt;span class="math"&gt;\(ȳ\)&lt;/span&gt;代表的是&lt;span class="math"&gt;\(\{y_{n}\}\)&lt;/span&gt;的平均值，式子中使用平方誤差來評估資料的離散程度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有了Impurity Function我們就有了指標，找出應該要使用哪個Feature、應該要怎麼切，才能使得Impurity Function總和最小，決定好這一刀後，接下來就從這一刀切下去，把Data一分為二，然後這兩組Data再各自去長出一棵Decision Tree，經過遞迴式的迭代，我們就可以得到一棵完整的Decision Tree了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Show C&amp;amp;RT" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.015.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;如果我們讓一棵樹完整的長成了，可以想到的後果想當然爾就是Overfitting，所以我們必須要做Regularization，&lt;strong&gt;Decision Tree常用的Regularization的方法是Pruning&lt;/strong&gt;，就是砍樹，我們將分支的數量&lt;span class="math"&gt;\(Ω(G)\)&lt;/span&gt;加進去&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;中做為Regularization，所以我們問題變成是去找到 &lt;span class="math"&gt;\(argmin\ E_{in}(G)+λΩ(G)\)&lt;/span&gt;，其中的λ可以利用Validation Data來做選擇，你會發現如果真正的要去找到&lt;span class="math"&gt;\(argmin\ E_{in}(G)+λΩ(G)\)&lt;/span&gt;的最佳解，這問題會非常的困難，因為你必須要把所有的可能的樹都考慮進去，所以有一個替代方案，&lt;strong&gt;我們可以先將樹整棵長完，然後在一一的去合併分支，看哪兩個分支合併之後可以使&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小就先合併，使用這樣的作法逐步減少分支的數量&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;順道一提，C&amp;amp;RT可以產生許多替代方案，這些替代方案稱為Surrogate Branch，當有一筆Data缺乏某個Feature，我們仍然有辦法使用替代方案來做決策，這是C&amp;amp;RT的一個大大的優點。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Random Forest（隨機森林）&lt;/h3&gt;
&lt;p&gt;如果我拿Decision Tree來做Bagging這樣可以嗎？當然OK，Aggregation Model的精髓就是可以綜合子Model，那Decision Tree也可以是看成一個子Model，所以我們在做的就是Aggregation of Aggregation，&lt;strong&gt;這種拿Decision Tree來做Bagging的Model叫做Random Forest&lt;/strong&gt;，這個名字取的很生動，有很多棵數的地方就是森林啦！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Decision Tree和Bagging其實是有互補的作用&lt;/strong&gt;，Decision Tree這種演算法是「變異度」很高的，因為它不像SVM這類的演算法，會去評估與Data之間的距離，空出最大的距離來避免Overfitting，而Bagging正可以拿來減少「變異度」，消除雜訊，所以&lt;strong&gt;Random Forest會比Decision Tree更不易Overfitting&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Random Forest" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.011.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;見上圖，我們來看一下Random Forest的流程，一開始先做和Bagging裡頭一樣做的事Bootstrap，藉此來產生新的Dataset，另外為了讓我們隨機程度變得更高，我也對我們Features來做點變化，將它乘上一個亂數產生的&lt;span class="math"&gt;\(P\)&lt;/span&gt;，如果&lt;span class="math"&gt;\(P_{i}=0\)&lt;/span&gt;代表我們完全不取這個Feature，如果&lt;span class="math"&gt;\(P_{i}=1\)&lt;/span&gt;代表我們完全取這個Feature，我們更可以以分數來代表我們對某個Feature的重視程度，這個手法叫做Random-subspace。接下來就是把弄的很亂的Dataset放進去長一顆Decision Tree，最後再把所有的Decision Tree平均就是Random Forest的結果。&lt;/p&gt;
&lt;p&gt;Random Forest發展出了一套獨特的Validation方法，我們知道Bootstrap的結果會造成有些Data取用而有些Data不使用，而取用的Data會拿來Training，這讓你想到什麼呢？沒錯，沒有用到的Data可以做Validation，我們可以拿那些沒有被取用的Data來評估Training的好壞，我們會稱那些沒被取用的Date叫做Out-of-Bag Data，而利用Out-of-Bag Data來Validation的Error，稱為Out-of-Bag Error，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Out-of-Bag Error &lt;span class="math"&gt;\(E_{oob}=(1/N) 𝚺_{n} err(y_{n}, {G_{n}}^{-}(x_{n}))\)&lt;/span&gt; &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(where:\ {G_{n}}^{-}(x) = Average(Models\ without\ using\ this\ data)\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Out-of-Bag Error提供一個很方便的Self-validation的方法。&lt;/p&gt;
&lt;p&gt;在以前Linear Model中，權重W代表每筆Feature對Model的貢獻度，我們可以由W的分量大小來評估每個Feature的重要程度。Random Forest則是可以利用&lt;span class="math"&gt;\(E_{oob}\)&lt;/span&gt;和Random-subspace來標示出每個Feature的重要程度，想法是這樣的，如果今天某一個Feature i 對Model很重要，所以說我只對Feature i 做Random-subspace，也就是只有&lt;span class="math"&gt;\(P_{i}\)&lt;/span&gt;是隨機的，可以想知&lt;span class="math"&gt;\(E_{oob}\)&lt;/span&gt;會大幅增加，因此利用這個想法我們可以用來定義Feature的重要程度，
&lt;/p&gt;
&lt;div class="math"&gt;$$
important(i) = E_{oob}(G) - E_{oob}(G with random-subspace at i)
$$&lt;/div&gt;
&lt;p&gt;
&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;在這一篇我們提了幾個基礎的Aggregation Models，從最簡單的Blending，Blending的方法本身不去產生子Model，而是使用兩階段學習，先自行挑選和訓練來產生很多的子Model，而Blending只在這些結果上做不同方式的結合。&lt;/p&gt;
&lt;p&gt;接下來，Learning-Aggregation的方法則化被動為主動，我們先提了Bagging，裡頭使用Bootstrap的技巧來造成資料的隨機性，利用這樣的變異來產生多個&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，再接下來我講了Decision Tree，Decision Tree由多個Decision Stump組合而成，每個Decision Stump就是&lt;span class="math"&gt;\(g_{t}\)&lt;/span&gt;，Decision Tree做的事就是，產生Decision Stump、切分Dataset、再產生Decision Stump...接續下去，最後綜合全部的Decision Stump成為Decision Tree。&lt;/p&gt;
&lt;p&gt;最後，我們結合Decision Tree和Bagging產生了Random Forest，利用彼此的互補，讓效果變得更好可以比單純Decision Tree更好。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (3)：Kernel Regression</title><link href="https://ycc.idv.tw/ml-course-techniques_3.html" rel="alternate"></link><published>2017-03-15T12:00:00+08:00</published><updated>2017-03-15T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-03-15:/ml-course-techniques_3.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋Probabilistic SVM、Kernel Logistic Regression、Kernel Ridge Regression、Support Vector Regression (SVR)&lt;/p&gt;</summary><content type="html">&lt;p&gt;在上一篇當中我們看到了Kernel Trick的強大，我們繼續運用這個數學工具在其他的Regression上看看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Soft-Margin SVM其實很像L2 Regularized Logistic Regression&lt;/h3&gt;
&lt;p&gt;上一篇中提到的Soft-Margin SVM其實很像&lt;a href="https://gitycc.github.io/YCNote/tag/ji-qi-xue-xi-ji-shi.html"&gt;《機器學習基石》&lt;/a&gt;裡頭提到的L2 Regularized Logistic Regression，如果你還記得的話，Logistic Regression是為了因應雜訊而給予每筆資料的描述賦予「機率」的性質，讓Model在看Data的時候不那麼的非黑及白，那時候有提到這叫做Soft Classification，而這個概念就非常接近於Soft-Margin的概念。&lt;/p&gt;
&lt;p&gt;從數學式來看會更清楚，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Soft-Margin SVM：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. (W^{T}W/2) + C×𝚺_{n} ξ_{n}\ \ \ s.t.\ \ \ y_{n}×(W^{T}Z_{n}+b) ≥ 1-ξ_{n}\ and\ ξ_{n} ≥ 0,\ n=1\cdots N\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的式子中，可以將限制條件由max取代掉，轉換成下面的Unbounded的表示方法，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Soft-Margin SVM：&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. C×𝚺_{n} Err_{hinge,n} + (W^{T}W/2)\)&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;其中，&lt;span class="math"&gt;\(Err_{hinge,n}=max[0,1-y_{n}×(W^{T}Z_{n}+b)]\)&lt;/span&gt;，稱之為Hinge Error Measure&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;接下來比較一下L2 Regularized Logistic Regression，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;L2 Regularized Logistic Regression：&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. (1/N)×𝚺_{n} Err_{ce,n} +  (λ/N)×W^{T}W\)&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;其中，&lt;span class="math"&gt;\(Err_{ce,n}=ln[1+exp(-y_{n}×(W^{T}Z_{n}))]\)&lt;/span&gt;，為Cross-Entropy Error Measure。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;你會發現Soft-Margin SVM和L2 Regularized Logistic Regression兩個式子的形式是很接近的，都有&lt;span class="math"&gt;\(W^{T}W\)&lt;/span&gt;這一項，只是意義上不同，在Soft-Margin SVM裡頭&lt;span class="math"&gt;\(W^{T}W\)&lt;/span&gt;所代表的是反比於空白區大小距離的函式，而在L2 Regularized Logistic Regression裡頭則是指Regularization。&lt;/p&gt;
&lt;p&gt;另外，我們來疊一下&lt;span class="math"&gt;\(Err_{hinge,n}\)&lt;/span&gt;和&lt;span class="math"&gt;\(Err_{ce,n}\)&lt;/span&gt;來看看這兩個函數像不像，&lt;/p&gt;
&lt;p&gt;&lt;img alt="compare:hinge and ce" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_03.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(Err_{hinge,n}\)&lt;/span&gt;和&lt;span class="math"&gt;\(Err_{ce,n}\)&lt;/span&gt;是非常接近的，所以我們可以說做Soft-Margin SVM，很像是在做L2 Regularized Logistic Regression。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;雖然說Soft-Margin SVM和L2 Regularized Logistic Regression非常的像，但是我在做完Soft-Margin SVM後，仍然沒辦法像Logistic Regression一樣得到一個具有機率分布的Target Function，以下提供了兩種方法，第一種是間接的方法，使用兩階段學習來達成Logistic的效果；第二種是直接將L2 Regularized Logistic Regression加入有如Soft-Margin SVM的Kernel性質。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;使用SVM做Logistic Regression：Probabilistic SVM&lt;/h3&gt;
&lt;p&gt;要讓Soft-Margin SVM在最後呈現的Target Function時具有機率性質，最簡單的作法就是透過兩階段的學習來達成，第一階段先用Soft-Margin SVM去解出切分資料的平面，第二階段再將Logistic Function套在這個平面上，並做Fitting，最後我們就得到一個以Logistic Function表示的Target Function，這個稱之為Probabilistic SVM。實際操作方法如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;使用Soft-Margin SVM解出切平面&lt;span class="math"&gt;\(W_{SVM}^{T}Z+b_{SVM}=0\)&lt;/span&gt;，並將所有Data進一步的轉換到 &lt;span class="math"&gt;\(Z'_{n}=W_{SVM}^{T}Z(X_{n})+b_{SVM}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;接下來用轉換後的結果&lt;span class="math"&gt;\(\{Z'_{n},\ y_{n}\}\)&lt;/span&gt;做Logistic Regression得到係數A和B。&lt;/li&gt;
&lt;li&gt;最後的Target Function就是 &lt;span class="math"&gt;\(g(x)=Θ(A\cdot (W_{SVM}^{T}Z(X_{n})+b_{SVM})+B)\)&lt;/span&gt;，&lt;span class="math"&gt;\(Θ\)&lt;/span&gt;為Logistic Function。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的方法有一個缺點，就是如果B的值不接近0時，SVM的切平面就會和Logistic Regression的邊界就會不同，而且一個Model要Fitting兩次也相當的麻煩，以下還有另外一個可以達到一樣的具有機率性質的效果的方法—Kernel Logistic Regression。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Trick的真正精髓：Representer Theorem&lt;/h3&gt;
&lt;p&gt;在說明Kernel Logistic Regression之前我們先來複習一下Kernel的概念，並且從中將他的重要觀念萃取出來。&lt;/p&gt;
&lt;p&gt;再來看一眼我們怎麼解Kernel Soft-Margin SVM的，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kernel Soft-Margin SVM：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;在&lt;span class="math"&gt;\(0 ≤ α_{n} ≤ C;\ 𝚺_{n} α_{n}y_{n} = 0\)&lt;/span&gt;的限制條件下，求解&lt;span class="math"&gt;\(min. [(1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}K(X_{n},X_{m})-𝚺_{n} α_{n}]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;得到&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;，然後&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(W = 𝚺_{n} α_{n}y_{n}Z_{n}\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(b=y_{sv}-𝚺_{n} α_{n}y_{n}K(X_{n},X_{sv})\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其中W可以想成是由&lt;span class="math"&gt;\(Z_{n}\)&lt;/span&gt;所組合而成的，而決定貢獻程度則反應在放在它前面的係數&lt;span class="math"&gt;\((α_{n}y_{n})\)&lt;/span&gt;，&lt;span class="math"&gt;\(y_{n}\)&lt;/span&gt;決定貢獻的方向，&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;決定影響的程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;數學上，有個理論Representer Theorem可以告訴我們，所有的最佳化問題中，&lt;span class="math"&gt;\(W\)&lt;/span&gt;的最佳解都是由&lt;span class="math"&gt;\(Z_{n}\)&lt;/span&gt;所組合而成的，以線性代數的角度，就是&lt;span class="math"&gt;\(W\)&lt;/span&gt;由&lt;span class="math"&gt;\(Z_{n}\)&lt;/span&gt;所展開(span)，數學上表示成&lt;span class="math"&gt;\(W^*=𝚺_{n} β_{n}Z_{n}\)&lt;/span&gt;。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;這個性質為Kernel Trick提供了一個良好的基礎，每次我們只要遇到&lt;span class="math"&gt;\(W^{*T}Z\)&lt;/span&gt;的部分，我們就可以使用Representer Theorem把問題轉換成&lt;span class="math"&gt;\(W^{*T}Z=𝚺_{n} β_{n}Z_{n}Z=𝚺_{n} β_{n}K(X_{n},X)\)&lt;/span&gt;，就可以使用Kernel Function了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="kernel trick" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_04.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上圖是老師在上課時列出來SVM、PLA和Logistic Regression的W的展開式，你會發現都可以表現成Representer Theorem的形式。&lt;/p&gt;
&lt;p&gt;有了這個概念，我們就可以把很多問題都利用Representer Theorem來轉換，並且套上Kernel Trick。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Logistic Regression&lt;/h3&gt;
&lt;p&gt;那我們有了Representer Theorem就可以直接來轉換L2 Regularized Logistic Regression，讓它有擁有Kernel的效果，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;L2 Regularized Logistic Regression：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. (1/N)×𝚺_{n} ln[1+exp(-y_{n}×(W^{T}Z_{n}))] +  (λ/N)×W^{T}W\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;使用&lt;span class="math"&gt;\(W^*=𝚺_{n} β_{n}Z_{n}\)&lt;/span&gt;代入得，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kernel Logistic Regression: &lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(min. (1/N)×𝚺_{n} ln[ 1+exp(-y_{n}×𝚺_{n} β_{n}K(X_{n},X)) ] +  (λ/N)×𝚺_{n}𝚺_{m} β_{n}β_{m}K(X_{n},X_{m})\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的式子可以使用Grandient Descent來求解&lt;span class="math"&gt;\(β_{n}\)&lt;/span&gt;，進而得到&lt;span class="math"&gt;\(W^*=𝚺_{n} β_{n}Z_{n}\)&lt;/span&gt;。而且在Kernel Function的幫助之下，我們更容易可以做到非常高次的特徵轉換。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Ridge Regression&lt;/h3&gt;
&lt;p&gt;同理，我們也可以把相同技巧套用到Ridge Regression，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ridge Regression：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. (1/N)×𝚺_{n} (y_{n}-W^{T}Z_{n})^{2} +  (λ/N)×W^{T}W\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;使用&lt;span class="math"&gt;\(W^*=𝚺_{n} β_{n}Z_{n}\)&lt;/span&gt;代入得，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kernel Ridge Regression：&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(min. (1/N)×𝚺_{n} (y_{n}-𝚺_{m} β_{m}K(X_{n},X_{m}))^{2} +  (λ/N)×𝚺_{n}𝚺_{m} β_{n}β_{m}K(X_{n},X_{m})\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;上面的式子也可以使用Grandient Descent來求解&lt;span class="math"&gt;\(β_{n}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;另外，這個式子有辦法推出解析解，先把上式可以寫成矩陣形式，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kernel Ridge Regression：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(min. E_{aug}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(E_{aug}=(1/N)×(β^{T}K^{T}Kβ-2β^{T}K^{T}y+y^{T}y) +  (λ/N)×β^{T}Kβ)\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以，由&lt;span class="math"&gt;\(∇E_{aug}=0\)&lt;/span&gt;就可以得到最小值成立的條件為&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(β^*=(λI+K)^{-1}y\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其實這個式子非常像之前在線性模型時使用的Pseudo-Inverse，&lt;/p&gt;
&lt;p&gt;Pseudo-Inverse：&lt;span class="math"&gt;\(W=(X^{T}X)^{-1}X^{T}y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;不過現在更為強大了，可以求得非線性模型+Regularization下的解析解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我們可以使用Kernel Ridge Regression來做分類問題，稱之為Least-Squares SVM (LSSVM) 。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Support Vector Regression (SVR)&lt;/h3&gt;
&lt;p&gt;其實，不管是Kernel Logistic Regression還是Kernel Ridge Regression，這種直接套用Representer Theorem在Regression上的都有一個缺點。&lt;/p&gt;
&lt;p&gt;那就是它們的&lt;strong&gt;&lt;span class="math"&gt;\(β_{n}\)&lt;/span&gt;並不確保大多數是0&lt;/strong&gt;，如果Data筆數非常多的話，這在計算上會是一種負荷。在之前我們討論Kernel SVM時有提到只有Support Vector的數據才會對Model最後的結果有所貢獻，Support Vector的&lt;span class="math"&gt;\(α_{n}&amp;gt;0\)&lt;/span&gt;；而不是Support Vector的數據則沒有貢獻，Non-Support Vector的&lt;span class="math"&gt;\(α_{n}=0\)&lt;/span&gt;。所以你可以想見的是，&lt;strong&gt;&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;大多數是0除了Support Vector外，我們稱這叫做「Sparse &lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;」性質&lt;/strong&gt;，有這樣的性質可以大大的減少計算量。&lt;/p&gt;
&lt;p&gt;因此接下來我們打算&lt;strong&gt;讓Regression具有Support Vector的性質，稱之為Support Vector Regression (SVR)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="SVR" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.006.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;見上圖說明，Support Vector Regression簡稱SVR，以往的Linear Regression是求一條擬合直線能使所有數據點到直線的Error最小，而現在我們賦予它Soft-Margin的能力，&lt;strong&gt;SVR將擬合直線向外擴張距離ε，在這個擴張的區域裡頭的數據點不去計算它的Error，只有在超出距離ε外的才去計算Error&lt;/strong&gt;，此時這個擬合直線有點像一條水管，水管外我們才計算Error，所以又稱之為Tube Regression。&lt;/p&gt;
&lt;p&gt;這個概念和Soft-Margin SVM有點像，都是在邊界給予犯錯的機會，不同的是Soft-Margin SVM因為是分類問題，所以不允許錯誤的數據超過界，所以評估Error的方向是向內的，而SVR是向外評估Error，在水管外部上方的Error我們記作&lt;span class="math"&gt;\(ξ_{n}^{⋀}\)&lt;/span&gt;，在水管外部下方的Error我們記作&lt;span class="math"&gt;\(ξ_{n}^{⋁}\)&lt;/span&gt;，&lt;strong&gt;所以SVR的目的就是在Regularization之下使得&lt;span class="math"&gt;\(ξ_{n}^{⋀}+ξ_{n}^{⋁}\)&lt;/span&gt;最小，並且調整距離ε和C來決定對Error的容忍程度&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;這個問題同樣的可以化作Dual問題，問題變成只需要最佳化&lt;span class="math"&gt;\(α_{n}^{⋀}\)&lt;/span&gt;和&lt;span class="math"&gt;\(α_{n}^{⋁}\)&lt;/span&gt;，再使用最佳化後的&lt;span class="math"&gt;\(α_{n}^{⋀}\)&lt;/span&gt;和&lt;span class="math"&gt;\(α_{n}^{⋁}\)&lt;/span&gt;就可以得到&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(b\)&lt;/span&gt;。其中&lt;span class="math"&gt;\(W=𝚺_{n} (α_{n}^{⋀}-α_{n}^{⋁}) Z_{n}\)&lt;/span&gt;這式子裡頭隱含著Representer Theorem，每筆數據的貢獻程度&lt;span class="math"&gt;\(β_{n}=(α_{n}^{⋀}-α_{n}^{⋁})\)&lt;/span&gt;，&lt;strong&gt;因此在管子內的&lt;span class="math"&gt;\(α_{n}^{⋀}=0\)&lt;/span&gt;且&lt;span class="math"&gt;\(α_{n}^{⋁}=0\)&lt;/span&gt;，不會有所貢獻，這使得SVR具有Sparse的性質，可以大大的減少計算&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;這一篇中，我們一開始揭露了「Soft-Margin SVM其實很像L2 Regularized Logistic Regression」的這個現象，所以在SVM中最小化&lt;span class="math"&gt;\(W^{T}W\)&lt;/span&gt;有點像是Regression中的Regularization，也因為形式上相當的接近，所以在SVM裡頭用到的數學技巧同樣的可以套到這些有Regularized的Regression上。&lt;/p&gt;
&lt;p&gt;然後，我們從Kernel Soft-Margin SVM中萃取出Kernel Trick的精華—Representer Theorem，最佳化的W可以由Data的Feature &lt;span class="math"&gt;\(Z_{n}\)&lt;/span&gt;所組成，記作&lt;span class="math"&gt;\(W^*=𝚺_{n} β_{n}Z_{n}\)&lt;/span&gt;，這提供了Kernel Trick背後的實踐基礎，接下來我們就開始運用Representer Theorem在L2 Regularized Logistic Regression和Ridge Regression上，讓這些Regression可以輕易的做非線性特徵轉換。&lt;/p&gt;
&lt;p&gt;最後，我們指出了直接套用Representer Theorem在Regression上的缺點就是參數並不Sparse，所以造成計算量大大增加。因此Support Vector Regression (SVR)參照Soft-Margin SVM的形式重新設計Regression，並且使用Dual Transformation和Kernel Function來轉化問題，最後SVR就具有Sparse的特性了。&lt;/p&gt;
&lt;p&gt;上一篇跟這一篇，談的是「Kernel Models」，在這樣的形式下我們可以讓我們的「特徵轉化」變得更為複雜，甚至是無窮多次方還是做得到的。下一篇，我們會進到另外一個主題—Aggregation Models。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (2)：Support Vector Machine (SVM)</title><link href="https://ycc.idv.tw/ml-course-techniques_2.html" rel="alternate"></link><published>2017-02-20T12:00:00+08:00</published><updated>2017-02-20T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-02-20:/ml-course-techniques_2.html</id><summary type="html">&lt;p&gt;本篇內容涵蓋Hard-Margin Support Vector Machine (SVM)、Kernel Function、Kernel Hard-Margin SVM、Soft-Margin SVM、Kernel Soft-Margin SVM、拉格朗日乘子法（Lagrange Multiplier）、Lagrangian Dual Problem&lt;/p&gt;</summary><content type="html">&lt;p&gt;在&lt;a href="http://www.ycc.idv.tw/ml-course-techniques_1.html"&gt;上一篇文章&lt;/a&gt;當中，我們掃過了《機器學習技法》 將會包含的內容，今天我們正式來看SVM。&lt;/p&gt;
&lt;p&gt;如果我想要使用無窮次高次方的非線性轉換加入我的Model，可以做到嗎？上一篇，我告訴大家，只要使用Dual Transformation加上Kernel Function等數學技巧就可以做到，我們今天就來看一下這是怎麼一回事。&lt;/p&gt;
&lt;p&gt;本篇文章分為兩個部分，第一部分我盡量不牽扯太多數學計算，而將數學證明放在第二個部分，數學證明的部分非常複雜，但我並不打算把它們忽略掉，因為這些數學計算是相當重要的，它所帶來的方法和概念是可以重複使用的，也有助於你了解和創造其他演算法，所以有心想要成為專家的你請耐心的把後半段的數學看完。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Hard-Margin Support Vector Machine (SVM)&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Hard-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.001.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;回到我們最熟悉的二元分類問題，如果問題的答案是線性可分的話，我們可以找到一條直線把兩類Data給切開來，而在以前PLA的方法，切在哪裡其實是沒辦法決定的，PLA只能幫你找到可以分開兩類的一刀，但不能幫你把這刀切的更好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我們希望這個切開兩類的邊界可以離兩類Data越遠越好，讓邊界到Data有一個較大的空白區，這就是Hard-Margin SVM做的事&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;我們先來看一下如何計算切平面到任意Data的距離，首先我先假設切平面的方程式為
&lt;/p&gt;
&lt;div class="math"&gt;$$
W^T X+b = 0 (切平面)
$$&lt;/div&gt;
&lt;p&gt;
回想一下高中數學，這個平面的法向量是W，垂直於平面，所以垂直於平面的單位法向量是 &lt;span class="math"&gt;\(W/|W|\)&lt;/span&gt;，今天如果我有一點Data Point落在&lt;span class="math"&gt;\(X\)&lt;/span&gt;，另外在平面上任意再找一點&lt;span class="math"&gt;\(X_0\)&lt;/span&gt;，從&lt;span class="math"&gt;\(X_0\)&lt;/span&gt;到&lt;span class="math"&gt;\(X\)&lt;/span&gt;的向量表示為&lt;span class="math"&gt;\(X-X_0\)&lt;/span&gt;，這個向量如果投影到單位法向量上，這個向量的大小正是Data Point到平面的最短距離，表示成
&lt;/p&gt;
&lt;div class="math"&gt;$$
d = |W\cdot (X - X_0)| / |W|
$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(X_0\)&lt;/span&gt;符合切平面的方程式&lt;span class="math"&gt;\(W^T X_0+b = 0\)&lt;/span&gt;代入，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
d = |W\cdot X + b| / |W|
$$&lt;/div&gt;
&lt;p&gt;
所以假如我有一群線性可分的二元分類Data，這個切平面我希望可以離兩類Data越遠越好，所以我會有一段全部都沒有Data的空白區，這邊假設這個空白區的邊界為
&lt;/p&gt;
&lt;div class="math"&gt;$$
W^TX+b = ±1
$$&lt;/div&gt;
&lt;p&gt;
這個假設是可以做到的，因為我們可以以比例去調整&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(b\)&lt;/span&gt;來達到縮放的效果，而不會影響切平面&lt;span class="math"&gt;\(W^T X+b = 0\)&lt;/span&gt; 。從上面的距離公式，我們知道在這個假設之下，空白區邊界距離切平面為
&lt;/p&gt;
&lt;div class="math"&gt;$$
margin = 1 / |W|
$$&lt;/div&gt;
&lt;p&gt;
而剛好落在這空白區邊界的Data會符合以下方程式&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(y_n\times (W^T X_n+b) = 1\ (Support\ Vector)\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;的正負剛好和&lt;span class="math"&gt;\((W^T X_n+b)\)&lt;/span&gt;相抵消，&lt;strong&gt;這些落在空白區邊界的Data被稱為Support Vector，就字面上的意義就像是空白區由這一些數據給「撐」起來，而切平面只由這些Support Vector的數據點所決定，和其他的數據點無關&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果考慮所有Data的話，應該要滿足
&lt;/p&gt;
&lt;div class="math"&gt;$$
y_n\times (W^T X_n+b) ≥ 1\ (All\ Data)
$$&lt;/div&gt;
&lt;p&gt;
&lt;strong&gt;綜合上述，Hard-Margin SVM的目標就是，在符合&lt;span class="math"&gt;\(y_n\times (W^T X_n+b) ≥ 1 ,\ n=1~N\)&lt;/span&gt;的條件下，求&lt;span class="math"&gt;\(Margin (1 / |W|)\)&lt;/span&gt;最大的情形，也可以等價於求&lt;span class="math"&gt;\((W^T W/2)\)&lt;/span&gt; 最小的情形，這個問題有辦法使用QP Solver來求解，詳見&lt;a href="https://en.wikipedia.org/wiki/Quadratic_programming"&gt;這裡&lt;/a&gt;，我就不多加介紹這個數學工具。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Function&lt;/h3&gt;
&lt;p&gt;Kernel Function是最終可以讓我們有無限多次方特徵的數學工具，但這個工具非常容易理解。&lt;/p&gt;
&lt;p&gt;假設考慮一個非線性轉換，將&lt;span class="math"&gt;\(X\)&lt;/span&gt;空間轉換到&lt;span class="math"&gt;\(Z\)&lt;/span&gt;空間，那如果我需要計算轉換過的兩個新Features相乘&lt;span class="math"&gt;\(Z_n (X_n)\times Z_m(X_m)\)&lt;/span&gt;，我有辦法&lt;strong&gt;不需要先做特徵轉換再相乘&lt;/strong&gt;，而是直接使用原有的Features &lt;span class="math"&gt;\(X_n\)&lt;/span&gt;和&lt;span class="math"&gt;\(X_m\)&lt;/span&gt;求出&lt;span class="math"&gt;\(Z_n(X_n)×Z_m(X_m)\)&lt;/span&gt;的最後結果？這種情形數學可以表示成&lt;span class="math"&gt;\(K(X_n,X_m)=Z_n(X_n)×Z_m(X_m)\)&lt;/span&gt;，這個函式就叫Kernel Function。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果有了Kernel Function這樣的數學工具，就可以簡化和優化因為「特徵轉換」所帶來的複雜計算。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我列出以下幾種Kernel Function：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Polynomial Kernel：&lt;span class="math"&gt;\(K_Q(X_n,X_m)=(ζ+γ X_n^T X_m)^Q\)&lt;/span&gt;等價於 「Q次方非線性轉換後的兩個新特徵相乘」。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Guassian Kernel：&lt;span class="math"&gt;\(K(X_n,X_m)=exp(-γ|X_n-X_m|^2)\)&lt;/span&gt;等價於 「無窮次方非線性轉換後的兩個新特徵相乘」。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此有了Guassian Kernel的幫忙，我們完全不需要管特徵轉換有多複雜，我們可以直接使用原有的Features 來計算「無窮次方的非線性轉換」。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;最後給予Kernel Function一個物理解釋，Kernel Function說穿了就是兩個向量轉換到Z空間後的「內積」，「內積」可以約略想成是「相似程度」，當兩個向量同向，內積是正的，相似度高，但當兩個向量反向，內積是負的，相似度極低，所以你會發現Guassian Kernel在&lt;span class="math"&gt;\(X_n=X_m\)&lt;/span&gt;會出現最大值，因為代表這兩個位置相似度極高。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Hard-Margin SVM&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Kernel Hard-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.002.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;那我們如何使用Kernel Function來使得Hard-Margin SVM更厲害呢？我們必須額外引入另外的數學工具，包括：Lagrange Multiplier和Lagrange Dual Problem，才有辦法把Kernel Function用上，不過這部份的數學有一些複雜，我將這部份的證明放在後面的附錄上，這邊就直接從結果講起。&lt;/p&gt;
&lt;p&gt;Kernel Hard-Margin SVM的公式是，在&lt;span class="math"&gt;\(α_n  ≥ 0; 𝚺_n α_n y_n = 0\)&lt;/span&gt;的限制條件下，求解&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;使得 &lt;span class="math"&gt;\([(1/2)𝚺_n 𝚺_m  α_n α_m y_n y_m K(X_n,X_m)-𝚺_n α_n]\)&lt;/span&gt;為最小值，&lt;/p&gt;
&lt;p&gt;其中&lt;span class="math"&gt;\(K(X_n,X_m)\)&lt;/span&gt;就是Kernel Function，由你的特徵轉換方式來決定，這個問題一樣可以使用QP Solver來求解。&lt;/p&gt;
&lt;p&gt;當我們已經有了每筆數據點的&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;了，接下來可以利用&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;求出切平面的W和b，在那之前來看一下&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;的意義，&lt;strong&gt;&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;可以看作是某個數據點對切平面的貢獻程度，&lt;span class="math"&gt;\(α_n=0\)&lt;/span&gt;的這些數據點為非Support Vector，而&lt;span class="math"&gt;\(α_n&amp;gt;0\)&lt;/span&gt;的這些數據點是Support Vector，所以對切平面有貢獻的只有Support Vector而已&lt;/strong&gt;，這和剛剛的結論相同。因此，W和b可由Support Vector決定，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(W = 𝚺_{n=sv} α_n y_n Z_n\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(b=y_{sv}-𝚺_n α_n y_n K(X_n,X_{sv})\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;最後提一個非常重要的概念，是什麼原因讓我們不需要管特徵轉換的複雜度？以往我們的作法是這樣的，我們有每筆Data的Features，接下來對每筆Data做特徵轉換，然後在用特徵轉換後的新Features去Train線性模型，這麼一來如果特徵轉換的次方非常高的話，計算的複雜度就會全落在特徵轉換上。&lt;strong&gt;所以我們巧妙的使用數學工具，讓我們可以單單使用Data的Labels來做優化，而將複雜的特徵轉換利用Kernel Function的方式「嵌入」到優化的過程裡頭，此時計算量就只與Data數量有關，所以可以完全不管特徵轉換所帶來的複雜度&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Hard-Margin SVM: 無窮次方的特徵轉換效果如何?&lt;/h3&gt;
&lt;p&gt;終於我們可以使用無窮次方的特徵轉換了，只要使用Kernel Hard-Margin SVM搭配上Guassian Kernel：&lt;span class="math"&gt;\(K(X_n,X_m)=exp(-γ|X_n-X_m|^2)\)&lt;/span&gt;就可以辦到，下圖是模擬的結果，是不是看起來很強大，隨著γ的不同會有不一樣的切分方法，&lt;strong&gt;你會發現γ越大時看起來的結果越接近Overfitting，所以必須小心挑選γ的大小。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Guassian Kernel in Hard-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_01.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/203_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/203_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Soft-Margin SVM&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Soft-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.003.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;剛剛Hard-Margin SVM會很容易Overfitting的原因在於它的機制無法&lt;strong&gt;容忍雜訊&lt;/strong&gt;，所以接下來要講的Soft-Margin SVM可以容忍部份的Data違反規則，讓它們可以超出空白區的邊界。&lt;/p&gt;
&lt;p&gt;見上圖，可以發現我們稍微修改了Hard-Margin SVM，加入了參數&lt;span class="math"&gt;\(ξ_n\)&lt;/span&gt;，&lt;span class="math"&gt;\(ξ_n\)&lt;/span&gt;代表錯誤的Data離空白區邊界有多遠，而我們將&lt;span class="math"&gt;\(ξ_n\)&lt;/span&gt;的總和加進去Cost裡面，在優化的過程中將使違反的狀況不會太多和離邊界太遠，&lt;strong&gt;而參數C負責控制&lt;span class="math"&gt;\(ξ_n\)&lt;/span&gt;總和的影響程度，如果C很大，代表不大能容忍雜訊；如果C很小，則代表對雜訊的容忍很寬鬆&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;因此我們現在有兩種Support Vector，一種是剛好落在空白區邊界的，稱為Free Support Vector；另外一種是違反規則並超出空白區的，稱為Bounded Support Vector，切平面一樣是由這些Support Vector所決定。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Soft-Margin SVM&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Kernel Soft-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.004.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;接下來同樣的對Soft-Margin SVM做數學上Lagrange Multiplier和Lagrange Dual Problem的轉換，再將Kernel Function用上，一樣的，我將這部份的證明放在後面的附錄上，這邊就直接從結果講起。&lt;/p&gt;
&lt;p&gt;Kernel Soft-Margin SVM的公式是，在&lt;span class="math"&gt;\(0 ≤ α_n ≤ C;\ 𝚺_n α_n y_n = 0\)&lt;/span&gt;的限制條件下，求解&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;使得 &lt;span class="math"&gt;\([(1/2)𝚺_n 𝚺_m α_n α_m y_n y_m K(X_n ,X_m)-𝚺_n α_n]\)&lt;/span&gt;為最小值，&lt;/p&gt;
&lt;p&gt;你會發現和Kernel Hard-Margin SVM唯一只差在&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;被&lt;span class="math"&gt;\(C\)&lt;/span&gt;所限制。&lt;/p&gt;
&lt;p&gt;當我們已經有了每筆數據點的&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;了，接下來可以利用&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;求出切平面的&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(b\)&lt;/span&gt;，&lt;span class="math"&gt;\(α_n\)&lt;/span&gt;一樣的可以看作是某個數據點對切平面的貢獻程度，&lt;span class="math"&gt;\(α_n=0\)&lt;/span&gt;的這些數據點為非Support Vector，而&lt;span class="math"&gt;\(α_n&amp;gt;0\)&lt;/span&gt;的這些數據點是Support Vector，可以進一步細分，&lt;span class="math"&gt;\(α_n &amp;lt; C\)&lt;/span&gt;為Free Support Vector，而&lt;span class="math"&gt;\(α_n＝C\)&lt;/span&gt;為Bounded Support Vector。相同的，W和b可由Support Vector (Free Support Vector和Bounded Support Vector)決定，跟Kernel Hard-Margin SVM公式一模一樣&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(W = 𝚺_{n=sv} α_n y_n Z_n\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(b=y_{sv} -𝚺_n α_n y_n K(X_n,X_{sv})\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Kernel Soft-Margin SVM: 容忍雜訊的無窮次方特徵轉換&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Guassian Kernel in Soft-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_02.png"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/204_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/204_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;來看看Kernel Soft-Margin SVM搭配上Guassian Kernel的效果如何，上圖是模擬的結果，我們會發現有部分Data違反分類規則，所以Soft-Margin SVM確實可以容忍雜訊，而且&lt;span class="math"&gt;\(C\)&lt;/span&gt;越小，容忍雜訊的能力越強，所以要特別注意&lt;span class="math"&gt;\(C\)&lt;/span&gt;的選取，如果沒有選好還是可能造成Overfitting的。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;結語&lt;/h3&gt;
&lt;p&gt;在這一篇當中，我們介紹了Hard-Margin SVM和Soft-Margin SVM，並且成功的利用數學工具將問題轉換成，可以單單使用Data的Labels來做優化，而將複雜的特徵轉換利用Kernel Function的方式「嵌入」到優化的過程裡頭，此時計算量就只與Data數量有關，所以可以完全不管特徵轉換所帶來的複雜度，因此利用Guassian Kernel就可以做到「無窮多次的特徵轉換」了。最後再次強調數學的部分非常重要，它提供的方法和概念是可以重複使用的，而這部份的數學是少不了的，所以有興趣的可以繼續往下看下去。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;[進階] 拉格朗日乘子法（Lagrange Multiplier）&lt;/h3&gt;
&lt;p&gt;如果是物理系學生修過古典力學，應該對這個數學工具不陌生。&lt;strong&gt;Lagrange Multiplier是用在有限制條件之下的求極值問題&lt;/strong&gt;，步驟如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;問題：在限制 &lt;span class="math"&gt;\(g_i (x_1,x_2, … , x_n) = 0,\ i=1\cdots k\)&lt;/span&gt;  之下，求 &lt;span class="math"&gt;\(f(x_1,x_2, … , x_n)\)&lt;/span&gt; 的極值&lt;/li&gt;
&lt;li&gt;假設Lagrange Function：   &lt;span class="math"&gt;\(L(x_1,x_2, … , x_n,λ_i) = f(x_1,x_2, … , x_n) + 𝚺_i λ_i × g_i(x_1,x_2, … , x_n)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;聯立方程式求解：&lt;/li&gt;
&lt;li&gt;找L的極值：&lt;span class="math"&gt;\(∇L = 0\)&lt;/span&gt;  [Stationarity Condition]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(g_i (x_1,x_2, … , x_n) = 0,\ i=1\cdots k\)&lt;/span&gt;  [Primal Feasibility Condition]&lt;/li&gt;
&lt;li&gt;求解以上聯立方程式得到最佳解 &lt;span class="math"&gt;\(x_{1},x_{2}, … , x_{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上面的聯立方程式不難理解，Primal Feasibility Condition就是我們的限制式，然後Stationarity Condition就是求極值的方法，非常直觀，滿足上面的式子我們就可以在限制上面找極值。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;上面是一般的Lagrange Multiplier，只有考慮到限制式是等式的情形，假如限制條件是不等式呢？我們來看一下加強版的Lagrange Multiplier：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;問題：在限制 &lt;span class="math"&gt;\(g_{i}(x_{1},x_{2}, … , x_{n}) = 0,\ i=1\cdots k\)&lt;/span&gt; 且  &lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) ≤ 0,\ j=1\cdots r\)&lt;/span&gt; 之下，求 &lt;span class="math"&gt;\(f(x_{1},x_{2}, … , x_{n})\)&lt;/span&gt; 的極值&lt;/li&gt;
&lt;li&gt;假設Lagrange Function：   &lt;span class="math"&gt;\(L(x_{1},x_{2}, … , x_{n}, λ_{i},μ_{j}) = f(x_{1},x_{2}, … , x_{n}) + 𝚺_{i} λ_{i} × g_{i}(x_{1},x_{2}, … , x_{n}) + 𝚺_{j} μ_{j} × h_{j}(x_{1},x_{2}, … , x_{n})\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;聯立方程式求解：&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;找&lt;span class="math"&gt;\(L\)&lt;/span&gt;的極值：&lt;span class="math"&gt;\(∇L = 0\)&lt;/span&gt;  [Stationarity Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(g_{i}(x_{1},x_{2}, … , x_{n}) = 0,\ i=1\cdots k\)&lt;/span&gt; 且 &lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) ≤ 0,\ j=1\cdots r\)&lt;/span&gt;  [Primal Feasibility Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="math"&gt;\(μ_{j}  × h_{j} (x_{1},x_{2}, … , x_{n}) = 0,\ j=1\cdots r\)&lt;/span&gt;  [Complementary Slackness Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;求&lt;span class="math"&gt;\(L\)&lt;/span&gt;的最小值時 &lt;span class="math"&gt;\(μ_{j} ≥ 0,\ j=1\cdots r\)&lt;/span&gt;；求&lt;span class="math"&gt;\(L\)&lt;/span&gt;的最大值時 &lt;span class="math"&gt;\(μ_{j} ≤ 0,\ j=1\cdots r\)&lt;/span&gt; [Dual Feasibility Condition]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;以上的條件包括Stationarity、Primal Feasibility、Complementary Slackness、Dual Feasibility通稱 KKT (Karush-Kuhn-Tucker) Conditions&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;加強版的Lagrange Multiplier和一般版的一樣有Stationarity Condition和Primal Feasibility Condition。唯一增加的是Complementary Slackness Condition和Dual Feasibility Condition。&lt;/p&gt;
&lt;p&gt;先來講一下Complementary Slackness Condition怎麼來的，我們來考慮不等式條件&lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) ≤ 0\)&lt;/span&gt;，會有兩個情形發生，一個是壓到邊界，也就是&lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) = 0\)&lt;/span&gt;，這個時候問題就回到一般版的Lagrange Multiplier，此時&lt;span class="math"&gt;\(μ_{j}\)&lt;/span&gt;和&lt;span class="math"&gt;\(λ_{i}\)&lt;/span&gt;效果是一樣的，&lt;span class="math"&gt;\(μ_{j}\)&lt;/span&gt;可以是任意值；另外一種情況是我沒壓到邊界，也就是&lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) &amp;lt; 0\)&lt;/span&gt;，這個時候我可以把這個限制看作不存，最簡易的方法就是令&lt;span class="math"&gt;\(μ_{j}=0\)&lt;/span&gt;，他在&lt;span class="math"&gt;\(L(x_{1},x_{2}, … , x_{n}, λ_{i},μ_{j})\)&lt;/span&gt; 中就不參與作用了。&lt;strong&gt;所以綜合壓到邊界和不壓到兩種情況，我們可以寫出一個有開關效果的方程式 &lt;span class="math"&gt;\(μ_{j} × h_{j}(x_{1},x_{2}, … , x_{n}) = 0\)&lt;/span&gt;，這就是Complementary Slackness Condition。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外一個是Dual Feasibility Condition，這個限制一樣是在不等式條件才會發生，&lt;span class="math"&gt;\(μ_{j}\)&lt;/span&gt;的正負號取決於&lt;span class="math"&gt;\(L\)&lt;/span&gt;是要求最大還是求最小值，稍微解釋一下，找極值我們用&lt;span class="math"&gt;\(∇L = 0\)&lt;/span&gt;這個式子來求，代入Lagrange Function後得&lt;span class="math"&gt;\(∇L = ∇f +𝚺_{i}λ_{i}×∇g_{i}+𝚺_{j}μ_{j}×∇h_{j}=0\)&lt;/span&gt;，先定性來看，假設不計&lt;span class="math"&gt;\(∇g_{i}\)&lt;/span&gt;的影響，當最後解落在&lt;span class="math"&gt;\(h ≤ 0\)&lt;/span&gt;的邊界上時&lt;span class="math"&gt;\(∇f＝- μ×∇h\)&lt;/span&gt;，因為&lt;span class="math"&gt;\(h ≤ 0\)&lt;/span&gt;的關係，所以&lt;span class="math"&gt;\(∇h\)&lt;/span&gt;是朝向可行區的外面，如果今天是求&lt;span class="math"&gt;\(f\)&lt;/span&gt;的極小值，那們&lt;span class="math"&gt;\(∇f\)&lt;/span&gt;應當朝著可行區才合理，如果不是的話則可行區內部有更小更佳的解，所以求極小值時&lt;span class="math"&gt;\(μ ≥ 0\)&lt;/span&gt;；如果是求&lt;span class="math"&gt;\(f\)&lt;/span&gt;的極大值，那&lt;span class="math"&gt;\(∇f\)&lt;/span&gt;應當朝著可行區的外面，所以&lt;span class="math"&gt;\(μ ≤ 0\)&lt;/span&gt;，這個條件待會會用在對偶問題上面。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;其實我們之前在《機器學習基石》裡的Regularization有偷用了Lagrange Multiplier的產物。&lt;/p&gt;
&lt;p&gt;Regularization將W的長度限制在一個範圍，表示成
&lt;/p&gt;
&lt;div class="math"&gt;$$
|W|^{2} ≤ C
$$&lt;/div&gt;
&lt;p&gt;
在這個條件下我們要找&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的極小值，使用加強版的Lagrange Multiplier：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;問題：在限制  &lt;span class="math"&gt;\(|W|^{2} - C ≤ 0\)&lt;/span&gt; 之下，求 &lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt; 的極小值&lt;/li&gt;
&lt;li&gt;假設Lagrange Function：   &lt;span class="math"&gt;\(L = E_{in} + μ × ( |W|^{2} - C)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;聯立方程式求解：&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉W = 𝞉E_{in} / 𝞉W + 2μ × |W| = 0\)&lt;/span&gt;  [Stationarity Condition]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(|W|^{2} - C ≤ 0\)&lt;/span&gt;  [Primal Feasibility Condition]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(μ × ( C - |W|^{2} ) = 0\)&lt;/span&gt;  [Complementary Slackness Condition]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stationarity Condition的結果就是Regularization的結果了，可以&lt;a href="http://www.ycc.idv.tw/ml-course-foundations_4.html"&gt;回去參照一下&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;[進階] Lagrangian Dual Problem&lt;/h3&gt;
&lt;p&gt;接下來來講對偶問題，這個部分很難，我也是反覆在網路上看了很多篇介紹才弄懂，推薦大家看&lt;a href="http://www.eng.newcastle.edu.au/eecs/cdsc/books/cce/Slides/Duality.pdf"&gt;這一篇&lt;/a&gt;，這篇介紹的很清楚，應該會對大家理解Lagrangian Dual有幫助。&lt;/p&gt;
&lt;p&gt;來考慮一下待會會用到的求極小值問題，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在限制 &lt;span class="math"&gt;\(g_{i}(x_{1},x_{2}, … , x_{n}) = 0,\ i=1\cdots k\)&lt;/span&gt; 且  &lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) ≤ 0,\ j=1\cdots r\)&lt;/span&gt; 之下，求 &lt;span class="math"&gt;\(f(x_{1},x_{2}, … , x_{n})\)&lt;/span&gt; 的極小值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果我們利用剛剛的解法，稱之為Lagrangian Primal Problem。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;而這個問題可以等效轉換成Lagrangian Dual Problem，利用以下關係式&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="math"&gt;\(Minimum Problem ≡ min. L  ≡ min. [max._{μ ≥ 0} L] ≥ max._{μ ≥ 0} [min. L(μ)]\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我們在將原本&lt;span class="math"&gt;\(min. L\)&lt;/span&gt; 換成&lt;span class="math"&gt;\(min. [max._{μ ≥ 0} L]\)&lt;/span&gt; 是不影響結果的，因為我們剛剛分析過了在求最小值時&lt;span class="math"&gt;\(μ ≥ 0\)&lt;/span&gt;是合理的，相反的如果&lt;span class="math"&gt;\(μ &amp;lt; 0\)&lt;/span&gt;，則求&lt;span class="math"&gt;\(max._{μ ≥ 0} L\)&lt;/span&gt;時會產生無限大的結果，接下來就是交換&lt;span class="math"&gt;\(min.\)&lt;/span&gt;和&lt;span class="math"&gt;\(max.\)&lt;/span&gt;的部分，數學上可以證明&lt;span class="math"&gt;\(min. [max._{μ ≥ 0} L] ≥ max._{μ ≥ 0} [min. L(μ)]\)&lt;/span&gt;這樣的關係，我們就稱左式轉到右式為Dual轉換。&lt;/p&gt;
&lt;p&gt;而上面式子右側的求法，我們可以先求出&lt;span class="math"&gt;\(Θ(λ_{i},μ_{j}) =\ given\ λ_{i},μ_{j}\ to\ find\ min. L(x_{1},x_{2}, … , x_{n}, λ_{i},μ_{j})\)&lt;/span&gt; ，作法是使用&lt;span class="math"&gt;\(∇L = 0\)&lt;/span&gt;所產生符合極值的參數代入&lt;span class="math"&gt;\(L(x_{1},x_{2}, … , x_{n}, λ_{i},μ_{j})\)&lt;/span&gt;，換成以&lt;span class="math"&gt;\(λ_{i}\)&lt;/span&gt;,&lt;span class="math"&gt;\(μ_{j}\)&lt;/span&gt;表示的&lt;span class="math"&gt;\(Θ(λ_{i},μ_{j})\)&lt;/span&gt;。然後，再求&lt;span class="math"&gt;\(Θ(λ_{i},μ_{j})\)&lt;/span&gt;的最大值，就可以了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;經過Dual轉換後，我們將原本在&lt;span class="math"&gt;\(x_{1},x_{2}, … , x_{n}\)&lt;/span&gt;的問題轉換到&lt;span class="math"&gt;\(λ_{i},μ_{j}\)&lt;/span&gt;的空間上。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;這個轉換我們可以使用下面的圖來解釋，&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lagrangian Dual Geometric Interpretation" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.005.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;我們先不管&lt;span class="math"&gt;\(g(x)\)&lt;/span&gt;的部分只看&lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(h(x)\)&lt;/span&gt;的部分，假設所有的Data &lt;span class="math"&gt;\(x\)&lt;/span&gt;映射到&lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(h(x)\)&lt;/span&gt;會產生一塊區域&lt;span class="math"&gt;\(G\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;在Primal Problem中我們可以很容易的找出&lt;span class="math"&gt;\(h_{j}(x_{1},x_{2}, … , x_{n}) ≤ 0\)&lt;/span&gt;的限制之下&lt;span class="math"&gt;\(f(x_{1},x_{2}, … , x_{n})\)&lt;/span&gt; 的最小值，見上圖左側。&lt;/p&gt;
&lt;p&gt;見上圖中間，Dual Problem採取另外一個方法，它先去找
&lt;/p&gt;
&lt;div class="math"&gt;$$
Θ(μ) = given\ μ\ to\ find\ min. L(x,μ),\ where: L(x,μ) = f(x)+μh(x)。
$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(f(x)+μh(x)=α\)&lt;/span&gt;在圖中的平面上是一條直線，而&lt;span class="math"&gt;\(f(x)+μh(x)\)&lt;/span&gt;的值也就是&lt;span class="math"&gt;\(α\)&lt;/span&gt;也正好是它的「截距」，所以在給定&lt;span class="math"&gt;\(μ\)&lt;/span&gt;後要最小化&lt;span class="math"&gt;\(f(x)+μh(x)\)&lt;/span&gt;的方法，就等效於固定直線斜率最小化截距，所以最後這個直線就必須要切於&lt;span class="math"&gt;\(G\)&lt;/span&gt;才能使得截距最小，所以我們得到一條切於&lt;span class="math"&gt;\(G\)&lt;/span&gt;且斜率&lt;span class="math"&gt;\((-μ)\)&lt;/span&gt;的直線， 因此我們就順利的得到&lt;span class="math"&gt;\(Θ(μ)\)&lt;/span&gt;的關係式了，接下來我要找出&lt;span class="math"&gt;\(Θ(μ)\)&lt;/span&gt;的最大值，所以就必須往上推，這個時候你就發現答案和前面Primal Problem答案一模一樣，這種最佳化答案相同的情況稱為「Strong Duality」，而最佳化答案不相同的情況就叫做「Weak Duality」，見上圖右側，在這種&lt;span class="math"&gt;\(G\)&lt;/span&gt;的形狀下，就會產生最佳化答案不相同的情況。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;[進階] Hard-Margin SVM Dual + Kernel Function = Kernel Hard-Margin SVM&lt;/h3&gt;
&lt;p&gt;那我們現在可以正式的把Lagrangian Dual的東西放到Hard-Margin SVM上面。&lt;/p&gt;
&lt;p&gt;回想一下Hard-Margin SVM的問題是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在&lt;span class="math"&gt;\(y_{n}×(W^{T}X_{n}+b) ≥ 1 ,\ n=1\cdots N\)&lt;/span&gt;的條件下，求&lt;span class="math"&gt;\((W^{T}W/2)\)&lt;/span&gt; 最小的情形。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;那如果加上非線性轉換，從&lt;span class="math"&gt;\(X\)&lt;/span&gt;空間轉到&lt;span class="math"&gt;\(Z\)&lt;/span&gt;空間，則問題變成&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在&lt;span class="math"&gt;\(y_{n}×(W^{T}Z_{n}+b) ≥ 1 ,\ n=1\cdots N\)&lt;/span&gt;的條件下，求&lt;span class="math"&gt;\((W^{T}W/2)\)&lt;/span&gt; 最小的情形。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以我們可以使用Lagrangian Multiplier來解決問題，依以下步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;假設Lagrange Function：   &lt;span class="math"&gt;\(L(W,b,α) = (W^{T}W/2) +  𝚺_{n} α_{n} × [1-y_{n}×(W^{T}Z_{n}+b)]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;考慮Primal Feasibility、Complementary Slackness、Dual Feasibility的限制&lt;/li&gt;
&lt;li&gt;Primal Feasibility Condition：&lt;span class="math"&gt;\(1-y_{n}×(W^{T}Z_{n}+b) ≤ 0\)&lt;/span&gt; [式1-1]&lt;/li&gt;
&lt;li&gt;Complementary Slackness Condition：&lt;span class="math"&gt;\(α_{n}  × [1-y_{n}×(W^{T}Z_{n}+b)] = 0\)&lt;/span&gt; [式1-2]&lt;/li&gt;
&lt;li&gt;Dual Feasibility Condition：&lt;span class="math"&gt;\(α_{n}  ≥ 0\)&lt;/span&gt; [式1-3]&lt;/li&gt;
&lt;li&gt;先求出&lt;span class="math"&gt;\(Θ(α) = given α to find min. L(W,b,α)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉b = - 𝚺_{n} α_{n}y_{n} = 0\)&lt;/span&gt; [式1-4]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉W_{n} =  |W|- 𝚺_{n} α_{n}y_{n}Z_{n} = 0\)&lt;/span&gt;，&lt;span class="math"&gt;\(y_{n}Z_{n}\)&lt;/span&gt;應該和&lt;span class="math"&gt;\(W\)&lt;/span&gt;同向，所以
     &lt;span class="math"&gt;\(W = 𝚺_{n} α_{n}y_{n}Z_{n}\)&lt;/span&gt; [式1-5]&lt;/li&gt;
&lt;li&gt;因此&lt;span class="math"&gt;\(L(W,b,α)\)&lt;/span&gt;只要滿足[式1-4]和[式1-5]就代表是極小值了&lt;/li&gt;
&lt;li&gt;所以[式1-4]和[式1-5]代入得&lt;span class="math"&gt;\(Θ(α,β) = (-1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}Z_{n}Z_{m}+𝚺_{n} α_{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;求&lt;span class="math"&gt;\(Θ(α)\)&lt;/span&gt;極大值&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(max.[Θ(α)]＝min.[-Θ(α)]=min.[(1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}Z_{n}Z_{m}-𝚺_{n} α_{n}]\)&lt;/span&gt; —[式1-6]&lt;/li&gt;
&lt;li&gt;綜合上述[式1-3]、[式1-4]、[式1-6]並改寫成Kernel的形式得，&lt;span class="math"&gt;\(min. [(1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}K(X_{n},X_{m})-𝚺_{n} α_{n}], s.t. α_{n} ≥ 0 ; \ 𝚺_{n} α_{n}y_{n} = 0\)&lt;/span&gt;，使用QP Solver可以求出 &lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;可以用&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;來求&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(b\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;涵義：觀察[式1-2]可得 (1) &lt;span class="math"&gt;\(α_{n} = 0\)&lt;/span&gt; 為Non-Support Vector； (2) &lt;span class="math"&gt;\(α_{n} &amp;gt; 0\)&lt;/span&gt; 代表&lt;span class="math"&gt;\(y_{n}×(W^{T}Z_{n}+b)=1\)&lt;/span&gt;，為Support Vector。&lt;/li&gt;
&lt;li&gt;由[式1-5]得，&lt;span class="math"&gt;\(W = 𝚺_{n} α_{n}y_{n}Z_{n}\)&lt;/span&gt;，從式子中你會發現對W有貢獻的只有Support Vector &lt;span class="math"&gt;\((α_{n}&amp;gt;0)\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;假設在某個Support Vector(&lt;span class="math"&gt;\(α_{n}&amp;gt;0\)&lt;/span&gt;)上，由[式1-2]可推得，&lt;span class="math"&gt;\(b=y_{sv}-𝚺_{n} α_{n}y_{n}K(X_{n},X_{sv})\)&lt;/span&gt;  (at Support Vector)。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;[進階] Soft-Margin SVM Dual + Kernel Function = Kernel Soft-Margin SVM&lt;/h3&gt;
&lt;p&gt;考慮Soft-Margin SVM和特徵轉換：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在&lt;span class="math"&gt;\(y_{n}×(W^{T}Z_{n}+b) ≥ 1-ξ_{n}\)&lt;/span&gt;且&lt;span class="math"&gt;\(ξ_{n} ≥ 0,\ n=1\cdots N\)&lt;/span&gt;的條件下，求&lt;span class="math"&gt;\((W^{T}W/2) + C 𝚺_{n} ξ_{n}\)&lt;/span&gt;最小的情形。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以我們可以使用Lagrangian Dual Problem來解決問題，依以下步驟：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;假設Lagrange Function：   &lt;span class="math"&gt;\(L(W,b,ξ,α,β) = (W^{T}W/2) + C 𝚺_{n} ξ_{n} +  𝚺_{n} α_{n} × [1-ξ_{n}-y_{n}×(W^{T}Z_{n}+b)] + 𝚺_{n} β_{n} × [-ξ_{n}]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;考慮Primal Feasibility、Complementary Slackness、Dual Feasibility的限制&lt;/li&gt;
&lt;li&gt;Primal Feasibility Condition：&lt;span class="math"&gt;\(1-ξ_{n}-y_{n}×(W^{T}Z_{n}+b) ≤ 0\)&lt;/span&gt; [式2-1]；&lt;span class="math"&gt;\(-ξ_{n} ≤ 0\)&lt;/span&gt; [式2-2]&lt;/li&gt;
&lt;li&gt;Complementary Slackness Condition：&lt;span class="math"&gt;\(α_{n}  × [1-ξ_{n}-y_{n}×(W^{T}Z_{n}+b)] = 0\)&lt;/span&gt; [式2-3]；&lt;span class="math"&gt;\(β_{n} × [-ξ_{n}] = 0\)&lt;/span&gt; [式2-4]&lt;/li&gt;
&lt;li&gt;Dual Feasibility Condition：&lt;span class="math"&gt;\(α_{n}  ≥ 0\)&lt;/span&gt; [式2-5]；&lt;span class="math"&gt;\(β_{n}  ≥ 0\)&lt;/span&gt; [式2-6]&lt;/li&gt;
&lt;li&gt;先求出&lt;span class="math"&gt;\(Θ(α,β) = given\ α,β\ to\ find\ min. L(W,b,ξ,α,β)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉b = - 𝚺_{n} α_{n}y_{n} = 0\)&lt;/span&gt; [式2-7]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉W_{n} =  |W|- 𝚺_{n} α_{n}y_{n}Z_{n} = 0\)&lt;/span&gt;，&lt;span class="math"&gt;\(y_{n}Z_{n}\)&lt;/span&gt;應該和&lt;span class="math"&gt;\(W\)&lt;/span&gt;同向，所以
     &lt;span class="math"&gt;\(W = 𝚺_{n} α_{n}y_{n}Z_{n}\)&lt;/span&gt; [式2-8]&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(𝞉L / 𝞉ξ_{n} = C - α_{n} - β_{n} = 0\)&lt;/span&gt; [式2-9]&lt;/li&gt;
&lt;li&gt;因此&lt;span class="math"&gt;\(L(W,b,ξ,α,β)\)&lt;/span&gt;只要滿足[式2-7]、[式2-8]和[式2-9]就代表是極小值了&lt;/li&gt;
&lt;li&gt;所以[式2-7]、[式2-8]和[式2-9]代入得&lt;span class="math"&gt;\(Θ(α,β) = (-1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}Z_{n}Z_{m}+𝚺_{n} α_{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;求&lt;span class="math"&gt;\(Θ(α,β)\)&lt;/span&gt;極大值&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(max.[Θ(α,β)]＝min.[-Θ(α,β)]=min.[(1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}Z_{n}Z_{m}-𝚺_{n} α_{n}]\)&lt;/span&gt; —[式2-10]&lt;/li&gt;
&lt;li&gt;綜合上述[式2-5]、[式2-6]、[式2-9]、[式2-10]並改寫成Kernel的形式得，&lt;span class="math"&gt;\(min. [(1/2)𝚺_{n}𝚺_{m} α_{n}α_{m}y_{n}y_{m}K(X_{n},X_{m})-𝚺_{n} α_{n}],\ s.t. 0 ≤ α_{n} ≤ C;\  𝚺_{n} α_{n}y_{n} = 0\)&lt;/span&gt;，使用QP Solver可以求出 &lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;。&lt;/li&gt;
&lt;li&gt;可以用&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;來求&lt;span class="math"&gt;\(W\)&lt;/span&gt;和&lt;span class="math"&gt;\(b\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(α_{n}\)&lt;/span&gt;涵義：觀察[式2-3]和[式2-4]可得 (1) &lt;span class="math"&gt;\(α_{n} = 0\)&lt;/span&gt; 為Non-Support Vector； (2) &lt;span class="math"&gt;\(0 &amp;lt; α_{n} &amp;lt; C\)&lt;/span&gt; 代表&lt;span class="math"&gt;\(y_{n}×(W^{T}Z_{n}+b)=1\)&lt;/span&gt;，為Free Support Vector；(3) &lt;span class="math"&gt;\(α_{n} = C\)&lt;/span&gt; 代表&lt;span class="math"&gt;\(y_{n}×(W^{T}Z_{n}+b)=1-ξ_{n}\)&lt;/span&gt;，為Bounded Support Vector。&lt;/li&gt;
&lt;li&gt;由[式2-8]得，&lt;span class="math"&gt;\(W = 𝚺_{n} α_{n}y_{n}Z_{n}\)&lt;/span&gt;，從式子中你會發現對W有貢獻的只有Support Vector (&lt;span class="math"&gt;\(α_{n}&amp;gt;0\)&lt;/span&gt;)。&lt;/li&gt;
&lt;li&gt;假設在某個Support Vector(&lt;span class="math"&gt;\(α_{n}&amp;gt;0\)&lt;/span&gt;且&lt;span class="math"&gt;\(β_{n}&amp;gt;0\)&lt;/span&gt;)上，由[式2-3]和[式2-4]可推得，&lt;span class="math"&gt;\(b=y_{sv}-𝚺_{n} α_{n}y_{n}K(X_{n},X_{sv})\)&lt;/span&gt;  (at Support Vector)。&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習技法 學習筆記 (1)：我們將會學到什麼? 先見林再來見樹</title><link href="https://ycc.idv.tw/ml-course-techniques_1.html" rel="alternate"></link><published>2017-01-12T12:00:00+08:00</published><updated>2017-01-12T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2017-01-12:/ml-course-techniques_1.html</id><summary type="html">&lt;p&gt;有什麼特徵可以使用？ / Embedding Numerous Features ：Kernel Models / Combining Predictive Features：Aggregation Models / Distilling Implicit Features：Extraction Models&lt;/p&gt;</summary><content type="html">&lt;p&gt;在之前四篇文章中，我總結了台大教授林軒田在Coursera上的《機器學習基石》16堂課程，我覺得這是機器學習初學很重要的基礎課程，接下來我要接續更進階的課程。&lt;/p&gt;
&lt;p&gt;林軒田教授的機器學習是兩學期的課，第一學期是《機器學習基石》，第二學期就是接下來這個系列要講的《機器學習技法》，這兩堂課程是有相當大的銜接關係的，所以如果想看這系列的文章，請先看&lt;a href="http://www.ycc.idv.tw/tag/ji-qi-xue-xi-ji-shi.html"&gt;這四篇《機器學習基石》的介紹&lt;/a&gt;或者&lt;a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations"&gt;直接到Coursera上學習&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;《機器學習技法》課程影片可以到老師的Youtube [ &lt;a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2"&gt;https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&lt;/a&gt; ]上收看，投影片可以到老師的個人網站上下載 [ &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/&lt;/a&gt; ]。&lt;/p&gt;
&lt;p&gt;以前，我曾經和實驗室的英國學長聊英國的教育方法，然後我驚人的發現，他的學校在大一就已經學過量子場論（物理上很難的學科XDD）了，我就很好奇量子場論不是需要很深厚的數學基礎嗎？大一是要怎麼教啊？他告訴我，他們大一就會完整走過物理的各大領域，不過是用非常概念的方式來學習，不牽涉到太困難的數學，但這概念的一系列課程卻是四年大學中相當重要的基礎，讓他在開始學細節前就可以知道這些東西未來會用在哪裡？產生了連結讓學習更有效率。&lt;/p&gt;
&lt;p&gt;所以，《機器學習技法》中會介紹很多厲害的機器學習的方法，但這一篇我不直接進去看每個方法的細節，我想帶大家坐著直升機來先看看這遊樂園中有哪些遊樂設施，先來見林再來見樹，會更容易了解。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;有什麼特徵可以使用？&lt;/h3&gt;
&lt;p&gt;在之前《機器學習基石》中，我們講到了Features（特徵）的選擇，&lt;strong&gt;Features（特徵）就是我的Model描述Data的方法，也可以說是影響Data的變數&lt;/strong&gt;，那在之前我們講過Features（特徵）的選擇可以是線性的，那也可以使用「特徵轉換」來產生非線性。&lt;/p&gt;
&lt;p&gt;在這系列文章，我們會看到更多種類的Features，可以分為三類：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Embedding Numerous Features（嵌入大量特徵）&lt;/li&gt;
&lt;li&gt;Combining Predictive Features（綜合預測結果的特徵）&lt;/li&gt;
&lt;li&gt;Distilling Implicit Features（抽取隱含特性的特徵）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我已經盡力用我的理解翻譯上面的英文，哈！&lt;/p&gt;
&lt;p&gt;這些不同種類的Features就會造成不同的Models，這些Models分別是&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Embedding Numerous Features ：Kernel Models（Kernel模型）&lt;/li&gt;
&lt;li&gt;Combining Predictive Features：Aggregation Models（集合模型）&lt;/li&gt;
&lt;li&gt;Distilling Implicit Features：Extraction Models（萃取模型）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;讓我們依序來看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Embedding Numerous Features ：Kernel Models&lt;/h3&gt;
&lt;p&gt;還記得《機器學習基石》中，我們講了哪些Model嗎？我們一開始講了二元分類問題，然後提出了Perceptron Learning Algorithm (PLA)來解決這個問題（&lt;a href="http://www.ycc.idv.tw/ml-course-foundations_1.html"&gt;詳見《機器學習基石》第一篇&lt;/a&gt;），如果數據是線性可分的話，我們就可以使用PLA劃分出一條邊界來區分兩種種類。&lt;/p&gt;
&lt;p&gt;接下來提到我們可以使用Regression的方法來做二元分類問題，其中Logistic Regression考慮了雜訊造成每個Label的出現呈機率分布，給予一個較為寬鬆的區分方法，我們會稱PLA為Hard Classification，而Logistic Regression為Soft Classification。（&lt;a href="http://www.ycc.idv.tw/ml-course-foundations_3.html"&gt;詳見《機器學習基石》第三篇&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;最後，我們引入「特徵轉換」將我們原本的線性區分推到非線性區分，讓我的Model有更大的複雜度，也因為如此，我們需要使用Regularization和Validation來避免 Overfitting。（&lt;a href="http://www.ycc.idv.tw/ml-course-foundations_4.html"&gt;詳見《機器學習基石》第四篇&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;那如果我想要使用無窮個高次方的非線性Features來當作我的Model，可以做到嗎？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;來看一下之前我們做特徵轉換怎麼做的？其實我們沒有多做什麼功夫，我們只是把高次項先產生出來，然後在把這每一項當作線性模型的Features去處理，我們就用線性模型的方法產生了非線性的效果。&lt;/p&gt;
&lt;p&gt;那如果非線性項目的個數無窮多個，顯然這種方法就做不了了啊！&lt;/p&gt;
&lt;p&gt;不過，數學總是會拯救我們，&lt;strong&gt;我們可以使用Dual Transformation加上Kernel Function的技巧，帶我們走捷徑，直接用解析解讓我們得出答案，繞過要考慮無窮多個Features後再處理的窘境。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第一堂課「Linear Support Vector Machine」中，提出Hard-Margin Support Vector Machine (SVM)的架構，他和PLA非常相近，屬於Hard Classification，不同的是Hard-Margin SVM還會讓這個切分的邊界落在最佳的位置上。&lt;/p&gt;
&lt;p&gt;第二堂課 「Dual Support Vector Machine」中，我們開始使用Dual Transformation，把大部分與Data中Features有關的計算，取代成計算與Data中Labels有關的計算，讓我們朝不需要計算Features邁進一步，但是因為有另外一部分還是需要計算Features，所以一樣的我們還是無法讓Features有無窮多個。&lt;/p&gt;
&lt;p&gt;第三堂課「Kernel Support Vector Machine」中，我們引入Kernel Function來幫助我們，現在真的可以不需去列出所有Features也能算出答案，所以我們就可以讓Features有無窮多項，但也因為Model太過複雜，我們不得不去面對Overfitting的問題。&lt;/p&gt;
&lt;p&gt;第四堂課「Soft-Margin Support Vector Machine」中，提出Soft-Margin SVM，它是一種Soft Classification，讓我們可以允許部分錯誤發生，並且同樣的使用Dual Transformation加上Kernel Function的技巧，來讓我可以使用無窮多項的Features，而且因為Soft-Margin SVM可以允許錯誤，也就是對雜訊有容忍度，因此可以幫助我們抑制Overfitting的發生。&lt;/p&gt;
&lt;p&gt;第五堂課「Kernel Logistic Regression」中，我們將Kernel的方法引入Logistic Regression當中來用不同於Soft-Margin SVM的方式做二元分類。&lt;/p&gt;
&lt;p&gt;第六堂課「Support Vector Regression」中，會介紹如何使用Kernel Model來做各類Regression的問題。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這6堂課，主要做的事是把《機器學習基石》裡面學到的東西，全部引入數學工具讓Model的Features可以擴展到無窮多項，產生更強大的Kernel Model。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Combining Predictive Features：Aggregation Models&lt;/h3&gt;
&lt;p&gt;那如果今天我有很多支的Model，我有辦法融合他們得到更好的效果嗎？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這就是Aggregation Models的精髓，Aggregation Models藉由類似於投票的方法綜合各個子Models的結果得到效果更好的Model。換個角度看，你可以把整個體系看成一個新的Model，而原本這些子Models當作轉換過後的新Features，所以Aggregation Model裡頭做了「特徵轉換」，這個轉換產生出許多有預測答案能力的Features，稱為Predictive Features，然後再綜合它們。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Aggregation Models可以分成兩大類，第一種的作法比較簡單，先Train出一個一個獨立的Predictive Features，然後在綜合它們，&lt;strong&gt;「集合」的動作是發生在得到Train好的Predictive Feature之後，這叫做「Blending Models」&lt;/strong&gt;；第二種作法則是，&lt;strong&gt;「集合」的動作和Training同步進行，這叫做「Aggregation-Learning Models」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;從「集合」的方法上也可以進一步細分三種類型，有票票等值的&lt;strong&gt;「Uniform Aggregation Type」&lt;/strong&gt;，有給予Predictive Features不同權重的&lt;strong&gt;「Linear Aggregation Type」&lt;/strong&gt;，甚至還可以用條件或任意Model來分配Predictive Features，這叫做&lt;strong&gt;「Non-linear Aggregation Type」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以兩種類型、三種Aggregation Type，交互產生六種Aggregation Models。&lt;/p&gt;
&lt;p&gt;第七堂課「Bootstrip Aggregation」中，一開始介紹Blending Models的三種Aggregation Type，第一種是直接平均所有的Predictive Features，第二種則是藉由每個Predictive Feature的預測能力，使用線性模型去調配它們的權重，第三種則是使用任意模型分配權重。接著又介紹了Aggregation-Learning Models的Uniform Aggregation Type，稱之為Bagging，它的特點在於它可以利用變換Dataset來造出很多個Predictive Features，並接著做Aggregation。&lt;/p&gt;
&lt;p&gt;第八堂課「Adaptive Boosting」中，介紹Aggregation-Learning Models的Linear Aggregation Type，稱之為AdaBoost，它的特點在於它可以使得每個Predictive Features彼此間可以截長補短。&lt;/p&gt;
&lt;p&gt;第九堂課「Decision Tree」中，介紹Aggregation-Learning Models的Non-linear Aggregation Type，稱之為Decision Tree。&lt;/p&gt;
&lt;p&gt;第十堂課「Random Forest」中，使用Bagging來做Decision Tree，這叫做Random Forest。&lt;/p&gt;
&lt;p&gt;第十一堂課「Gradient Boosted Decision Tree」中，會介紹AdaBoost的Regression版本稱為GradientBoost，並且運用AdaBoost和GradientBoost在Decision Tree上面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這5堂課，我們將會介紹Aggregation Models，引入綜合、集合Predictive Feature的概念來使我們造出更好的Model。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Distilling Implicit Features：Extraction Models&lt;/h3&gt;
&lt;p&gt;那最後這個部分則是介紹現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，在這裡我們通稱Extraction Models。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extraction Models的特色在於它「特徵轉換」的方法，使用一層一層神經元來做非線性的特徵轉換，如果具有多層神經元，那就是做了多次的非線性特徵轉換，這就是「深度學習」，藉由Data機器會自行學習出這每一層的特徵轉換，找出隱含的Features。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第十二堂課「Neural Network」中，介紹Neural Network，並介紹Neural Network的演算法—Back-Propagation（反向傳遞法），在概念上Gradient Descent就是Back-Propagation的源頭，另外介紹避免Overfitting的方法—Early Stopping。&lt;/p&gt;
&lt;p&gt;第十三堂課「Deep Learning」中，開始介紹「深度學習」，考慮多層神經元的Neural Network就叫做Deep Learning，我們會探討如何在Deep Learning中加入Regularization，並介紹一種叫做Auto-encoder的特殊Deep Learning方法。&lt;/p&gt;
&lt;p&gt;第十四堂課「Radial Basis Function Network」中，介紹Radial Basis Function (RBF) Network，並且介紹K-means等非監督分類法。&lt;/p&gt;
&lt;p&gt;第十五堂課「Matrix Factorization」中，我們會探討類別的匹配問題，例如：我想要知道用戶喜歡看什麼電影，而我的Data只有用戶的ID和電影的編號。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這4堂課，我們將會介紹Extraction Model，使用神經元的概念來萃取出Data中的Features。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;後話&lt;/h3&gt;
&lt;p&gt;最後總結一下《機器學習技法》會講哪些東西？我們會講具有三種不同「特徵轉換」方式的Models。&lt;strong&gt;Kernel Model的「特徵轉換」是將非線性Features擴張到無窮多個；Aggregation Model的「特徵轉換」是產生出有預測能力的Features；Extraction Model的「特徵轉換」是利用神經元的方式來做到萃取出隱含的資訊。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;跟《機器學習基石》不一樣的地方，《機器學習技法》中介紹更厲害的「特徵轉換」來產生更厲害的Model，不過因為會有Overfitting的狀況，所以我們還需要介紹相應的配套措施。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在未來一系列的文章，我會帶大家一一的來看這些內容，不過和之前一樣，我不會以課堂當作單位來講，而是以單元式的方式，而且我主要的目的是去點出概念，並盡可能的不去牽涉太多的數學計算，但是數學計算的部分是很重要的，這會影響到你真正的實作，數學的部份可以去看林軒田老師的影片或投影片，裡頭都有很詳細的介紹。&lt;/p&gt;</content><category term="AI.ML"></category><category term="機器學習技法"></category></entry><entry><title>機器學習基石 學習筆記 (4)：機器可以怎麼學得更好?</title><link href="https://ycc.idv.tw/ml-course-foundations_4.html" rel="alternate"></link><published>2016-09-18T12:00:00+08:00</published><updated>2016-09-18T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2016-09-18:/ml-course-foundations_4.html</id><summary type="html">&lt;p&gt;特徵轉換 / Overfitting / Regularization / Validation&lt;/p&gt;</summary><content type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回中，我們已經了解了機器學習基本的操作該怎麼做。而這一篇中，我們來看&lt;strong&gt;機器可以怎麼學得更好?&lt;/strong&gt; 基本上有三招：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），我們來看看。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Feature Transformation（特徵轉換）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.013.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;在上一回當中我們講了很多的線性模型，大家有沒有懷疑說，數據呈現的方式一定可以用線性描述嗎？我的答案是通常線性描述會表現不錯，但不是絕對，&lt;strong&gt;那我們怎麼用非線性的方法來描述我們的數據，這邊提供一個方法叫做「非線性轉換」，或者又稱為「特徵轉換」（還記得變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;又可以稱為特徵Features）&lt;/strong&gt;，聽起來有點困難齁～其實不會啦！&lt;/p&gt;
&lt;p&gt;假設今天你的Data分布是圓圈狀的分布，顯而易見的你很難用一條線去區分他們，那我們應該怎麼做呢？假設今天有一個轉換可以把這個圓圈狀分布的空間轉換到另外一個空間，而且在這個新的空間，我們可以做到線性可分，這樣問題就解決了，我們非常擅長做線性可分啊！怎麼做呢？我們知道這個空間分布是圓圈分布，所以套用以前學過的公式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(x_1, x_2) = sign(-A\times x_1^2-B\times x_2^2+C)
$$&lt;/div&gt;
&lt;p&gt;，如此一來，令 &lt;span class="math"&gt;\(z_1=-x_1^2\)&lt;/span&gt;; &lt;span class="math"&gt;\(z_2=-x_2^2\)&lt;/span&gt;，所以問題就變成一個線性問題
&lt;/p&gt;
&lt;div class="math"&gt;$$
H(z_1, z_2) = sign(A\times z_1+B\times z_2+C)
$$&lt;/div&gt;
&lt;p&gt;
在做這個操作時，我們會用到非線性項，也就是高次項或交叉項，所以會稱這個轉換叫做「非線性轉換」。&lt;strong&gt;藉由人為觀察數據，並給予適當的特徵轉換，找出其中隱藏的非線性項當作新的特徵，又稱為特徵工程（Feature Engineering）。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但如果我們需要去人為定義這個「非線性轉換」，這就很弱啦！我們當然希望機器可以自行從Data中學習到這個轉換，作法是這樣的，我們先把變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;做個變化和擴充，讓它們互相的相乘創造出高次項，再把這些項等價的放到Linear Model裡，所以我們就用了線性的作法來做到Non-linear Model，而因為有權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;在非線性項前面的關係，所以機器會針對Data自行去調配非線性項或線性項的權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;，這效果就等同於機器自行學習到「非線性轉換」。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;機器自己學習特徵轉換的這個概念應該是現今ML最重要的概念之一，最近很夯的深度學習就擁有強大的特徵轉換功能，這些轉換都是機器從Data自行學來的。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;特徵轉換讓ML變得很強大，但要特別注意，因為我們增加了非線性項，所以等於是增加了模型的複雜度，這麼做的確可以壓低&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;沒有錯，但也可能使得&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;不再成立，也就是Overfitting，所以建議要逐步的增加非線性項，從低次方的項開始加起，避免Overfitting。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Overfitting&lt;/h3&gt;
&lt;p&gt;Overfitting是一個大怪獸，在學習怎麼對付牠之前，我們先來好好的了解牠！&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overfitting" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.000.03.png"&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上面這張圖用很簡單的方法說明了Overfitting是怎麼一回事，假設藍色的線是Target，也就是我們抽樣的母群體，因為雜訊的關係，抽樣出來的點可能會稍微偏離Target，而如果這個時候我們用二次式來描述這些抽樣出來的Data（上圖中的左側）會發現&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;不能壓到0，所以這個時候可能有人想說加進去更高次項來試試看（上圖中的右側），此時會發現&lt;span class="math"&gt;\(E_{in}=0\)&lt;/span&gt;，所有數據都可以被完整描述了，但是你會發現Fit的曲線已經完全偏離了Target，反而是使用低次項還描述比較好，低次項描述的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和 &lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;(Target Function) 比較接近，所以結論是&lt;strong&gt;如果我們把「隨機雜訊」（Stochastic Noise）Fit進去Model裡面就會因此產生Overfitting，要避免這種情況發生，就要小心使用高次項&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overfitting2" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.000.04.png"&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;但可別以為沒有「隨機雜訊」鬧場就不會出現Overfitting，上圖假設一個沒有「隨機雜訊」的情形，但是這次Target Function的複雜度很高（上圖右側），當我們從中採樣一些Data來進行Fitting，如上圖左側，我們分別使用2次和10次來做Fitting，這個時候你會發現雖然2次和10次都和Target曲線差很遠，但是小次方的還是Fit的比較好一點，造成Overfitting的原因是因為當Target很複雜的情況下，如果採樣的數據不大，根本無法反應Target本身，所以就算使用了和Target一樣複雜的Model，也只是在瞎猜而已。&lt;strong&gt;這種因為Target本身的複雜度所帶來的雜訊，我們稱為「決定性雜訊」(Deterministic Noise)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Noise" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.000.05.png"&gt;&lt;/p&gt;
&lt;p&gt;From: &lt;a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf"&gt;https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們來看一下「隨機雜訊」（Stochastic Noise）和「決定性雜訊」（Deterministic Noise）怎麼造成Overfitting的，上圖中的兩張漸層圖表示的是Overfitting的程度，越接近紅色代表Overfitting越嚴重；反之，越接近藍色則Overfitting越輕微。左邊的漸層圖是考慮「隨機誤差」的影響，右邊的漸層圖則是考慮「決定性雜訊」的影響。從這兩張圖我們可以觀察出下面四點特性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data數量&lt;span class="math"&gt;\(N\)&lt;/span&gt;越少，越容易Overfitting&lt;/li&gt;
&lt;li&gt;「隨機雜訊」越多，越容易Overfitting&lt;/li&gt;
&lt;li&gt;「決定性雜訊」越多，越容易Overfitting&lt;/li&gt;
&lt;li&gt;Model本身越複雜，越容易Overfitting&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;那有什麼方法可以防止Overfitting嗎？有的，包括之前講過的一些方法，我們來看一下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;從簡單的模型開始做起，從低次模型開始做起，在慢慢加入高次項&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提升資料的正確性：Data Cleaning/Pruning（資料清洗）將錯誤的Data修正或刪除&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Hinting（製造資料），使用合理的方法擴增原有的資料，例如：在圖形辨識問題中，可以用平移和旋轉來擴增出更多Data&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Regularization（正規化）：限制權重W的大小以控制高次的影響。&lt;/strong&gt;（接下來會詳述...）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Validation（驗證）：將部分Data保留不進去Fitting，然後用這個Validation Data來檢驗Overfitting的程度。&lt;/strong&gt;（接下來會詳述...）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Regularization（正規化）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="regularation" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.014.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;剛剛我們提到了Overfitting所造成的影響很大一部分是因為Model複雜度所造成的，但是為了可以把&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;給壓下去，我們又的確需要去增加高次項，所以依照建議需要從低次項開始慢慢的加，這樣感覺很麻煩啊！&lt;strong&gt;有沒有辦法讓機器自己去限制高次項的出現呢？有的，這就是Regularization（正規化）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;還記得剛剛在講「特徵轉換」時，有提到一點，ML有辦法自行學習「特徵轉換」的關鍵是因為高次項前面有一個可調控的權重，而機器會針對Data來調整權重大小，那其實就是等價於機器自己學習到了「特徵轉換」，同理可知，&lt;strong&gt;我們只要限制權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;的大小就等同於限制了機器無所忌憚的使用高次項&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;經數學證明，&lt;strong&gt;限制權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;的大小可以等價於在&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;上面加上「&lt;span class="math"&gt;\(W\)&lt;/span&gt;大小的平方」乘上定值&lt;span class="math"&gt;\(λ\)&lt;/span&gt;，&lt;span class="math"&gt;\(λ\)&lt;/span&gt;越大代表&lt;span class="math"&gt;\(W\)&lt;/span&gt;大小限制越緊；&lt;span class="math"&gt;\(λ\)&lt;/span&gt;越小代表&lt;span class="math"&gt;\(W\)&lt;/span&gt;大小限制越鬆&lt;/strong&gt;，這也非常容易想像，訓練Model的方法是去降低&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;，但是如果使用了大的&lt;span class="math"&gt;\(W\)&lt;/span&gt;，就會使得&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;增大，自然而然在訓練的過程中，機器會去尋找小一點的&lt;span class="math"&gt;\(W\)&lt;/span&gt;，也就等同於限制了&lt;span class="math"&gt;\(W\)&lt;/span&gt;的大小。&lt;/p&gt;
&lt;p&gt;見上圖左側，我們修改了Gradient Descent讓它受到Regularization的限制。&lt;/p&gt;
&lt;p&gt;而上圖左側下方，顯示了在&lt;span class="math"&gt;\(λ\)&lt;/span&gt;增大的同時，限制&lt;span class="math"&gt;\(W\)&lt;/span&gt;的大小會越來越緊，所以Fitting的結果從原本的Overfitting變成Underfitting。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Underfitting所代表的是Model本身的複雜度不夠，不足以使得&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;降的夠小，如果你經過Validation（待會會講）後發現沒有Overfitting的現象，但是你的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;始終壓不下來，那就有可能是Underfitting，那你該考慮的是增加Model複雜度或者放寬Regularization，反而不是Regularization。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regularizer的選擇常見的有兩種L2和L1，L2使用「&lt;span class="math"&gt;\(W\)&lt;/span&gt;大小的平方」，L1則使用「&lt;span class="math"&gt;\(W\)&lt;/span&gt;大小的絕對值」。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;當Linear Regression使用Regularization限制，統計上有一個名稱稱為Ridge Regression，你可以使用Gradient Descent來做，又或者使用解析解的方法。&lt;/p&gt;
&lt;p&gt;最後提一個Regularization的細節，你會發現因為高次項是彼此兩兩相乘的結果，所以項目的個數會隨著次方增加而增加，這麼一來在做Regularization時可能會過度懲罰高次項，因此，我們可以將Feature轉換成Legendre Polynomials來避免這個問題。&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;Validation（驗證）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="validation" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.015.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;講了這麼多Overfitting，但到底要怎麼去量化Overfitting呢？Overfitting就是&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;不成立，但是&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;我們不會知道啊！因為我們不會知道Target Function是什麼，那該怎麼得到量化Overfitting的值呢？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有一個方法叫做Validation可以拿來量化Overfitting的值，這個方法是先將採樣的數據做分離，一部分將會拿來做Model Fitting（Model Training），另外一部分保留起來評估訓練完畢的Model，因為保留的這一部分源自於母群體，而且又沒有被Model給看過，所以它可以很客觀的反應出&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;的大小。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我們的Model和Algorithm從以前講到現在已經是越來越複雜了，來複習一下Model和Algorithm受哪些參數影響，Algorithm的選擇就有很多了，包括：PLA、Linear Regression、Logistic Regression；Learning Rate &lt;span class="math"&gt;\(η\)&lt;/span&gt;也需要去選擇大小決定學習速率；Feature Transformation中Feature的決定和次方大小的決定；Regularization也有L2、L1 Regularizer的選擇；還有Regularization的&lt;span class="math"&gt;\(λ\)&lt;/span&gt;值也必須被決定。&lt;/p&gt;
&lt;p&gt;這些條件彼此交互搭配會產生很多組的Model，那該如何挑選Model呢？我們就可以使用Validation來當作一個依據來選擇Model，選擇出&lt;span class="math"&gt;\(E_{val}\)&lt;/span&gt;最小的Model，如上圖所示。&lt;/p&gt;
&lt;p&gt;另外實作上有一些方法：Leave-One-Out Cross Validation和V-Fold Cross Validation，他們的精髓就是保留&lt;span class="math"&gt;\(k\)&lt;/span&gt;筆Data當作未來Validation用，另外一些拿下去Train Model，然後再用這k筆去評估並得到&lt;span class="math"&gt;\(E_{val,1}\)&lt;/span&gt;，還沒結束，為了讓&lt;span class="math"&gt;\(E_{val}\)&lt;/span&gt;盡可能的正確，所以我們會在把Data作一個迴轉，這次使用另外一組k組Data來Validation，其餘的再拿去Train Model，然後在評估出，&lt;span class="math"&gt;\(E_{val,2}\)&lt;/span&gt; … 以此類推，當轉完一輪之後，在把這些&lt;span class="math"&gt;\(E_{val,1}\)&lt;/span&gt;, &lt;span class="math"&gt;\(E_{val,2}\)&lt;/span&gt;, ...做平均得到一個較為精確&lt;span class="math"&gt;\(E_{val}\)&lt;/span&gt;。那Leave-One-Out Cross Validation顧名思義就是&lt;span class="math"&gt;\(k=1\)&lt;/span&gt;，但這樣做要付出的代價就是計算量太大了，所以V-Fold Cross Validation則使用&lt;span class="math"&gt;\(k=V\)&lt;/span&gt;來做。實務上，我常常做Validation時根本不會去Cross它們，我大都只是保留一部分的Data來驗證而已，給大家參考。&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;總結&lt;/h3&gt;
&lt;p&gt;來到了這四篇有關於林軒田教授機器學習基石學習筆記的尾聲了，讓我們重溫看看我們學會了什麼？&lt;/p&gt;
&lt;p&gt;一開始我帶大家初探ML的基本架構，建立Model、使用Data訓練、最後達到描述Target Function的目的，也帶大家認識各種機器學習的類型。&lt;/p&gt;
&lt;p&gt;接下來，我們用理論告訴大家，ML是不是真的可以做到，那在什麼時候可以做到？要符合哪些條件？我們知道要有好的Model，VC Dimension越小越好，也就是可調控的參數越少越好，才會使得&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;成立；要有足夠的Data；要有好的Learning Algorithm能把&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;壓低，這三種條件成立後，如此一來Model在描述訓練數據很好的同時也可以很好的去預測母群體，但我們發現&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;壓低和可調控的參數越少越好兩者是Trade-off，所以我們必須取適當的VC Dimension。&lt;/p&gt;
&lt;p&gt;再接下來我們開始看實際上ML該怎麼做，引入相當重要的Learning Algorithm，也就是Gradient Descent，並且說明了Linear Regression和Logistic Regression，而且還可以使用這兩種Regression來做分類問題。&lt;/p&gt;
&lt;p&gt;那最後就真正亮出ML的三大絕招啦：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），Feature Transformation使得Model更為強大，所以&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;更能夠壓低，但是為了避免Overfitting我們必須去限制它，Regularization可以限制高次項的貢獻，另外，Validation可以量化Overfitting的程度，有了這個我們就可以去選出體質健康而且&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;又小的Model。&lt;/p&gt;
&lt;p&gt;機器學習基石的這些概念都很重要，往後如果你開始學習其他的ML技巧，例如：深度學習，這些知識都是你強大的基礎，所以多看幾次吧！&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習基石"></category></entry><entry><title>機器學習基石 學習筆記 (3)：機器可以怎麼樣學習?</title><link href="https://ycc.idv.tw/ml-course-foundations_3.html" rel="alternate"></link><published>2016-08-07T12:00:00+08:00</published><updated>2016-08-07T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2016-08-07:/ml-course-foundations_3.html</id><summary type="html">&lt;p&gt;Gradient Descent / Linear Regression / Logistic Regression / 使用迴歸法做二元分類問題&lt;/p&gt;</summary><content type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回中，我們已經了解了機器學習在理論上有怎樣的條件才可以達成，所以接下來我們就可以正式的來看有哪一些機器學習的方法。&lt;/p&gt;
&lt;p&gt;在這一篇中，我會帶大家初探：&lt;strong&gt;機器可以怎麼樣學習?&lt;/strong&gt; 內容包括：Gradient Descent、Linear Regression、Logistic Regression、使用迴歸法做二元分類問題等等。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Gradient Descent（梯度下降）&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.009.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;還記得上一回我們歸納出了一套ML的流程，複習一下&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;準備好足夠的數據&lt;/li&gt;
&lt;li&gt;把Model建立好，&lt;span class="math"&gt;\(d_{VC}\)&lt;/span&gt;必須要是有限的，而且大小要適中&lt;/li&gt;
&lt;li&gt;定義好評估&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的Error Measurement&lt;/li&gt;
&lt;li&gt;使用演算法找出最佳參數把&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;降低&lt;/li&gt;
&lt;li&gt;最後評估一下是否有Overfitting的狀況，確保&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;請容許我先不管Model這部份該怎麼建立，我們先來看如何找到最佳參數這部份，&lt;strong&gt;假設今天我知道&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的評估方法，我該如何找到最佳的參數來使得&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;更小？有一套普遍的方法叫做Gradient Descent&lt;/strong&gt;，很強大，甚至連現今流行的「深度學習」找最佳解的機制也是從Gradient Descent衍生出來的。&lt;/p&gt;
&lt;p&gt;想像一下你是一位登山客，你在爬一座由&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;所決定的高山，你的目標是去這座山最低的山谷，也就是&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小的地方，因為村莊正在那裡，但是很不幸的你沒有地圖，這個時候有什麼方法可以知道低谷在哪裡呢？最簡單的答案就是一直下坡就對啦！反正我知道村莊在山谷裡，那我就一路下山應該就可以找到村莊了，這就是Gradient Descent的精髓。&lt;/p&gt;
&lt;p&gt;在數學上有一個衡量函數變化的東西，這就是Gradient（梯度），Gradient是一個向量，它的「方向」指向函數值增加量最大的方向，而它的「大小」反應這個變化有多大，其實就是一次微分啦！只不過Gradient推廣到高維度而已。所以我們和這個登山客做一樣的事情，我們朝著下降最多的方向前進，這就是Gradient Descent（梯度下降法），我剛剛說了，梯度是指向函數值增加量最大的方向，那顯然我們往反方向走就可以達到最大下降，所以如果我們有一個Error函數&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;，它的Gradient就是&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;，那我們的下降方向就是&lt;span class="math"&gt;\(- \nabla E_{in}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;來看一下上圖中Gradient Descent的流程，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定義出Error函數&lt;/li&gt;
&lt;li&gt;Error函數讓我們可以去評估&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;算出它的梯度&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;朝著&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;的反方向更新參數W，而每次只跨出&lt;span class="math"&gt;\(η\)&lt;/span&gt;大小的一步&lt;/li&gt;
&lt;li&gt;反覆的計算新參數&lt;span class="math"&gt;\(W\)&lt;/span&gt;的梯度，並一再的更新參數&lt;span class="math"&gt;\(W\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;這邊要特別注意，流程中的第四項中，有提到&lt;span class="math"&gt;\(η\)&lt;/span&gt;，&lt;strong&gt;&lt;span class="math"&gt;\(η\)&lt;/span&gt;稱為Learning Rate，它影響的是更新步伐的大小&lt;/strong&gt;，&lt;span class="math"&gt;\(η\)&lt;/span&gt;的選擇要適當，如果&lt;span class="math"&gt;\(η\)&lt;/span&gt;太小的時候，我們可能要花很多時間才可以走到低點，但如果&lt;span class="math"&gt;\(η\)&lt;/span&gt;太大的話，又可能導致我們在兩個山腰間跳來跳去，甚至越更新越往高處跑，&lt;strong&gt;所以選擇適當的&lt;span class="math"&gt;\(η\)&lt;/span&gt;相當的重要，所以下次如果你發現&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;一直降不下來甚至在增大，試著將&lt;span class="math"&gt;\(η\)&lt;/span&gt;減小看看&lt;/strong&gt;。另外&lt;span class="math"&gt;\(η\)&lt;/span&gt;也可以不是定值，我們可以直接設&lt;span class="math"&gt;\(η＝|\nabla E_{in}|\)&lt;/span&gt;，這麼一來遇到陡坡的時候它就會跨大一點的步伐，遇到緩坡的時候就會跨小步一點，隨狀況調整&lt;span class="math"&gt;\(η\)&lt;/span&gt;的值。&lt;/p&gt;
&lt;p&gt;Gradient Descent (GD, 梯度下降) 有兩個變形，分別為Stochastic Gradient Descent (SGD, 隨機梯度下降) 和 Batch Gradient Descent (BGD, 批次梯度下降)，這差別只在於評估&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;的時候所考慮的Data數量，正常來說必須要考慮所有的Data，我們才會得到真正的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;，才有辦法算出正確的&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;，但這樣所要付出的代價就是較大的計算量。&lt;/p&gt;
&lt;p&gt;所以&lt;strong&gt;Stochastic Gradient Descent的作法是一次只拿一筆Data來求&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;，並且更新參數&lt;span class="math"&gt;\(W\)&lt;/span&gt;&lt;/strong&gt;，這樣的更新方法顯然會比較不穩定，但我們假設，經過好幾輪的更新後，已經完整看過整個數據了，所以平均來說效果和一般的Gradient Descent一樣。&lt;/p&gt;
&lt;p&gt;另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，&lt;strong&gt;Batch Gradient Descent一次評估k組數據，並更新參數W&lt;/strong&gt;，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的計算方法你可以想像成在做矩陣計算，矩陣元素在計算的時候往往是可以拆開計算的，此時GPU利用它強大的平行化運算將這些元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，&lt;strong&gt;所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Batch Gradient Descent，是既有效率又穩定的作法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Gradient Descent求最佳解其實不是完美的，還記得我們的目標嗎？我們希望可以走到最低點的山谷裡，所以我們採取的策略是不斷的下降，這個時候如果遇到兩種情形就會導致還沒到山谷就已經動彈不得，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;小山谷，數學上稱為&lt;strong&gt;Local Minimum&lt;/strong&gt;，雖然在那點看起來，那邊的確是相對的低點，所以&lt;span class="math"&gt;\(\nabla E_{in}=0\)&lt;/span&gt;，但卻不是整個&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的最低點，但也因為&lt;span class="math"&gt;\(\nabla E_{in}=0\)&lt;/span&gt;的關係，更新就不會再進行。&lt;/li&gt;
&lt;li&gt;平原，數學上稱為&lt;strong&gt;Saddle Point（鞍點）&lt;/strong&gt;，在一片很平的區域，&lt;span class="math"&gt;\(\nabla E_{in}=0\)&lt;/span&gt;，所以就停止不動了。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;針對這些問題有一些改良後的演算法，在這裡不詳述，請參考&lt;a href="http://ruder.io/optimizing-gradient-descent/"&gt;S. Ruder的整理&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;好！我們已經了解了怎麼使用Gradient Descent去找到&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小的最佳參數，那我們可以回頭看Model有哪一些？Error Measure該怎麼定？&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Linear Regression&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.010.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;先從最簡單的看起，那就是線性迴歸（Linear Regression），假設今天我要用三種變數&lt;span class="math"&gt;\((x_1, x_2, x_3)\)&lt;/span&gt;來建立一個簡單的線性模型，那就是&lt;/p&gt;
&lt;div class="math"&gt;$$
w_0+w_1 x_1+w_2 x_2+w_3 x_3
$$&lt;/div&gt;
&lt;p&gt;這個又稱為Score，標為&lt;span class="math"&gt;\(s\)&lt;/span&gt;，為了方便起見，我們會額外增加&lt;span class="math"&gt;\(x_0=1\)&lt;/span&gt;的參數，這麼一來Score就可以寫成矩陣形式&lt;/p&gt;
&lt;div class="math"&gt;$$
s = w_0 x_0+w_1 x_1+w_2 x_2+w_3 x_3=W^T x
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
where: W = [w_0, w_1, w_2, w_3], x = [x_0=1, x_1, x_2, x_3]
$$&lt;/div&gt;
&lt;p&gt;
在線性模型中，這個 s 就正好是我們Model預測的值，通常我們會把預測得來的 &lt;span class="math"&gt;\(y\)&lt;/span&gt; 記作&lt;span class="math"&gt;\(\widehat{y}\)&lt;/span&gt; (y hat)，如果今天這個 y 和 ŷ 是實數的話，那這就是一個標準的Linear Regression問題，那如何去衡量預測的好或不好呢？&lt;strong&gt;我們可以使用Squared Error來衡量，&lt;span class="math"&gt;\(err(\widehat{y},y)=(\widehat{y}-y)^2\)&lt;/span&gt;&lt;/strong&gt;，所以 &lt;span class="math"&gt;\(\widehat{y}\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(y\)&lt;/span&gt; 越靠近Error就越小。&lt;/p&gt;
&lt;p&gt;Squared Error的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;平面是一個單純的開口向上的拋物線，所以它的最低點其實是有解析解的，我們可以靠著數學上的&lt;strong&gt;Pseudo-Inverse方法&lt;/strong&gt;把最佳參數給算出來，但是Pseudo-Inverse計算非常龐大，當數據量很大時這個方法是不可行的，而剛剛介紹的Gradient Descent計算複雜度只有&lt;span class="math"&gt;\(O(N)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.011.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;在上一回討論二元分類問題時，我們的評估模型都是非黑即白的&lt;/p&gt;
&lt;p&gt;在上一回討論二元分類問題時，我們考慮的狀況是「沒有雜訊」的情形，不過在實際情況下，「雜訊」是一定需要考慮的。在「沒有雜訊」的情形下，一筆Data只會有一個確定的答案要嘛是 &lt;span class="math"&gt;\(- 1\)&lt;/span&gt; 不然就是 &lt;span class="math"&gt;\(+ 1\)&lt;/span&gt;，&lt;strong&gt;如果考慮「雜訊」，一筆Data出現的答案可能呈現機率分布&lt;/strong&gt;，介於 &lt;span class="math"&gt;\(- 1\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(+ 1\)&lt;/span&gt; 之間，舉例可能會產生像下面一樣的情況，
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}(◯|X^{(1)}) = 0.9,\  \mathbb{P}(✕|X^{(1)}) = 0.1
$$&lt;/div&gt;
&lt;p&gt;之前PLA的分類方法是屬於非黑及白的，預測的結果不存在模糊地帶，這種分類法我們稱為Hard Classification，這種分類法並不能描述機率分布，所以我們來考慮另外一種分類法，稱之為Soft Classification。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Soft Classification看待每個答案不是非黑及白的，而是去評估每個答案出現的機會有多大，以此作為分類&lt;/strong&gt;，我們打算使用Regression的連續特性來產生Soft Classification，我們需要引入一個重要的函數—Logistic Function，這個函數可以將所有實數映射到0到1之間，如上圖下方中間的圖示所示，&lt;strong&gt;Logistic Function會將極大的值映射成1，而將極小值映射成0，這個0到1的值剛剛好可以拿來當作機率的大小&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以我們就可以來建立一個有機率概念的模型，這個Model的預測值是一個機率，一樣的先給予輸入變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;和權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;求出Score &lt;span class="math"&gt;\(s\)&lt;/span&gt;，再把 &lt;span class="math"&gt;\(s\)&lt;/span&gt; 放到Logistic Function當中，我們就可以映射出在一個機率空間，我們藉由調整&lt;span class="math"&gt;\(W\)&lt;/span&gt;來改變Model來擬合我們的Data，有了這個新的Model，我們就可以用機率的方式來描述二元分類，&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}(◯|X^{(1)}) = Θ(s)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\mathbb{P}(✕|X^{(1)}) = 1 - Θ(s) = Θ(-s)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
where:\ Θ(s)=1/[1+e^{-s}]
$$&lt;/div&gt;
&lt;p&gt;OK! 決定好Model，我們就可以來定義它的Error Measurement的方式了，這個時候如果使用Squared Error來作為Error Measurement你會發現這種評估方式有一點失焦了，如果採用Squared Error，我們做的事是將機率的值給擬合精準，但我們知道這個機率的產生是來自於雜訊，預測雜訊是沒有意義的，要做的事應該是要在考慮雜訊之下盡可能提升模型會產生取樣資料的可能機率。&lt;/p&gt;
&lt;p&gt;這就是Max Likelihood的概念，&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}(likelihood) = \mathbb{P}(x^{(1)})\mathbb{P}(◯|x^{(1)},H) \times \mathbb{P}(x^{(2)})\mathbb{P}(✕|x^{(2)},H) \times … \times \mathbb{P}(x^{(N)})\mathbb{P}(◯|x^{(N)},H)
$$&lt;/div&gt;
&lt;p&gt;我們的任務就是找一個function set &lt;span class="math"&gt;\(H\)&lt;/span&gt;使得我可以最大化likelihood，
&lt;/p&gt;
&lt;div class="math"&gt;$$
argmax_{({H})} \mathbb{P}(likelihood)
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=argmax_{({H})} \sum_{y^{(n)}\ is\ ◯} \{ln[\mathbb{P}(x^{(n)})]+ln[\mathbb{P}(◯|x^{(n)},H)] \}+ \sum_{y^{(n)}\ is\ ✕} \{ln[\mathbb{P}(x^{(n)})]+ln[\mathbb{P}(✕|x^{(n)},H)]\}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=argmax_{({H})} \sum_{y^{(n)}\ is\ ◯} ln[\mathbb{P}(◯|x^{(n)},H)] + \sum_{x^{(n)}\ is\ ✕} ln[\mathbb{P}(✕|y^{(n)},H)]
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
=argmin_{({H})} \sum_{y^{(n)}\ is\ ◯} -ln[\mathbb{P}(◯|x^{(n)},H)] + \sum_{y^{(n)}\ is\ ✕} -ln[\mathbb{P}(✕|x^{(n)},H)]
$$&lt;/div&gt;
&lt;p&gt;假設 &lt;span class="math"&gt;\(◯ \equiv  (y=+1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(✕ \equiv  (y=0)\)&lt;/span&gt;，則上式可以化簡，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
=argmin_{({H})} \sum_{n} -y^{(n)}ln\mathbb{P}(◯|x^{(n)},H) -(1-y^{(n)})ln[1-\mathbb{P}(◯|x^{(n)},H)]
$$&lt;/div&gt;
&lt;p&gt;其中&lt;span class="math"&gt;\(E_{ce}=-yln\mathbb{P}(◯|x,H)-(1-y)ln[1-\mathbb{P}(◯|x,H)]\)&lt;/span&gt; 就是Cross-Entropy Error。&lt;/p&gt;
&lt;p&gt;而對於logistic regression model而言，&lt;span class="math"&gt;\(\mathbb{P}(◯|x,H)=Θ(s)\)&lt;/span&gt;，代入Cross-Entropy Error得
&lt;/p&gt;
&lt;div class="math"&gt;$$
E_{ce,logistic}=-ylnΘ(s)-(1-y)ln(1-Θ(s))=-ylnΘ(s)-(1-y)lnΘ(-s)
$$&lt;/div&gt;
&lt;p&gt;
&lt;strong&gt;我們可以使用Gradient Descent來降低Cross-Entropy，這又稱為Logistic Regression，在這個問題中就沒有簡單的解析解可以直接算，只能使用Gradient Descent來求取近似解。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h3&gt;使用迴歸法做二元分類問題&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ML" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.012.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;剛剛介紹了Logistic Regression，其實我們是可以將Logistic Regression運用來做二元分類問題。&lt;/p&gt;
&lt;p&gt;線性模型的標準方法，我們會將變數&lt;span class="math"&gt;\(x\)&lt;/span&gt;做線性組合得到Linear Scoring Function — &lt;span class="math"&gt;\(s\)&lt;/span&gt;，線性組合的係數和Threshold稱為權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;，我們可以調整權重&lt;span class="math"&gt;\(W\)&lt;/span&gt;來改變Model，那針對看待&lt;span class="math"&gt;\(s\)&lt;/span&gt;的不同方式就衍生出不同的方法。那為了可以將Regression問題轉換成二元分類問題，所以通常我們會假設&lt;span class="math"&gt;\((y=+1)\)&lt;/span&gt;為&lt;span class="math"&gt;\(◯\)&lt;/span&gt;，&lt;span class="math"&gt;\((y=-1)\)&lt;/span&gt;為&lt;span class="math"&gt;\(✕\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;先回顧一下之前&lt;a href="http://www.ycc.idv.tw/ml-course-foundations_1.html"&gt;PLA的作法&lt;/a&gt;，我們把 &lt;span class="math"&gt;\(s&amp;gt;0\)&lt;/span&gt; 的狀況視為&lt;span class="math"&gt;\(◯\)&lt;/span&gt;，也就是&lt;span class="math"&gt;\((y=+1)\)&lt;/span&gt;；然後把&lt;span class="math"&gt;\(s&amp;lt;0\)&lt;/span&gt; 的狀況視為&lt;span class="math"&gt;\(✕\)&lt;/span&gt;，也就是&lt;span class="math"&gt;\((y=-1)\)&lt;/span&gt;，把這個概念畫成上圖右側的圖，圖中藍色的階梯函數就是PLA的Error Measurement，正是因為它是一個階梯函數，所以我們不能使用Gradient Descent等Regression方法來處理，&lt;strong&gt;因為在階梯的每一點&lt;span class="math"&gt;\(\nabla E_{in}\)&lt;/span&gt;都是0（除了原點外），也就是如此PLA在更新的過程才無法確保趨近於最佳解，而需要使用Pocket PLA來解決這個問題&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;那如果我們用Linear Regression來做這件事呢？我們把Squared Error畫在上圖右側小圖的紅線，你會發現它的低點會落在&lt;span class="math"&gt;\(y\times s=1\)&lt;/span&gt;的地方，這應該不是我們要的結果，雖然它一樣可以把錯誤的判斷修正回正確，但是面對過度確定的正確答案，它反而會去修正它往錯誤的方向，很顯然這不是我們想要的。&lt;/p&gt;
&lt;p&gt;最好的方式就是Logistic Regression了，我們將&lt;span class="math"&gt;\(s\)&lt;/span&gt;做Logistic Function的轉換，轉換成機率，並在評估最大化Likelihood的條件下定義出Cross-Entropy來當作Error Measurement，在上圖右側的小圖，我們稍微調整Cross-Entropy，使得它的Error Function可以在&lt;span class="math"&gt;\(y\times s=0\)&lt;/span&gt;的地方和Squared Error相切，&lt;strong&gt;這張圖告訴我們的是隨著Grandient Descent每次的更新，Logistic Regression會把分類做的越來越好&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;後話&lt;/h3&gt;
&lt;p&gt;在這一篇當中，我們介紹了Grandient Descent這一個相當重要的演算法，並且運用在兩種Regression上：Linear Regression和Logistic Regression，Linear Regression是最簡單的Regression方法，甚至它還可以使用Pseudo-Inverse的方法直接算出最佳解，Logistic Regression考慮了有雜訊的Data產生的機率分布，我們可以用Logistic Regression做Soft Binary Classification，而且我們也說明了Logistic Regression為何適合拿來用在二元分類上。本篇我們對於ML的實際作法有了基本認識，在下一篇，我們繼續討論還有沒有什麼方式可以讓ML做的更好。&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習基石"></category></entry><entry><title>機器學習基石 學習筆記 (2)：為什麼機器可以學習?</title><link href="https://ycc.idv.tw/ml-course-foundations_2.html" rel="alternate"></link><published>2016-06-26T12:00:00+08:00</published><updated>2016-06-26T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2016-06-26:/ml-course-foundations_2.html</id><summary type="html">&lt;p&gt;機器可以學習嗎? / &lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;的差異 / VC Generalization Bound / 機器要能學習的三要素 / 學習架構&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;在上一回當中，我們初探了機器學習，了解了什麼時候適合使用機器學習，而不是一般的Hard Coding，那今天這篇文章要繼續問下去。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;為什麼機器可以學習(Why Can Machines Learn?)&lt;/strong&gt;，本篇會介紹學理上機器學習（ML）必須要有哪些條件才可行，這些理論有非常多的數學，但卻是了解機器學習非常重要的內功，我會盡量避開繁複的數學運算，而帶大家直接的了解式子所要告訴我們的觀念。&lt;/p&gt;
&lt;h3&gt;機器可以學習嗎?&lt;/h3&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.001" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.001.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;還記得上面這張圖嗎? 上次帶大家初探了Machine Learning(ML)的基本架構，可以把整個概念總結成上面這張圖。&lt;/p&gt;
&lt;p&gt;我們來複習一下，先從最上面的盆子開始看起，我們用Target Function代表你想要學習的技能，在非常理想的情況下，也就是沒有noise的情況，每組輸入變數 &lt;span class="math"&gt;\(X_n\)&lt;/span&gt;都會找到一組精確的輸出 &lt;span class="math"&gt;\(y_n\)&lt;/span&gt;，而這個Target Function能產生多個Data，圖中那些小球就是代表由Target Function產生的Data，今天我從中隨機抽取出&lt;span class="math"&gt;\(N\)&lt;/span&gt;組Data來做機器學習，接下來Learning Algorithm會利用這些取出的Data去找出最吻合的Hypothesis，那這組Hypothesis就成了我們學習出來的結果，我們可以利用這個結果來預測新的問題。&lt;/p&gt;
&lt;p&gt;那麼上面這張圖真的合理嗎? 我們真的有辦法用上面的方法讓機器學習嗎? &lt;/p&gt;
&lt;p&gt;先介紹幾個名詞，我們會稱&lt;strong&gt;抽樣的Data為In-sample Data&lt;/strong&gt;，並且稱&lt;strong&gt;Hypothesis預測In-sample Data的誤差為In-sample Error，記作&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;&lt;/strong&gt;，因此Learning Algorithm的目的就是找出那組Hypothesis使得&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小。&lt;/p&gt;
&lt;p&gt;回想一下二元分類問題，在上一篇當中我們使用PLA來挑選Hypothesis Set，還記得我們做了什麼事來確保我們可以得到最佳解嗎? 那就是Pocket的方法，Pocket的目的就是去留住一組能預測最好的Hypothesis，也就是能保留一組最佳參數使得&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;最小。&lt;/p&gt;
&lt;p&gt;但如果&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;真的已經可以壓到0了，我們就可以說機器學習已經完成了嗎？&lt;/p&gt;
&lt;p&gt;並不是這樣的，回到目的，我們真正希望的是機器有辦法預測新的問題，所以真正的目標是能將「沒有看過的Data」也可以預測好，而不是單單將取樣的Data預測好就夠了。&lt;/p&gt;
&lt;p&gt;我們會稱&lt;strong&gt;未被取樣的Data為Out-of-sample Data&lt;/strong&gt;，並且稱&lt;strong&gt;Hypothesis預測Out-of-sample Data的誤差為Out-of-sample Error，記作&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;，我們最終目的就是把&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;壓下來，也就代表可以預測新的問題&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;但遺憾的是我們不會真正知道&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;，除非我們知道Target Function，所以我們只能評估&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;來選取Model參數，因此重要的是需要&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;這個條件要成立，否則一切的學習都是無效的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;總結一下機器學習的條件，我們必須建立一個 Learning Model可以確保&lt;span class="math"&gt;\(E_{in}\approx E_{out}\)&lt;/span&gt;，所以在Learning Algorithm選出最小&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的Hypothesis，同時這組Hypothesis也可以很好的預測Out-sample，我們就可以說機器已經會學習了。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;的差異&lt;/h3&gt;
&lt;p&gt;&lt;img alt="image" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.005.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;剛剛我們已經提到了如果機器能學習，那就必須先確保&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，下面我會引入Hoeffding不等式來說明這個條件怎麼成立。&lt;/p&gt;
&lt;p&gt;先想像一下我有一個桶子，這個桶子裝了兩種顏色的小球，分別為橘色和綠色，今天如果桶子內橘色球佔的比例為&lt;span class="math"&gt;\(μ\)&lt;/span&gt;，而今天我們從中隨機抽樣出&lt;span class="math"&gt;\(N\)&lt;/span&gt;顆小球，並且計算出這&lt;span class="math"&gt;\(N\)&lt;/span&gt;顆小球中橘色佔的比例為&lt;span class="math"&gt;\(ν\)&lt;/span&gt;，此時我們可以想像的到，&lt;span class="math"&gt;\(μ=ν\)&lt;/span&gt;不一定會成立，但&lt;span class="math"&gt;\(μ\)&lt;/span&gt;也不至於離&lt;span class="math"&gt;\(ν\)&lt;/span&gt;太遠，所以Hoeffding不等式就告訴我們&lt;span class="math"&gt;\(|μ-ν|\)&lt;/span&gt;會被限制在一個範圍內，表示為：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}[|ν-μ|&amp;gt;ε] \leq 2 exp(-2ε^2N)
$$&lt;/div&gt;
&lt;p&gt;
當&lt;span class="math"&gt;\(ε\)&lt;/span&gt;越大，出現的機率就越低。&lt;/p&gt;
&lt;p&gt;接下來我們再把橘球和綠球的意義換成是，一組Hypothesis預測每筆Data的好或壞，預測正確的是綠球，預測失敗的是橘球，所以對於In-Sample來說，&lt;span class="math"&gt;\(μ\)&lt;/span&gt; 就是 &lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
μ = (1/N) \sum_{n=1}^N ⟦h(x)\neq y_n⟧ = E_{in}(h)
$$&lt;/div&gt;
&lt;p&gt;
對於Out-Sample來說，&lt;span class="math"&gt;\(ν\)&lt;/span&gt; 就是 &lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
ν =  ε_{x \sim P} ⟦h(x)\neq f(x)⟧ = E_{out}(h)
$$&lt;/div&gt;
&lt;p&gt;
套入剛剛的不等式，得
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}[|E_{in}(h)-E_{out}(h)|&amp;gt;ε] \leq 2 exp(-2ε^2N)
$$&lt;/div&gt;
&lt;p&gt;
上面這個式子告訴我們&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;差距超過&lt;span class="math"&gt;\(ε\)&lt;/span&gt;的可能性是被限制住的，只要抽樣的數量&lt;span class="math"&gt;\(N\)&lt;/span&gt;夠多，基本上&lt;span class="math"&gt;\(E_{in}\approx E_{out}\)&lt;/span&gt;就成立，我們這邊定義那些超出&lt;span class="math"&gt;\(ε\)&lt;/span&gt;的Data為Bad Data(不好的數據)，Bad Data出現的可能是被Bound住的，所以機器學習是有可能的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.006.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;而事實上，我們的hypothesis不會只有一個，所以接下來來考慮如果有M個Hypotheses的情況下我們的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;的差異會怎麼被參數影響。&lt;/p&gt;
&lt;p&gt;如果我們考慮M組Hypotheses，就會發現每種Hypothesis出現Bad Data的地方可能不一樣，因此大大的減少能使用的Data，如上圖左側所示。&lt;/p&gt;
&lt;p&gt;今天如果我有1000份從Target Function取&lt;span class="math"&gt;\(N\)&lt;/span&gt;個Data的情形，然後只用一個Hypothesis來衡量，根據Hoeffding's Inequality，1000份裡面假設大概5份會出現Bad Data，但今天我再增加一組Hypothesis來衡量，對於這個Hypothesis也可能有自己的5份Bad Data，如果很不幸的，剛剛好這5份Bad Data和前5份沒有重疊，因此用這兩個hypotheses來評估的話，1000份裡頭將會出現10份的Bad Data，由此類推，如果有&lt;span class="math"&gt;\(M\)&lt;/span&gt;組Hypotheses，最差的情況會發生在什麼時候呢? 那就是&lt;span class="math"&gt;\(M\)&lt;/span&gt;個Hypotheses的每份Bad Data彼此都沒有交集，夠慘吧! 所以把這些出現Bad Data的機率取聯集得到以下式子：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}[\exists h\in \mathbb{H}\ s.t.\ |E_{in}(h)-E_{out}(h)|&amp;gt;ε] \leq 2M exp(-2ε^2N)
$$&lt;/div&gt;
&lt;p&gt;
大家現在回想一下上一篇所提到的Perceptron Hypothesis Set就會發現，糟糕了! Perceptron Hypothesis Set 裡有無限多組的Hypotheses，也就是&lt;span class="math"&gt;\(M→∞\)&lt;/span&gt;，那我們不就需要無限多的Data才能做到&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，否則機器根本不會學習，所以前一篇的內容都在亂講，PLA根本無法學習，因為&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，就算&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;很小也不代表學習成立，機器學習是不可能的。等一下！先沉住氣，聽我接下來慢慢解釋，你就會發現還有一線生機。&lt;/p&gt;
&lt;h3&gt;VC Generalization Bound&lt;/h3&gt;
&lt;p&gt;&lt;img alt="image" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.007.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;問題出在這裡，我們在Multi-Bin Hoeffding’s Inequality中採用了一個假設，就是假設每組Hypotheses的Bad Data彼此間都沒有重疊，所以在&lt;span class="math"&gt;\(M→∞\)&lt;/span&gt;的情況下，當然會有一個無限大的上限值，但如果考慮了Bad Data重疊的情形，縱使&lt;span class="math"&gt;\(M→∞\)&lt;/span&gt;的情況下還是有機會把Bad Data的出現機率壓在一個有限的定值之下。&lt;/p&gt;
&lt;p&gt;我們回到二元分類問題，看一下上圖中左側的圖例，如果今天在二維平面上做二元分類，當數據量只有1個&lt;span class="math"&gt;\(n=1\)&lt;/span&gt;時，就算你的切法有無窮多種，但對於一組Data來說就只有兩類Hypotheses而已，再來看&lt;span class="math"&gt;\(n=2\)&lt;/span&gt;的情況，一樣的無限多組的切法但Hypotheses也只能歸類成4類。&lt;/p&gt;
&lt;p&gt;所以Hypotheses用來描述數據的情況是彼此有所重疊的，也就是Bad Data出現的情形在許多Hypotheses是相同的。&lt;/p&gt;
&lt;p&gt;但是聰明的你一定想到，如果今天&lt;span class="math"&gt;\(n\)&lt;/span&gt;的數量不斷的增加，則Hypotheses被分類的數量就會成指數 &lt;span class="math"&gt;\(2^n\)&lt;/span&gt; 增加，Hypotheses彼此之間Bad Data的重疊情況就會漸漸減少，因此仍然無法限制住Bad Data的數量。&lt;/p&gt;
&lt;p&gt;先別緊張，我們繼續看下去，當&lt;span class="math"&gt;\(n=3\)&lt;/span&gt;，沒有意外的Hypotheses會被分類為8類，那接下來&lt;span class="math"&gt;\(n=4\)&lt;/span&gt;時，你就會發現一個有趣的現象，開始有一些分類情況是不會出現的，因為它無法被一分為二，因此我們擔心因為Data數量增加而造成Hypotheses的種類暴增的情形被排除了，有一些狀況是不會出現的，Hypotheses是有重疊的。&lt;/p&gt;
&lt;p&gt;剛剛所提到的分類方式的數量稱為Dichotomy。在&lt;span class="math"&gt;\(n=1\)&lt;/span&gt;、&lt;span class="math"&gt;\(n=2\)&lt;/span&gt;到&lt;span class="math"&gt;\(n=3\)&lt;/span&gt;的情形，所有列得出來的方式都可被完整分類開來，我們稱這情形為Shatter，但是到了&lt;span class="math"&gt;\(n=4\)&lt;/span&gt;的時候，有些不可能被分類的情形出現了，稱為不可被Shatter，另外我們又稱此情形開始發生的那點為Break Point，這邊注意一下喔! 會不會存有Break Point取決於你的Hypothesis Set長怎麼樣，現在這個例子的Break Point在&lt;span class="math"&gt;\(n=4\)&lt;/span&gt;，其他的Hypothesis Set就不一定了。&lt;/p&gt;
&lt;p&gt;Break Point的出現非常重要，他所代表的是Bad Data的出現機率不會無所限制的大下去，因此把這概念帶入Multi-Bin Hoeffding’s Inequality，經過繁複的計算，就可以得到以下公式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mathbb{P}[\exists h\in \mathbb{H}\ s.t.\ |E_{in}(h)-E_{out}(h)|&amp;gt;ε] \leq 4m_{\mathbb{H}}(2N) exp(-ε^2N/8)
$$&lt;/div&gt;
&lt;p&gt;
，原本的&lt;span class="math"&gt;\(M\)&lt;/span&gt;消失了，取而代之的是Growth Function  &lt;span class="math"&gt;\(m_{\mathbb{H}}(2N)\)&lt;/span&gt;，Growth Function與Data數量&lt;span class="math"&gt;\(N\)&lt;/span&gt;有關，這就是我們剛剛解說的，決定Hypothesis Set的種類的其實是 Data的數量&lt;span class="math"&gt;\(N\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;那麼Growth Function要怎麼和Break Point連結起來呢？&lt;/p&gt;
&lt;p&gt;先定義一下VC Dimension：&lt;span class="math"&gt;\(d_{VC}= Break Point-1\)&lt;/span&gt;，Break Point代表首次出現不Shatter的情況，那比它小一級代表的正是最大可以Shatter的點，上面的例子中&lt;span class="math"&gt;\(d_{VC}=3\)&lt;/span&gt;。而這個VC Dimension就可以和我們在意的Growth Function連接起來，經過數學推倒可以得到以下關係式：
&lt;/p&gt;
&lt;div class="math"&gt;$$
m_{\mathbb{H}}(n) = n^{d_{VC}},\ d_{VC} = BreakPoint-1
$$&lt;/div&gt;
&lt;p&gt;
所以我們就知道啦！&lt;strong&gt;只要有Break Point存在，VC Dimension就是一個有限的值，也因此Growth Function是一個有限的值，VC Bound就產生了，就可以確保Bad Data出現的機率被壓在一個定值之下，所以一樣的只要資料量&lt;span class="math"&gt;\(N\)&lt;/span&gt;夠多就可以確保&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，機器將可以學習。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;另外一件重要的事，VC Dimension在數學上是有意義的，&lt;strong&gt;&lt;span class="math"&gt;\(d_{VC} \approx 可調控變數的個數\)&lt;/span&gt;&lt;/strong&gt;，像是上述的二維二元分類問題，它的可調控變數有&lt;span class="math"&gt;\(w_0\)&lt;/span&gt;, &lt;span class="math"&gt;\(w_1\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(w_2\)&lt;/span&gt;，總共3個，所以&lt;span class="math"&gt;\(d_{VC}=3\)&lt;/span&gt;。&lt;strong&gt;也就是說Hypothesis Set的可調變參數如果是有限，大部分都可以做機器學習。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;機器要能學習的三要素&lt;/h3&gt;
&lt;p&gt;前面拉哩拉雜的講了一堆，終於要推出我們的結論了! 所以如果剛剛的數學讓你感到很挫敗，沒關係，讀懂這段那就足夠了。&lt;/p&gt;
&lt;p&gt;從VC Generalization Bound，我們可以知道機器學習是可能的，只要它具備三點要素：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Good Hypothesis Set: Hypothesis Set 必須有Break Point的存在，也意味著VC Dimension是有限的，而且越小越好，在意義上代表可以調控的變數不要太多。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Good Data: 數據量越大越好，可以壓低VC Generalization Bound&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Good Learning Algorithm: 以上兩點可以確定的是&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，接下來好的Learning Algorithm要有能力找到&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt; 最小的參數。很直觀的，當我們可以調控的變數越多，我們的選擇就越多，也就是我們可以找到更小&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt; 的機會變多了，所以可以調控的變數不可以太少。&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;眼尖的你有沒有發現矛盾啊! 可以調控的變數很少，我們能確保&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;，但是如果我想要找到更小的&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt; 又必須有更多的調控變數，這個矛盾是機器學習上一個重要的課題，&lt;strong&gt;解法是我們必須要能找到適當的調控變數數量，也就是適當大小的&lt;span class="math"&gt;\(d_{VC}\)&lt;/span&gt; &lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.000.02.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://d396qusza40orc.cloudfront.net/ntumlone/lecture_slides/07_handout.pdf"&gt;https://d396qusza40orc.cloudfront.net/ntumlone/lecture_slides/07_handout.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上圖中，我們把VC Generalization Bound公式帶入Growth Function和&lt;span class="math"&gt;\(d_{VC}\)&lt;/span&gt;的關係式，並且設&lt;span class="math"&gt;\(δ\)&lt;/span&gt; 為最大可以容忍的Bad Data出現機率，把它帶入取代掉&lt;span class="math"&gt;\(ε\)&lt;/span&gt;，整理一下，就可以推出上圖的公式，&lt;span class="math"&gt;\(\Omega (N,\mathbb{H},δ)\)&lt;/span&gt;稱為Model Complexity，這一項代表的是Hypothesis Set的大小造成的模型複雜度，它隨著&lt;span class="math"&gt;\(d_{VC}\)&lt;/span&gt;增加而增加。Model Complexity越大代表Bad Data更容易出現，所以&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;開始被帶開了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;這個現象有一個很常見的名字叫做Overfitting，指的是使用非常複雜的Model來Fitting，雖然可以把手頭上的數據Fit的很漂亮，但是拿到其他的數據來看就會發現這Model的預測性非常的差，原因就是因為Model Complexity造成&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;和&lt;span class="math"&gt;\(E_{out}\)&lt;/span&gt;脫鉤了，所以選擇一個複雜度適中的Model是很重要的。&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;機器學習架構一般化&lt;/h3&gt;
&lt;p&gt;&lt;img alt="image" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.008.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;最後我們來總結一下機器學習的流程，上圖中是之前提到的機器學習的架構並額外考慮一些真實情形，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每筆Data出現的機會不一定，同樣的採樣結果也是會受機率的影響，所以上圖中標示為&lt;span class="math"&gt;\(\mathbb{P} (x)\)&lt;/span&gt;，這個修改並不會影響機器學習的流程和結果。&lt;/li&gt;
&lt;li&gt;Data可能會受到Noise的影響，所以給定&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;並不一定會百分之一百得到&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;，他存在著可能會出錯，上圖標示為&lt;span class="math"&gt;\(\mathbb{P}(y|x)\)&lt;/span&gt;，我們可以增大我們採樣的數量&lt;span class="math"&gt;\(N\)&lt;/span&gt;來減少Noise的影響。&lt;/li&gt;
&lt;li&gt;我們是採用&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;來當作選擇Model參數的指標，因此我們需要訂出Error的評估方式，常見的有Squared Error &lt;span class="math"&gt;\(E_{squared} = (y_n - y_{prediction})^2\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;跟著架構我們就有一套機器學習的&lt;strong&gt;標準流程&lt;/strong&gt;，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;準備好足夠的數據&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;把Model建立好，&lt;span class="math"&gt;\(d_{VC}\)&lt;/span&gt;必須要是有限的，而且大小要適中&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;定義好評估&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;的Error Measurement&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;使用演算法找出最佳參數把&lt;span class="math"&gt;\(E_{in}\)&lt;/span&gt;降低&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最後評估一下是否有Overfitting的狀況，確保&lt;span class="math"&gt;\(E_{in} \approx E_{out}\)&lt;/span&gt;&lt;/strong&gt;（未來會講怎麼做）&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習基石"></category></entry><entry><title>機器學習基石 學習筆記 (1)：何時可以使用機器學習?</title><link href="https://ycc.idv.tw/ml-course-foundations_1.html" rel="alternate"></link><published>2016-06-06T12:00:00+08:00</published><updated>2016-06-06T12:00:00+08:00</updated><author><name>YC Chen</name></author><id>tag:ycc.idv.tw,2016-06-06:/ml-course-foundations_1.html</id><summary type="html">&lt;p&gt;什麼是Machine Learning / ML的使用時機 / 二元分類問題 / 多元學習&lt;/p&gt;</summary><content type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;經過幾個月的努力，終於完成田神在Coursera上machine learning的兩門課中的第一門課—&lt;a href="https://www.coursera.org/course/ntumlone"&gt;機器學習基石&lt;/a&gt;，田神不愧為田神的名號，整門課上起來非常流暢，每個觀念講得非常得清晰，考究學理，但是又不會單單只有理論而已，課程中會舉很多實用的例子，讓你了解每個觀念如何實踐。因此，非常推薦大家去把Coursera上面的課程完整聽一次，應該會收益良多，接下來一系列的文章，我會摘要出《機器學習基石》之中主要的概念，適合對Machine Learning（ML）有興趣的初學者來一窺它的脈絡。&lt;/p&gt;
&lt;p&gt;《機器學習基石》一共有16堂課，主要分為四個方向，第一個方向，&lt;strong&gt;何時可以使用機器學習(When Can Machines Learn? )&lt;/strong&gt;，點出什麼是機器學習，適合在哪些情形下使用，並引入貫穿整個課程的二元分類問題，第二個方向，&lt;strong&gt;為什麼機器可以學習(Why Can Machines Learn?)&lt;/strong&gt;，介紹學理上機器學習必須要有哪些條件才可行，這些理論是了解機器學習非常重要的內功，第三個方向，&lt;strong&gt;機器可以怎麼樣學習(How Can Machines Learn?)&lt;/strong&gt;，學習完了學理，我們來看機器學習有哪些的使用方法，最後一個方向，&lt;strong&gt;機器可以怎麼樣學得更好(How Can Machines Learn Better?)&lt;/strong&gt;，探討哪些問題會造成機器學不好，然後怎麼去改善。&lt;/p&gt;
&lt;h3&gt;什麼是Machine Learning (ML)&lt;/h3&gt;
&lt;p&gt;在了解機器學習之前，我們不妨來想想「你」從小是怎麼學習的，有人會說學習就是一個不斷記憶的過程，但這樣的說法顯然不夠全面，你總不會認為把考題的所有答案都背起來的學生就已經學會一門知識了吧！所以，考題只是表象，我們真正要學習的是它背後的觀念，可以拿來推敲未知的知識。&lt;/p&gt;
&lt;p&gt;同樣的，ML的學習方式也有點類似於人類的學習，機器從Data中開始學習起，這些Data就像是一道一道的考題，而ML做的事正是去學習Data後面的觀念，而不是單純把Data給儲存起來，有了Data背後的觀念才能舉一反三，才算是真正的學會了。&lt;/p&gt;
&lt;p&gt;所以，做ML有點像是手把手的造一顆大腦，並且訓練它學會Data背後的知識。那這個大腦要怎麼設計呢？這個大腦用我們學物理的人的說法就是建一個Model，而餵給它Data的過程就是Model Fitting。&lt;/p&gt;
&lt;p&gt;那什麼是Model呢？讓我來解釋一下，&lt;strong&gt;所謂的Model就是給一個未知現象的框架來試圖描述它&lt;/strong&gt;，舉個例子，我們都知道力的公式是&lt;span class="math"&gt;\(F=ma\)&lt;/span&gt;（力＝質量x加速度），但如果你今天拿一顆皮球來，你就會發現這個公式並不那麼正確，因為皮球會形變，那怎麼辦呢？我們可以假設形變會把部份的力給抵消掉，所以式子改寫成&lt;span class="math"&gt;\((F-F_1)=ma\)&lt;/span&gt;，在這邊&lt;span class="math"&gt;\(F_1\)&lt;/span&gt;就是那個抵消的力，這樣就是設計了一個Model來描述這個現象，而&lt;span class="math"&gt;\(F_1\)&lt;/span&gt;是一個未知的值，我們可以用實驗數據來推估&lt;span class="math"&gt;\(F_1\)&lt;/span&gt;，這就是所謂的Model Fitting。&lt;/p&gt;
&lt;p&gt;物理上的Model通常是這樣做的，我們先觀察未知現象，然後從中猜測可能造成這現象的原因，總結這些原因來設計一個Model，Model中可能有一些參數還沒被決定，此時我們就可以用數據來決定它，這就是Model Fitting。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.001" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.001.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;了解了Model的概念就相當好了解ML的架構，上圖是ML的基本架構，&lt;strong&gt;假設我們今天要讓機器學一樣技術，這個技術我們用一個函數來表示，稱之為Target Function，這個Target Function就是隱藏在Data後面的真正道理&lt;/strong&gt;，每個變數&lt;span class="math"&gt;\(X\)&lt;/span&gt;會有相應的正確答案&lt;span class="math"&gt;\(Y\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;今天我從Target Function中取出&lt;span class="math"&gt;\(N\)&lt;/span&gt;組當作Data來給我的機器學習，那目標是什麼?&lt;strong&gt;目標當然是讓機器學習出這個Target Function啦！&lt;/strong&gt;所以我們要先設計我們的Model，最終目的是決定Model裡的參數之後，這個被選擇的Model就是Target Function。&lt;/p&gt;
&lt;p&gt;Model就是上圖中的Hypothesis Set，在Model參數還沒被決定之前，你可以想像它就像一個集合包含很多可以選擇的函數，而使用數據Model Fitting以後，選出一組最佳化的參數，就好像從這個集合中挑選一組函數一樣。&lt;/p&gt;
&lt;p&gt;在這個找最佳化參數的過程，我們需要一個機制，這個機制可以評估Hypothesis Set中每組函數描述Data的好壞，並且找出描述Data最好的那組參數，這個機制就是上圖中的Learning Algorithm。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;建立Model，使用Data加上Learning Algorithm找出最佳參數，這就是ML的架構輪廓&lt;/strong&gt;。當然這邊要補充一下，物理上的Model通常是建基在已知的知識之上，而常見的ML強大之處是不需要太多的人為的智慧，機器可以自行學習，所以我這裡指的Model是比物理上的Model更加廣義的。&lt;/p&gt;
&lt;h3&gt;Machine Learning (ML)的使用時機&lt;/h3&gt;
&lt;p&gt;剛剛帶大家初探了ML的架構，接下來帶大家了解什麼時候我們適合使用ML。&lt;/p&gt;
&lt;p&gt;舉幾個例子，大家可能比較有感覺，譬如說Netflix曾辦過一場競賽，競賽的內容是利用客戶的影片評分紀錄，來預測未評分影片的得分，如果可以增進預測率10%，就可以獨得100萬美元獎金，這個問題就可以使用ML，Data是過去得評分紀錄，Target Function是用戶評分的規律，如此一來，機器學到了這個技術，未來就可以舉一反三的推出未評分影片的分數，和用戶喜歡的影片可能有哪些。&lt;/p&gt;
&lt;p&gt;再多看幾個例子，例如設計火星勘查機，人類目前對火星的了解仍相當有限，所以我們沒辦法完全猜測勘查機在火星會遇到什麼問題，所以必須讓勘查機有ML的能力去學習各種問題的解決方法。&lt;/p&gt;
&lt;p&gt;再來個例子，現在很夯的汽車自動駕駛也需要ML技術，機器去學習辨識交通號誌。&lt;/p&gt;
&lt;p&gt;看了這麼多例子，我們會發現這些例子都很難以寫出簡單的規則，但是卻又存在著一種規律，這種情形正是適合用ML來做。&lt;/p&gt;
&lt;p&gt;在以往電腦工程幾乎都是由工程師用嚴謹的邏輯去逐條的把規則一一的寫上，這樣的機器不具有學習能力，或稱得上人工智慧，因為它只是單純反應工程師的工人智慧而已，但如果遇到一些困難的問題，譬如告訴機器什麼是狗，這時候你就會發現很難用人為規則來描述它，有尾巴，可是是怎樣的尾巴？有耳朵，那這耳朵怎麼和貓的耳朵區分開來？此時要用人為寫出規則就太困難了，我們不這麼做，反過來我們設計架構讓機器自己去從Data中學習。&lt;/p&gt;
&lt;p&gt;總結一下上面的重點，ML的最佳使用時機包含下面三種情形&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;你想要學習的技術存在一種模式&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;要學習的技術不容易簡單的列出規則&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;存在可以代表這個要學習的模式的Data&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;二元分類問題&lt;/h3&gt;
&lt;p&gt;&lt;img alt="img" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.000.01.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;from: &lt;a href="https://class.coursera.org/ntumlone-003/lecture/17"&gt;https://class.coursera.org/ntumlone-003/lecture/17&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;好! 大家現在應該對於機器學習有一些認識了，那接下來我們來實作一些例子來了解機器學習架構怎麼運作。像個小學生一樣，我們先從簡單的是非題來學起，是非題學究一點的講法就是「二元分類問題」。&lt;/p&gt;
&lt;p&gt;舉個例子，今天有一家銀行想要開發一款ML的軟體，這個軟體可以根據過去信用卡核發用戶的資料，去判斷要不要核發信用卡給這個新的申請人，這些過去的資料可能包括：用戶年齡、用戶性別、用戶年薪等等，讓機器藉由這些資料去學習判斷要不要核發信用卡。把這樣的二元分類問題化作&lt;/p&gt;
&lt;p&gt;Target Function：&lt;span class="math"&gt;\(f: X → y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(X\)&lt;/span&gt;有年齡、性別和年薪這些變數，而&lt;span class="math"&gt;\(y\)&lt;/span&gt;則是個二元類別，不是&lt;span class="math"&gt;\(y=1\)&lt;/span&gt;(核發)就是&lt;span class="math"&gt;\(y= -1\)&lt;/span&gt;(不核發)。&lt;/p&gt;
&lt;p&gt;那接下來，我們就要決定我們的Learning Model，也就是Hypothesis Set。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.002" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.002.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;引入&lt;strong&gt;Perceptron(感知器) Hypothesis Set&lt;/strong&gt;來當作我們的Hypothesis Set，如上圖，我們給予我們的輸入變數個別的權重，然後相加起來，並且看這個值是正還是負，來決定輸出值是&lt;span class="math"&gt;\(+1\)&lt;/span&gt;或&lt;span class="math"&gt;\(-1\)&lt;/span&gt;，&lt;span class="math"&gt;\(sign\)&lt;/span&gt;函數的作用是假設輸入的值為正則輸出&lt;span class="math"&gt;\(+1\)&lt;/span&gt;，反之則輸出&lt;span class="math"&gt;\(-1\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;對應核發信用卡這個例子，&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(x_1\)&lt;/span&gt; = 用戶年齡; &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; = 用戶性別; &lt;span class="math"&gt;\(x_3\)&lt;/span&gt; = 用戶年薪，&lt;/p&gt;
&lt;p&gt;在分別乘上weight &lt;span class="math"&gt;\(w_1\)&lt;/span&gt;, &lt;span class="math"&gt;\(w_2\)&lt;/span&gt;, &lt;span class="math"&gt;\(w_3\)&lt;/span&gt;，這個變數前面的weight代表這個變數對於答案&lt;span class="math"&gt;\(Y\)&lt;/span&gt;有什麼影響，如果是正向影響，&lt;span class="math"&gt;\(weight &amp;gt; 0\)&lt;/span&gt;，如果沒有影響，&lt;span class="math"&gt;\(weight = 0\)&lt;/span&gt;，如果負向影響，&lt;span class="math"&gt;\(weight &amp;lt; 0\)&lt;/span&gt;，舉個例子，高年薪也許可以提升核發信用卡的機會，那它前面的weight應該就是正的，也許性別並不影響核發信用卡的機會，則&lt;span class="math"&gt;\(weight = 0\)&lt;/span&gt;，那麼考慮到這些input變數對結果影響的評估，我們會得到一個數值 &lt;span class="math"&gt;\((w_1\times x_1+w_2\times x_2+...)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;此時我們要用這個數值去做「二元分類」，也就是一分為二，怎麼做呢? 很簡單，給他一分水嶺，高於一個閥值我就給他 &lt;span class="math"&gt;\(y=+1\)&lt;/span&gt;，低於一個閥值我就給他&lt;span class="math"&gt;\(y=-1\)&lt;/span&gt;，假設這個閥值為&lt;span class="math"&gt;\((-w_0)\)&lt;/span&gt;，則分類依據就可以表示為 &lt;span class="math"&gt;\(sign(w_0+w_1\times x_1+w_2\times x_2+...)\)&lt;/span&gt; 。&lt;/p&gt;
&lt;p&gt;上圖中的 &lt;span class="math"&gt;\(s = w_0+w_1\times x_1+w_2\times x_2+...\)&lt;/span&gt; 就像一個分數(score)一樣，高分 &lt;span class="math"&gt;\(s&amp;gt;0\)&lt;/span&gt; 的我就核發(&lt;span class="math"&gt;\(+1\)&lt;/span&gt;)，低分 &lt;span class="math"&gt;\(s &amp;lt; 0\)&lt;/span&gt; 的我就不核發(&lt;span class="math"&gt;\(-1\)&lt;/span&gt;)，其中權重 &lt;span class="math"&gt;\(w_0, w_1, w_2, ...\)&lt;/span&gt; 都可以由機器學習去調整，這些不同的weight就構成了Hypothesis Set，也就是Model，那接下來我們還需要Learning Algorithm來取出最佳參數，也就是決定一組最佳weight來選出最吻合數據的Hypothesis。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.003" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.003.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;如上圖所示，&lt;strong&gt;Perceptron Learning Algorithm(PLA)&lt;/strong&gt;是用於處理Perceptron Hypothesis Set的一種演算法。&lt;/p&gt;
&lt;p&gt;它的作法簡單來講是，藉由一筆一筆的數據去逐步的更新它的weight使得Model可以描述這筆數據，直到不需要再更新為止，此時所有的Data都可以用這個Model表示，更新的方法是先判斷進來的這筆數據是否符合目前的Model預測，如果不符合，此時&lt;span class="math"&gt;\(\left[...\right]\)&lt;/span&gt;為&lt;span class="math"&gt;\(+ 1\)&lt;/span&gt;，則必須朝變數向量&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;的方向，前進或後退大小為Learning Rate的一步來更新weight，前進還是後退端看你的Data是&lt;span class="math"&gt;\(y=-1\)&lt;/span&gt;或&lt;span class="math"&gt;\(+1\)&lt;/span&gt;，&lt;span class="math"&gt;\(y=+1\)&lt;/span&gt;就往前進，&lt;span class="math"&gt;\(y=-1\)&lt;/span&gt;就往後退。&lt;/p&gt;
&lt;p&gt;因此，這個跨步更新的動作必須可以使Model接近正確答案，這麼神奇，真的假的？不太直覺，先從score來想起，假設有一筆資料為&lt;span class="math"&gt;\((X_n,y_n)\)&lt;/span&gt;，則Score：&lt;span class="math"&gt;\(s = W_t・X_n\)&lt;/span&gt;，在&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;和&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;向量彼此有同向分量的情況下，&lt;span class="math"&gt;\(s &amp;gt; 0\)&lt;/span&gt;，如果這個時候&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;剛好為&lt;span class="math"&gt;\(+1\)&lt;/span&gt;，則&lt;span class="math"&gt;\(sign(s)=y_n\)&lt;/span&gt;，這個時候&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;描述這個數據就很好啊，我們就不需要去更新它；如果相反&lt;span class="math"&gt;\(y_n=-1\)&lt;/span&gt;，這個&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;描述這個數據就不正確，也就是說&lt;span class="math"&gt;\(W_t\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(X_n\)&lt;/span&gt;不應該同向，所以我們讓&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;加上&lt;span class="math"&gt;\(-X_n\)&lt;/span&gt;(&lt;span class="math"&gt;\(=y_n\times X_n\)&lt;/span&gt;)，把&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;從原本與&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;同向的狀態反向拉離開來。那如果在&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;和&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;向量彼此不同向的情況下，&lt;span class="math"&gt;\(s &amp;lt; 0\)&lt;/span&gt;，這個時候如果&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;剛好為&lt;span class="math"&gt;\(-1\)&lt;/span&gt;，則&lt;span class="math"&gt;\(sign(s)=y_n\)&lt;/span&gt;，很好我們不去更新它；如果相反&lt;span class="math"&gt;\(y_n=+1\)&lt;/span&gt;，這個&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;描述這個數據不正確，也就是說&lt;span class="math"&gt;\(W_t\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(X_n\)&lt;/span&gt;不應該反向，所以我們讓&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;加上&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;(&lt;span class="math"&gt;\(=y_n\times X_n\)&lt;/span&gt;)，把&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;拉到和&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;同向一點。這就是PLA找到更好&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;的機制。&lt;/p&gt;
&lt;p&gt;&lt;img alt="MachineLearningFoundations.004" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.004.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;Seeing is believing，上面這張圖帶我們來看PLA如何運作，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initially: 在最一開始的時候，我們weight &lt;span class="math"&gt;\(W_t\)&lt;/span&gt;先設成零向量&lt;/li&gt;
&lt;li&gt;Update 1: PLA更新把零向量的&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;拉成&lt;span class="math"&gt;\(W_{t+1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Update 2: 上一輪的&lt;span class="math"&gt;\(W_{t+1}\)&lt;/span&gt;已經是這一輪的&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;，也就是紅色的那個向量，&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;決定了一條壁壘分明的二元分類邊界，這條線的方程式其實就是 &lt;span class="math"&gt;\(w_0+w_1x_1+... = 0\)&lt;/span&gt;，如果你還記得高中數學的話，這條邊界必然會和&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;垂直，如圖所示，而&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;的方向是屬於&lt;span class="math"&gt;\(y=+1\)&lt;/span&gt;的區域，這一輪剛剛好找到一個圈(&lt;span class="math"&gt;\(y=+1\)&lt;/span&gt;)落在&lt;span class="math"&gt;\(y=-1\)&lt;/span&gt;的區域，因此我們需要更新weight，做法是把&lt;span class="math"&gt;\(W_t\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(y_n\times X_n\)&lt;/span&gt;(=&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;)相加成為新的weight &lt;span class="math"&gt;\(W_{t+1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;...........以此類推&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;如果資料線性可分的話，PLA在迭代多次後，是可以用一條線完全區分兩種數據&lt;/strong&gt;。但如果數據不是線性可分，不存在一條線來區分數據，此時最佳解就必須評估整體犯錯有多少，找出犯錯最少的那條直線就是最佳解，但可惜的是PLA方法並不會在迭代中趨向於犯錯最少的那條線，什麼時候該停止迭代是個世紀難解的NP-Hard問題（如果不了解這個名詞，&lt;a href="http://www.ycc.idv.tw/algorithm-complexity-theory.html"&gt;詳見&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;因此要改變一下PLA，這個方法我們稱之為Pocket，當每次得到一組weight的時候，都拿它來評估它對所有Data的區分能力好或壞，而只留下一組最好的放進口袋裡，所以當迭代次數做多了，保留在口袋的這組解就可以看成是最佳解，就這麼簡單。&lt;/p&gt;
&lt;h3&gt;多元學習&lt;/h3&gt;
&lt;p&gt;機器學習和人類學習一樣，有各式各樣的學習型態。剛剛的&lt;strong&gt;「二元分類問題」&lt;/strong&gt;就像考「是非題」一樣，答案要嘛是Yes不然就是No，表示為 &lt;strong&gt;&lt;span class="math"&gt;\(y=\{-1, 1\}\)&lt;/span&gt;&lt;/strong&gt;，這就像是機器在小學時代的問題，較為簡單。&lt;/p&gt;
&lt;p&gt;現在機器脫離國小來到了國中，考試題目開始出現「選擇題」，這和機器學習中的&lt;strong&gt;「多元分類問題」&lt;/strong&gt;一樣，必須從兩個以上有限的答案中作選擇，表示為 &lt;strong&gt;&lt;span class="math"&gt;\(y=\{1, 2, ... , k\}\)&lt;/span&gt;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;另外機器還可能遇到傷透腦筋的「計算題」，在機器學習裏頭稱為&lt;strong&gt;「Regression 問題」&lt;/strong&gt;，這個時候答案已經放寬到整個實數系了，表示為 &lt;strong&gt;&lt;span class="math"&gt;\(y∈R\)&lt;/span&gt;&lt;/strong&gt;，舉個例子，譬如利用過去天氣的數據去預測明日氣溫，或者利用歷史股價資料預測未來股價，都是Regression的應用。&lt;/p&gt;
&lt;p&gt;此時，機器到了大學，開始碰到不那麼容易回答，甚至不存在單一答案的「申論題」，這在ML中像是&lt;strong&gt;「Structure Learning 問題」&lt;/strong&gt;，答案的選擇換成了各種結構，表示為 &lt;strong&gt;&lt;span class="math"&gt;\(y=\{structures\}\)&lt;/span&gt;&lt;/strong&gt;，舉個例子可能比較好理解，例如：自然語言，我們都希望有一天電腦可以理解我們的語言，我們可以不再需要以機器語言來和電腦溝通，而是用人類的語言直接和電腦溝通，聽起來很棒對吧! 這個部分的ML就需要Structure Learning來學習語言的文法結構。&lt;/p&gt;
&lt;p&gt;我們教機器學習也有各種不同的教育方法。&lt;/p&gt;
&lt;p&gt;有像是填鴨式教育的&lt;strong&gt;「Supervised Learning」(監督式學習）&lt;/strong&gt;，直接告訴機器考題和答案，讓機器從中學習，這種情況下每筆資料&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;對應的&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;都有明確Label，Data中有明顯的答案。&lt;/p&gt;
&lt;p&gt;有像是培養科學家教育方法一樣的&lt;strong&gt;「Unsupervised Learning」(非監督式學習）&lt;/strong&gt;，此時每筆資料&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;對應的&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;都沒有Label，所以機器要自己歸納整理，然後從中學到規律，通常用於分群問題，對資料做分類找出規律性。&lt;/p&gt;
&lt;p&gt;那還有折衷於上述兩種方法的啟發式教育，&lt;strong&gt;「Semi-supervised Learning」(半監督式學習）&lt;/strong&gt;，在這個情形下有部分資料&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;是有Label的，機器可以藉由有Label的正確答案和資料的規律性來做更好的學習，一個有名的例子是Facebook的人臉辨識標記功能，有部分已經被用戶標記的照片，這屬於有Label的&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;，但有更多沒有標記的照片，這些照片也可以幫助ML學習。&lt;/p&gt;
&lt;p&gt;那還有像是訓練小狗的方法，當我跟小狗說坐下，如果牠真的坐下了，這個時候我就給牠獎勵，譬如說餵牠好吃的食物，久而久之牠就會學會聽從這個命令，&lt;strong&gt;「Reinforcement Learning」(強化式學習）&lt;/strong&gt;就是不直接表明&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;的Label，但是機器能在嘗試中得到&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;結果的好壞，再從這個好壞當作回饋去優化它的學習。&lt;/p&gt;
&lt;p&gt;Data給的方法也可以有很多種類。&lt;/p&gt;
&lt;p&gt;剛剛舉的ML例子都是屬於&lt;strong&gt;「Batch Learning」&lt;/strong&gt;，也就是一次給你所有的Data。另外一種給Data的方法叫做&lt;strong&gt;「Online Learning」&lt;/strong&gt;，這個情形下Data會一個一個以序列的方式餵給機器，這麼方式下的Model可以隨時更新。最後一種方式是&lt;strong&gt;「Active Learning」&lt;/strong&gt;，機器不僅是被動的接受 Data，而是會根據它自己的需求向使用者索取它想要的Data。&lt;/p&gt;
&lt;p&gt;另外，除了有輸出值&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;有多種種類之外，輸入的變數&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;的來源也有很多種，我們稱之為Features。&lt;/p&gt;
&lt;p&gt;如果具有物理意義的輸入變數，稱之為&lt;strong&gt;「Concrete Features」&lt;/strong&gt;，這些變數建立在人類知識的預先處理。還有輸入變數並不具有物理含意的情形，這稱之為&lt;strong&gt;「Abstract Features」&lt;/strong&gt;。那有些情形下直接採用不加以處理的原始數據，稱為&lt;strong&gt;「Raw Features」&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;而使用工人智慧由人力從Raw Features中萃取出Concrete Features，這叫做Feature Engineering。相反的，現在很夯的Deep Learning厲害的地方是他可以自行從Data中學習 Features。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;總結一下，機器學習有很多種型態，從Data的給予方式可分為Batch Learning、Online Learning和Active Learning。Data的表達形式由輸入變數&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;和輸出值&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;所決定，從輸入變數&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;的來源可分為Concrete Features、Raw Features和Abstract Features，從輸出值&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;的種類上可以分為二元分類、多元分類、Regression和Structured Learning 問題，從輸出值&lt;span class="math"&gt;\(y_n\)&lt;/span&gt;的Label給予情況可分為Supervised Learning、Unsupervised Learning、Semi-supervised Learning 和 Reinforcement Learning。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;順道一提，這16堂課裡頭主要聚焦在探討Batch Supervised Learning with Concrete Features。&lt;/p&gt;
&lt;h3&gt;後話&lt;/h3&gt;
&lt;p&gt;這篇文章帶大家初探了一眼機器學習，介紹了機器學習的架構和種類，以及它的使用時機，還有介紹了整門課非常重要的二元分類問題。但是講這麼多，機器學習真的可能嗎? 那如果可以做到，會需要哪一些要素呢? 這就必須深入理論之中，才能找到答案，在下一篇文章裡，我將介紹這門課的第二個部分：Why Can Machines Learn? &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#0B5345 ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="AI.ML"></category><category term="機器學習基石"></category></entry></feed>