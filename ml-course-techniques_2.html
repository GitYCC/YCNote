<!DOCTYPE html>
<html lang="zh">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="本篇內容涵蓋Hard-Margin Support Vector Machine (SVM)、Kernel Function、Kernel Hard-Margin SVM、Soft-Margin SVM、Kernel Soft-Margin SVM、拉格朗日乘子法（Lagrange Multiplier）、Lagrangian Dual Problem。...">
        <meta name="keywords" content="機器學習技法">
        <link rel="icon" href="./static/img/favicon.png">

        <title>機器學習技法 學習筆記 (2)：Support Vector Machine (SVM) - YC Note</title>

        <!-- Stylesheets -->
        <link href="./theme/css/all.min.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script type="text/x-mathjax-config"> 
            MathJax.Hub.Config({ 
                "HTML-CSS": { scale: 80, linebreaks: { automatic: true } }, 
                SVG: { linebreaks: { automatic:true } }, 
                displayAlign: "left" });
        </script>




    </head>

    <body>

        <!-- Header -->
    <div class="header-container" style="background: linear-gradient(rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0.2)), url('./images/ai_front_board.jpg'); background-position: center; background-size: cover;">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="./"><img class="logo" src="./static/img/favicon.png" alt="logo">YC Note</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="./category/coding.html">Coding</a>
                                <a href="./category/aiml.html">AI.ML</a>
                                <a href="./category/reading.html">Reading</a>
                                <a href="./category/recording.html">Recording</a>
                                <a href="./about-me.html">About Me</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">機器學習技法 學習筆記 (2)：Support Vector Machine (SVM)</h1>
                      <p class="header-date">By <a href="./author/yc-chen.html">YC Chen</a>, 2017 / 2月 20, in category <a href="./category/aiml.html">AI.ML</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="./tag/ji-qi-xue-xi-ji-fa.html">機器學習技法</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <blockquote>
<p>本篇內容涵蓋Hard-Margin Support Vector Machine (SVM)、Kernel Function、Kernel Hard-Margin SVM、Soft-Margin SVM、Kernel Soft-Margin SVM、拉格朗日乘子法（Lagrange Multiplier）、Lagrangian Dual Problem。</p>
</blockquote>
<p>在<a href="http://www.ycc.idv.tw/ml-course-techniques_1.html">上一篇文章</a>當中，我們掃過了《機器學習技法》 將會包含的內容，今天我們正式來看SVM。</p>
<p>如果我想要使用無窮次高次方的非線性轉換加入我的Model，可以做到嗎？上一篇，我告訴大家，只要使用Dual Transformation加上Kernel Function等數學技巧就可以做到，我們今天就來看一下這是怎麼一回事。</p>
<p>本篇文章分為兩個部分，第一部分我盡量不牽扯太多數學計算，而將數學證明放在第二個部分，數學證明的部分非常複雜，但我並不打算把它們忽略掉，因為這些數學計算是相當重要的，它所帶來的方法和概念是可以重複使用的，也有助於你了解和創造其他演算法，所以有心想要成為專家的你請耐心的把後半段的數學看完。</p>
<p><br/></p>
<h3>Hard-Margin Support Vector Machine (SVM)</h3>
<p><img alt="Hard-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.001.jpeg"></p>
<p>回到我們最熟悉的二元分類問題，如果問題的答案是線性可分的話，我們可以找到一條直線把兩類Data給切開來，而在以前PLA的方法，切在哪裡其實是沒辦法決定的，PLA只能幫你找到可以分開兩類的一刀，但不能幫你把這刀切的更好。</p>
<p><strong>我們希望這個切開兩類的邊界可以離兩類Data越遠越好，讓邊界到Data有一個較大的空白區，這就是Hard-Margin SVM做的事</strong>。</p>
<p>我們先來看一下如何計算切平面到任意Data的距離，首先我先假設切平面的方程式為</p>
<p>W<sup>T</sup>X+b = 0 (切平面)</p>
<p>回想一下高中數學，這個平面的法向量是W，垂直於平面，所以垂直於平面的單位法向量是 W/|W|，今天如果我有一點Data Point落在X，另外在平面上任意再找一點X<sub>0</sub>，從X<sub>0</sub>到X的向量表示為X-X<sub>0</sub>，這個向量如果投影到單位法向量上，這個向量的大小正是Data Point到平面的最短距離，表示成</p>
<p>d = |W・(X - X<sub>0</sub>)| / |W|</p>
<p>X<sub>0</sub>符合切平面的方程式W<sup>T</sup>X<sub>0</sub>+b = 0代入，得</p>
<p>d = |W・X + b| / |W|</p>
<p>所以假如我有一群線性可分的二元分類Data，這個切平面我希望可以離兩類Data越遠越好，所以我會有一段全部都沒有Data的空白區，這邊假設這個空白區的邊界為</p>
<p>W<sup>T</sup>X+b = ±1</p>
<p>這個假設是可以做到的，因為我們可以以比例去調整W和b來達到縮放的效果，而不會影響切平面W<sup>T</sup>X+b = 0 。從上面的距離公式，我們知道在這個假設之下，空白區邊界距離切平面為</p>
<p>margin = 1 / |W|</p>
<p>而剛好落在這空白區邊界的Data會符合以下方程式</p>
<p><strong>y<sub>n</sub>×(W<sup>T</sup>X<sub>n</sub>+b) = 1 (Support Vector)</strong></p>
<p>y<sub>n</sub>的正負剛好和(W<sup>T</sup>X<sub>n</sub>+b)相抵消，<strong>這些落在空白區邊界的Data被稱為Support Vector，就字面上的意義就像是空白區由這一些數據給「撐」起來，而切平面只由這些Support Vector的數據點所決定，和其他的數據點無關</strong>。</p>
<p>如果考慮所有Data的話，應該要滿足</p>
<p>y<sub>n</sub>×(W<sup>T</sup>X<sub>n</sub>+b) ≥ 1 (All Data)</p>
<p><strong>綜合上述，Hard-Margin SVM的目標就是，在符合y<sub>n</sub>×(W<sup>T</sup>X<sub>n</sub>+b) ≥ 1 , n=1~N的條件下，求Margin (1 / |W|)最大的情形，也可以等價於求 (W<sup>T</sup>W/2) 最小的情形，這個問題有辦法使用QP Solver來求解，詳見<a href="https://en.wikipedia.org/wiki/Quadratic_programming">這裡</a>，我就不多加介紹這個數學工具。</strong></p>
<p><br/></p>
<h3>Kernel Function</h3>
<p>Kernel Function是最終可以讓我們有無限多次方特徵的數學工具，但這個工具非常容易理解。</p>
<p>假設考慮一個非線性轉換，將X空間轉換到Z空間，那如果我需要計算轉換過的兩個新Features相乘Z<sub>n</sub>(X<sub>n</sub>)×Z<sub>m</sub>(X<sub>m</sub>)，我有辦法<strong>不需要先做特徵轉換再相乘</strong>，而是直接使用原有的Features X<sub>n</sub>和X<sub>m</sub>求出Z<sub>n</sub>(X<sub>n</sub>)×Z<sub>m</sub>(X<sub>m</sub>)的最後結果？這種情形數學可以表示成K(X<sub>n</sub>,X<sub>m</sub>)=Z<sub>n</sub>(X<sub>n</sub>)×Z<sub>m</sub>(X<sub>m</sub>)，這個函式就叫Kernel Function。</p>
<p><strong>如果有了Kernel Function這樣的數學工具，就可以簡化和優化因為「特徵轉換」所帶來的複雜計算。</strong></p>
<p>我列出以下幾種Kernel Function：</p>
<ul>
<li><strong>Polynomial Kernel：K<sub>Q</sub>(X<sub>n</sub>,X<sub>m</sub>)=(ζ+γ X<sub>n</sub><sup>T</sup>X<sub>m</sub>)<sup>Q</sup>等價於 「Q次方非線性轉換後的兩個新特徵相乘」。</strong></li>
<li><strong>Guassian Kernel：K(X<sub>n</sub>,X<sub>m</sub>)=exp(-γ|X<sub>n</sub>-X<sub>m</sub>|<sup>2</sup>)等價於 「無窮次方非線性轉換後的兩個新特徵相乘」。</strong></li>
</ul>
<p>因此有了Guassian Kernel的幫忙，我們完全不需要管特徵轉換有多複雜，我們可以直接使用原有的Features 來計算「無窮次方的非線性轉換」。</p>
<p><strong>最後給予Kernel Function一個物理解釋，Kernel Function說穿了就是兩個向量轉換到Z空間後的「內積」，「內積」可以約略想成是「相似程度」，當兩個向量同向，內積是正的，相似度高，但當兩個向量反向，內積是負的，相似度極低，所以你會發現Guassian Kernel在X<sub>n</sub>=X<sub>m</sub>會出現最大值，因為代表這兩個位置相似度極高。</strong></p>
<p><br/></p>
<h3>Kernel Hard-Margin SVM</h3>
<p><img alt="Kernel Hard-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.002.jpeg"></p>
<p>那我們如何使用Kernel Function來使得Hard-Margin SVM更厲害呢？我們必須額外引入另外的數學工具，包括：Lagrange Multiplier和Lagrange Dual Problem，才有辦法把Kernel Function用上，不過這部份的數學有一些複雜，我將這部份的證明放在後面的附錄上，這邊就直接從結果講起。</p>
<p>Kernel Hard-Margin SVM的公式是，在α<sub>n</sub>  ≥ 0; 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0的限制條件下，求解α<sub>n</sub></p>
<p>使得 [(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)-𝚺<sub>n</sub> α<sub>n</sub>]為最小值，</p>
<p>其中K(X<sub>n</sub>,X<sub>m</sub>)就是Kernel Function，由你的特徵轉換方式來決定，這個問題一樣可以使用QP Solver來求解。</p>
<p>當我們已經有了每筆數據點的α<sub>n</sub>了，接下來可以利用α<sub>n</sub>求出切平面的W和b，在那之前來看一下α<sub>n</sub>的意義，<strong>α<sub>n</sub>可以看作是某個數據點對切平面的貢獻程度，α<sub>n</sub>=0的這些數據點為非Support Vector，而α<sub>n</sub>&gt;0的這些數據點是Support Vector，所以對切平面有貢獻的只有Support Vector而已</strong>，這和剛剛的結論相同。因此，W和b可由Support Vector決定，</p>
<p><strong>W = 𝚺<sub>n=sv</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub></strong></p>
<p><strong>b=y<sub>sv</sub>-𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>K(X<sub>n</sub>,X<sub>sv</sub>)</strong></p>
<p>最後提一個非常重要的概念，是什麼原因讓我們不需要管特徵轉換的複雜度？以往我們的作法是這樣的，我們有每筆Data的Features，接下來對每筆Data做特徵轉換，然後在用特徵轉換後的新Features去Train線性模型，這麼一來如果特徵轉換的次方非常高的話，計算的複雜度就會全落在特徵轉換上。<strong>所以我們巧妙的使用數學工具，讓我們可以單單使用Data的Labels來做優化，而將複雜的特徵轉換利用Kernel Function的方式「嵌入」到優化的過程裡頭，此時計算量就只與Data數量有關，所以可以完全不管特徵轉換所帶來的複雜度</strong>。</p>
<p><br/></p>
<h3>Kernel Hard-Margin SVM: 無窮次方的特徵轉換效果如何?</h3>
<p>終於我們可以使用無窮次方的特徵轉換了，只要使用Kernel Hard-Margin SVM搭配上Guassian Kernel：K(X<sub>n</sub>,X<sub>m</sub>)=exp(-γ|X<sub>n</sub>-X<sub>m</sub>|<sup>2</sup>)就可以辦到，下圖是模擬的結果，是不是看起來很強大，隨著γ的不同會有不一樣的切分方法，<strong>你會發現γ越大時看起來的結果越接近Overfitting，所以必須小心挑選γ的大小。</strong></p>
<p><img alt="Guassian Kernel in Hard-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_01.png"></p>
<p>from: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/203_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/203_handout.pdf</a></p>
<p><br/></p>
<h3>Soft-Margin SVM</h3>
<p><img alt="Soft-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.003.jpeg"></p>
<p>剛剛Hard-Margin SVM會很容易Overfitting的原因在於它的機制無法<strong>容忍雜訊</strong>，所以接下來要講的Soft-Margin SVM可以容忍部份的Data違反規則，讓它們可以超出空白區的邊界。</p>
<p>見上圖，可以發現我們稍微修改了Hard-Margin SVM，加入了參數ξ<sub>n</sub>，ξ<sub>n</sub>代表錯誤的Data離空白區邊界有多遠，而我們將ξ<sub>n</sub>的總和加進去Cost裡面，在優化的過程中將使違反的狀況不會太多和離邊界太遠，<strong>而參數C負責控制ξ<sub>n</sub>總和的影響程度，如果C很大，代表不大能容忍雜訊；如果C很小，則代表對雜訊的容忍很寬鬆</strong>。</p>
<p><strong>因此我們現在有兩種Support Vector，一種是剛好落在空白區邊界的，稱為Free Support Vector；另外一種是違反規則並超出空白區的，稱為Bounded Support Vector，切平面一樣是由這些Support Vector所決定。</strong></p>
<p><br/></p>
<h3>Kernel Soft-Margin SVM</h3>
<p><img alt="Kernel Soft-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.004.jpeg"></p>
<p>接下來同樣的對Soft-Margin SVM做數學上Lagrange Multiplier和Lagrange Dual Problem的轉換，再將Kernel Function用上，一樣的，我將這部份的證明放在後面的附錄上，這邊就直接從結果講起。</p>
<p>Kernel Soft-Margin SVM的公式是，在0 ≤ <strong>α<sub>n</sub> ≤ C</strong>; 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0的限制條件下，求解α<sub>n</sub></p>
<p>使得 [(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)-𝚺<sub>n</sub> α<sub>n</sub>]為最小值，</p>
<p>你會發現和Kernel Hard-Margin SVM唯一只差在α<sub>n</sub>被C所限制。</p>
<p>當我們已經有了每筆數據點的α<sub>n</sub>了，接下來可以利用α<sub>n</sub>求出切平面的W和b，α<sub>n</sub>一樣的可以看作是某個數據點對切平面的貢獻程度，α<sub>n</sub>=0的這些數據點為非Support Vector，而α<sub>n</sub>&gt;0的這些數據點是Support Vector，可以進一步細分，α<sub>n</sub> &lt; C為Free Support Vector，而α<sub>n</sub>＝C為Bounded Support Vector。相同的，W和b可由Support Vector (Free Support Vector和Bounded Support Vector)決定，跟Kernel Hard-Margin SVM公式一模一樣</p>
<p><strong>W = 𝚺<sub>n=sv</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub></strong></p>
<p><strong>b=y<sub>sv</sub>-𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>K(X<sub>n</sub>,X<sub>sv</sub>)</strong></p>
<p><br/></p>
<h3>Kernel Soft-Margin SVM: 容忍雜訊的無窮次方特徵轉換</h3>
<p><img alt="Guassian Kernel in Soft-Margin SVM" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_02.png"></p>
<p>from: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/204_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/204_handout.pdf</a></p>
<p>來看看Kernel Soft-Margin SVM搭配上Guassian Kernel的效果如何，上圖是模擬的結果，我們會發現有部分Data違反分類規則，所以Soft-Margin SVM確實可以容忍雜訊，而且C越小，容忍雜訊的能力越強，所以要特別注意C的選取，如果沒有選好還是可能造成Overfitting的。</p>
<p><br/></p>
<h3>結語</h3>
<p>在這一篇當中，我們介紹了Hard-Margin SVM和Soft-Margin SVM，並且成功的利用數學工具將問題轉換成，可以單單使用Data的Labels來做優化，而將複雜的特徵轉換利用Kernel Function的方式「嵌入」到優化的過程裡頭，此時計算量就只與Data數量有關，所以可以完全不管特徵轉換所帶來的複雜度，因此利用Guassian Kernel就可以做到「無窮多次的特徵轉換」了。最後再次強調數學的部分非常重要，它提供的方法和概念是可以重複使用的，而這部份的數學是少不了的，所以有興趣的可以繼續往下看下去。</p>
<p><br/><br/></p>
<h3>[進階] 拉格朗日乘子法（Lagrange Multiplier）</h3>
<p>如果是物理系學生修過古典力學，應該對這個數學工具不陌生。<strong>Lagrange Multiplier是用在有限制條件之下的求極值問題</strong>，步驟如下：</p>
<ol>
<li>問題：在限制 g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, i=1~k  之下，求 f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) 的極值</li>
<li>假設Lagrange Function：   L(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>,λ<sub>i</sub>) = f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) + 𝚺<sub>i</sub> λ<sub>i</sub> × g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>)</li>
<li>聯立方程式求解：</li>
<li>找L的極值：∇L = 0  [Stationarity Condition]</li>
<li>g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, i=1~k  [Primal Feasibility Condition]</li>
<li>求解以上聯立方程式得到最佳解 x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub></li>
</ol>
<p>上面的聯立方程式不難理解，Primal Feasibility Condition就是我們的限制式，然後Stationarity Condition就是求極值的方法，非常直觀，滿足上面的式子我們就可以在限制上面找極值。</p>
<p><br/></p>
<p>上面是一般的Lagrange Multiplier，只有考慮到限制式是等式的情形，假如限制條件是不等式呢？我們來看一下加強版的Lagrange Multiplier：</p>
<ol>
<li>問題：在限制 g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, i=1~k 且  h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) ≤ 0, j=1~r 之下，求 f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) 的極值</li>
<li>假設Lagrange Function：   L(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>, λ<sub>i</sub>,μ<sub>j</sub>) = f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) + 𝚺<sub>i</sub> λ<sub>i</sub> × g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) + 𝚺<sub>j</sub> μ<sub>j</sub> × h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>)</li>
<li>聯立方程式求解：</li>
<li><strong>找L的極值：∇L = 0  [Stationarity Condition]</strong></li>
<li><strong>g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, i=1~k 且 h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) ≤ 0, j=1~r  [Primal Feasibility Condition]</strong></li>
<li><strong>μ<sub>j</sub>  × h<sub>j</sub> (x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, j=1~r  [Complementary Slackness Condition]</strong></li>
<li><strong>求L的最小值時 μ<sub>j</sub> ≥ 0, j=1~r；求L的最大值時 μ<sub>j</sub> ≤ 0, j=1~r [Dual Feasibility Condition]</strong></li>
<li><strong>以上的條件包括Stationarity、Primal Feasibility、Complementary Slackness、Dual Feasibility通稱 KKT (Karush-Kuhn-Tucker) Conditions</strong></li>
</ol>
<p>加強版的Lagrange Multiplier和一般版的一樣有Stationarity Condition和Primal Feasibility Condition。唯一增加的是Complementary Slackness Condition和Dual Feasibility Condition。</p>
<p>先來講一下Complementary Slackness Condition怎麼來的，我們來考慮不等式條件h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) ≤ 0，會有兩個情形發生，一個是壓到邊界，也就是h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0，這個時候問題就回到一般版的Lagrange Multiplier，此時μ<sub>j</sub>和λ<sub>i</sub>效果是一樣的，μ<sub>j</sub>可以是任意值；另外一種情況是我沒壓到邊界，也就是h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) &lt; 0，這個時候我可以把這個限制看作不存，最簡易的方法就是令μ<sub>j</sub>=0，他在L(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>, λ<sub>i</sub>,μ<sub>j</sub>) 中就不參與作用了。<strong>所以綜合壓到邊界和不壓到兩種情況，我們可以寫出一個有開關效果的方程式 μ<sub>j</sub> × h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0，這就是Complementary Slackness Condition。</strong></p>
<p>另外一個是Dual Feasibility Condition，這個限制一樣是在不等式條件才會發生，μ<sub>j</sub>的正負號取決於L是要求最大還是求最小值，稍微解釋一下，找極值我們用∇L = 0這個式子來求，代入Lagrange Function後得∇L = ∇f +𝚺<sub>i</sub>λ<sub>i</sub>×∇g<sub>i</sub>+𝚺<sub>j</sub>μ<sub>j</sub>×∇h<sub>j</sub>=0，先定性來看，假設不計∇g<sub>i</sub>的影響，當最後解落在h ≤ 0的邊界上時∇f＝- μ×∇h，因為h ≤ 0的關係，所以∇h是朝向可行區的外面，如果今天是求f的極小值，那們∇f應當朝著可行區才合理，如果不是的話則可行區內部有更小更佳的解，所以求極小值時μ ≥ 0；如果是求f的極大值，那∇f應當朝著可行區的外面，所以μ ≤ 0，這個條件待會會用在對偶問題上面。</p>
<p><br/></p>
<p>其實我們之前在《機器學習基石》裡的Regularization有偷用了Lagrange Multiplier的產物。</p>
<p>Regularization將W的長度限制在一個範圍，表示成</p>
<p>|W|<sup>2</sup> ≤ C</p>
<p>在這個條件下我們要找E<sub>in</sub>的極小值，使用加強版的Lagrange Multiplier：</p>
<ol>
<li>問題：在限制  |W|<sup>2</sup> - C ≤ 0 之下，求 E<sub>in</sub> 的極小值</li>
<li>假設Lagrange Function：   L = E<sub>in</sub> + μ × ( |W|<sup>2</sup> - C)</li>
<li>聯立方程式求解：</li>
<li>𝞉L / 𝞉W = 𝞉E<sub>in</sub> / 𝞉W + 2μ × |W| = 0  [Stationarity Condition]</li>
<li>|W|<sup>2</sup> - C ≤ 0  [Primal Feasibility Condition]</li>
<li>μ × ( C - |W|<sup>2</sup> ) = 0  [Complementary Slackness Condition]</li>
</ol>
<p>Stationarity Condition的結果就是Regularization的結果了，可以<a href="http://www.ycc.idv.tw/ml-course-foundations_4.html">回去參照一下</a>。</p>
<p><br/></p>
<h3>[進階] Lagrangian Dual Problem</h3>
<p>接下來來講對偶問題，這個部分很難，我也是反覆在網路上看了很多篇介紹才弄懂，推薦大家看<a href="http://www.eng.newcastle.edu.au/eecs/cdsc/books/cce/Slides/Duality.pdf">這一篇</a>，這篇介紹的很清楚，應該會對大家理解Lagrangian Dual有幫助。</p>
<p>來考慮一下待會會用到的求極小值問題，</p>
<blockquote>
<p>在限制 g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, i=1~k 且  h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) ≤ 0, j=1~r 之下，求 f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) 的極小值。</p>
</blockquote>
<p>如果我們利用剛剛的解法，稱之為Lagrangian Primal Problem。</p>
<p><strong>而這個問題可以等效轉換成Lagrangian Dual Problem，利用以下關係式</strong></p>
<p><strong>Minimum Problem ≡ min. L  ≡ min. [max.<sub>μ ≥ 0</sub> L] ≥ max.<sub>μ ≥ 0</sub> [min. L(μ)]</strong></p>
<p>我們在將原本min. L 換成min. [max.<sub>μ ≥ 0</sub> L] 是不影響結果的，因為我們剛剛分析過了在求最小值時μ ≥ 0是合理的，相反的如果μ &lt; 0，則求max.<sub>μ ≥ 0</sub> L時會產生無限大的結果，接下來就是交換min.和max.的部分，數學上可以證明min. [max.<sub>μ ≥ 0</sub> L] ≥ max.<sub>μ ≥ 0</sub> [min. L(μ)]這樣的關係，我們就稱左式轉到右式為Dual轉換。</p>
<p>而上面式子右側的求法，我們可以先求出Θ(λ<sub>i</sub>,μ<sub>j</sub>) = given λ<sub>i</sub>,μ<sub>j</sub> to find min. L(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>, λ<sub>i</sub>,μ<sub>j</sub>) ，作法是使用∇L = 0所產生符合極值的參數代入L(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>, λ<sub>i</sub>,μ<sub>j</sub>)，換成以λ<sub>i</sub>,μ<sub>j</sub>表示的Θ(λ<sub>i</sub>,μ<sub>j</sub>)。然後，再求Θ(λ<sub>i</sub>,μ<sub>j</sub>)的最大值，就可以了。</p>
<p><strong>經過Dual轉換後，我們將原本在x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>的問題轉換到λ<sub>i</sub>,μ<sub>j</sub>的空間上。</strong></p>
<p>這個轉換我們可以使用下面的圖來解釋，</p>
<p><img alt="Lagrangian Dual Geometric Interpretation" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.005.jpeg"></p>
<p>我們先不管g(x)的部分只看f(x)和h(x)的部分，假設所有的Data x映射到f(x)和h(x)會產生一塊區域G。</p>
<p>在Primal Problem中我們可以很容易的找出h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) ≤ 0的限制之下f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) 的最小值，見上圖左側。</p>
<p>見上圖中間，Dual Problem採取另外一個方法，它先去找</p>
<p>Θ(μ) = given μ to find min. L(x,μ)，其中 L(x,μ) = f(x)+μh(x)。</p>
<p>f(x)+μh(x)=α在圖中的平面上是一條直線，而f(x)+μh(x)的值也就是α也正好是它的「截距」，所以在給定μ後要最小化f(x)+μh(x)的方法，就等效於固定直線斜率最小化截距，所以最後這個直線就必須要切於G才能使得截距最小，所以我們得到一條切於G且斜率(-μ)的直線， 因此我們就順利的得到Θ(μ)的關係式了，接下來我要找出Θ(μ)的最大值，所以就必須往上推，這個時候你就發現答案和前面Primal Problem答案一模一樣，這種最佳化答案相同的情況稱為「Strong Duality」，而最佳化答案不相同的情況就叫做「Weak Duality」，見上圖右側，在這種G的形狀下，就會產生最佳化答案不相同的情況。</p>
<p><br/></p>
<h3>[進階] Hard-Margin SVM Dual + Kernel Function = Kernel Hard-Margin SVM</h3>
<p>那我們現在可以正式的把Lagrangian Dual的東西放到Hard-Margin SVM上面。</p>
<p>回想一下Hard-Margin SVM的問題是：</p>
<blockquote>
<p>在y<sub>n</sub>×(W<sup>T</sup>X<sub>n</sub>+b) ≥ 1 , n=1~N的條件下，求(W<sup>T</sup>W/2) 最小的情形。</p>
</blockquote>
<p>那如果加上非線性轉換，從X空間轉到Z空間，則問題變成</p>
<blockquote>
<p>在y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b) ≥ 1 , n=1~N的條件下，求(W<sup>T</sup>W/2) 最小的情形。</p>
</blockquote>
<p>所以我們可以使用Lagrangian Multiplier來解決問題，依以下步驟：</p>
<ol>
<li>假設Lagrange Function：   L(W,b,α) = (W<sup>T</sup>W/2) +  𝚺<sub>n</sub> α<sub>n</sub> × [1-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)]</li>
<li>考慮Primal Feasibility、Complementary Slackness、Dual Feasibility的限制</li>
<li>Primal Feasibility Condition：1-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b) ≤ 0 [式1-1]</li>
<li>Complementary Slackness Condition：α<sub>n</sub>  × [1-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)] = 0 [式1-2]</li>
<li>Dual Feasibility Condition：α<sub>n</sub>  ≥ 0 [式1-3]</li>
<li>先求出Θ(α) = given α to find min. L(W,b,α)</li>
<li>𝞉L / 𝞉b = - 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0 [式1-4]</li>
<li>𝞉L / 𝞉W<sub>n</sub> =  |W|- 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub> = 0，y<sub>n</sub>Z<sub>n</sub>應該和W同向，所以
     W = 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub> [式1-5]</li>
<li>因此L(W,b,α)只要滿足[式1-4]和[式1-5]就代表是極小值了</li>
<li>所以[式1-4]和[式1-5]代入得Θ(α,β) = (-1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>Z<sub>n</sub>Z<sub>m</sub>+𝚺<sub>n</sub> α<sub>n</sub></li>
<li>求Θ(α)極大值</li>
<li>max.[Θ(α)]＝min.[-Θ(α)]=min.[(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>Z<sub>n</sub>Z<sub>m</sub>-𝚺<sub>n</sub> α<sub>n</sub>] —[式1-6]</li>
<li>綜合上述[式1-3]、[式1-4]、[式1-6]並改寫成Kernel的形式得，min. [(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)-𝚺<sub>n</sub> α<sub>n</sub>], s.t. α<sub>n</sub> ≥ 0 ;  𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0，使用QP Solver可以求出 α<sub>n</sub>。</li>
<li>可以用α<sub>n</sub>來求W和b</li>
<li>α<sub>n</sub>涵義：觀察[式1-2]可得 (1) α<sub>n</sub> = 0 為Non-Support Vector； (2) α<sub>n</sub> &gt; 0 代表y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)=1，為Support Vector。</li>
<li>由[式1-5]得，W = 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub>，從式子中你會發現對W有貢獻的只有Support Vector (α<sub>n</sub>&gt;0)。</li>
<li>假設在某個Support Vector(α<sub>n</sub>&gt;0)上，由[式1-2]可推得，b=y<sub>sv</sub>-𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>K(X<sub>n</sub>,X<sub>sv</sub>)  (at Support Vector)。</li>
</ol>
<p><br/></p>
<h3>[進階] Soft-Margin SVM Dual + Kernel Function = Kernel Soft-Margin SVM</h3>
<p>考慮Soft-Margin SVM和特徵轉換：</p>
<blockquote>
<p>在y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b) ≥ 1-ξ<sub>n</sub>且ξ<sub>n</sub> ≥ 0, n=1~N的條件下，求(W<sup>T</sup>W/2) + C 𝚺<sub>n</sub> ξ<sub>n</sub>最小的情形。</p>
</blockquote>
<p>所以我們可以使用Lagrangian Dual Problem來解決問題，依以下步驟：</p>
<ol>
<li>假設Lagrange Function：   L(W,b,ξ,α,β) = (W<sup>T</sup>W/2) + C 𝚺<sub>n</sub> ξ<sub>n</sub> +  𝚺<sub>n</sub> α<sub>n</sub> × [1-ξ<sub>n</sub>-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)] + 𝚺<sub>n</sub> β<sub>n</sub> × [-ξ<sub>n</sub>]</li>
<li>考慮Primal Feasibility、Complementary Slackness、Dual Feasibility的限制</li>
<li>Primal Feasibility Condition：1-ξ<sub>n</sub>-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b) ≤ 0 [式2-1]；-ξ<sub>n</sub> ≤ 0 [式2-2]</li>
<li>Complementary Slackness Condition：α<sub>n</sub>  × [1-ξ<sub>n</sub>-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)] = 0 [式2-3]；β<sub>n</sub> × [-ξ<sub>n</sub>] = 0 [式2-4]</li>
<li>Dual Feasibility Condition：α<sub>n</sub>  ≥ 0 [式2-5]；β<sub>n</sub>  ≥ 0 [式2-6]</li>
<li>先求出Θ(α,β) = given α,β to find min. L(W,b,ξ,α,β)</li>
<li>𝞉L / 𝞉b = - 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0 [式2-7]</li>
<li>𝞉L / 𝞉W<sub>n</sub> =  |W|- 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub> = 0，y<sub>n</sub>Z<sub>n</sub>應該和W同向，所以
     W = 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub> [式2-8]</li>
<li>𝞉L / 𝞉ξ<sub>n</sub> = C - α<sub>n</sub> - β<sub>n</sub> = 0 [式2-9]</li>
<li>因此L(W,b,ξ,α,β)只要滿足[式2-7]、[式2-8]和[式2-9]就代表是極小值了</li>
<li>所以[式2-7]、[式2-8]和[式2-9]代入得Θ(α,β) = (-1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>Z<sub>n</sub>Z<sub>m</sub>+𝚺<sub>n</sub> α<sub>n</sub></li>
<li>求Θ(α,β)極大值</li>
<li>max.[Θ(α,β)]＝min.[-Θ(α,β)]=min.[(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>Z<sub>n</sub>Z<sub>m</sub>-𝚺<sub>n</sub> α<sub>n</sub>] —[式2-10]</li>
<li>綜合上述[式2-5]、[式2-6]、[式2-9]、[式2-10]並改寫成Kernel的形式得，min. [(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)-𝚺<sub>n</sub> α<sub>n</sub>], s.t. 0 ≤ α<sub>n</sub> ≤ C;  𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0，使用QP Solver可以求出 α<sub>n</sub>。</li>
<li>可以用α<sub>n</sub>來求W和b</li>
<li>α<sub>n</sub>涵義：觀察[式2-3]和[式2-4]可得 (1) α<sub>n</sub> = 0 為Non-Support Vector； (2) 0 &lt; α<sub>n</sub> &lt; C 代表y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)=1，為Free Support Vector；(3) α<sub>n</sub> = C 代表y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)=1-ξ<sub>n</sub>，為Bounded Support Vector。</li>
<li>由[式2-8]得，W = 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub>，從式子中你會發現對W有貢獻的只有Support Vector (α<sub>n</sub>&gt;0)。</li>
<li>假設在某個Support Vector(α<sub>n</sub>&gt;0且β<sub>n</sub>&gt;0)上，由[式2-3]和[式2-4]可推得，b=y<sub>sv</sub>-𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>K(X<sub>n</sub>,X<sub>sv</sub>)  (at Support Vector)。</li>
</ol>


        <br/><br/>

<div id="disqus_thread"></div>
<script type="text/javascript">
/* <![CDATA[ */

    var disqus_shortname = 'ycnote-1';
    var disqus_identifier = "ml-course-techniques_2.html";

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
/* ]]> */
</script>
<noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



        
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="./archives.html">Archives</a></li>
                            <li><a href="./tags.html">Tags</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Contact Me</div>
                        <ul class="list-unstyled">
                            <li><a href="./about-me.html" target="_blank">About Me</a></li>
                            <li><a href="https://github.com/GitYCC" target="_blank">Github</a></li>
                            <li><a href="mailto:ycc.tw.email@gmail.com" target="_blank">Email</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; YC Note 2018</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>