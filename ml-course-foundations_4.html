<!DOCTYPE html>
<html lang="zh">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="前言 在上一回中，我們已經了解了機器學習基本的操作該怎麼做。而這一篇中，我們來看機器可以怎麼學得更好? 基本上有三招：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），我們來看看。 Feature Transformation（特徵轉換）...">
        <meta name="keywords" content="機器學習基石">
        <link rel="icon" href="./static/img/favicon.png">

        <title>機器學習基石 學習筆記 (4)：機器可以怎麼學得更好? - YC Note</title>

        <!-- Stylesheets -->
        <link href="./theme/css/all.min.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script type="text/x-mathjax-config"> 
            MathJax.Hub.Config({ 
                "HTML-CSS": { scale: 90, linebreaks: { automatic: true } }, 
                SVG: { linebreaks: { automatic:true } }, 
                displayAlign: "left" });
        </script>




    </head>

    <body>

        <!-- Header -->
    <div class="header-container" style="background: linear-gradient(rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0.2)), url('./images/ai_front_board.jpg'); background-position: center; background-size: cover;">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="./"><img class="logo" src="./static/img/favicon.png" alt="logo">YC Note</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="./category/coding.html">Coding</a>
                                <a href="./category/aiml.html">AI.ML</a>
                                <a href="./category/reading.html">Reading</a>
                                <a href="./category/recording.html">Recording</a>
                                <a href="./about-me.html">About Me</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">機器學習基石 學習筆記 (4)：機器可以怎麼學得更好?</h1>
                      <p class="header-date">By <a href="./author/yc-chen.html">YC Chen</a>, 2016 / 9月 18, in category <a href="./category/aiml.html">AI.ML</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="./tag/ji-qi-xue-xi-ji-shi.html">機器學習基石</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <h3>前言</h3>
<p>在上一回中，我們已經了解了機器學習基本的操作該怎麼做。而這一篇中，我們來看<strong>機器可以怎麼學得更好?</strong> 基本上有三招：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），我們來看看。</p>
<p><br/></p>
<h3>Feature Transformation（特徵轉換）</h3>
<p><img alt="ML" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.013.jpeg"></p>
<p>在上一回當中我們講了很多的線性模型，大家有沒有懷疑說，數據呈現的方式一定可以用線性描述嗎？我的答案是通常線性描述會表現不錯，但不是絕對，<strong>那我們怎麼用非線性的方法來描述我們的數據，這邊提供一個方法叫做「非線性轉換」，或者又稱為「特徵轉換」（還記得變數x又可以稱為特徵Features）</strong>，聽起來有點困難齁～其實不會啦！</p>
<p>假設今天你的Data分布是圓圈狀的分布，顯而易見的你很難用一條線去區分他們，那我們應該怎麼做呢？假設今天有一個轉換可以把這個圓圈狀分布的空間轉換到另外一個空間，在這個新的空間可以做到線性可分，這樣的問題不就解決了嗎，我們會做線性可分的問題啊！</p>
<p>這個轉換就叫做「非線性轉換」，那這個轉換要怎麼得到呢？可以用人為定義，譬如你知道這個空間的分布狀況是圓圈分布，記作 </p>
<p>H(x<sub>1</sub>, x<sub>2</sub>) = sign(-A*x<sub>1</sub><sup>2</sup>-B*x<sub>2</sub><sup>2</sup>+C)</p>
<p>，那只要做一件事我就可以把它轉換成線性可描述的，令 z<sub>1</sub>=-x<sub>1</sub><sup>2</sup>; z<sub>2</sub>=-x<sub>2</sub><sup>2</sup>，所以問題就變成</p>
<p>H(z<sub>1</sub>, z<sub>2</sub>) = sign(A*z<sub>1</sub>+B*z<sub>2</sub>+C)</p>
<p>此時這個問題就變成一個線性問題啦！</p>
<p><strong>藉由人為觀察數據並給予適當的特徵轉換是特徵工程（Feature Engineering）中一件重要的事。</strong></p>
<p>但如果我們需要去人為定義這個「非線性轉換」，這就很弱啦！我們當然希望機器可以自行從Data中學習到這個轉換，作法是這樣的，我們先把變數x做個變化和擴充，讓它們互相的相乘創造出高次項，再把這些項等價的放到Linear Model裡，所以我們就用了線性的作法來做到Non-linear Model，而因為有權重W在非線性項前面的關係，所以機器會針對Data自行去調配非線性項，這效果就等同於機器自行學習到「非線性轉換」。</p>
<p><strong>機器自己學習特徵轉換的這個概念應該是現今ML最重要的概念之一，最近很夯的深度學習甚至不只做一次性的特徵轉換，而是做了多層的特徵轉換，而這些轉換都是機器自動從Data中學來的。</strong></p>
<p><strong>特徵轉換讓ML變得很強大，但要特別注意，因為我們增加了非線性項，所以等於是增加了模型的複雜度，這麼做的確可以壓低E<sub>in</sub>沒有錯，但也可能使得E<sub>in</sub> ≈ E<sub>out</sub>不再成立，也就是Overfitting，所以建議要逐步的增加非線性項，從低次方的項開始加起，避免Overfitting。</strong></p>
<p><br /></p>
<h3>Overfitting</h3>
<p>Overfitting是一個大怪獸，在學習怎麼對付牠之前，我們先來好好的了解牠！</p>
<p><img alt="Overfitting" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.000.03.png"></p>
<p>From: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf</a></p>
<p>上面這張圖用很簡單的方法說明了Overfitting是怎麼一回事，假設藍色的線是Target，也就是我們抽樣的母群體，因為雜訊的關係，抽樣出來的點可能會稍微偏離Target，而如果這個時候我們用二次式來描述這些抽樣出來的Data（上圖中的左側）會發現E<sub>in</sub>不能壓到0，所以這個時候可能有人想說加進去更高次項來試試看（上圖中的右側），此時會發現E<sub>in</sub>=0，所有數據都可以被完整描述了，但是你會發現Fit的曲線已經完全偏離了Target，反而是使用低次項還描述的比較好，所以結論是<strong>如果我們把「隨機雜訊」（Stochastic Noise）Fit進去Model裡面就會因此產生Overfitting</strong>。</p>
<p><img alt="Overfitting2" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.000.04.png"></p>
<p>From: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf</a></p>
<p>但可別以為沒有「隨機雜訊」鬧場就不會出現Overfitting，上圖假設一個沒有「隨機雜訊」的情形，但是Target Function的複雜度很高（上圖右側），當我們從中採樣一些Data來進行Fitting，如上圖左側，我們分別使用2次和10次來做Fitting，這個時候你會發現雖然2次和10次都和Target曲線差很遠，但是小次方的還是Fit的比較好一點，造成Overfitting的原因是因為當Target很複雜的情況下，如果採樣的數據不大，根本無法反應Target本身，所以就算使用了和Target一樣複雜的Model，也只是在瞎猜而已。<strong>這種因為Target本身的複雜度所帶來的雜訊，我們稱為「決定性雜訊」(Deterministic Noise)</strong>。</p>
<p><img alt="Noise" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.000.05.png"></p>
<p>From: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf</a></p>
<p>我們來看一下「隨機雜訊」（Stochastic Noise）和「決定性雜訊」（Deterministic Noise）怎麼造成Overfitting的，上圖中的兩張漸層圖表示的是Overfitting的程度，越接近紅色代表Overfitting越嚴重；反之，越接近藍色則Overfitting越輕微。左邊的漸層圖是考慮「隨機誤差」的影響，右邊的漸層圖則是考慮「決定性雜訊」的影響。從這兩張圖我們可以觀察出下面四點，</p>
<ol>
<li>Data數量N越少，越容易Overfitting</li>
<li>「隨機雜訊」越多，越容易Overfitting</li>
<li>「決定性雜訊」越多，越容易Overfitting</li>
<li>Model本身越複雜，越容易Overfitting</li>
</ol>
<p>那有什麼方法可以防止Overfitting嗎？有的，有一些之前提過，而有一些我接下來會講，我們來看一下：</p>
<ol>
<li><strong>從簡單的模型開始做起，從低次模型開始做起，在慢慢加入高次項</strong></li>
<li><strong>提升資料的正確性：Data Cleaning/Pruning（資料清洗）將錯誤的Data修正或刪除</strong></li>
<li><strong>Data Hinting（製造資料），使用合理的方法擴增原有的資料，例如：在圖形辨識問題中，可以用平移和旋轉來擴增出更多Data</strong></li>
<li><strong>Regularization（正規化）：限制權重W的大小以控制高次的影響。</strong>（接下來會詳述...）</li>
<li><strong>Validation（驗證）：將部分Data保留不進去Fitting，然後用這個Validation Data來檢驗Overfitting的程度。</strong>（接下來會詳述...）</li>
</ol>
<p><br /></p>
<h3>Regularization（正規化）</h3>
<p><img alt="regularation" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.014.jpeg"></p>
<p>剛剛我們提到了Overfitting所造成的影響很大一部分是因為Model複雜度所造成的，但是為了可以把E<sub>in</sub>給壓下去，我們又的確需要去增加高次項，所以依照建議需要從低次項開始慢慢的加，這樣感覺很麻煩啊！<strong>有沒有辦法讓機器自己去限制高次項的出現呢？有的，這就是Regularization（正規化）</strong>。</p>
<p>還記得剛剛在講「特徵轉換」時，有提到一點，ML有辦法自行學習「特徵轉換」的關鍵是因為高次項前面有一個可調控的權重，而機器會針對Data來調整權重大小，那其實就是等價於機器自己學習到了「特徵轉換」，同理可知，<strong>我們只要限制權重W的大小就等同於限制了機器無所忌憚的使用高次項</strong>。</p>
<p>經數學證明，<strong>限制權重W的大小可以等價於在E<sub>in</sub>上面加上「W大小的平方」乘上定值λ，λ越大代表W大小限制越緊；λ越小代表W大小限制越鬆</strong>，這也非常容易想像，訓練Model的方法是去降低E<sub>in</sub>，但是如果使用了大的W，就會使得E<sub>in</sub>增大，自然而然在訓練的過程中，機器會去尋找小一點的W，也就等同於限制了W的大小。</p>
<p>見上圖左側，我們修改了Gradient Descent讓它受到Regularization的限制。</p>
<p>而上圖左側下方，顯示了在λ增大的同時，限制W的大小會越來越緊，所以Fitting的結果從原本的Overfitting變成Underfitting。</p>
<p><strong>Underfitting所代表的是Model本身的複雜度不足以使得E<sub>in</sub>減小，如果你經過Validation（待會會講）後發現沒有Overfitting的現象，但是你的E<sub>in</sub>始終壓不下來，那就有可能是Underfitting，那你可以考慮增加Model複雜度或者放寬Regularization。</strong></p>
<p><strong>Regularizer的選擇常見的有兩種L2和L1，L2使用「W大小的平方」，L1則使用「W大小的絕對值」。</strong></p>
<p>當Linear Regression使用Regularization限制，統計上有一個名稱稱為Ridge Regression，你可以使用Gradient Descent來做，又或者使用解析解的方法。</p>
<p>最後提一個Regularization的細節，你會發現因為高次項是彼此兩兩相乘的結果，所以項目的個數會隨著次方增加而增加，這麼一來在做Regularization時可能會過度懲罰高次項，因此，我們可以將Feature轉換成Legendre Polynomials來避免這個問題。</p>
<p><br /></p>
<h3>Validation（驗證）</h3>
<p><img alt="validation" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.015.jpeg"></p>
<p>講了這麼多Overfitting，但到底要怎麼去量化Overfitting呢？Overfitting就是E<sub>in</sub> ≈ E<sub>out</sub>不成立，但是E<sub>out</sub>我們不會知道啊！因為我們不會知道Target Function是什麼，那該怎麼得到量化Overfitting的值呢？</p>
<p><strong>有一個方法叫做Validation可以拿來量化Overfitting的值，這個方法是先將採樣的數據做分離，一部分將會拿來做Model Fitting（Model Training），另外一部分保留起來評估訓練完畢的Model，因為保留的這一部分源自於母群體，而且又沒有被Model給看過，所以它可以很客觀的反應出E<sub>out</sub>的大小。</strong></p>
<p>我們的Model和Algorithm從以前講到現在已經是越來越複雜了，來複習一下Model和Algorithm受哪些參數影響，Algorithm的選擇就有很多了，包括：PLA、Linear Regression、Logistic Regression；Learning Rate η也需要去選擇大小決定學習速率；Feature Transformation中Feature的決定和次方大小的決定；Regularization也有L2、L1 Regularizer的選擇；還有Regularization的λ值也必須被決定。</p>
<p>這些條件彼此交互搭配會產生很多組的Model，那該如何挑選Model呢？我們就可以使用Validation來當作一個依據來選擇Model，選擇出E<sub>val</sub>最小的Model，如上圖所示。</p>
<p>另外實作上有一些方法：Leave-One-Out Cross Validation和V-Fold Cross Validation，他們的精髓就是保留k筆Data當作未來Validation用，另外一些拿下去Train Model，然後再用這k筆去評估並得到E<sub>val</sub>1，還沒結束，為了讓E<sub>val</sub>盡可能的正確，所以我們會在把Data作一個迴轉，這次使用另外一組k組Data來Validation，其餘的再拿去Train Model，然後在評估出E<sub>val</sub>2， … 以此類推，當轉完一輪之後，在把這些E<sub>val</sub>1, E<sub>val</sub>2, ...做平均得到一個較為精確E<sub>val</sub>。那Leave-One-Out Cross Validation顧名思義就是k=1，但這樣做要付出的代價就是計算量太大了，所以V-Fold Cross Validation則使用k=V來做。實務上，我常常做Validation時根本不會去Cross它們，我大都只是保留一部分的Data來驗證而已，給大家參考。</p>
<p><br /></p>
<h3>總結</h3>
<p>來到了這四篇有關於林軒田教授機器學習基石學習筆記的尾聲了，讓我們重溫看看我們學會了什麼？</p>
<p>一開始我帶大家初探ML的基本架構，建立Model、使用Data訓練、最後達到描述Target Function的目的，也帶大家認識各種機器學習的類型。</p>
<p>接下來，我們用理論告訴大家，ML是不是真的可以做到，那在什麼時候可以做到？要符合哪些條件？我們知道要有好的Model，VC Dimension越小越好，也就是可調控的參數越少越好，才會使得E<sub>in</sub> ≈ E<sub>out</sub>成立；要有足夠的Data；要有好的Learning Algorithm能把E<sub>in</sub>壓低，這三種條件成立後，如此一來Model在描述訓練數據很好的同時也可以很好的去預測母群體，但我們發現E<sub>in</sub>壓低和可調控的參數越少越好兩者是Trade-off，所以我們必須取適當的VC Dimension。</p>
<p>再接下來我們開始看實際上ML該怎麼做，引入相當重要的Learning Algorithm，也就是Gradient Descent，並且說明了Linear Regression和Logistic Regression，而且還可以使用這兩種Regression來做分類問題。</p>
<p>那最後就真正亮出ML的三大絕招啦：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），Feature Transformation使得Model更為強大，所以E<sub>in</sub>更能夠壓低，但是為了避免Overfitting我們必須去限制它，Regularization可以限制高次項的貢獻，另外，Validation可以量化Overfitting的程度，有了這個我們就可以去選出體質健康而且E<sub>in</sub>又小的Model。</p>
<p>機器學習基石的這些概念都很重要，往後如果你開始學習其他的ML技巧，例如：深度學習，這些知識都是你強大的基礎，所以多看幾次吧！</p>


        <br/><br/>

<div id="disqus_thread"></div>
<script type="text/javascript">
/* <![CDATA[ */

    var disqus_shortname = 'ycnote-1';
    var disqus_identifier = "ml-course-foundations_4.html";

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
/* ]]> */
</script>
<noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



        
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="./archives.html">Archives</a></li>
                            <li><a href="./tags.html">Tags</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Contact Me</div>
                        <ul class="list-unstyled">
                            <li><a href="./about-me.html" target="_blank">About Me</a></li>
                            <li><a href="https://github.com/GitYCC" target="_blank">Github</a></li>
                            <li><a href="mailto:ycc.tw.email@gmail.com" target="_blank">Email</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; YC Note 2018</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>