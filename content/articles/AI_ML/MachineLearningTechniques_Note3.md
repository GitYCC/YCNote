Title: æ©Ÿå™¨å­¸ç¿’æŠ€æ³• å­¸ç¿’ç­†è¨˜ (3)ï¼šKernel Regression
Date: 2017-03-15 12:00
Category: AI.ML
Tags: æ©Ÿå™¨å­¸ç¿’æŠ€æ³•
Slug: ml-course-techniques_3
Author: YC Chen
Illustration: ml-course-techniques.jpeg
Alias: /YCNote/post/32.html
related_posts: ml-course-techniques_1,ml-course-techniques_2,ml-course-techniques_4,ml-course-techniques_5
Summary: æœ¬ç¯‡å…§å®¹æ¶µè“‹Probabilistic SVMã€Kernel Logistic Regressionã€Kernel Ridge Regressionã€Support Vector Regression (SVR)


åœ¨ä¸Šä¸€ç¯‡ç•¶ä¸­æˆ‘å€‘çœ‹åˆ°äº†Kernel Trickçš„å¼·å¤§ï¼Œæˆ‘å€‘ç¹¼çºŒé‹ç”¨é€™å€‹æ•¸å­¸å·¥å…·åœ¨å…¶ä»–çš„Regressionä¸Šçœ‹çœ‹ã€‚

<br/>

### Soft-Margin SVMå…¶å¯¦å¾ˆåƒL2 Regularized Logistic Regression

ä¸Šä¸€ç¯‡ä¸­æåˆ°çš„Soft-Margin SVMå…¶å¯¦å¾ˆåƒ[ã€Šæ©Ÿå™¨å­¸ç¿’åŸºçŸ³ã€‹](https://gitycc.github.io/YCNote/tag/ji-qi-xue-xi-ji-shi.html)è£¡é ­æåˆ°çš„L2 Regularized Logistic Regressionï¼Œå¦‚æœä½ é‚„è¨˜å¾—çš„è©±ï¼ŒLogistic Regressionæ˜¯ç‚ºäº†å› æ‡‰é›œè¨Šè€Œçµ¦äºˆæ¯ç­†è³‡æ–™çš„æè¿°è³¦äºˆã€Œæ©Ÿç‡ã€çš„æ€§è³ªï¼Œè®“Modelåœ¨çœ‹Dataçš„æ™‚å€™ä¸é‚£éº¼çš„éé»‘åŠç™½ï¼Œé‚£æ™‚å€™æœ‰æåˆ°é€™å«åšSoft Classificationï¼Œè€Œé€™å€‹æ¦‚å¿µå°±éå¸¸æ¥è¿‘æ–¼Soft-Marginçš„æ¦‚å¿µã€‚

å¾æ•¸å­¸å¼ä¾†çœ‹æœƒæ›´æ¸…æ¥šï¼Œ

> Soft-Margin SVMï¼š<br/>
>
> $min. (W^{T}W/2) + CÃ—ğšº_{n} Î¾_{n}\ \ \ s.t.\ \ \ y_{n}Ã—(W^{T}Z_{n}+b) â‰¥ 1-Î¾_{n}\ and\ Î¾_{n} â‰¥ 0,\ n=1\cdots N$

ä¸Šé¢çš„å¼å­ä¸­ï¼Œå¯ä»¥å°‡é™åˆ¶æ¢ä»¶ç”±maxå–ä»£æ‰ï¼Œè½‰æ›æˆä¸‹é¢çš„Unboundedçš„è¡¨ç¤ºæ–¹æ³•ï¼Œ

> Soft-Margin SVMï¼š<br>
>
> $min. CÃ—ğšº_{n} Err_{hinge,n} + (W^{T}W/2)$<br/>
>
> **å…¶ä¸­ï¼Œ$Err_{hinge,n}=max[0,1-y_{n}Ã—(W^{T}Z_{n}+b)]$ï¼Œç¨±ä¹‹ç‚ºHinge Error Measure**ã€‚

æ¥ä¸‹ä¾†æ¯”è¼ƒä¸€ä¸‹L2 Regularized Logistic Regressionï¼Œ

> L2 Regularized Logistic Regressionï¼š<br>
>
> $min. (1/N)Ã—ğšº_{n} Err_{ce,n} +  (Î»/N)Ã—W^{T}W$<br/>
>
> å…¶ä¸­ï¼Œ$Err_{ce,n}=ln[1+exp(-y_{n}Ã—(W^{T}Z_{n}))]$ï¼Œç‚ºCross-Entropy Error Measureã€‚

ä½ æœƒç™¼ç¾Soft-Margin SVMå’ŒL2 Regularized Logistic Regressionå…©å€‹å¼å­çš„å½¢å¼æ˜¯å¾ˆæ¥è¿‘çš„ï¼Œéƒ½æœ‰$W^{T}W$é€™ä¸€é …ï¼Œåªæ˜¯æ„ç¾©ä¸Šä¸åŒï¼Œåœ¨Soft-Margin SVMè£¡é ­$W^{T}W$æ‰€ä»£è¡¨çš„æ˜¯åæ¯”æ–¼ç©ºç™½å€å¤§å°è·é›¢çš„å‡½å¼ï¼Œè€Œåœ¨L2 Regularized Logistic Regressionè£¡é ­å‰‡æ˜¯æŒ‡Regularizationã€‚

å¦å¤–ï¼Œæˆ‘å€‘ä¾†ç–Šä¸€ä¸‹$Err_{hinge,n}$å’Œ$Err_{ce,n}$ä¾†çœ‹çœ‹é€™å…©å€‹å‡½æ•¸åƒä¸åƒï¼Œ

![compare:hinge and ce](http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_03.png)

from:Â [https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf](https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf)

**$Err_{hinge,n}$å’Œ$Err_{ce,n}$æ˜¯éå¸¸æ¥è¿‘çš„ï¼Œæ‰€ä»¥æˆ‘å€‘å¯ä»¥èªªåšSoft-Margin SVMï¼Œå¾ˆåƒæ˜¯åœ¨åšL2 Regularized Logistic Regressionã€‚**

**é›–ç„¶èªªSoft-Margin SVMå’ŒL2 Regularized Logistic Regressionéå¸¸çš„åƒï¼Œä½†æ˜¯æˆ‘åœ¨åšå®ŒSoft-Margin SVMå¾Œï¼Œä»ç„¶æ²’è¾¦æ³•åƒLogistic Regressionä¸€æ¨£å¾—åˆ°ä¸€å€‹å…·æœ‰æ©Ÿç‡åˆ†å¸ƒçš„Target Functionï¼Œä»¥ä¸‹æä¾›äº†å…©ç¨®æ–¹æ³•ï¼Œç¬¬ä¸€ç¨®æ˜¯é–“æ¥çš„æ–¹æ³•ï¼Œä½¿ç”¨å…©éšæ®µå­¸ç¿’ä¾†é”æˆLogisticçš„æ•ˆæœï¼›ç¬¬äºŒç¨®æ˜¯ç›´æ¥å°‡L2 Regularized Logistic RegressionåŠ å…¥æœ‰å¦‚Soft-Margin SVMçš„Kernelæ€§è³ªã€‚**

<br/>

### ä½¿ç”¨SVMåšLogistic Regressionï¼šProbabilistic SVM

è¦è®“Soft-Margin SVMåœ¨æœ€å¾Œå‘ˆç¾çš„Target Functionæ™‚å…·æœ‰æ©Ÿç‡æ€§è³ªï¼Œæœ€ç°¡å–®çš„ä½œæ³•å°±æ˜¯é€éå…©éšæ®µçš„å­¸ç¿’ä¾†é”æˆï¼Œç¬¬ä¸€éšæ®µå…ˆç”¨Soft-Margin SVMå»è§£å‡ºåˆ‡åˆ†è³‡æ–™çš„å¹³é¢ï¼Œç¬¬äºŒéšæ®µå†å°‡Logistic Functionå¥—åœ¨é€™å€‹å¹³é¢ä¸Šï¼Œä¸¦åšFittingï¼Œæœ€å¾Œæˆ‘å€‘å°±å¾—åˆ°ä¸€å€‹ä»¥Logistic Functionè¡¨ç¤ºçš„Target Functionï¼Œé€™å€‹ç¨±ä¹‹ç‚ºProbabilistic SVMã€‚å¯¦éš›æ“ä½œæ–¹æ³•å¦‚ä¸‹ï¼š

> 1. ä½¿ç”¨Soft-Margin SVMè§£å‡ºåˆ‡å¹³é¢$W_{SVM}^{T}Z+b_{SVM}=0$ï¼Œä¸¦å°‡æ‰€æœ‰Dataé€²ä¸€æ­¥çš„è½‰æ›åˆ° $Z'_{n}=W_{SVM}^{T}Z(X_{n})+b_{SVM}$ã€‚
> 2. æ¥ä¸‹ä¾†ç”¨è½‰æ›å¾Œçš„çµæœ$\{Z'_{n},\ y_{n}\}$åšLogistic Regressionå¾—åˆ°ä¿‚æ•¸Aå’ŒBã€‚
> 3. æœ€å¾Œçš„Target Functionå°±æ˜¯ $g(x)=Î˜(A\cdot (W_{SVM}^{T}Z(X_{n})+b_{SVM})+B)$ï¼Œ$Î˜$ç‚ºLogistic Functionã€‚

ä¸Šé¢çš„æ–¹æ³•æœ‰ä¸€å€‹ç¼ºé»ï¼Œå°±æ˜¯å¦‚æœBçš„å€¼ä¸æ¥è¿‘0æ™‚ï¼ŒSVMçš„åˆ‡å¹³é¢å°±æœƒå’ŒLogistic Regressionçš„é‚Šç•Œå°±æœƒä¸åŒï¼Œè€Œä¸”ä¸€å€‹Modelè¦Fittingå…©æ¬¡ä¹Ÿç›¸ç•¶çš„éº»ç…©ï¼Œä»¥ä¸‹é‚„æœ‰å¦å¤–ä¸€å€‹å¯ä»¥é”åˆ°ä¸€æ¨£çš„å…·æœ‰æ©Ÿç‡æ€§è³ªçš„æ•ˆæœçš„æ–¹æ³•â€”Kernel Logistic Regressionã€‚

<br/>

### Kernel Trickçš„çœŸæ­£ç²¾é«“ï¼šRepresenter Theorem

åœ¨èªªæ˜Kernel Logistic Regressionä¹‹å‰æˆ‘å€‘å…ˆä¾†è¤‡ç¿’ä¸€ä¸‹Kernelçš„æ¦‚å¿µï¼Œä¸¦ä¸”å¾ä¸­å°‡ä»–çš„é‡è¦è§€å¿µèƒå–å‡ºä¾†ã€‚

å†ä¾†çœ‹ä¸€çœ¼æˆ‘å€‘æ€éº¼è§£Kernel Soft-Margin SVMçš„ï¼Œ

> Kernel Soft-Margin SVMï¼š<br/>
>
> åœ¨$0 â‰¤ Î±_{n} â‰¤ C;\ ğšº_{n} Î±_{n}y_{n} = 0$çš„é™åˆ¶æ¢ä»¶ä¸‹ï¼Œæ±‚è§£$min. [(1/2)ğšº_{n}ğšº_{m} Î±_{n}Î±_{m}y_{n}y_{m}K(X_{n},X_{m})-ğšº_{n} Î±_{n}]$
>
> å¾—åˆ°$Î±_{n}$ï¼Œç„¶å¾Œ
>
> **$W = ğšº_{n} Î±_{n}y_{n}Z_{n}$**
>
> $b=y_{sv}-ğšº_{n} Î±_{n}y_{n}K(X_{n},X_{sv})$

å…¶ä¸­Wå¯ä»¥æƒ³æˆæ˜¯ç”±$Z_{n}$æ‰€çµ„åˆè€Œæˆçš„ï¼Œè€Œæ±ºå®šè²¢ç»ç¨‹åº¦å‰‡åæ‡‰åœ¨æ”¾åœ¨å®ƒå‰é¢çš„ä¿‚æ•¸$(Î±_{n}y_{n})$ï¼Œ$y_{n}$æ±ºå®šè²¢ç»çš„æ–¹å‘ï¼Œ$Î±_{n}$æ±ºå®šå½±éŸ¿çš„ç¨‹åº¦ã€‚

**æ•¸å­¸ä¸Šï¼Œæœ‰å€‹ç†è«–Representer Theoremå¯ä»¥å‘Šè¨´æˆ‘å€‘ï¼Œæ‰€æœ‰çš„æœ€ä½³åŒ–å•é¡Œä¸­ï¼Œ$W$çš„æœ€ä½³è§£éƒ½æ˜¯ç”±$Z_{n}$æ‰€çµ„åˆè€Œæˆçš„ï¼Œä»¥ç·šæ€§ä»£æ•¸çš„è§’åº¦ï¼Œå°±æ˜¯$W$ç”±$Z_{n}$æ‰€å±•é–‹(span)ï¼Œæ•¸å­¸ä¸Šè¡¨ç¤ºæˆ$W^*=ğšº_{n} Î²_{n}Z_{n}$ã€‚**

é€™å€‹æ€§è³ªç‚ºKernel Trickæä¾›äº†ä¸€å€‹è‰¯å¥½çš„åŸºç¤ï¼Œæ¯æ¬¡æˆ‘å€‘åªè¦é‡åˆ°$W^{*T}Z$çš„éƒ¨åˆ†ï¼Œæˆ‘å€‘å°±å¯ä»¥ä½¿ç”¨Representer TheoremæŠŠå•é¡Œè½‰æ›æˆ$W^{*T}Z=ğšº_{n} Î²_{n}Z_{n}Z=ğšº_{n} Î²_{n}K(X_{n},X)$ï¼Œå°±å¯ä»¥ä½¿ç”¨Kernel Functionäº†ã€‚

![kernel trick](http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_04.png)

from:Â [https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf](https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf)

ä¸Šåœ–æ˜¯è€å¸«åœ¨ä¸Šèª²æ™‚åˆ—å‡ºä¾†SVMã€PLAå’ŒLogistic Regressionçš„Wçš„å±•é–‹å¼ï¼Œä½ æœƒç™¼ç¾éƒ½å¯ä»¥è¡¨ç¾æˆRepresenter Theoremçš„å½¢å¼ã€‚

æœ‰äº†é€™å€‹æ¦‚å¿µï¼Œæˆ‘å€‘å°±å¯ä»¥æŠŠå¾ˆå¤šå•é¡Œéƒ½åˆ©ç”¨Representer Theoremä¾†è½‰æ›ï¼Œä¸¦ä¸”å¥—ä¸ŠKernel Trickã€‚

<br/>

### Kernel Logistic Regression

é‚£æˆ‘å€‘æœ‰äº†Representer Theoremå°±å¯ä»¥ç›´æ¥ä¾†è½‰æ›L2 Regularized Logistic Regressionï¼Œè®“å®ƒæœ‰æ“æœ‰Kernelçš„æ•ˆæœï¼Œ

> L2 Regularized Logistic Regressionï¼š<br/>
>
> $min. (1/N)Ã—ğšº_{n} ln[1+exp(-y_{n}Ã—(W^{T}Z_{n}))] +  (Î»/N)Ã—W^{T}W$

ä½¿ç”¨$W^*=ğšº_{n} Î²_{n}Z_{n}$ä»£å…¥å¾—ï¼Œ

> **Kernel Logistic Regression: <br/>**
>
> **$min. (1/N)Ã—ğšº_{n} ln[ 1+exp(-y_{n}Ã—ğšº_{n} Î²_{n}K(X_{n},X)) ] +  (Î»/N)Ã—ğšº_{n}ğšº_{m} Î²_{n}Î²_{m}K(X_{n},X_{m})$**

ä¸Šé¢çš„å¼å­å¯ä»¥ä½¿ç”¨Grandient Descentä¾†æ±‚è§£$Î²_{n}$ï¼Œé€²è€Œå¾—åˆ°$W^*=ğšº_{n} Î²_{n}Z_{n}$ã€‚è€Œä¸”åœ¨Kernel Functionçš„å¹«åŠ©ä¹‹ä¸‹ï¼Œæˆ‘å€‘æ›´å®¹æ˜“å¯ä»¥åšåˆ°éå¸¸é«˜æ¬¡çš„ç‰¹å¾µè½‰æ›ã€‚

<br/>

### Kernel Ridge Regression

åŒç†ï¼Œæˆ‘å€‘ä¹Ÿå¯ä»¥æŠŠç›¸åŒæŠ€å·§å¥—ç”¨åˆ°Ridge Regressionï¼Œ

> Ridge Regressionï¼š<br/>
>
> $min. (1/N)Ã—ğšº_{n} (y_{n}-W^{T}Z_{n})^{2} +  (Î»/N)Ã—W^{T}W$

ä½¿ç”¨$W^*=ğšº_{n} Î²_{n}Z_{n}$ä»£å…¥å¾—ï¼Œ

> **Kernel Ridge Regressionï¼š<br/>**
>
> **$min. (1/N)Ã—ğšº_{n} (y_{n}-ğšº_{m} Î²_{m}K(X_{n},X_{m}))^{2} +  (Î»/N)Ã—ğšº_{n}ğšº_{m} Î²_{n}Î²_{m}K(X_{n},X_{m})$**

ä¸Šé¢çš„å¼å­ä¹Ÿå¯ä»¥ä½¿ç”¨Grandient Descentä¾†æ±‚è§£$Î²_{n}$ã€‚

å¦å¤–ï¼Œé€™å€‹å¼å­æœ‰è¾¦æ³•æ¨å‡ºè§£æè§£ï¼Œå…ˆæŠŠä¸Šå¼å¯ä»¥å¯«æˆçŸ©é™£å½¢å¼ï¼Œ

> Kernel Ridge Regressionï¼š<br/>
>
> $min. E_{aug}$
>
> $E_{aug}=(1/N)Ã—(Î²^{T}K^{T}KÎ²-2Î²^{T}K^{T}y+y^{T}y) +  (Î»/N)Ã—Î²^{T}KÎ²)$

æ‰€ä»¥ï¼Œç”±$âˆ‡E_{aug}=0$å°±å¯ä»¥å¾—åˆ°æœ€å°å€¼æˆç«‹çš„æ¢ä»¶ç‚º

**$Î²^*=(Î»I+K)^{-1}y$**

å…¶å¯¦é€™å€‹å¼å­éå¸¸åƒä¹‹å‰åœ¨ç·šæ€§æ¨¡å‹æ™‚ä½¿ç”¨çš„Pseudo-Inverseï¼Œ

Pseudo-Inverseï¼š$W=(X^{T}X)^{-1}X^{T}y$

ä¸éç¾åœ¨æ›´ç‚ºå¼·å¤§äº†ï¼Œå¯ä»¥æ±‚å¾—éç·šæ€§æ¨¡å‹+Regularizationä¸‹çš„è§£æè§£ã€‚

**æˆ‘å€‘å¯ä»¥ä½¿ç”¨Kernel Ridge Regressionä¾†åšåˆ†é¡å•é¡Œï¼Œç¨±ä¹‹ç‚ºLeast-Squares SVM (LSSVM) ã€‚**

<br/>

### Support Vector Regression (SVR)

å…¶å¯¦ï¼Œä¸ç®¡æ˜¯Kernel Logistic Regressioné‚„æ˜¯Kernel Ridge Regressionï¼Œé€™ç¨®ç›´æ¥å¥—ç”¨Representer Theoremåœ¨Regressionä¸Šçš„éƒ½æœ‰ä¸€å€‹ç¼ºé»ã€‚

é‚£å°±æ˜¯å®ƒå€‘çš„**$Î²_{n}$ä¸¦ä¸ç¢ºä¿å¤§å¤šæ•¸æ˜¯0**ï¼Œå¦‚æœDataç­†æ•¸éå¸¸å¤šçš„è©±ï¼Œé€™åœ¨è¨ˆç®—ä¸Šæœƒæ˜¯ä¸€ç¨®è² è·ã€‚åœ¨ä¹‹å‰æˆ‘å€‘è¨è«–Kernel SVMæ™‚æœ‰æåˆ°åªæœ‰Support Vectorçš„æ•¸æ“šæ‰æœƒå°Modelæœ€å¾Œçš„çµæœæœ‰æ‰€è²¢ç»ï¼ŒSupport Vectorçš„$Î±_{n}>0$ï¼›è€Œä¸æ˜¯Support Vectorçš„æ•¸æ“šå‰‡æ²’æœ‰è²¢ç»ï¼ŒNon-Support Vectorçš„$Î±_{n}=0$ã€‚æ‰€ä»¥ä½ å¯ä»¥æƒ³è¦‹çš„æ˜¯ï¼Œ**$Î±_{n}$å¤§å¤šæ•¸æ˜¯0é™¤äº†Support Vectorå¤–ï¼Œæˆ‘å€‘ç¨±é€™å«åšã€ŒSparse $Î±_{n}$ã€æ€§è³ª**ï¼Œæœ‰é€™æ¨£çš„æ€§è³ªå¯ä»¥å¤§å¤§çš„æ¸›å°‘è¨ˆç®—é‡ã€‚

å› æ­¤æ¥ä¸‹ä¾†æˆ‘å€‘æ‰“ç®—**è®“Regressionå…·æœ‰Support Vectorçš„æ€§è³ªï¼Œç¨±ä¹‹ç‚ºSupport Vector Regression (SVR)**ã€‚

![SVR](http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.006.jpeg)

è¦‹ä¸Šåœ–èªªæ˜ï¼ŒSupport Vector Regressionç°¡ç¨±SVRï¼Œä»¥å¾€çš„Linear Regressionæ˜¯æ±‚ä¸€æ¢æ“¬åˆç›´ç·šèƒ½ä½¿æ‰€æœ‰æ•¸æ“šé»åˆ°ç›´ç·šçš„Erroræœ€å°ï¼Œè€Œç¾åœ¨æˆ‘å€‘è³¦äºˆå®ƒSoft-Marginçš„èƒ½åŠ›ï¼Œ**SVRå°‡æ“¬åˆç›´ç·šå‘å¤–æ“´å¼µè·é›¢Îµï¼Œåœ¨é€™å€‹æ“´å¼µçš„å€åŸŸè£¡é ­çš„æ•¸æ“šé»ä¸å»è¨ˆç®—å®ƒçš„Errorï¼Œåªæœ‰åœ¨è¶…å‡ºè·é›¢Îµå¤–çš„æ‰å»è¨ˆç®—Error**ï¼Œæ­¤æ™‚é€™å€‹æ“¬åˆç›´ç·šæœ‰é»åƒä¸€æ¢æ°´ç®¡ï¼Œæ°´ç®¡å¤–æˆ‘å€‘æ‰è¨ˆç®—Errorï¼Œæ‰€ä»¥åˆç¨±ä¹‹ç‚ºTube Regressionã€‚

é€™å€‹æ¦‚å¿µå’ŒSoft-Margin SVMæœ‰é»åƒï¼Œéƒ½æ˜¯åœ¨é‚Šç•Œçµ¦äºˆçŠ¯éŒ¯çš„æ©Ÿæœƒï¼Œä¸åŒçš„æ˜¯Soft-Margin SVMå› ç‚ºæ˜¯åˆ†é¡å•é¡Œï¼Œæ‰€ä»¥ä¸å…è¨±éŒ¯èª¤çš„æ•¸æ“šè¶…éç•Œï¼Œæ‰€ä»¥è©•ä¼°Errorçš„æ–¹å‘æ˜¯å‘å…§çš„ï¼Œè€ŒSVRæ˜¯å‘å¤–è©•ä¼°Errorï¼Œè¶…å‡ºæ°´ç®¡ä¹‹ä¸Šçš„Erroræˆ‘å€‘è¨˜ä½œ$Î¾_{n}^{â‹€}$ï¼Œä½æ–¼æ°´ç®¡ä¹‹ä¸‹çš„Erroræˆ‘å€‘è¨˜ä½œ$Î¾_{n}^{â‹}$ï¼Œ**æ‰€ä»¥SVRçš„ç›®çš„å°±æ˜¯åœ¨Regularizationä¹‹ä¸‹ä½¿å¾—$Î¾_{n}^{â‹€}+Î¾_{n}^{â‹}$æœ€å°ï¼Œä¸¦ä¸”èª¿æ•´è·é›¢Îµå’ŒCä¾†æ±ºå®šå°Errorçš„å®¹å¿ç¨‹åº¦**ã€‚

é€™å€‹å•é¡ŒåŒæ¨£çš„å¯ä»¥åŒ–ä½œDualå•é¡Œï¼Œå•é¡Œè®Šæˆåªéœ€è¦æœ€ä½³åŒ–$Î±_{n}^{â‹€}$å’Œ$Î±_{n}^{â‹}$ï¼Œå†ä½¿ç”¨æœ€ä½³åŒ–å¾Œçš„$Î±_{n}^{â‹€}$å’Œ$Î±_{n}^{â‹}$å°±å¯ä»¥å¾—åˆ°$W$å’Œ$b$ã€‚å…¶ä¸­$W=ğšº_{n} (Î±_{n}^{â‹€}-Î±_{n}^{â‹}) Z_{n}$é€™å¼å­è£¡é ­éš±å«è‘—Representer Theoremï¼Œæ¯ç­†æ•¸æ“šçš„è²¢ç»ç¨‹åº¦$Î²_{n}=(Î±_{n}^{â‹€}-Î±_{n}^{â‹})$ï¼Œ**å› æ­¤åœ¨ç®¡å­å…§çš„$Î±_{n}^{â‹€}=0$ä¸”$Î±_{n}^{â‹}=0$ï¼Œä¸æœƒæœ‰æ‰€è²¢ç»ï¼Œé€™ä½¿å¾—SVRå…·æœ‰Sparseçš„æ€§è³ªï¼Œå¯ä»¥å¤§å¤§çš„æ¸›å°‘è¨ˆç®—**ã€‚

<br/>

### çµèª

é€™ä¸€ç¯‡ä¸­ï¼Œæˆ‘å€‘ä¸€é–‹å§‹æ­éœ²äº†ã€ŒSoft-Margin SVMå…¶å¯¦å¾ˆåƒL2 Regularized Logistic Regressionã€çš„é€™å€‹ç¾è±¡ï¼Œæ‰€ä»¥åœ¨SVMä¸­æœ€å°åŒ–$W^{T}W$æœ‰é»åƒæ˜¯Regressionä¸­çš„Regularizationï¼Œä¹Ÿå› ç‚ºå½¢å¼ä¸Šç›¸ç•¶çš„æ¥è¿‘ï¼Œæ‰€ä»¥åœ¨SVMè£¡é ­ç”¨åˆ°çš„æ•¸å­¸æŠ€å·§åŒæ¨£çš„å¯ä»¥å¥—åˆ°é€™äº›æœ‰Regularizedçš„Regressionä¸Šã€‚

ç„¶å¾Œï¼Œæˆ‘å€‘å¾Kernel Soft-Margin SVMä¸­èƒå–å‡ºKernel Trickçš„ç²¾è¯â€”Representer Theoremï¼Œæœ€ä½³åŒ–çš„Wå¯ä»¥ç”±Dataçš„Feature $Z_{n}$æ‰€çµ„æˆï¼Œè¨˜ä½œ$W^*=ğšº_{n} Î²_{n}Z_{n}$ï¼Œé€™æä¾›äº†Kernel TrickèƒŒå¾Œçš„å¯¦è¸åŸºç¤ï¼Œæ¥ä¸‹ä¾†æˆ‘å€‘å°±é–‹å§‹é‹ç”¨Representer Theoremåœ¨L2 Regularized Logistic Regressionå’ŒRidge Regressionä¸Šï¼Œè®“é€™äº›Regressionå¯ä»¥è¼•æ˜“çš„åšéç·šæ€§ç‰¹å¾µè½‰æ›ã€‚

æœ€å¾Œï¼Œæˆ‘å€‘æŒ‡å‡ºäº†ç›´æ¥å¥—ç”¨Representer Theoremåœ¨Regressionä¸Šçš„ç¼ºé»å°±æ˜¯åƒæ•¸ä¸¦ä¸Sparseï¼Œæ‰€ä»¥é€ æˆè¨ˆç®—é‡å¤§å¤§å¢åŠ ã€‚å› æ­¤Support Vector Regression (SVR)åƒç…§Soft-Margin SVMçš„å½¢å¼é‡æ–°è¨­è¨ˆRegressionï¼Œä¸¦ä¸”ä½¿ç”¨Dual Transformationå’ŒKernel Functionä¾†è½‰åŒ–å•é¡Œï¼Œæœ€å¾ŒSVRå°±å…·æœ‰Sparseçš„ç‰¹æ€§äº†ã€‚

ä¸Šä¸€ç¯‡è·Ÿé€™ä¸€ç¯‡ï¼Œè«‡çš„æ˜¯ã€ŒKernel Modelsã€ï¼Œåœ¨é€™æ¨£çš„å½¢å¼ä¸‹æˆ‘å€‘å¯ä»¥è®“æˆ‘å€‘çš„ã€Œç‰¹å¾µè½‰åŒ–ã€è®Šå¾—æ›´ç‚ºè¤‡é›œï¼Œç”šè‡³æ˜¯ç„¡çª®å¤šæ¬¡æ–¹é‚„æ˜¯åšå¾—åˆ°çš„ã€‚ä¸‹ä¸€ç¯‡ï¼Œæˆ‘å€‘æœƒé€²åˆ°å¦å¤–ä¸€å€‹ä¸»é¡Œâ€”Aggregation Modelsã€‚







 



