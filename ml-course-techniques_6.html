<!DOCTYPE html>
<html class="no-js" lang="en">
<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <title>機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning) - YC Note</title>
    <meta name="description" content="本篇內容涵蓋神經網路(Neural Network, NN)、深度學習(Deep Learning, DL)、反向傳播算法(Backpropagation, BP)、Weight-elimination Regularizer、Early Stop、Autoencoder、Principal Component Analysis (PCA)">
    <meta name="author" content="YC Chen">

    <meta property="og:type" content="article" />
    <meta property="og:title" content="機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)" />
    <meta property="og:description" content="本篇內容涵蓋神經網路(Neural Network, NN)、深度學習(Deep Learning, DL)、反向傳播算法(Backpropagation, BP)、Weight-elimination Regularizer、Early Stop、Autoencoder、Principal Component Analysis (PCA)" />
    <meta property="og:image" content="https://www.ycc.idv.tw/images/ml-course-techniques.jpeg" />
    <meta property="og:url" content="https://www.ycc.idv.tw/ml-course-techniques_6.html" />
    <meta property="og:site_name" content="YC Note" />

    <script type="application/ld+json">
        {
          "@context": "https://schema.org",
          "@type": "BreadcrumbList",
          "itemListElement": [{
            "@type": "ListItem",
            "position": 1,
            "name": "AI.ML",
            "item": "https://www.ycc.idv.tw/category/aiml.html"
          },{
            "@type": "ListItem",
            "position": 2,
            "name": "機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)",
            "item": "https://www.ycc.idv.tw/ml-course-techniques_6.html"
          }]
        }
    </script>
    <script type="application/ld+json">
        {
          "@context" : "http://schema.org",
          "@type" : "Article",
          "name" : "機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning) - YC Note",
          "author" : {
            "@type" : "Person",
            "name" : "YC Chen"
          },
          "datePublished" : "2017-04-17",
          "image" : "https://www.ycc.idv.tw/images/ml-course-techniques.jpeg",
          "articleSection" : "AI.ML",
          "articleBody" : "本篇內容涵蓋神經網路(Neural Network, NN)、深度學習(Deep Learning, DL)、反向傳播算法(Backpropagation, BP)、Weight-elimination Regularizer、Early Stop、Autoencoder、Principal Component Analysis (PCA)",
          "url" : "https://www.ycc.idv.tw/ml-course-techniques_6.html",
          "publisher" : {
            "@type" : "Organization",
            "name" : "YC Note",
            "logo" : {
                "@type" : "ImageObject",
                "url": "https://www.ycc.idv.tw/theme/images/favicon.png"
            }
          },
          "headline" : "機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)"
        }
    </script>

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="https://www.ycc.idv.tw/theme/css/base.css">
    <link rel="stylesheet" href="https://www.ycc.idv.tw/theme/css/vendor.css">
    <link rel="stylesheet" href="https://www.ycc.idv.tw/theme/css/main.css">
    <!-- <link rel="stylesheet" href="https://www.ycc.idv.tw/theme/css/all.min.css"> -->
    <link rel='stylesheet' id='font-awesome-css'  href='https://mk0athemesdemon3j7s5.kinstacdn.com/wp-content/themes/astrid/fonts/font-awesome.min.css?ver=5.2.4' type='text/css' media='all' />

    <!-- script
    ================================================== -->
    <script src="https://www.ycc.idv.tw/theme/js/modernizr.js"></script>

    <!-- favicons
    ================================================== -->
    <link rel="icon" type="image/png" sizes="32x32" href="https://www.ycc.idv.tw/theme/images/favicon.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://www.ycc.idv.tw/theme/images/favicon.png">

    <!-- Google Analytics
    ================================================== -->
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-68393177-2', 'auto');
        ga('send', 'pageview');
    </script>

    <script type="text/x-mathjax-config"> 
        MathJax.Hub.Config({ 
            "HTML-CSS": { scale: 90, linebreaks: { automatic: true } }, 
            SVG: { linebreaks: { automatic:true } } 
            });
    </script>

</head>

<body class="ss-bg-white">

    <!-- preloader
    ================================================== -->
    <div id="preloader">
        <div id="loader" class="dots-fade">
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>

    <div id="top" class="s-wrap site-wrapper">

        <!-- site header
        ================================================== -->
        <header class="s-header header">

            <div class="header__top">
                <div class="header__logo">
                    <a class="site-logo" href="https://www.ycc.idv.tw/">
                        <img src="https://www.ycc.idv.tw/theme/images/favicon.png" alt="Homepage">
                    </a>
                </div>

                <!-- toggles -->
                <a href="#0" class="header__menu-toggle"><span>Menu</span></a>

            </div>

            <nav class="header__nav-wrap">

                <ul class="header__nav">
                    <li><a href="https://www.ycc.idv.tw/" title="">Home</a></li>
                    <li class="has-children">
                        <a href="#0" title="">Categories</a>
                        <ul class="sub-menu">
                            <li><a href="https://www.ycc.idv.tw/category/aiml.html">AI.ML</a></li>
                            <li><a href="https://www.ycc.idv.tw/category/coding.html">Coding</a></li>
                            <li><a href="https://www.ycc.idv.tw/category/life.html">Life</a></li>
                            <li><a href="https://www.ycc.idv.tw/category/reading.html">Reading</a></li>
                        </ul>
                    </li>
                    <li class="has-children">
                        <a href="#0" title="">Tags</a>
                        <ul class="sub-menu">
                            <li><a href="https://www.ycc.idv.tw/tag/ai-ji.html">埃及</a></li>
                            <li><a href="https://www.ycc.idv.tw/tag/ji-qi-xue-xi-ji-fa.html">機器學習技法</a></li>
                            <li><a href="https://www.ycc.idv.tw/tag/ji-qi-xue-xi-ji-shi.html">機器學習基石</a></li>
                            <li><a href="https://www.ycc.idv.tw/tag/ji-ta.html">吉他</a></li>
                            <li><a href="https://www.ycc.idv.tw/tag/papers.html">Papers</a></li>
                            <li><a href="https://www.ycc.idv.tw/tag/pou-xi-shen-du-xue-xi.html">剖析深度學習</a></li>
                            <li><a href="https://www.ycc.idv.tw/tag/pythonwan-shu-ju.html">Python玩數據</a></li>
                            <li><a href="https://www.ycc.idv.tw/tag/ruan-ti-she-ji.html">軟體設計</a></li>
                            <li><a href="https://www.ycc.idv.tw/tag/tensorflow.html">Tensorflow</a></li>
                            <li><a href="https://www.ycc.idv.tw/tag/you-ji.html">遊記</a></li>
                        </ul>
                    </li>
                    <li></li>
                    <li><a href="https://www.ycc.idv.tw/#about" title="">About</a></li>
                </ul> <!-- end header__nav -->

                <ul class="header__social">
                    <li class="ss-facebook">
                        <a href="https://www.facebook.com/pg/yc.note" target="_blank">
                            <span class="screen-reader-text">Facebook</span>
                        </a>
                    </li>
                    <li class="ss-github">
                        <a href="https://github.com/GitYCC" target="_blank">
                            <span class="screen-reader-text">Github</span>
                        </a>
                    </li>
                    <li class="ss-linkedin">
                        <a href="https://www.linkedin.com/in/yi-chang-chen-aba1b6114/" target="_blank">
                            <span class="screen-reader-text">Linkedin</span>
                        </a>
                    </li>
                    <li class="ss-email">
                        <a href="mailto:ycc.tw.email@gmail.com" target="_blank">
                            <span class="screen-reader-text">Email</span>
                        </a>
                    </li>

                </ul>

            </nav> <!-- end header__nav-wrap -->

        </header> <!-- end s-header -->


        <!-- site content
        ================================================== -->
        <div class="s-content content">
            <main class="row content__page">

                <article class="column large-full entry format-standard">

                    <div class="media-wrap entry__media">
                        <div class="entry__post-thumb">
                            <img src="https://www.ycc.idv.tw/images/ml-course-techniques.jpeg" 
                                 srcset="https://www.ycc.idv.tw/images/ml-course-techniques.jpeg 2000w, 
                                 https://www.ycc.idv.tw/images/ml-course-techniques.jpeg 1000w, 
                                 https://www.ycc.idv.tw/images/ml-course-techniques.jpeg 500w" sizes="(max-width: 2000px) 100vw, 2000px" alt="">
                        </div>
                    </div>

                    <div class="content__page-header entry__header">
                        <h1 class="display-1 entry__title">
                        機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)
                        </h1>
                        <ul class="entry__header-meta">
                            <li class="author"><i class="fa fa-user"></i> YC Chen</a></li>
                            <li class="date"><i class="fa fa-calendar"></i> 2017-04-17</li>
                            <li class="cat-links">
                                <i class="fa fa-archive"></i> <a href="https://www.ycc.idv.tw/category/aiml.html">AI.ML</a>
                            </li>
                            <li>
                                <i class="fa fa-tags"></i> 
<a href="https://www.ycc.idv.tw/tag/ji-qi-xue-xi-ji-fa.html">機器學習技法</a>                            </li>
                        </ul>
                    </div> <!-- end entry__header -->

                    <div class="entry__content">
                        <div style="background-color: rgba(0, 0, 0, 0.0470588);padding: 20px;margin-bottom:  50px;">
                            本篇內容涵蓋神經網路(Neural Network, NN)、深度學習(Deep Learning, DL)、反向傳播算法(Backpropagation, BP)、Weight-elimination Regularizer、Early Stop、Autoencoder、Principal Component Analysis (PCA)
                        </div>

                        <h3>神經網路(Neural Network)</h3>
<p>最後一個主題，我們要來講第三種「特徵轉換」— Extraction Models，其實就是現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，包括下圍棋的AlphaGo、Tesla的自動駕駛都是採用這一類的Machine Learning。</p>
<p>Extraction Models的基本款就是廣為人知的「神經網路」(Neural Network)，它的特色是使用神經元來做非線性的特徵轉換，那如果具有多層神經元，就是做了多次的非線性特徵轉換，這就是所謂的「深度學習」(Deep Learning)。</p>
<p><img alt="Neural Network" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.016.jpeg"></p>
<p>上圖左側就是具有一層神經元的Neural Network，首先我們有一組特徵<span class="math">\(X\)</span>，通常我們會加入一個維度<span class="math">\(X_{0}=1\)</span>，這是為了可以讓結構變得更好看，未來可以與<span class="math">\(W_{0}\)</span>相乘產生常數項。使用<span class="math">\(W\)</span>來給予特徵<span class="math">\(X\)</span>權重，最後總和的結果稱之為Score，<span class="math">\(s = W_{0}X_{0}+𝚺_{i=1}W_{i}X_{i} = 𝚺_{i=0}W_{i}X_{i}\)</span>。</p>
<p>這個Score會被輸入到一個Activation Function裡頭，<strong>Activation Function的用意就是開關</strong>，當Score大於某個閥值，就打通線路讓這條路的貢獻可以繼續向後傳遞；當Score小於某個閥值，就關閉線路，所以Activation Function可以是Binary Function，但在實際操作之下不會使用像Binary Function這類不可以微分的Activation Function，所以我們會找具有相似特性但又可以微分的函數，例如<span class="math">\(tanh\)</span>或者是<span class="math">\(ReLU\)</span>這類比較接近開關效果的函數，經過Activation Function轉換後的輸出表示成<span class="math">\(g_{t} = σ(𝚺_{i}W_{i}X_{i})\)</span>，這個<span class="math">\(g_{t}\)</span>就稱為神經元、<span class="math">\(σ\)</span>為Activation Function、<span class="math">\(𝚺_{i} W_{i}X_{i}\)</span>是Score。</p>
<p>如果我們有多組權重<span class="math">\(W\)</span>就能產生多組神經元<span class="math">\(g_{t}\)</span>，然後最後把<span class="math">\(g_{t}\)</span>做線性組合並使用Output Function <span class="math">\(h(x)\)</span>來衡量出最後的答案，Output Function可以是Linear Classification的Binary Function <span class="math">\(h(x)=sign(x)\)</span>，不過一樣的問題，它不可以微分，通常不會被使用，常見的是使用Linear Regression <span class="math">\(h(x)=x\)</span>，或者Logistic Regression <span class="math">\(h(x)=Θ(x)\)</span>來當作Output Function，最後的結果可以表示成 <span class="math">\(y=h(𝚺_{t}α_{t}g_{t})\)</span>，看到這個式子有沒有覺得很熟悉，它就像我們上一回講的Aggregation，將特徵X使用特徵轉換轉成使用<span class="math">\(g_{t}\)</span>表示，再來組合這些<span class="math">\(g_{t}\)</span>成為最後的Model，所以單層的Neural Network就使用到了Aggregation，它繼承了Aggregation的優點。</p>
<p>有了這個Model的形式了，我們可以使用Gradient Descent的手法來做最佳化，這也就是為什麼要讓操作過程當中所使用的函數都可以微分的原因。Gradient Descent在Neural Network的領域裡面發展出一套方法稱為Backpropagation，我們待會會介紹。<strong>因此實現Backpropagation，我只要餵Data進去，Model就會去尋找可以描述這組Data的特徵轉換<span class="math">\(g_{t}\)</span>，這就好像是可以從Data中萃取出隱含的Feature一樣，所以這類的Models才會被統稱為Extraction Models</strong>。</p>
<p><br/></p>
<h3>深度學習(Deep Learning)</h3>
<p>剛剛我們介紹了最基本款的Neural Network，那如果這個Neural Network有好幾層，我還會稱它為Deep Learning，所以基本上Deep Neural Network和Deep Learning是指同一件事，那為什麼會有兩個名字呢？其實是有歷史典故的。</p>
<p>Neural Network的歷史相當悠久，早在1958年就有人提出以Perceptron當作Activation Function的單層Neural Network，大家也知道一層的Neural Network是不Powerful的，所以在1969年，就有人寫了論文叫做「perceptron has limitation」，從那時起Neural Network的方法就很少人研究了。</p>
<p>直到1980年代，有人開始使用多層的Neural Network，並在1989年，Yann LeCun博士等人就已經將反向傳播演算法(Backpropagation, BP)應用於Neural Network，當時Neural Network的架構已經和現在的Deep Learning很接近了，不過礙於當時的硬體設備計算力不足，Neural Network無法發揮功效，並且緊接的<strong>有人在1989年證明了只要使用一層Neural Network就可以代表任意函數，那為何還要Deep呢？</strong>所以Deep Neural Network這方法就徹底黑掉了。</p>
<p>一直到了最近，<strong>G. E. Hinton博士為了讓Deep Neural Network起死回生，重新給了它一個新名字「Deep Learning」</strong>，再加上他在2006年提出的RBM初始化方法，這是一個非常複雜的方法，所以在學術界就造成了一股流行，雖然後來被證明RBM是沒有用的，不過卻因為很多人參與研究Deep Learning的關係，也找出了解決Deep Learning痛處的方法，<strong>2009年開始有人發現使用GPU可以大大的加速Deep Learning</strong>，從這一刻起，Deep Learning就開始流行起來，直到去年的2016年3月，圍棋程式Alpha GO運用Deep Learning技術以4:1擊敗世界頂尖棋手李世乭，Deep Learning正式掀起了AI的狂潮。</p>
<p>聽完這個故事我們知道改名字的重要性XDD，不過大家是否還有看到什麼關鍵，「使用一層Neural Network就可以代表任意函數，那為何還要Deep呢？」這句話，這不就否定了我們今天做的事情了嗎？的確，使用一層的Neural Network就可以形成任意函數，而且完全可以用一層的神經元來表示任何多層的神經元，數學上是行得通的，但重點是參數量。Deep Learning的學習方法和人有點類似，我們在學習一個艱深的理論時，會先單元式的針對幾個簡單的概念學習，然後在整合這些概念去理解更高層次的問題，Deep Learning透過多層結構學習，雖然第一層的神經元沒有很多，能學到的也只是簡單的概念而已，不過第二層再重組這些簡單概念，第三層再用更高層次的方法看問題，所以同樣的問題使用一層Neural Network可能需要很多神經元才有辦法描述，但是Deep Learning卻可以使用更少的神經元做到一樣的效果，</p>
<blockquote>
<p>同樣表示的數學轉換過程，雖然單層和多層都是做得到相同轉換的，但是多層所用的參數量是比單層來得少的，依照VC Generalization Bound理論 (請參考：<a href="https://www.ycc.idv.tw/ml-course-foundations_2.html">機器學習基石 學習筆記 (2)：為什麼機器可以學習?</a>) 告訴我們可調控的參數量代表模型的複雜度，所以多層的NN比單層的有個優勢是在做到同樣的數學轉換的情況下更不容易Overfitting。</p>
</blockquote>
<p><strong>因此，Deep Learning中每一層當中做了Aggregation，在增加模型複雜度的同時，也因為平均的效果而做到截長補短，這具有Regularization的效果，並且在採用多層且瘦的結構也同時因為「模組化」而做到降低參數使用量，來減少模型複雜度，這就不難想像Deep Learning為何如此強大。</strong></p>
<p><br/></p>
<h3>反向傳播算法(Backpropagation, BP)</h3>
<p><img alt="Neural Network" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.017.jpeg"></p>
<p>我們接下來就來看一下Deep Learning的演算法—反向傳播法，我們來看要怎麼從Gradient Descent來推出這個算法。</p>
<p>看一下上面的圖，我畫出了具有<span class="math">\(L\)</span>層深的Deep Learning，每一層都有一個權重<span class="math">\(W_{ij}^{(ℓ)}\)</span>，因此我們可以估計出每一層的Score <span class="math">\(s_{j}^{(ℓ)}= 𝚺_{i} W_{ij}^{(ℓ)}X_{i}^{(ℓ-1)}\)</span>，把Score <span class="math">\(s_{j}^{(ℓ)}\)</span>通過Activation Function，就可以得到下一層的Input，如此不斷的疊上去，直到最後一層L為Output Layer，Output最後的結果<span class="math">\(y\)</span>，這裡我使用Linear Function來當作Output Function，這就是Deep Learning最簡單的架構。</p>
<p>而我們需要Training的就是這些權重<span class="math">\(W_{ij}^{(ℓ)}\)</span>，我們如何一步一步的更新<span class="math">\(W_{ij}^{(ℓ)}\)</span>，使得它可以Fit數據呢？回想一下Gradient Descent的流程：</p>
<ol>
<li>定義出Error函數</li>
<li>Error函數讓我們可以去評估<span class="math">\(E_{in}\)</span></li>
<li>算出它的梯度<span class="math">\(∇E_{in}\)</span></li>
<li>朝著<span class="math">\(∇E_{in}\)</span>的反方向更新參數W，而每次只跨出<span class="math">\(η\)</span>大小的一步</li>
<li>反覆的計算新參數<span class="math">\(W\)</span>的梯度，並一再的更新參數<span class="math">\(W\)</span></li>
</ol>
<p>假設使用平方誤差的話，Error函數在這邊就是</p>
<p><span class="math">\(L = (1/2) (y-\overline{y})^{2}\)</span>，</p>
<p>因此我們的更新公式可以表示成</p>
<p><span class="math">\(W_{ij}^{(ℓ)} ←  W_{ij}^{(ℓ)}-η×∂L/∂W_{ij}^{(ℓ)}\)</span> </p>
<p>那我們要怎麼解這個式子呢？關鍵就在<span class="math">\(∂L/∂W_{ij}^{(ℓ)}\)</span>這項要怎麼計算，這一項在Output Layer (<span class="math">\(ℓ=L\)</span>)是很好計算的，</p>
<p><span class="math">\(∂L/∂W_{ij}^{(L)}\)</span></p>
<p><span class="math">\(= \frac{∂L}{∂s_{j}^{(L)}} \frac{∂s_{j}^{(L)}}{{∂W_{ij}^{(L)}}}\)</span>  (連鎖率)</p>
<p><span class="math">\(= {δ_{j}^{(L)}}×{X_{i}^{(L-1)}}\)</span></p>
<p>上式當中我們使用了微分的連鎖率，並且令</p>
<p><strong><span class="math">\(δ_{j}^{(L)} = ∂L/∂s_{j}^{(L)}\)</span></strong></p>
<p><span class="math">\(δ_{j}^{(L)}\)</span>這一項被稱為Backward Pass Term，而<span class="math">\(X_{i}^{(L-1)}\)</span>這項被稱為Forward Pass Term，所以<span class="math">\(L\)</span>層權重的更新取決於Forward Pass Term和Backward Pass Term相乘<span class="math">\(δ_{j}^{(L)}×X_{i}^{(L-1)}\)</span>。</p>
<p>我們先來看一下<span class="math">\(L\)</span>層的Forward Pass Term要怎麼計算，<span class="math">\(X_{i}^{(L-1)}\)</span>這項是很容易求的，我們只要讓數據一路從<span class="math">\(0\)</span>層傳遞上來就可以自然而然的得到<span class="math">\(X_{i}^{(L-1)}\)</span>的值，所以我們會稱<span class="math">\(X_{i}^{(L-1)}\)</span>這一項為Forward Pass Term，因為我們必須要往前傳遞才可以得到這個值。</p>
<p>再來看一下<span class="math">\(L\)</span>層的Backward Pass Term要怎麼計算，<span class="math">\(δ_{j}^{(L)}\)</span>一樣是很容易求得的，</p>
<p><span class="math">\(δ_{j}^{(L)} = ∂L/∂s_{j}^{(L)} = ∂[(1/2) (y-\overline{y})^{2}]/∂y = (y-\overline{y})\)</span></p>
<p>你會發現這一項的計算需要得到誤差的資訊，而誤差資訊要等到Forward的動作做完才有辦法得到，所以資訊的傳遞方向是從尾巴一路回到頭，是一個Backword的動作。</p>
<p>因此，最後一層也是Output Layer的更新公式如下：</p>
<p><span class="math">\(W_{ij}^{(L)} ←  W_{ij}^{(L)}-η×δ_{j}^{(L)}×X_{i}^{(L-1)}\)</span></p>
<p>權重的更新取決於Input和Error的影響，需要考慮Forward Pass Term和Backward Pass Term。</p>
<p>那除了Output這一層以外的權重應該怎麼更新？來看一下<span class="math">\((ℓ)\)</span>層，</p>
<p><span class="math">\(∂L/∂W_{ij}^{(ℓ)}\)</span></p>
<p><span class="math">\(= \frac{∂L}{∂s_{j}^{(ℓ)}}\frac{∂s_{j}^{(ℓ)}}{∂W_{ij}^{(ℓ)}}\)</span> (連鎖率)</p>
<p><span class="math">\(= δ_{j}^{(ℓ)}×X_{i}^{(ℓ-1)}\)</span></p>
<p>一樣是Forward Pass Term和Backword Pass Term相乘，不過<span class="math">\(δ_{j}^{(ℓ)}\)</span>這一項的計算有點技巧性，來看一下，</p>
<p><span class="math">\(δ_{j}^{(ℓ)}\)</span></p>
<p><span class="math">\(= ∂L/∂s_{j}^{(ℓ)}\)</span></p>
<p><span class="math">\(= 𝚺_{k} \frac{∂L}{∂s_{k}^{(ℓ+1)}}\frac{∂s_{k}^{(ℓ+1)}}{∂X_{jk}^{(ℓ)}}\frac{∂X_{jk}^{(ℓ)}}{∂s_{j}^{(ℓ)}}\)</span> (連鎖率)</p>
<p><span class="math">\(= 𝚺_{k} {δ_{k}^{(ℓ+1)}}×{W_{jk}^{(ℓ)}}×{σ'(s_{j}^{(ℓ)})}\)</span></p>
<p><span class="math">\(W_{jk}^{(ℓ)}\)</span>和<span class="math">\(σ'(s_{j}^{(ℓ)})\)</span>都是Forward之後就會得到的資訊，而<span class="math">\(δ_{k}^{(ℓ+1)}\)</span> 而是需要Backward才可以得到，我們已經知道<span class="math">\(δ_{j}^{(ℓ=L)}\)</span>的值，就可以從<span class="math">\(δ_{j}^{(ℓ=L)}\)</span>開始利用上面的公式，一路Backward把所有的<span class="math">\(δ_{j}\)</span>都找齊。好！那現在我們已經找到了更新所有Weights的方法了。</p>
<p>看一下上圖中的最下面的Flow，一開始我們Forward，把所有<span class="math">\(X\)</span>和<span class="math">\(s\)</span>都得到，到了Output Layer，我們得到了<span class="math">\(δ_{j}^{(ℓ=L)}\)</span>，再Backward回去找出所有的<span class="math">\(δ\)</span>，接下來就可以用Forward Pass Term和Backword Pass Term來Update所有的<span class="math">\(W\)</span>了。</p>
<p>總結一下，反向傳播算法(Backpropagation, BP)更新權重的方法為</p>
<blockquote>
<p><strong><span class="math">\(W_{ij}^{(ℓ)} ←  W_{ij}^{(ℓ)}-η×δ_{j}^{(ℓ)}×X_{i}^{(ℓ-1)}\)</span>  <br/></strong></p>
<p><strong>If output layer (<span class="math">\(ℓ=L\)</span>), <span class="math">\(δ_{j}^{(ℓ=L)}=(y-ŷ)\)</span>  <br/></strong></p>
<p><strong>If other layer, <span class="math">\(δ_{j}^{(ℓ)}= σ'(s_{j}^{(ℓ)}) × 𝚺_{k} δ_{k}^{(ℓ+1)}×W_{jk}^{(ℓ)}\)</span>  <br/></strong></p>
<p><strong><span class="math">\(δ_{j}^{(ℓ)}\)</span>為Backword Pass Term；<span class="math">\(X_{i}^{(ℓ-1)}\)</span>為Forward Pass Term。</strong></p>
</blockquote>
<p><br/></p>
<h3>Regularization in Deep Learning</h3>
<p>那麼使用Deep Learning的時候，我們要怎麼避免Overfitting呢？有五個方法。</p>
<p>第一個方法，就是我們剛剛提過的<strong>「設計Deep Neural Network的結構」</strong>，藉由限縮一層當中的神經元來達到一種限制，做到Regularization。</p>
<p>第二個方法是<strong>「限制W的大小」</strong>，和標準Regularization作一樣的事情，我們將<span class="math">\(W\)</span>的大小加進去Cost裡頭做Fitting，例如使用L2 Regularizer <span class="math">\(Ω(W)=𝚺(W_{jk}^{(ℓ)})^{2}\)</span>，但這樣使用有一個問題就是<span class="math">\(W\)</span>並不是Sparse的，L2 Regularizer在抑制<span class="math">\(W\)</span>的方法是，如果W的分量大的話就抑制多一點，如果分量小就抑制少一點（因為<span class="math">\(W^{2}\)</span>微分為1次），所以最後會留下很多很小的分量，造成計算量大大增加，尤其像是Deep Learing這麼龐大的Model，這樣的Regularization顯然不夠好，L1 Regularizer顯然可以解決這個問題（因為在大部分位置微分為常數），但不幸的是它無法微分，所以就有了L2 Regularizer的衍生版本，</p>
<p>Weight-elimination L2 regularizer: <span class="math">\(𝚺\frac{(W_{jk}^{(ℓ)})^{2}}{1+(W_{jk}^{(ℓ)})^{2}}\)</span></p>
<p>這麼一來不管<span class="math">\(W\)</span>大或小，它受到抑制的值大小是接近的 (因為Weight-elimination L2 regularizer微分為 <span class="math">\(-1\)</span>次方)，因此就可以使得部分<span class="math">\(W\)</span>可以為<span class="math">\(0\)</span>，大大便利於我們做計算。</p>
<p>第三種方法是最常使用的<strong>「Early Stopping」</strong>，所謂的Early Stopping就是，在做Backpropagation的過程去觀察Validation Data的Error有沒有脫離Training Data的Error太多，如果開始出現差異，我們就立刻停止計算，這樣就可以確保Model裡的參數沒有使得Model產生Overfitting，是一個很直接的作法。</p>
<p>第四種方法是<strong>「Drop-out」</strong>，在Deep Learing Fitting的過程中，隨機的關閉部分神經元，藉由這樣的作法使得Fitting的過程使用較少的神經元，並且使得結構是瘦長狀的，來達到Regularization。</p>
<p>第五種方法是接下來會用更大篇幅介紹的<strong>「Denoising Autoencoder」</strong>，在Deep Neural Network前面加入這樣的結構有助於抑制雜訊。</p>
<p><br/></p>
<h3>Autoencoder</h3>
<p><img alt="Regularization in Deep Learning" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.018.jpeg"></p>
<p>Neural Network針對不同需要發展出很多不同的型態，包括CNN, RNN，還有接下來要介紹的Autoencoder，<strong>Autoencoder是一種可以將資料重要資訊保留下來的Neural Network</strong>，效果有點像是資料壓縮，在做資料壓縮時，會有一個稱為Encoder的方法可以將資料壓縮，那當然還要有另外一個方法將它還原回去，這方法稱為Decoder，壓縮的過程就是用更精簡的方式保存了資料。<strong>Autoencoder同樣的有Encoder和Decoder，不過它不像資料壓縮一樣可以百分之一百還原，不過特別之處是Autoencoder會試著從Data中自己學習出Encoder和Decoder，並盡量讓資料在壓縮完了可以還原回去原始數據</strong>。</p>
<p>見上圖中Basic Autoencoder的部分，透過兩層的轉換，我們試著讓Input <span class="math">\(X\)</span>可以完整還原回去，通常中間這一層會使用比較少的神經元，因為我們想要將資訊做壓縮，所以第一層的部分就是一個Encoder，而第二層則是Decoder，他們由權重<span class="math">\(W_{jk}^{(ℓ)}\)</span>決定，而在Training的過程，Autoencoder會試著找出最好的<span class="math">\(W_{jk}^{(ℓ)}\)</span>來使得資訊可以盡量完整還原回去，這也代表Autoencoder可以自行找出了Encoder和Decoder。</p>
<p><strong>Encoder這一段就是在做一個Demension Reduction</strong>，Encoder轉換原本數據到一個新的空間，這個空間可以比原本Features描述的空間更能精準的描述這群數據，而中間這層Layer的數值就是新空間裡頭的座標，有些時候我們會用這個新空間來判斷每筆Data之間彼此的接近程度。</p>
<p>我們也可以讓Encoder和Decoder可以設計的更複雜一點，所以你同樣的可以使用多層結構，稱之為Deep Autoencoder。另外，也有人使用Autoencoder的方法來Pre-train Deep Neural Network的各個權重。</p>
<p>緊接著介紹兩種特殊的例子，第一個是Linear Autoencoder，我們把所有的Activation Function改成線性的，這個方法可以等效於待會要講的Principal Component Analysis (PCA)的方法，PCA是一個全然線性的方法，所以它的效力會比Autoencoder差一點。</p>
<p>第二個是剛剛提到的Denoising Autoencoder，我們在原本Autoencoder的前面加了一道增加人工雜訊的流程，但是又要讓Autoencoder試著去還原出原來沒有加入雜訊的資訊，這麼一來<strong>我們將可以找到一個Autoencoder是可以消除雜訊的</strong>，把這個Denoising Autoencoder加到正常Neural Network的前面，那這個Neural Network就擁有了抑制雜訊的功用，所以可以當作一種Regularization的方法。</p>
<p><br/></p>
<h3>Principal Component Analysis (PCA)</h3>
<p>最後來講一下Principal Component Analysis (PCA)，它不太算是Deep Learning的範疇，不過它是一個傳統且重要的Dimension Reduction的方法，我們就來看一下。</p>
<p><img alt="PCA" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.019.jpeg"></p>
<p>PCA的演算法是這樣的，第一步先求出資料Features的平均值，並且將各個Features減掉平均值，令為<span class="math">\(ζ\)</span>，第二步求出由<span class="math">\(ζ^{T}ζ\)</span>產生的矩陣的Eigenvalue和Eigenvector，第三步，從這些Eigenvalue和Eigenvector中挑選前面<span class="math">\(k\)</span>個，並組成轉換矩陣<span class="math">\(W\)</span>，而最終PCA的轉換就是<span class="math">\(Φ(x)=W^{T}(X-mean(X))\)</span>，這個轉換做的就是Dimension Reduction，將數據降維到<span class="math">\(k\)</span>維。</p>
<p>PCA做的事是這樣的，每一個Eigenvector代表新空間裡頭的一個軸，而Eigenvalue代表站在這個軸上看資料的離散程度，當然我們如果可以描述每筆資料越分離，就代表這樣的描述方法越好，所以Eigenvalue越大的Eigenvector越是重要，<strong>所以取前面<span class="math">\(k\)</span>個Eigenvector的用意是在降低維度的過程，還可以盡量的保持對數據的描述力，而且Eigenvector彼此是正交的，也就是說在新空間裡頭的每個軸是彼此垂直，彼此沒有Dependent的軸是最精簡的，所以PCA所做的Dimension Reduction一定是線性模型中最好、最有效率的</strong>。</p>
<p>另外，剛剛有提到的Linear Autoencode幾乎是等效於PCA，大家可以看上圖中的描述，這裡不多贅述，不過不同的是，Linear Autoencoder並沒有限制新空間軸必須是正交的特性，所以它的效率一定會比PCA來的差。</p>
<p><br/></p>
<h3>結語</h3>
<p>這一篇當中，我們介紹了Neural Network，並且探討多層Neural Network—Deep Neural Network，也等同於Deep Learning，並且說明為什麼需要「Deep」，然後介紹Deep Learning最重要的演算法—反向傳播算法，接著介紹五種常用的Regularization的方法：設計Deep Neural Network的結構、限制W的大小、Early Stopping、Drop-out和Denoising Autoencoder。</p>
<p>介紹完以上內容，我們就已經對於Deep Learning的全貌有了一些認識了，緊接著來看Deep Learning的特殊例子—Autoencoder，Autoencoder可以用來做Dimension Reduction，那既然提到了Dimension Reduction，那就不得不在講一下重要的線性方法PCA。</p>
<p>那在下一回，我們會繼續探討Neural Network還有哪些特殊的分支。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

                    </div> <!-- end entry content -->
                    
                    <div class="entry__pagenav">
                        <div class="entry__nav">
                            <div class="entry__prev">
                                <a href="https://www.ycc.idv.tw/big-data-a-revolution.html" rel="prev">
                                    <span>Previous Post</span>
                                    大數據 Big Data:A Revolution That Will Transform How We Live, Work, and Think
                                </a>
                            </div>
                            <div class="entry__next">
                                <a href="https://www.ycc.idv.tw/python-play-with-data_2.html" rel="next">
                                    <span>Next Post</span>
                                    Python玩數據 (2)：Numpy [1/2]
                                </a>
                            </div>
                        </div>
                    </div> <!-- end entry__pagenav -->

                    <div class="entry__related">
                        <h3 class="h2">Related Articles</h3>
                        <ul class="related">
                            <li class="related__item">
                                <a href="https://www.ycc.idv.tw/ml-course-techniques_4.html" class="related__link">
                                    <img src="https://www.ycc.idv.tw/images/ml-course-techniques.jpeg" alt="">
                                    <h5 class="related__post-title">機器學習技法 學習筆記 (4)：Basic Aggregation Models</h5>
                                </a>
                            </li>
                            <li class="related__item">
                                <a href="https://www.ycc.idv.tw/ml-course-techniques_5.html" class="related__link">
                                    <img src="https://www.ycc.idv.tw/images/ml-course-techniques.jpeg" alt="">
                                    <h5 class="related__post-title">機器學習技法 學習筆記 (5)：Boost Aggregation Models</h5>
                                </a>
                            </li>
                            <li class="related__item">
                                <a href="https://www.ycc.idv.tw/ml-course-techniques_7.html" class="related__link">
                                    <img src="https://www.ycc.idv.tw/images/ml-course-techniques.jpeg" alt="">
                                    <h5 class="related__post-title">機器學習技法 學習筆記 (7)：Radial Basis Function Network與Matrix Factorization</h5>
                                </a>
                            </li>
                            <li class="related__item">
                                <a href="https://www.ycc.idv.tw/tensorflow-tutorial_1.html" class="related__link">
                                    <img src="https://www.ycc.idv.tw/images/tensorflow-logo.jpg" alt="">
                                    <h5 class="related__post-title">實作Tensorflow (1)：Simple Logistic Classification on MNIST</h5>
                                </a>
                            </li>
                            <li class="related__item">
                                <a href="https://www.ycc.idv.tw/tensorflow-tutorial_2.html" class="related__link">
                                    <img src="https://www.ycc.idv.tw/images/tensorflow-logo.jpg" alt="">
                                    <h5 class="related__post-title">實作Tensorflow (2)：Build First Deep Neurel Network (DNN)</h5>
                                </a>
                            </li>
                            <li class="related__item">
                                <a href="https://www.ycc.idv.tw/tensorflow-tutorial_3.html" class="related__link">
                                    <img src="https://www.ycc.idv.tw/images/tensorflow-logo.jpg" alt="">
                                    <h5 class="related__post-title">實作Tensorflow (3)：Build First Convolutional Neurel Network (CNN)</h5>
                                </a>
                            </li>
                        </ul>
                    </div> <!-- end entry related -->

                </article> <!-- end column large-full entry-->

<div id="disqus-wrapper">
    <div id="disqus_thread"></div>
</div>
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = "ycnote-1";
        this.page.identifier = "ml-course-techniques_6.html";
        this.page.title = "機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)";
        this.language = "zh_TW";
    };

    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://ycnote-1.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


            </main>
        </div> <!-- end s-content -->

        <!-- footer
        ================================================== -->
        <footer class="s-footer footer">
            <div class="row">
                <div class="column large-full footer__content">
                    <div class="footer__copyright">
                        <span>© Copyright YC Note 2019</span> 
                        <span>Design by <a href="https://www.styleshout.com/">StyleShout</a></span>
                    </div>
                </div>
            </div>

            <div class="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"></a>
            </div>
        </footer>

    </div> <!-- end s-wrap -->


    <!-- Java Script
    ================================================== -->
    <script src="https://www.ycc.idv.tw/theme/js/jquery-3.2.1.min.js"></script>
    <script src="https://www.ycc.idv.tw/theme/js/plugins.js"></script>
    <script src="https://www.ycc.idv.tw/theme/js/main.js"></script>
    <script>
        var elements = document.getElementsByTagName("h3");
        for(i = 0; i < elements.length; i++)
        {
            elements[i].setAttribute("id", "anchor"+i);
        }
    </script>

</body>