
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="True" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="index, follow" name="robots"/>
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&amp;family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&amp;display=swap" rel="stylesheet"/>
<link href="https://ycc.idv.tw/theme/stylesheet/style.less" rel="stylesheet/less" type="text/css"/>
<script src="//cdnjs.cloudflare.com/ajax/libs/less.js/2.5.1/less.min.js" type="text/javascript"></script>
<link href="https://ycc.idv.tw/theme/pygments/monokai.min.css" id="pygments-light-theme" rel="stylesheet" type="text/css"/>
<link href="https://ycc.idv.tw/theme/stork/stork.css" rel="stylesheet" type="text/css">
<link href="https://ycc.idv.tw/theme/font-awesome/css/fontawesome.css" rel="stylesheet" type="text/css"/>
<link href="https://ycc.idv.tw/theme/font-awesome/css/brands.css" rel="stylesheet" type="text/css"/>
<link href="https://ycc.idv.tw/theme/font-awesome/css/solid.css" rel="stylesheet" type="text/css"/>
<link href="/images/favicon.png" rel="shortcut icon" type="image/x-icon"/>
<link href="/images/favicon.png" rel="icon" type="image/x-icon"/>
<!-- Chrome, Firefox OS and Opera -->
<meta content="#FFFFFF" name="theme-color"/>
<!-- Windows Phone -->
<meta content="#FFFFFF" name="msapplication-navbutton-color"/>
<!-- iOS Safari -->
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"/>
<!-- Microsoft EDGE -->
<meta content="#FFFFFF" name="msapplication-TileColor"/>
<link href="https://ycc.idv.tw/feeds/all.atom.xml" rel="alternate" title="YC Note Atom" type="application/atom+xml"/>
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68393177-2', 'auto');
  ga('send', 'pageview');
</script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-LXDD9FZFX2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LXDD9FZFX2');
</script>
<meta content="YC Chen" name="author">
<meta content="本篇內容涵蓋AdaBoost (Adaptive Boost)、Gradient Boost、AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)。" name="description">
<meta content="機器學習技法" name="keywords"/>
<meta content="YC Note" property="og:site_name">
<meta content="機器學習技法 學習筆記 (5)：Boost Aggregation Models" property="og:title">
<meta content="本篇內容涵蓋AdaBoost (Adaptive Boost)、Gradient Boost、AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)。" property="og:description">
<meta content="en_US" property="og:locale">
<meta content="https://ycc.idv.tw/ml-course-techniques_5.html" property="og:url"/>
<meta content="article" property="og:type"/>
<meta content="2017-04-02 12:00:00+08:00" property="article:published_time"/>
<meta content="" property="article:modified_time"/>
<meta content="https://ycc.idv.tw/author/yc-chen.html" property="article:author"/>
<meta content="AI.ML" property="article:section">
<meta content="機器學習技法" property="article:tag"/>
<meta content="" property="og:image"/>
<title>YC Note – 機器學習技法 學習筆記 (5)：Boost Aggregation Models</title>
<script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-5639899546876072",
      enable_page_level_ads: true
    });
  </script>
</meta></meta></meta></meta></meta></meta></meta></link><link href="https://ycc.idv.tw/ml-course-techniques_5.html" rel="canonical"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "BreadcrumbList", "itemListElement": [{"@type": "ListItem", "position": 1, "name": "YC Note", "item": "https://ycc.idv.tw"}, {"@type": "ListItem", "position": 2, "name": "Ml course techniques_5", "item": "https://ycc.idv.tw/ml-course-techniques_5.html"}]}</script><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "author": {"@type": "Person", "name": "YC Chen"}, "publisher": {"@type": "Organization", "name": "YC Note"}, "headline": "機器學習技法 學習筆記 (5)：Boost Aggregation Models", "about": "AI.ML", "datePublished": "2017-04-02 12:00"}</script></head>
<body class="light-theme">
<aside>
<div>
<a href="https://ycc.idv.tw/">
<img alt="YC Note" src="https://ycc.idv.tw/theme/img/profile.png" title="YC Note"/>
</a>
<h1>
<a href="https://ycc.idv.tw/">YC Note</a>
</h1>
<p style="text-align: center;">ML/DL Tech Blog (Total Views: 515,668) </p>
<div class="stork">
<input autocomplete="off" class="stork-input" data-stork="sitesearch" name="q" onclick="loadStorkIndex(); this.onclick=null;" placeholder="Search (beta feature) ..." type="text"/>
<div class="stork-output" data-stork="sitesearch-output"></div>
</div>
<!-- <script>
      window.addEventListener('load', 
        function() { 
          loadStorkIndex();
        }, false);
    </script> -->
<p>This blog is a resource for anyone interested in data science and machine learning, featuring tutorials, research papers, and the latest industry technologies.</p>
<p>Hello, I am YC, an ML engineer/researcher with experience in CV, NLP/NLU, and Recommender. I also have experience in high-QPS ML systems. In my spare time, I'm a blogger and guitar singer. <a href="https://ycc.idv.tw/about-me.html#anchor" style="color:yellow">More about me.</a></p>
<ul class="social">
<li>
<a class="sc-facebook" href="https://www.facebook.com/yc.note" target="_blank">
<i class="fa-brands fa-facebook"></i>
</a>
</li>
<li>
<a class="sc-github" href="https://github.com/GitYCC" target="_blank">
<i class="fa-brands fa-github"></i>
</a>
</li>
<li>
<a class="sc-linkedin" href="https://www.linkedin.com/in/yi-chang-chen-aba1b6114/" target="_blank">
<i class="fa-brands fa-linkedin"></i>
</a>
</li>
</ul>
</div>
</aside>
<main>
<nav id="anchor">
<a href="https://ycc.idv.tw/">Home</a>
<a href="/about-me.html#anchor">About Me</a>
<a href="/categories.html#anchor">Categories</a>
<a href="/tags.html#anchor">Tags</a>
<a href="https://ycc.idv.tw/feeds/all.atom.xml">Atom</a>
</nav>
<article class="single">
<header>
<h1 id="ml-course-techniques_5">機器學習技法 學習筆記 (5)：Boost Aggregation Models</h1>
<p>
      Posted on April 02, 2017 in <a href="https://ycc.idv.tw/category/aiml.html">AI.ML</a>. View: 4,737

    </p>
</header>
<div class="tag-cloud">
<p>
<a href="https://ycc.idv.tw/tag/ji-qi-xue-xi-ji-fa.html">機器學習技法</a>
</p>
</div>
<script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle ads-responsive" data-ad-client="ca-pub-5639899546876072" data-ad-slot="5718861428"></ins>
<script>
      (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
<div class="main-contents">
<h3 id="boost">Boost的精髓</h3>
<p>在上一回當中，我們介紹的Aggregation Models都屬於沒有Boost的，不管是Bagging或Decision Tree都沒有要試著在Training的過程中改善Model，<strong>而這篇將要提到的Boost方法，則是在產生每個<span class="math">\(g_{t}\)</span>時試圖讓Model整體更完善，更能發揮Aggregation Models中截長補短中的「補短」的效果，也就是說<span class="math">\(g_{t}\)</span>可以彼此互補不足之處</strong>。</p>
<p>那實際上我們應該怎麼做才能實踐Boost呢？其實方法的道理早就透漏在上一回中的Bagging和Decision Tree裡頭了，不管是Bagging和Decision Tree都是使用變換Data來做到變異度，在這個方法下Model的架構可以本身是不變的，這帶來相當的便利性，而今天我們要講的Boost也同樣的利用「變換Data」來做到變異度，但不同的是Boost的過程中「變換Data」這件事是有目標性的。</p>
<p><strong>Boost方法在「變換Data」時會試著去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的<span class="math">\(g_{t}\)</span>可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善，這就是Boost的主要精髓。</strong></p>
<p><br/></p>
<h3 id="adaboost-adaptive-boost-for-classification">AdaBoost (Adaptive Boost) for Classification</h3>
<p>剛剛上一段的最後我已經揭露了Boost的真正精髓，拿這樣的概念來做分類問題，就是我們接下來要談的AdaBoost，全名稱為Adaptive Boost。</p>
<p>在分類問題中我們怎麼做到「凸顯原先做錯的Data」？簡單的想法是這樣的，我們可以減少原本已經是正確分類的Data的數量，然後增加原本錯誤分類的Data的數量，<strong>增減Data的數量其實是等效於改變每筆Data的權重</strong>，假如我們給每筆資料權重，要做的事是拉低正確分類Data的權重，而且拉高錯誤分類Data的權重。</p>
<p>那我們應該要提升權重或降低權重到什麼程度才是OK的呢？換個方式思考，我們為什麼要去調整權重？目的其實是要去凸顯原先做錯的部分，降低原本做對的部分，也就是想<strong>藉由調整每筆Data的多寡或權重來做到「弭平原先的預測性」，最好可以讓原本的預測方法看起來是隨機分布</strong>，也就是「錯誤率＝正確率」，讓它像是擲銅板一樣，沒有什麼預測能力。</p>
<p><img alt="AdaBoost" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.012.jpeg"/></p>
<p>有了概念之後，我們來看實際應該要怎麼做？見上圖說明，首先我們需要先將Data權重<span class="math">\(u^{(1)}\)</span>先初始化，接下來就可以開始找<span class="math">\(g_{t}\)</span>了，我們使用任意一個分類問題的Model搭配上Data的權重，求得一組<span class="math">\(g_{t}\)</span>，接下來計算這組<span class="math">\(g_{t}\)</span>的<strong>「錯誤率」<span class="math">\(ε_{t}\)</span></strong>，</p>
<p><strong><span class="math">\(ε_{t}= 𝚺_{n} u_{n}^{(t)} ⟦y_{n}≠g_{t}(x_{n})⟧ / 𝚺_{n} u_{n}^{(t)}\)</span></strong></p>
<p>有注意到考慮「錯誤率」<span class="math">\(ε_{t}\)</span>的時候必須要評估<span class="math">\(u_{n}^{(t)}\)</span>，要記得會有Data權重是為了表示增加或減少原本的Data的數量，所以依照每筆Data的出現機會不同，會有不同的權重，也就會有對「錯誤率」不同的貢獻程度。</p>
<p>那為了待會要對權重重新分配，我們先定義了<span class="math">\(β_{t}\)</span>，在未來我會將錯誤的Data的權重乘上<span class="math">\(β_{t}\)</span>，即<span class="math">\(u_{n}^{(t+1)}=u_{n}^{(t)}×β_{t}\)</span>，並且把正確的Data權重除以<span class="math">\(β_{t}\)</span>，即<span class="math">\(u_{n}^{(t+1)}=u_{n}^{(t)}/β_{t}\)</span>，<strong>而期望的結果是重新分配的Dataset在<span class="math">\(g_{t}\)</span>的預測下可以表現的像隨機的一樣，於是乎下一次使用這組Dataset訓練出來的<span class="math">\(g_{t+1}\)</span>將會彌補<span class="math">\(g_{t}\)</span>的不足</strong>，根據這樣的原則我們來推一下<span class="math">\(β_{t}\)</span>，</p>
<p><span class="math">\(𝚺_{n} u_{n}^{(t+1)} ⟦y_{n}≠g_{t}(x_{n})⟧ / 𝚺_{n} u_{n}^{(t+1)}=1/2\)</span> (預測能力像隨機分布)</p>
<p>⇒  <span class="math">\(𝚺_{n} u_{n}^{(t+1)} ⟦y_{n}≠g_{t}(x_{n})⟧ = 𝚺_{n} u_{n}^{(t+1)} ⟦y_{n}=g_{t}(x_{n})⟧\)</span></p>
<p>⇒  <span class="math">\(𝚺_{n} (u_{n}^{(t)}×β_{t})  ⟦y_{n}≠g_{t}(x_{n})⟧ = 𝚺_{n} (u_{n}^{(t)}/β_{t}) ⟦y_{n}=g_{t}(x_{n})⟧\)</span></p>
<p>⇒  <span class="math">\(β_{t}^{2} = \frac{𝚺_{n} u_{n}^{(t)} ⟦y_{n}=g_{t}(x_{n})⟧ }{𝚺_{n} u_{n}^{(t)}  ⟦y_{n}≠g_{t}(x_{n})⟧}\)</span></p>
<p>⇒  <span class="math">\(β_{t}^{2} = \frac{𝚺_{n} u_{n}^{(t)} ⟦y_{n}=g_{t}(x_{n})⟧ /  𝚺_{n} u_{n}^{(t)}}{ 𝚺_{n} u_{n}^{(t)}  ⟦y_{n}≠g_{t}(x_{n})⟧ / 𝚺_{n} u_{n}^{(t)}}\)</span></p>
<p>⇒  <span class="math">\(β_{t}^{2} = \frac{1-ε_{t}}{ε_{t}}\)</span></p>
<p>⇒  <strong><span class="math">\(β_{t} = \sqrt{\frac{1-ε_{t}}{ε_{t}}}\)</span></strong></p>
<p>所以我們就可以利用這個<span class="math">\(β_{t}\)</span>來更新我的Data權重，並且在多次迭代後，得到很多個<span class="math">\(g_{t}\)</span>。而將來我們會把所有的<span class="math">\(g_{t}\)</span>做線性組合，而我們希望<strong>「錯誤率」越低的<span class="math">\(g_{t}\)</span>可以有更高的貢獻度<span class="math">\(α_{t}\)</span></strong>，所以使用<span class="math">\(β_{t}\)</span>緊接著計算「<span class="math">\(g_{t}\)</span>的權重」<span class="math">\(α_{t}\)</span>，定義為</p>
<p><strong><span class="math">\(α_{t} = ln(β_t)\)</span></strong></p>
<p>所以當一個百分之一百可以完全預測的<span class="math">\(g_{t}\)</span>出現時，它的<span class="math">\(ε_{t}=0\)</span>，此時它的<span class="math">\(β_{t} →∞\)</span>，同時<span class="math">\(α_{t} →∞\)</span>，所以這樣的<span class="math">\(g_{t}\)</span>會有完全的貢獻。</p>
<p>如果一個預測效果很差的<span class="math">\(g_{t}\)</span>出現，它的<span class="math">\(ε_{t}=1/2\)</span>，此時它的<span class="math">\(β_{t}=1\)</span>，同時<span class="math">\(α_{t}=0\)</span>，所以這樣的<span class="math">\(g_{t}\)</span>並沒有任何參考價值。</p>
<p>那如果出現一個<span class="math">\(g_{t}\)</span>它的<span class="math">\(ε_{t} &gt; 1/2\)</span>，那這樣的<span class="math">\(g_{t}\)</span>並不能說它沒有用處，反而是一個很好的反指標，我們只需要反著看就好了，當<span class="math">\(ε_{t} &gt; 1/2\)</span>時，<span class="math">\(β_{t} &lt; 1\)</span>，所以<span class="math">\(α_{t} &lt; 0\)</span>，這樣的<span class="math">\(g_{t}\)</span>具有逆向的貢獻。</p>
<p>最後只要把這些訓練好的<span class="math">\(g_{t}\)</span>乘上各自的<span class="math">\(α_{t}\)</span>再加總起來，我們就完成了AdaBoost啦！</p>
<p><br/></p>
<h3 id="gradient-boost-for-regression">Gradient Boost for Regression</h3>
<p>剛剛我們講了AdaBoost，是個很神奇的方法，當我們做錯了，沒關係！從哪裡跌倒就從哪裡站起來，利用這種精神我們就可以做到Boost的效果，但美中不足的是上面的方法只能用在「分類問題」上，那如果我也想在「Regression問題」也做到Boost呢？這就是接下來要講的GradientBoost的方法。</p>
<p>在課程中林軒田教授是從AdaBoost出發經過推導後，得到一個很像是Gradient Decent的式子，接下來將式子一般化成為可以使用任意Error Measure的形式，我稍微列一下：</p>
<blockquote>
<p>GradientBoost: <span class="math">\(min_{η}\ min_{h}\ (1/N) 𝚺_{n} Error[𝚺_{τ=1}^{τ=t-1} α_{τ} g_{τ}(x_{n}) + η h(x_{n}), y_{n}]\)</span></p>
</blockquote>
<p>我們這邊會考慮Error為平方誤差<span class="math">\((s-y)^{2}\)</span>的結果，詳細的推導這邊就不多加討論，可以到影片中學習，這裡我想要從我觀察出來的觀點，概念性的來看這個GradientBoost的方法。</p>
<p>「從哪裡跌倒就從哪裡站起來」就是Boost的精神，所以今天你有一個Regression問題沒做好，<strong>留下了餘數Residual，怎麼辦？那我就把這個餘數當作另外一個Regression問題來做它</strong>，再把這個結果附到先前的那個就好啦！如果第一次Regression後的Model是<span class="math">\(g_{1}(x)\)</span>，那剩下的沒做好的餘數就應該是<span class="math">\(y(x)-g_{1}(x)\)</span>，我們拿這個餘數下去在做一次Regression得到另外一個Model <span class="math">\(g_{2}(x)\)</span>，此時合併這兩個結果的餘數就變成了<span class="math">\(y(x)-g_{1}(x)-g_{2}(x)\)</span>，就可以使用這個餘數繼續做下去，最後組合所有的<span class="math">\(g_{t}(x)\)</span>就會得到一個更好的Model。</p>
<p><img alt="Gradient Boost" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.013.jpeg"/></p>
<p>依循這樣的概念我們來看GradientBoost作法，如上圖，一開始我們先初始化每一筆Data的預測值<span class="math">\(s_{n}\)</span>為0，再接下來開始產生<span class="math">\(g_{t}\)</span>，我們先把Data的 <span class="math">\(y_{n}\)</span> 減去每一筆Data當前的預測值<span class="math">\(s_{n}\)</span>，就會產生餘數<span class="math">\((y_{n}-s_{n})\)</span>，當然，在一開始<span class="math">\(s_{n}=0\)</span>，所以<span class="math">\(y_{n}-s_{n}=y_{n}\)</span>，等於是對原問題求解。</p>
<p>接下來因為最後我們要線性組合<span class="math">\(g_{t}(x)\)</span>，所以需要決定<span class="math">\(g_{t}(x)\)</span>前面的係數<span class="math">\(α_{t}\)</span>，也就是貢獻度，這個<span class="math">\(α_{t}\)</span>的決定方式是去求解一個One-Variable-Linear-Regression (單變數線性迴歸)，目的是<strong>去縮放<span class="math">\(g_{t}(x)\)</span>使得它更接近剛剛的餘數<span class="math">\((y_{n}-s_{n})\)</span>，而找到這個縮放值就是<span class="math">\(α_{t}\)</span></strong>。所以每一次<span class="math">\(g_{t}(x)\)</span>的產生都是為了可以把G(x)描述的更好，最後<span class="math">\(G(x)=𝚺_{t} α_{t}g_{t}(x)\)</span>。</p>
<p>看到這裡有人一定會認為One-Variable-Linear-Regression求<span class="math">\(α_{t}\)</span>這一步是多餘的，因為在一開始做<span class="math">\(\{x_{n},y_{n}-s_{n}\}\)</span>的Regression中我們已經最佳化過<span class="math">\(g_{t}(x)\)</span>，那為什麼還要把<span class="math">\(g_{t}(x)\)</span>乘上<span class="math">\(α_{t}\)</span>再做同樣的事呢？<span class="math">\(α_{t}\)</span>一定是1的啊！就像我一開始舉的例子一樣啊！其實問題就出在於你把<span class="math">\(g_{t}(x)\)</span>理所當然的看成是線性模型，你才會覺得這一步是多餘的，如果<span class="math">\(g_{t}(x)\)</span>不是線性的，求<span class="math">\(α_{t}\)</span>就很重要的，因為你要使用線性組合來組出<span class="math">\(G(x)\)</span>，但是你的<span class="math">\(g_{t}(x)\)</span>不是線性的，所以你只好在外面再用線性模型來包裝一遍。</p>
<p><br/></p>
<h3 id="adaboosted-decision-treegradient-boosted-decision-tree-gbdt">AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)</h3>
<p><img alt="AdaBoosted and GrandientBoosted DTree" src="http://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.014.jpeg"/></p>
<p>和Random Forest一樣，我們也可以將AdaBoost和GradientBoost套用到Decision Tree上面，<strong>如果是處理分類問題就使用AdaBoosted Decision Tree；那如果是處理Regression問題可以使用Gradient Boosted Decision Tree</strong>。</p>
<p>但要特別注意的是，這邊的Decision Tree都必須是弱的，也就是Pruning過後的樹，如果直接使用完全長成的樹，你會發現在AdaBoosted Decision Tree中，因為<span class="math">\(ε_{t}=0\)</span>所以<span class="math">\(α_{t}→∞\)</span>；在Gradient Boosted Decision Tree中，<span class="math">\(y_{n}-s_{n}→0\)</span>，因為錯誤出現的太少了，所以造成我們不能真正使用到Boost的效果，也就失去做Boost的意義了，<strong>因此在做AdaBoosted Decision Tree或Gradient Boosted Decision Tree時要使用「弱」一點的Decision Tree</strong>。</p>
<p><br/></p>
<h3 id="_1">結語</h3>
<p>這一篇當中，我們完整提了Boost的方法，Boost的精神就是從哪裡跌倒就從哪裡站起來，使用變換Data權重的手法去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的<span class="math">\(g_{t}\)</span>可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善。</p>
<p>我們提了兩種Boost的方法，如果是處理分類問題就使用AdaBoost；如果是處理Regression問題可以使用GradientBoost，而且這兩種方法都可以和Decision Tree做結合。</p>
<p>以上兩回，我們已經完成了Aggregation Models了，接下來的下一回將要探討的就是現今很流行的類神經網路和深度學習等等。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>
<div class="neighbors">
<a class="btn float-left" href="https://ycc.idv.tw/algorithm-complexity-theory.html#anchor" title="輕鬆談演算法的複雜度分界：什麼是P, NP, NP-Complete, NP-Hard問題">
<i class="fa fa-angle-left"></i> Previous Post
    </a>
<a class="btn float-right" href="https://ycc.idv.tw/big-data-a-revolution.html#anchor" title="大數據 Big Data:A Revolution That Will Transform How We Live, Work, and Think">
      Next Post <i class="fa fa-angle-right"></i>
</a>
</div>
<div class="related-posts">
<h4>You might enjoy</h4>
<ul class="related-posts">
<li><a href="https://ycc.idv.tw/ml-course-techniques_3.html">機器學習技法 學習筆記 (3)：Kernel Regression</a></li>
<li><a href="https://ycc.idv.tw/ml-course-techniques_4.html">機器學習技法 學習筆記 (4)：Basic Aggregation Models</a></li>
<li><a href="https://ycc.idv.tw/ml-course-techniques_6.html">機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)</a></li>
<li><a href="https://ycc.idv.tw/ml-course-techniques_7.html">機器學習技法 學習筆記 (7)：Radial Basis Function Network與Matrix Factorization</a></li>
</ul>
</div>
<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'ycnote-1';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
    Please enable JavaScript to view comments.
</noscript>
<!-- End Disqus -->
</article>
<footer>
<p>
  © 2023  - This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" rel="license" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p><p>
<a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license" target="_blank">
<img alt="Creative Commons License" height="15" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" style="border-width:0" title="Creative Commons License" width="80"/>
</a>
</p></footer> </main>
<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " YC Note ",
  "url" : "https://ycc.idv.tw",
  "image": "",
  "description": "YC Note - ML/DL Tech Blog"
}
</script> <script>
    window.loadStorkIndex = function () {
      stork.register("sitesearch", "https://ycc.idv.tw/search-index.st", { showProgress: false });
    }
  </script>
<script src="https://files.stork-search.net/releases/v1.5.0/stork.js"></script>
</body>
</html>