<!DOCTYPE html>
<html lang="zh">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="前言 在上一回中，我們已經了解了機器學習在理論上有怎樣的條件才可以達成，所以接下來我們就可以正式的來看有哪一些機器學習的方法。 在這一篇中，我會帶大家初探：機器可以怎麼樣學習? 內容包括：Gradient Descent、Linear Regression、Logistic Regression、使用迴歸法做二元分類問題等等。 Gradient Descent（梯度下降）...">
        <meta name="keywords" content="機器學習基石">
        <link rel="icon" href="./static/img/favicon.png">

        <title>機器學習基石 學習筆記 (3)：機器可以怎麼樣學習? - YC Note</title>

        <!-- Stylesheets -->
        <link href="./theme/css/all.min.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->



    </head>

    <body>

        <!-- Header -->
    <div class="header-container" style="background: linear-gradient(rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0.2)), url('./images/ai_front_board.jpg'); background-position: center; background-size: cover;">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="./"><img class="logo" src="./static/img/favicon.png" alt="logo">YC Note</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="./category/coding.html">Coding</a>
                                <a href="./category/aiml.html">AI.ML</a>
                                <a href="./category/reading.html">Reading</a>
                                <a href="./category/recording.html">Recording</a>
                                <a href="./about-me.html">About Me</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">機器學習基石 學習筆記 (3)：機器可以怎麼樣學習?</h1>
                      <p class="header-date">By <a href="./author/yc-chen.html">YC Chen</a>, 2016 / 8月 07, in category <a href="./category/aiml.html">AI.ML</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="./tag/ji-qi-xue-xi-ji-shi.html">機器學習基石</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <h3>前言</h3>
<p>在上一回中，我們已經了解了機器學習在理論上有怎樣的條件才可以達成，所以接下來我們就可以正式的來看有哪一些機器學習的方法。</p>
<p>在這一篇中，我會帶大家初探：<strong>機器可以怎麼樣學習?</strong> 內容包括：Gradient Descent、Linear Regression、Logistic Regression、使用迴歸法做二元分類問題等等。</p>
<p><br/></p>
<h3>Gradient Descent（梯度下降）</h3>
<p><img alt="ML" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.009.jpeg"></p>
<p>還記得上一回我們歸納出了一套ML的流程，複習一下</p>
<ol>
<li>準備好足夠的數據</li>
<li>把Model建立好，d<sub>VC</sub>必須要是有限的，而且大小要適中</li>
<li>定義好評估E<sub>in</sub>的Error Measurement</li>
<li>使用演算法找出最佳參數把E<sub>in</sub>降低</li>
<li>最後評估一下是否有Overfitting的狀況，確保E<sub>in</sub> ≈ E<sub>out</sub></li>
</ol>
<p>請容許我先不管Model這部份該怎麼建立，我們先來看如何找到最佳參數這部份，<strong>假設今天我知道E<sub>in</sub>的評估方法，我該如何找到最佳的參數來使得E<sub>in</sub>更小？有一套普遍的方法叫做Gradient Descent</strong>，很強大，甚至連現今流行的「深度學習」找最佳解的機制也是從Gradient Descent衍生出來的。</p>
<p>想像一下你是一位登山客，你在爬一座由E<sub>in</sub>所決定的高山，你的目標是去這座山最低的山谷，也就是E<sub>in</sub>最小的地方，因為村莊正在那裡，但是很不幸的你沒有地圖，這個時候有什麼方法可以知道低谷在哪裡呢？答案是就一直下坡吧！反正我知道村莊在山谷裡，那我就一路下山應該就可以找到村莊了，這就是Gradient Descent的精髓。</p>
<p>在數學上有一個衡量函數變化的東西，這就是Gradient（梯度），Gradient是一個向量，它的「方向」指向函數值增加量最大的方向，而它的「大小」反應這個變化有多大，其實就是一次微分啦！只不過Gradient推廣到高維度而已。所以我們和這個登山客做一樣的事情，我們朝著下降最多的方向前進，這就是Gradient Descent（梯度下降法），我剛剛說了，梯度是指向函數值增加量最大的方向，那顯然我們往反方向走就可以達到最大下降，所以如果我們有一個Error函數E<sub>in</sub>，它的Gradient就是∇E<sub>in</sub>，那我們的下降方向就是-∇E<sub>in</sub>。</p>
<p>來看一下上圖中Gradient Descent的流程，</p>
<ol>
<li>定義出Error函數</li>
<li>Error函數讓我們可以去評估E<sub>in</sub></li>
<li>算出它的梯度∇E<sub>in</sub></li>
<li>朝著∇E<sub>in</sub>的反方向更新參數W，而每次只跨出η大小的一步</li>
<li>反覆的計算新參數W的梯度，並一再的更新參數W</li>
</ol>
<p>這邊要特別注意，流程中的第四項中，有提到η，<strong>η稱為Learning Rate，它影響的是更新步伐的大小</strong>，η的選擇要適當，如果η太小的時候，我們可能要花很多時間才可以走到低點，但如果η太大的話，又可能導致我們在兩個山腰間跳來跳去，甚至越更新越往高處跑，<strong>所以選擇適當的η相當的重要，所以下次如果你發現E<sub>in</sub>一直降不下來甚至在增大，試著將η減小看看</strong>。另外η也可以是變動的值，我們可以直接設η＝|∇E<sub>in</sub>|，這麼一來遇到陡坡的時候它就會跨大一點的步伐，遇到緩坡的時候就會跨小步一點，隨狀況調整η的值。</p>
<p>Gradient Descent (GD, 梯度下降) 有兩個變形，分別為Stochastic Gradient Descent (SGD, 隨機梯度下降) 和 Batch Gradient Descent (BGD, 批次梯度下降)，這差別只在於評估∇E<sub>in</sub>的時候所考慮的Data數量，正常來說必須要考慮所有的Data，我們才會得到真正的E<sub>in</sub>，才有辦法算出正確的∇E<sub>in</sub>，但這樣所要付出的代價就是較大的計算量。</p>
<p>所以<strong>Stochastic Gradient Descent的作法是一次只拿一筆Data來求E<sub>in</sub>'，並且更新參數W</strong>，這樣的更新方法顯然會比較不穩定，但我們假設，經過好幾輪的更新後，已經完整看過整個數據了，所以平均來說效果和一般的Gradient Descent一樣。</p>
<p>另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，<strong>Batch Gradient Descent一次評估k組數據，並更新參數W</strong>，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的計算方法你可以想像成在做矩陣計算，矩陣元素在計算的時候往往是可以拆開計算的，此時GPU利用它強大的平行化運算將這些元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，<strong>所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Batch Gradient Descent，是既有效率又穩定的作法</strong>。</p>
<p>Gradient Descent求最佳解其實是會產生問題的，還記得我們的目標嗎？我們希望可以走到最低點的山谷裡，所以我們採取的策略是不斷的下降，這個時候如果遇到兩種情形就會動彈不得，</p>
<ol>
<li>小山谷，數學上稱為<strong>Local Minimum</strong>，雖然在那點看起來，那邊的確是低點，但卻不是整個E<sub>in</sub>的最低點</li>
<li>平原，數學上稱為<strong>Saddle Point（鞍點）</strong>，在一片很平的區域，∇E<sub>in</sub>=0，所以就停止不動了</li>
</ol>
<p>針對這些問題有一些改良後的演算法，在這裡不詳述，請參考<a href="http://ruder.io/optimizing-gradient-descent/">S. Ruder的整理</a>。</p>
<p>好！我們已經了解了怎麼使用Gradient Descent去找到E<sub>in</sub>最小的最佳參數，那我們可以回頭看Model有哪一些？Error Measure該怎麼定？</p>
<p><br/></p>
<h3>Linear Regression</h3>
<p><img alt="ML" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.010.jpeg"></p>
<p>先從最簡單的看起，那就是線性迴歸（Linear Regression），假設今天我要用三種變數(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>)來建立一個簡單的線性模型，那就是</p>
<p>w<sub>0</sub>+w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>+w<sub>3</sub>x<sub>3</sub>，</p>
<p>這個又稱為Score，標為s，為了方便起見，我們會額外增加x<sub>0</sub>=1的參數，這麼一來Score就可以寫成矩陣形式</p>
<p>s = w<sub>0</sub>x<sub>0</sub>+w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>+w<sub>3</sub>x<sub>3</sub>=W<sup>T</sup>x</p>
<p>W = [w<sub>0</sub>, w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>]</p>
<p>x = [x<sub>0</sub>=1, x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>]</p>
<p>在線性模型中，這個 s 就正好是我們Model預測的 y，通常我們會把預測得來的 y 記作 ŷ (y hat)，如果今天這個 y 和 ŷ 是實數的話，那這就是一個標準的Linear Regression問題，那如何去衡量預測的好或不好呢？<strong>我們可以使用Squared Error來衡量，err(ŷ,y)=(ŷ-y)<sup>2</sup></strong>，所以 ŷ 和 y 越靠近Error就越小。</p>
<p>Squared Error的E<sub>in</sub>平面比較簡單，就是一個單純的開口向上的拋物線，所以它的最低點其實是有解析解的，我們可以靠著數學上的<strong>Pseudo-Inverse方法</strong>在評估完全部的Data之後把最佳參數給算出來，這麼簡單的E<sub>in</sub>平面是很難見到的，我們之前介紹的Gradient Descent則是靠著逐步更新的方式去尋找近似解，這個方法是不管E<sub>in</sub>平面有多麼複雜都可以處理，但是需要特別注意別卡在Local Minimum和Saddle Point。</p>
<p><br/></p>
<h3>Logistic Regression</h3>
<p><img alt="ML" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.011.jpeg"></p>
<p>在上一回討論二元分類問題時，我們考慮的狀況是「沒有雜訊」的情形，不過在實際情況下，「雜訊」是一定需要考慮的。在「沒有雜訊」的情形下，一筆Data只會有一個確定的答案，<strong>如果考慮「雜訊」，一筆Data有可能有多個答案，呈現機率分布</strong>，對於正確答案的機率也許會高一點，但因為雜訊的干擾的原因並非能百分之一百的出現正確答案。</p>
<p>在二元分類的答案因為雜訊出現了機率分布，可能會產生像下面一樣的情況，</p>
<p>ℙ(◯|X<sup>1</sup>) = 0.9 ;   ℙ(✕|X<sup>1</sup>) = 0.1</p>
<p>而之前PLA的分類方法是屬於非黑及白的，這種分類法我們稱為Hard Classification，並不能描述這種機率分布，所以我們來考慮另外一種分類法，稱之為Soft Classification。</p>
<p><strong>Soft Classification看待每個答案不是非黑及白的，而是去評估每個答案出現的機會有多大，以此作為分類</strong>，我們打算使用Regression的連續特性來產生Soft Classification，我們需要引入一個重要的函數—Logistic Function，這個函數可以將所有實數映射到0到1之間，如上圖下方中間的圖示所示，<strong>Logistic Function會將極大的值映射成1，而將極小值映射成0，這個0到1的值剛剛好可以拿來當作機率的大小</strong>。</p>
<p>所以我們就可以來建立一個有機率概念的模型，這個Model的預測值是一個機率，一樣的先給予輸入變數x權重W求出Score s，再把 s 放到Logistic Function當中，我們就可以映射出在一個機率空間，我們藉由調整W來改變Model以描述我們的Data，有了這個新的Model，我們就可以用機率的方式來描述二元分類，</p>
<p>ℙ(◯|X<sup>1</sup>) = Θ(s) ;   ℙ(✕|X<sup>1</sup>) = 1 - Θ(s) = Θ(-s)</p>
<p>OK! 決定好Model，我們就可以來定義它的Error Measurement的方式了，這個時候如果使用Squared Error來作為Error Measurement你會發現這種評估方式有一點失焦了，我們並不是要將雜訊給放進去Model之中，而是要在考慮雜訊之下盡可能的去描述數據背後真正的機制。</p>
<p>所以我們來探討一下「可能性」，在考慮採樣數據過程因為雜訊造成的機率分布的前提下，我們去看會採樣到這組Data的可能性，我們應該合理的認為採樣出來的這組Data應該具有最大的「可能性」，這個「可能性」可以表示成</p>
<p>Assume ◯ ≡ (y=+1) and ✕ ≡ (y=-1)</p>
<p>ℙ(likelihood of ◯) = ℙ(x<sup>1</sup>)Θ(y<sup>1</sup>×s<sup>1</sup>) × ℙ(x<sup>2</sup>)Θ(y<sup>2</sup>×s<sup>2</sup>) × … × ℙ(x<sup>N</sup>)Θ(y<sup>N</sup>×s<sup>N</sup>)</p>
<p><strong>所以我們需要設計一組Error Measurement，使得Error降低的同時可以使得ℙ of likelihood可以增大，這個Error Measurement就是Cross-Entropy，Error<sub>ce</sub>=ln[1+exp(-ys)]。</strong></p>
<p>來推導一下Cross-Entropy怎麼來的，</p>
<p>Max. ℙ(likelihood of ◯) </p>
<p>= Max. Θ(y<sup>1</sup>×s<sup>1</sup>) × Θ(y<sup>2</sup>×s<sup>2</sup>) × … × Θ(y<sup>N</sup>×s<sup>N</sup>)</p>
<p>= Min. 𝚺 -ln[Θ(y<sup>n</sup>×s<sup>n</sup>)]</p>
<p>= Min. 𝚺 ln[1+exp(-y<sup>n</sup>×s<sup>n</sup>)]</p>
<p>= Min. 𝚺 Error<sub>ce, n</sub></p>
<p><strong>我們可以使用Gradient Descent來降低Cross-Entropy，這又稱為Logistic Regression，在這個問題中就沒有簡單的解析解可以直接算，只能使用近似解來處理。</strong></p>
<p><br/></p>
<h3>使用迴歸法做二元分類問題</h3>
<p><img alt="ML" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.012.jpeg"></p>
<p>剛剛介紹了Logistic Regression，我們可以使用Regression方式來做二元分類問題，我們來看一下實際上該怎麼做？</p>
<p>線性模型的標準方法，我們會將變數x做線性組合得到Linear Scoring Function — s，線性組合的係數和Threshold稱為權重W，我們可以調整權重W來改變Model，那針對看待s的不同方式就衍生出不同的方法。那為了可以將Regression問題轉換成二元分類問題，所以通常我們會假設(y=+1)為◯，(y=-1)為✕。</p>
<p>先回顧一下之前<a href="http://www.ycc.idv.tw/ml-course-foundations_1.html">PLA的作法</a>，我們把 <code>s&gt;0</code> 的狀況視為◯，也就是(y=+1)；然後把 <code>s&lt;0</code> 的狀況視為✕，也就是(y=-1)，把這個概念畫成上圖右側的圖，圖中藍色的階梯函數就是PLA的Error Measurement，正是因為它是一個階梯函數，所以我們不能使用Gradient Descent等Regression方法來處理，<strong>因為在階梯的每一點∇E<sub>in</sub>都是0（除了原點外），也就是如此PLA在更新的過程才無法確保趨近於最佳解，而需要使用Pocket PLA來解決這個問題</strong>。</p>
<p>那如果我們用Linear Regression來做這件事呢？我們把Squared Error畫在上圖右側小圖的紅線，你會發現它的低點會落在ys=1的地方，這應該不是我們要的結果，雖然它一樣可以把錯誤的判斷修正回正確，但是面對過度確定的正確答案，它反而會去修正它往錯誤的方向，很顯然這不是我們想要的。</p>
<p>最好的方式就是Logistic Regression了，我們將s做Logistic Function的轉換，轉換成機率，並在評估最大化Likelihood的條件下定義出Cross-Entropy來當作Error Measurement，在上圖右側的小圖，我們稍微調整Cross-Entropy，使得它的Error Function可以在ys=0的地方和Squared Error相切，<strong>這張圖告訴我們的是隨著Grandient Descent每次的更新，Logistic Regression會把分類做的越來越好，把◯和✕拉的更遠</strong>。</p>
<p><br /></p>
<h3>後話</h3>
<p>在這一篇當中，我們介紹了Grandient Descent這一個相當重要的演算法，並且運用在兩種Regression上：Linear Regression和Logistic Regression，Linear Regression是最簡單的Regression方法，甚至它還可以使用Pseudo-Inverse的方法直接算出最佳解，Logistic Regression考慮了有雜訊的Data產生的機率分布，我們可以用Logistic Regression做Soft Binary Classification，而且我們也說明了Logistic Regression為何適合拿來用在二元分類上。本篇我們對於ML的實際作法有了基本認識，在下一篇，我們繼續討論還有沒有什麼方式可以讓ML做的更好。</p>


        <br/><br/>

<div id="disqus_thread"></div>
<script type="text/javascript">
/* <![CDATA[ */

    var disqus_shortname = 'ycnote-1';
    var disqus_identifier = "ml-course-foundations_3.html";

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
/* ]]> */
</script>
<noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



        
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="./archives.html">Archives</a></li>
                            <li><a href="./tags.html">Tags</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Contact Me</div>
                        <ul class="list-unstyled">
                            <li><a href="./about-me.html" target="_blank">About Me</a></li>
                            <li><a href="https://github.com/GitYCC" target="_blank">Github</a></li>
                            <li><a href="mailto:ycc.tw.email@gmail.com" target="_blank">Email</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; YC Note 2018</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>