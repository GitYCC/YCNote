
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="True" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="index, follow" name="robots"/>
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&amp;family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&amp;display=swap" rel="stylesheet"/>
<link href="https://ycc.idv.tw/theme/stylesheet/style.less" rel="stylesheet/less" type="text/css"/>
<script src="//cdnjs.cloudflare.com/ajax/libs/less.js/2.5.1/less.min.js" type="text/javascript"></script>
<link href="https://ycc.idv.tw/theme/pygments/default.min.css" id="pygments-light-theme" rel="stylesheet" type="text/css"/>
<link href="https://ycc.idv.tw/theme/stork/stork.css" rel="stylesheet" type="text/css">
<link href="https://ycc.idv.tw/theme/font-awesome/css/fontawesome.css" rel="stylesheet" type="text/css"/>
<link href="https://ycc.idv.tw/theme/font-awesome/css/brands.css" rel="stylesheet" type="text/css"/>
<link href="https://ycc.idv.tw/theme/font-awesome/css/solid.css" rel="stylesheet" type="text/css"/>
<link href="/images/favicon.png" rel="shortcut icon" type="image/x-icon"/>
<link href="/images/favicon.png" rel="icon" type="image/x-icon"/>
<!-- Chrome, Firefox OS and Opera -->
<meta content="#FFFFFF" name="theme-color"/>
<!-- Windows Phone -->
<meta content="#FFFFFF" name="msapplication-navbutton-color"/>
<!-- iOS Safari -->
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"/>
<!-- Microsoft EDGE -->
<meta content="#FFFFFF" name="msapplication-TileColor"/>
<link href="https://ycc.idv.tw/feeds/all.atom.xml" rel="alternate" title="YC Note Atom" type="application/atom+xml"/>
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68393177-2', 'auto');
  ga('send', 'pageview');
</script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-LXDD9FZFX2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LXDD9FZFX2');
</script>
<meta content="YC Chen" name="author">
<meta content="Gradient Descent / Linear Regression / Logistic Regression / 使用迴歸法做二元分類問題" name="description">
<meta content="機器學習基石" name="keywords"/>
<meta content="YC Note" property="og:site_name">
<meta content="機器學習基石 學習筆記 (3)：機器可以怎麼樣學習?" property="og:title">
<meta content="Gradient Descent / Linear Regression / Logistic Regression / 使用迴歸法做二元分類問題" property="og:description">
<meta content="en_US" property="og:locale">
<meta content="https://ycc.idv.tw/ml-course-foundations_3.html" property="og:url"/>
<meta content="article" property="og:type"/>
<meta content="2016-08-07 12:00:00+08:00" property="article:published_time"/>
<meta content="" property="article:modified_time"/>
<meta content="https://ycc.idv.tw/author/yc-chen.html" property="article:author"/>
<meta content="AI.ML" property="article:section">
<meta content="機器學習基石" property="article:tag"/>
<meta content="" property="og:image"/>
<title>YC Note – 機器學習基石 學習筆記 (3)：機器可以怎麼樣學習?</title>
<script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-5639899546876072",
      enable_page_level_ads: true
    });
  </script>
</meta></meta></meta></meta></meta></meta></meta></link><link href="https://ycc.idv.tw/ml-course-foundations_3.html" rel="canonical"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "BreadcrumbList", "itemListElement": [{"@type": "ListItem", "position": 1, "name": "YC Note", "item": "https://ycc.idv.tw"}, {"@type": "ListItem", "position": 2, "name": "Ml course foundations_3", "item": "https://ycc.idv.tw/ml-course-foundations_3.html"}]}</script><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "author": {"@type": "Person", "name": "YC Chen"}, "publisher": {"@type": "Organization", "name": "YC Note"}, "headline": "機器學習基石 學習筆記 (3)：機器可以怎麼樣學習?", "about": "AI.ML", "datePublished": "2016-08-07 12:00"}</script></head>
<body class="light-theme">
<aside>
<div>
<a href="https://ycc.idv.tw/">
<img alt="YC Note" src="https://ycc.idv.tw/theme/img/profile.png" title="YC Note"/>
</a>
<h1>
<a href="https://ycc.idv.tw/">YC Note</a>
</h1>
<p style="text-align: center;">ML/DL Tech Blog (Total Views: 571,238) </p>
<div class="stork">
<input autocomplete="off" class="stork-input" data-stork="sitesearch" name="q" onclick="loadStorkIndex(this); this.onclick=null;" placeholder="Search (beta feature) ..." type="text"/>
<div class="stork-output" data-stork="sitesearch-output"></div>
</div>
<!-- <script>
      window.addEventListener('load', 
        function() { 
          loadStorkIndex();
        }, false);
    </script> -->
<p>Hello, I am YC, an ML engineer/researcher with experience in CV, NLP/NLU, and Recommender. I also have experience in high-QPS ML systems. In my spare time, I'm a blogger and guitar singer. <a href="https://ycc.idv.tw/about-me.html#anchor" style="color:yellow">More about me.</a></p>
<p>This blog is a resource for anyone interested in data science and machine learning, featuring tutorials, research papers, and the latest industry technologies.</p>
<ul class="social">
<li>
<a class="sc-facebook" href="https://www.facebook.com/yc.note" target="_blank">
<i class="fa-brands fa-facebook"></i>
</a>
</li>
<li>
<a class="sc-github" href="https://github.com/GitYCC" target="_blank">
<i class="fa-brands fa-github"></i>
</a>
</li>
<li>
<a class="sc-linkedin" href="https://www.linkedin.com/in/yi-chang-chen-aba1b6114/" target="_blank">
<i class="fa-brands fa-linkedin"></i>
</a>
</li>
</ul>
</div>
</aside>
<main>
<nav id="anchor">
<a href="https://ycc.idv.tw/">Home</a>
<a href="/about-me.html#anchor">About Me</a>
<a href="/category/aiml.html#anchor">AI.ML</a>
<a href="/category/cs.html#anchor">CS</a>
<a href="/categories.html#anchor">Categories</a>
<a href="/tags.html#anchor">Tags</a>
<a href="https://ycc.idv.tw/feeds/all.atom.xml">Atom</a>
</nav>
<article class="single">
<header>
<h1 id="ml-course-foundations_3">機器學習基石 學習筆記 (3)：機器可以怎麼樣學習?</h1>
<p>
      Posted on August 07, 2016 in <a href="https://ycc.idv.tw/category/aiml.html">AI.ML</a>. View: 7,667

    </p>
</header>
<div class="tag-cloud">
<p>
<a href="https://ycc.idv.tw/tag/ji-qi-xue-xi-ji-shi.html">機器學習基石</a>
</p>
</div>
<script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle ads-responsive" data-ad-client="ca-pub-5639899546876072" data-ad-slot="5718861428"></ins>
<script>
      (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
<div class="main-contents">
<h3 id="_1">前言</h3>
<p>在上一回中，我們已經了解了機器學習在理論上有怎樣的條件才可以達成，所以接下來我們就可以正式的來看有哪一些機器學習的方法。</p>
<p>在這一篇中，我會帶大家初探：<strong>機器可以怎麼樣學習?</strong> 內容包括：Gradient Descent、Linear Regression、Logistic Regression、使用迴歸法做二元分類問題等等。</p>
<p><br/></p>
<h3 id="gradient-descent">Gradient Descent（梯度下降）</h3>
<p><img alt="ML" src="/media/MachineLearningFoundations/MachineLearningFoundations.009.jpeg"/></p>
<p>還記得上一回我們歸納出了一套ML的流程，複習一下</p>
<ol>
<li>準備好足夠的數據</li>
<li>把Model建立好，<span class="math">\(d_{VC}\)</span>必須要是有限的，而且大小要適中</li>
<li>定義好評估<span class="math">\(E_{in}\)</span>的Error Measurement</li>
<li>使用演算法找出最佳參數把<span class="math">\(E_{in}\)</span>降低</li>
<li>最後評估一下是否有Overfitting的狀況，確保<span class="math">\(E_{in} \approx E_{out}\)</span></li>
</ol>
<p>請容許我先不管Model這部份該怎麼建立，我們先來看如何找到最佳參數這部份，<strong>假設今天我知道<span class="math">\(E_{in}\)</span>的評估方法，我該如何找到最佳的參數來使得<span class="math">\(E_{in}\)</span>更小？有一套普遍的方法叫做Gradient Descent</strong>，很強大，甚至連現今流行的「深度學習」找最佳解的機制也是從Gradient Descent衍生出來的。</p>
<p>想像一下你是一位登山客，你在爬一座由<span class="math">\(E_{in}\)</span>所決定的高山，你的目標是去這座山最低的山谷，也就是<span class="math">\(E_{in}\)</span>最小的地方，因為村莊正在那裡，但是很不幸的你沒有地圖，這個時候有什麼方法可以知道低谷在哪裡呢？最簡單的答案就是一直下坡就對啦！反正我知道村莊在山谷裡，那我就一路下山應該就可以找到村莊了，這就是Gradient Descent的精髓。</p>
<p>在數學上有一個衡量函數變化的東西，這就是Gradient（梯度），Gradient是一個向量，它的「方向」指向函數值增加量最大的方向，而它的「大小」反應這個變化有多大，其實就是一次微分啦！只不過Gradient推廣到高維度而已。所以我們和這個登山客做一樣的事情，我們朝著下降最多的方向前進，這就是Gradient Descent（梯度下降法），我剛剛說了，梯度是指向函數值增加量最大的方向，那顯然我們往反方向走就可以達到最大下降，所以如果我們有一個Error函數<span class="math">\(E_{in}\)</span>，它的Gradient就是<span class="math">\(\nabla E_{in}\)</span>，那我們的下降方向就是<span class="math">\(- \nabla E_{in}\)</span>。</p>
<p>來看一下上圖中Gradient Descent的流程，</p>
<ol>
<li>定義出Error函數</li>
<li>Error函數讓我們可以去評估<span class="math">\(E_{in}\)</span></li>
<li>算出它的梯度<span class="math">\(\nabla E_{in}\)</span></li>
<li>朝著<span class="math">\(\nabla E_{in}\)</span>的反方向更新參數W，而每次只跨出<span class="math">\(η\)</span>大小的一步</li>
<li>反覆的計算新參數<span class="math">\(W\)</span>的梯度，並一再的更新參數<span class="math">\(W\)</span></li>
</ol>
<p>這邊要特別注意，流程中的第四項中，有提到<span class="math">\(η\)</span>，<strong><span class="math">\(η\)</span>稱為Learning Rate，它影響的是更新步伐的大小</strong>，<span class="math">\(η\)</span>的選擇要適當，如果<span class="math">\(η\)</span>太小的時候，我們可能要花很多時間才可以走到低點，但如果<span class="math">\(η\)</span>太大的話，又可能導致我們在兩個山腰間跳來跳去，甚至越更新越往高處跑，<strong>所以選擇適當的<span class="math">\(η\)</span>相當的重要，所以下次如果你發現<span class="math">\(E_{in}\)</span>一直降不下來甚至在增大，試著將<span class="math">\(η\)</span>減小看看</strong>。另外<span class="math">\(η\)</span>也可以不是定值，我們可以直接設<span class="math">\(η＝|\nabla E_{in}|\)</span>，這麼一來遇到陡坡的時候它就會跨大一點的步伐，遇到緩坡的時候就會跨小步一點，隨狀況調整<span class="math">\(η\)</span>的值。</p>
<p>Gradient Descent (GD, 梯度下降) 有兩個變形，分別為Stochastic Gradient Descent (SGD, 隨機梯度下降) 和 Batch Gradient Descent (BGD, 批次梯度下降)，這差別只在於評估<span class="math">\(\nabla E_{in}\)</span>的時候所考慮的Data數量，正常來說必須要考慮所有的Data，我們才會得到真正的<span class="math">\(E_{in}\)</span>，才有辦法算出正確的<span class="math">\(\nabla E_{in}\)</span>，但這樣所要付出的代價就是較大的計算量。</p>
<p>所以<strong>Stochastic Gradient Descent的作法是一次只拿一筆Data來求<span class="math">\(\nabla E_{in}\)</span>，並且更新參數<span class="math">\(W\)</span></strong>，這樣的更新方法顯然會比較不穩定，但我們假設，經過好幾輪的更新後，已經完整看過整個數據了，所以平均來說效果和一般的Gradient Descent一樣。</p>
<p>另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，<strong>Batch Gradient Descent一次評估k組數據，並更新參數W</strong>，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的計算方法你可以想像成在做矩陣計算，矩陣元素在計算的時候往往是可以拆開計算的，此時GPU利用它強大的平行化運算將這些元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，<strong>所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Batch Gradient Descent，是既有效率又穩定的作法</strong>。</p>
<p>Gradient Descent求最佳解其實不是完美的，還記得我們的目標嗎？我們希望可以走到最低點的山谷裡，所以我們採取的策略是不斷的下降，這個時候如果遇到兩種情形就會導致還沒到山谷就已經動彈不得，</p>
<ol>
<li>小山谷，數學上稱為<strong>Local Minimum</strong>，雖然在那點看起來，那邊的確是相對的低點，所以<span class="math">\(\nabla E_{in}=0\)</span>，但卻不是整個<span class="math">\(E_{in}\)</span>的最低點，但也因為<span class="math">\(\nabla E_{in}=0\)</span>的關係，更新就不會再進行。</li>
<li>平原，數學上稱為<strong>Saddle Point（鞍點）</strong>，在一片很平的區域，<span class="math">\(\nabla E_{in}=0\)</span>，所以就停止不動了。</li>
</ol>
<p>針對這些問題有一些改良後的演算法，在這裡不詳述，請參考<a href="http://ruder.io/optimizing-gradient-descent/">S. Ruder的整理</a>。</p>
<p>好！我們已經了解了怎麼使用Gradient Descent去找到<span class="math">\(E_{in}\)</span>最小的最佳參數，那我們可以回頭看Model有哪一些？Error Measure該怎麼定？</p>
<p><br/></p>
<h3 id="linear-regression">Linear Regression</h3>
<p><img alt="ML" src="/media/MachineLearningFoundations/MachineLearningFoundations.010.jpeg"/></p>
<p>先從最簡單的看起，那就是線性迴歸（Linear Regression），假設今天我要用三種變數<span class="math">\((x_1, x_2, x_3)\)</span>來建立一個簡單的線性模型，那就是</p>
<div class="math">$$
w_0+w_1 x_1+w_2 x_2+w_3 x_3
$$</div>
<p>這個又稱為Score，標為<span class="math">\(s\)</span>，為了方便起見，我們會額外增加<span class="math">\(x_0=1\)</span>的參數，這麼一來Score就可以寫成矩陣形式</p>
<div class="math">$$
s = w_0 x_0+w_1 x_1+w_2 x_2+w_3 x_3=W^T x
$$</div>
<div class="math">$$
where: W = [w_0, w_1, w_2, w_3], x = [x_0=1, x_1, x_2, x_3]
$$</div>
<p>
在線性模型中，這個 s 就正好是我們Model預測的值，通常我們會把預測得來的 <span class="math">\(y\)</span> 記作<span class="math">\(\widehat{y}\)</span> (y hat)，如果今天這個 y 和 ŷ 是實數的話，那這就是一個標準的Linear Regression問題，那如何去衡量預測的好或不好呢？<strong>我們可以使用Squared Error來衡量，<span class="math">\(err(\widehat{y},y)=(\widehat{y}-y)^2\)</span></strong>，所以 <span class="math">\(\widehat{y}\)</span> 和 <span class="math">\(y\)</span> 越靠近Error就越小。</p>
<p>Squared Error的<span class="math">\(E_{in}\)</span>平面是一個單純的開口向上的拋物線，所以它的最低點其實是有解析解的，我們可以靠著數學上的<strong>Pseudo-Inverse方法</strong>把最佳參數給算出來，但是Pseudo-Inverse計算非常龐大，當數據量很大時這個方法是不可行的，而剛剛介紹的Gradient Descent計算複雜度只有<span class="math">\(O(N)\)</span>。</p>
<p><br/></p>
<h3 id="logistic-regression">Logistic Regression</h3>
<p><img alt="ML" src="/media/MachineLearningFoundations/MachineLearningFoundations.011.jpeg"/></p>
<p>在上一回討論二元分類問題時，我們的評估模型都是非黑即白的</p>
<p>在上一回討論二元分類問題時，我們考慮的狀況是「沒有雜訊」的情形，不過在實際情況下，「雜訊」是一定需要考慮的。在「沒有雜訊」的情形下，一筆Data只會有一個確定的答案要嘛是 <span class="math">\(- 1\)</span> 不然就是 <span class="math">\(+ 1\)</span>，<strong>如果考慮「雜訊」，一筆Data出現的答案可能呈現機率分布</strong>，介於 <span class="math">\(- 1\)</span> 和 <span class="math">\(+ 1\)</span> 之間，舉例可能會產生像下面一樣的情況，
</p>
<div class="math">$$
\mathbb{P}(◯|X^{(1)}) = 0.9,\  \mathbb{P}(✕|X^{(1)}) = 0.1
$$</div>
<p>之前PLA的分類方法是屬於非黑及白的，預測的結果不存在模糊地帶，這種分類法我們稱為Hard Classification，這種分類法並不能描述機率分布，所以我們來考慮另外一種分類法，稱之為Soft Classification。</p>
<p><strong>Soft Classification看待每個答案不是非黑及白的，而是去評估每個答案出現的機會有多大，以此作為分類</strong>，我們打算使用Regression的連續特性來產生Soft Classification，我們需要引入一個重要的函數—Logistic Function，這個函數可以將所有實數映射到0到1之間，如上圖下方中間的圖示所示，<strong>Logistic Function會將極大的值映射成1，而將極小值映射成0，這個0到1的值剛剛好可以拿來當作機率的大小</strong>。</p>
<p>所以我們就可以來建立一個有機率概念的模型，這個Model的預測值是一個機率，一樣的先給予輸入變數<span class="math">\(x\)</span>和權重<span class="math">\(W\)</span>求出Score <span class="math">\(s\)</span>，再把 <span class="math">\(s\)</span> 放到Logistic Function當中，我們就可以映射出在一個機率空間，我們藉由調整<span class="math">\(W\)</span>來改變Model來擬合我們的Data，有了這個新的Model，我們就可以用機率的方式來描述二元分類，</p>
<div class="math">$$
\mathbb{P}(◯|X^{(1)}) = Θ(s)
$$</div>
<div class="math">$$
\mathbb{P}(✕|X^{(1)}) = 1 - Θ(s) = Θ(-s)
$$</div>
<div class="math">$$
where:\ Θ(s)=1/[1+e^{-s}]
$$</div>
<p>OK! 決定好Model，我們就可以來定義它的Error Measurement的方式了，這個時候如果使用Squared Error來作為Error Measurement你會發現這種評估方式有一點失焦了，如果採用Squared Error，我們做的事是將機率的值給擬合精準，但我們知道這個機率的產生是來自於雜訊，預測雜訊是沒有意義的，要做的事應該是要在考慮雜訊之下盡可能提升模型會產生取樣資料的可能機率。</p>
<p>這就是Max Likelihood的概念，</p>
<div class="math">$$
\mathbb{P}(likelihood) = \mathbb{P}(x^{(1)})\mathbb{P}(◯|x^{(1)},H) \times \mathbb{P}(x^{(2)})\mathbb{P}(✕|x^{(2)},H) \times … \times \mathbb{P}(x^{(N)})\mathbb{P}(◯|x^{(N)},H)
$$</div>
<p>我們的任務就是找一個function set <span class="math">\(H\)</span>使得我可以最大化likelihood，
</p>
<div class="math">$$
argmax_{({H})} \mathbb{P}(likelihood)
$$</div>
<div class="math">$$
=argmax_{({H})} \sum_{y^{(n)}\ is\ ◯} \{ln[\mathbb{P}(x^{(n)})]+ln[\mathbb{P}(◯|x^{(n)},H)] \}+ \sum_{y^{(n)}\ is\ ✕} \{ln[\mathbb{P}(x^{(n)})]+ln[\mathbb{P}(✕|x^{(n)},H)]\}
$$</div>
<div class="math">$$
=argmax_{({H})} \sum_{y^{(n)}\ is\ ◯} ln[\mathbb{P}(◯|x^{(n)},H)] + \sum_{x^{(n)}\ is\ ✕} ln[\mathbb{P}(✕|y^{(n)},H)]
$$</div>
<div class="math">$$
=argmin_{({H})} \sum_{y^{(n)}\ is\ ◯} -ln[\mathbb{P}(◯|x^{(n)},H)] + \sum_{y^{(n)}\ is\ ✕} -ln[\mathbb{P}(✕|x^{(n)},H)]
$$</div>
<p>假設 <span class="math">\(◯ \equiv  (y=+1)\)</span> and <span class="math">\(✕ \equiv  (y=0)\)</span>，則上式可以化簡，得
</p>
<div class="math">$$
=argmin_{({H})} \sum_{n} -y^{(n)}ln\mathbb{P}(◯|x^{(n)},H) -(1-y^{(n)})ln[1-\mathbb{P}(◯|x^{(n)},H)]
$$</div>
<p>其中<span class="math">\(E_{ce}=-yln\mathbb{P}(◯|x,H)-(1-y)ln[1-\mathbb{P}(◯|x,H)]\)</span> 就是Cross-Entropy Error。</p>
<p>而對於logistic regression model而言，<span class="math">\(\mathbb{P}(◯|x,H)=Θ(s)\)</span>，代入Cross-Entropy Error得
</p>
<div class="math">$$
E_{ce,logistic}=-ylnΘ(s)-(1-y)ln(1-Θ(s))=-ylnΘ(s)-(1-y)lnΘ(-s)
$$</div>
<p>
<strong>我們可以使用Gradient Descent來降低Cross-Entropy，這又稱為Logistic Regression，在這個問題中就沒有簡單的解析解可以直接算，只能使用Gradient Descent來求取近似解。</strong></p>
<p><br/></p>
<h3 id="_2">使用迴歸法做二元分類問題</h3>
<p><img alt="ML" src="/media/MachineLearningFoundations/MachineLearningFoundations.012.jpeg"/></p>
<p>剛剛介紹了Logistic Regression，其實我們是可以將Logistic Regression運用來做二元分類問題。</p>
<p>線性模型的標準方法，我們會將變數<span class="math">\(x\)</span>做線性組合得到Linear Scoring Function — <span class="math">\(s\)</span>，線性組合的係數和Threshold稱為權重<span class="math">\(W\)</span>，我們可以調整權重<span class="math">\(W\)</span>來改變Model，那針對看待<span class="math">\(s\)</span>的不同方式就衍生出不同的方法。那為了可以將Regression問題轉換成二元分類問題，所以通常我們會假設<span class="math">\((y=+1)\)</span>為<span class="math">\(◯\)</span>，<span class="math">\((y=-1)\)</span>為<span class="math">\(✕\)</span>。</p>
<p>先回顧一下之前<a href="/ml-course-foundations_1.html">PLA的作法</a>，我們把 <span class="math">\(s&gt;0\)</span> 的狀況視為<span class="math">\(◯\)</span>，也就是<span class="math">\((y=+1)\)</span>；然後把<span class="math">\(s&lt;0\)</span> 的狀況視為<span class="math">\(✕\)</span>，也就是<span class="math">\((y=-1)\)</span>，把這個概念畫成上圖右側的圖，圖中藍色的階梯函數就是PLA的Error Measurement，正是因為它是一個階梯函數，所以我們不能使用Gradient Descent等Regression方法來處理，<strong>因為在階梯的每一點<span class="math">\(\nabla E_{in}\)</span>都是0（除了原點外），也就是如此PLA在更新的過程才無法確保趨近於最佳解，而需要使用Pocket PLA來解決這個問題</strong>。</p>
<p>那如果我們用Linear Regression來做這件事呢？我們把Squared Error畫在上圖右側小圖的紅線，你會發現它的低點會落在<span class="math">\(y\times s=1\)</span>的地方，這應該不是我們要的結果，雖然它一樣可以把錯誤的判斷修正回正確，但是面對過度確定的正確答案，它反而會去修正它往錯誤的方向，很顯然這不是我們想要的。</p>
<p>最好的方式就是Logistic Regression了，我們將<span class="math">\(s\)</span>做Logistic Function的轉換，轉換成機率，並在評估最大化Likelihood的條件下定義出Cross-Entropy來當作Error Measurement，在上圖右側的小圖，我們稍微調整Cross-Entropy，使得它的Error Function可以在<span class="math">\(y\times s=0\)</span>的地方和Squared Error相切，<strong>這張圖告訴我們的是隨著Grandient Descent每次的更新，Logistic Regression會把分類做的越來越好</strong>。</p>
<p><br/></p>
<h3 id="_3">後話</h3>
<p>在這一篇當中，我們介紹了Grandient Descent這一個相當重要的演算法，並且運用在兩種Regression上：Linear Regression和Logistic Regression，Linear Regression是最簡單的Regression方法，甚至它還可以使用Pseudo-Inverse的方法直接算出最佳解，Logistic Regression考慮了有雜訊的Data產生的機率分布，我們可以用Logistic Regression做Soft Binary Classification，而且我們也說明了Logistic Regression為何適合拿來用在二元分類上。本篇我們對於ML的實際作法有了基本認識，在下一篇，我們繼續討論還有沒有什麼方式可以讓ML做的更好。</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>
<div class="center social-share">
<p>Like this article? Share it with your friends!</p>
<div class="addthis_native_toolbox"></div>
<div class="addthis_sharing_toolbox"></div>
<div class="addthis_inline_share_toolbox"></div>
</div>
<div class="neighbors">
<a class="btn float-left" href="https://ycc.idv.tw/ml-course-foundations_2.html#anchor" title="機器學習基石 學習筆記 (2)：為什麼機器可以學習?">
<i class="fa fa-angle-left"></i> Previous Post
    </a>
<a class="btn float-right" href="https://ycc.idv.tw/ml-course-foundations_4.html#anchor" title="機器學習基石 學習筆記 (4)：機器可以怎麼學得更好?">
      Next Post <i class="fa fa-angle-right"></i>
</a>
</div>
<div class="addthis_relatedposts_inline"></div>
<div class="related-posts">
<h4>You might enjoy</h4>
<ul class="related-posts">
<li><a href="https://ycc.idv.tw/ml-course-foundations_1.html">機器學習基石 學習筆記 (1)：何時可以使用機器學習?</a></li>
<li><a href="https://ycc.idv.tw/ml-course-foundations_2.html">機器學習基石 學習筆記 (2)：為什麼機器可以學習?</a></li>
<li><a href="https://ycc.idv.tw/ml-course-foundations_4.html">機器學習基石 學習筆記 (4)：機器可以怎麼學得更好?</a></li>
</ul>
</div>
<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'ycnote-1';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
    Please enable JavaScript to view comments.
</noscript>
<!-- End Disqus -->
</article>
<footer>
<p>
  © 2024  - This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" rel="license" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p><p>
<a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license" target="_blank">
<img alt="Creative Commons License" height="15" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" style="border-width:0" title="Creative Commons License" width="80"/>
</a>
</p></footer> </main>
<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " YC Note ",
  "url" : "https://ycc.idv.tw",
  "image": "",
  "description": "[ YC Note - ML/DL Tech Blog ] Hello, I am YC, an ML engineer/researcher with experience in CV, NLP/NLU, and Recommender. I built this blog for anyone interested in data science and machine learning."
}
</script><script async="async" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-63b4eabb5e84e9fb" type="text/javascript"></script>
<script>
    window.loadStorkIndex = async (input_obj) => {
      input_obj.disabled = true;
      input_obj.placeholder = 'Downloading index file, please wait ...'
      await stork.register("sitesearch", "https://ycc.idv.tw/search-index.st", { showProgress: false });
      input_obj.placeholder = 'Search ...'
      input_obj.disabled = false;
    }
  </script>
<script src="https://files.stork-search.net/releases/v1.5.0/stork.js"></script>
</body>
</html>