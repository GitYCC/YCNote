<!DOCTYPE html>
<html lang="zh">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="在之前四篇文章中，我總結了台大教授林軒田在Coursera上的《機器學習基石》16堂課程，我覺得這是機器學習初學很重要的基礎課程，接下來我要接續更進階的課程。...">
        <meta name="keywords" content="機器學習技法">
        <link rel="icon" href="./static/img/favicon.png">

        <title>機器學習技法 學習筆記 (1)：我們將會學到什麼? 先見林再來見樹 - YC Note</title>

        <!-- Stylesheets -->
        <link href="./theme/css/all.min.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <link href="YCNote/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="YC Note Full Atom Feed" />
        <link href="YCNote/feeds/aiml.atom.xml" type="application/atom+xml" rel="alternate" title="YC Note Categories Atom Feed" />
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->



    </head>

    <body>

        <!-- Header -->
    <div class="header-container" style="background: linear-gradient(rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0.2)), url('./images/ai_front_board.jpg'); background-position: center; background-size: cover;">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="./"><img class="logo" src="./static/img/favicon.png" alt="logo">YC Note</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="./category/coding.html">Coding</a>
                                <a href="./category/aiml.html">AI.ML</a>
                                <a href="./category/reading.html">Reading</a>
                                <a href="./category/recording.html">Recording</a>
                                <a href="./about-me.html">About Me</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">機器學習技法 學習筆記 (1)：我們將會學到什麼? 先見林再來見樹</h1>
                      <p class="header-date">By <a href="./author/yc-chen.html">YC Chen</a>, 2017 / 1月 12, in category <a href="./category/aiml.html">Ai.ml</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="./tag/ji-qi-xue-xi-ji-fa.html">機器學習技法</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <p>在之前四篇文章中，我總結了台大教授林軒田在Coursera上的《機器學習基石》16堂課程，我覺得這是機器學習初學很重要的基礎課程，接下來我要接續更進階的課程。</p>
<p>林軒田教授的機器學習是兩學期的課，第一學期是《機器學習基石》，第二學期就是接下來這個系列要講的《機器學習技法》，這兩堂課程是有相當大的銜接關係的，所以如果想看這系列的文章，請先看<a href="http://www.ycc.idv.tw/tag__筆記：機器學習基石/">這四篇《機器學習基石》的介紹</a>或者<a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations">直接到Coursera上學習</a>。</p>
<p>《機器學習技法》課程影片可以到老師的Youtube [ <a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2">https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2</a> ]上收看，投影片可以到老師的個人網站上下載 [ <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/</a> ]。</p>
<p>以前，我曾經和實驗室的英國學長聊英國的教育方法，然後我驚人的發現，他的學校在大一就已經學過量子場論（物理上很難的學科XDD）了，我就很好奇量子場論不是需要很深厚的數學基礎嗎？大一是要怎麼教啊？他告訴我，他們大一就會完整走過物理的各大領域，不過是用非常概念的方式來學習，不牽涉到太困難的數學，但這概念的一系列課程卻是四年大學中相當重要的基礎，讓他在開始學細節前就可以知道這些東西未來會用在哪裡？產生了連結讓學習更有效率。</p>
<p>所以，《機器學習技法》中會介紹很多厲害的機器學習的方法，但這一篇我不直接進去看每個方法的細節，我想帶大家坐著直升機來先看看這遊樂園中有哪些遊樂設施，先來見林再來見樹，會更容易了解。</p>
<p><br/></p>
<h3>有什麼特徵可以使用？</h3>
<p>在之前《機器學習基石》中，我們講到了Features（特徵）的選擇，<strong>Features（特徵）就是我的Model描述Data的方法，也可以說是影響Data的變數</strong>，那在之前我們講過Features（特徵）的選擇可以是線性的，那也可以使用「特徵轉換」來產生非線性。</p>
<p>在這系列文章，我們會看到更多種類的Features，可以分為三類：</p>
<ol>
<li>Embedding Numerous Features（嵌入大量特徵）</li>
<li>Combining Predictive Features（綜合預測結果的特徵）</li>
<li>Distilling Implicit Features（抽取隱含特性的特徵）</li>
</ol>
<p>我已經盡力用我的理解翻譯上面的英文，哈！</p>
<p>這些不同種類的Features就會造成不同的Models，這些Models分別是</p>
<ol>
<li>Embedding Numerous Features ：Kernel Models（Kernel模型）</li>
<li>Combining Predictive Features：Aggregation Models（集合模型）</li>
<li>Distilling Implicit Features：Extraction Models（萃取模型）</li>
</ol>
<p>讓我們依序來看。</p>
<p><br/></p>
<h3>Embedding Numerous Features ：Kernel Models</h3>
<p>還記得《機器學習基石》中，我們講了哪些Model嗎？我們一開始講了二元分類問題，然後提出了Perceptron Learning Algorithm (PLA)來解決這個問題（<a href="http://www.ycc.idv.tw/YCNote/post/25">詳見《機器學習基石》第一篇</a>），如果數據是線性可分的話，我們就可以使用PLA劃分出一條邊界來區分兩種種類。</p>
<p>接下來提到我們可以使用Regression的方法來做二元分類問題，其中Logistic Regression考慮了雜訊造成每個Label的出現呈機率分布，給予一個較為寬鬆的區分方法，我們會稱PLA為Hard Classification，而Logistic Regression為Soft Classification。（<a href="http://www.ycc.idv.tw/YCNote/post/27">詳見《機器學習基石》第三篇</a>）</p>
<p>最後，我們引入「特徵轉換」將我們原本的線性區分推到非線性區分，讓我的Model有更大的複雜度，也因為如此，我們需要使用Regularization和Validation來避免 Overfitting。（<a href="http://www.ycc.idv.tw/YCNote/post/28">詳見《機器學習基石》第四篇</a>）</p>
<p><strong>那如果我想要使用無窮個高次方的非線性Features來當作我的Model，可以做到嗎？</strong></p>
<p>來看一下之前我們做特徵轉換怎麼做的？其實我們沒有多做什麼功夫，我們只是把高次項先產生出來，然後在把這每一項當作線性模型的Features去處理，我們就用線性模型的方法產生了非線性的效果。</p>
<p>那如果非線性項目的個數無窮多個，顯然這種方法就做不了了啊！</p>
<p>不過，數學總是會拯救我們，<strong>我們可以使用Dual Transformation加上Kernel Function的技巧，帶我們走捷徑，直接用解析解讓我們得出答案，繞過要考慮無窮多個Features後再處理的窘境。</strong></p>
<p>第一堂課「Linear Support Vector Machine」中，提出Hard-Margin Support Vector Machine (SVM)的架構，他和PLA非常相近，屬於Hard Classification，不同的是Hard-Margin SVM還會讓這個切分的邊界落在最佳的位置上。</p>
<p>第二堂課 「Dual Support Vector Machine」中，我們開始使用Dual Transformation，把大部分與Data中Features有關的計算，取代成計算與Data中Labels有關的計算，讓我們朝不需要計算Features邁進一步，但是因為有另外一部分還是需要計算Features，所以一樣的我們還是無法讓Features有無窮多個。</p>
<p>第三堂課「Kernel Support Vector Machine」中，我們引入Kernel Function來幫助我們，現在真的可以不需去列出所有Features也能算出答案，所以我們就可以讓Features有無窮多項，但也因為Model太過複雜，我們不得不去面對Overfitting的問題。</p>
<p>第四堂課「Soft-Margin Support Vector Machine」中，提出Soft-Margin SVM，它是一種Soft Classification，讓我們可以允許部分錯誤發生，並且同樣的使用Dual Transformation加上Kernel Function的技巧，來讓我可以使用無窮多項的Features，而且因為Soft-Margin SVM可以允許錯誤，也就是對雜訊有容忍度，因此可以幫助我們抑制Overfitting的發生。</p>
<p>第五堂課「Kernel Logistic Regression」中，我們將Kernel的方法引入Logistic Regression當中來用不同於Soft-Margin SVM的方式做二元分類。</p>
<p>第六堂課「Support Vector Regression」中，會介紹如何使用Kernel Model來做各類Regression的問題。</p>
<p><strong>這6堂課，主要做的事是把《機器學習基石》裡面學到的東西，全部引入數學工具讓Model的Features可以擴展到無窮多項，產生更強大的Kernel Model。</strong></p>
<p><br/></p>
<h3>Combining Predictive Features：Aggregation Models</h3>
<p>那如果今天我有很多支的Model，我有辦法融合他們得到更好的效果嗎？</p>
<p><strong>這就是Aggregation Models的精髓，Aggregation Models藉由類似於投票的方法綜合各個子Models的結果得到效果更好的Model。換個角度看，你可以把整個體系看成一個新的Model，而原本這些子Models當作轉換過後的新Features，所以Aggregation Model裡頭做了「特徵轉換」，這個轉換產生出許多有預測答案能力的Features，稱為Predictive Features，然後再綜合它們。</strong></p>
<p>Aggregation Models可以分成兩大類，第一種的作法比較簡單，先Train出一個一個獨立的Predictive Features，然後在綜合它們，<strong>「集合」的動作是發生在得到Train好的Predictive Feature之後，這叫做「Blending Models」</strong>；第二種作法則是，<strong>「集合」的動作和Training同步進行，這叫做「Aggregation-Learning Models」</strong>。</p>
<p>從「集合」的方法上也可以進一步細分三種類型，有票票等值的<strong>「Uniform Aggregation Type」</strong>，有給予Predictive Features不同權重的<strong>「Linear Aggregation Type」</strong>，甚至還可以用條件或任意Model來分配Predictive Features，這叫做<strong>「Non-linear Aggregation Type」</strong>。</p>
<p>所以兩種類型、三種Aggregation Type，交互產生六種Aggregation Models。</p>
<p>第七堂課「Bootstrip Aggregation」中，一開始介紹Blending Models的三種Aggregation Type，第一種是直接平均所有的Predictive Features，第二種則是藉由每個Predictive Feature的預測能力，使用線性模型去調配它們的權重，第三種則是使用任意模型分配權重。接著又介紹了Aggregation-Learning Models的Uniform Aggregation Type，稱之為Bagging，它的特點在於它可以利用變換Dataset來造出很多個Predictive Features，並接著做Aggregation。</p>
<p>第八堂課「Adaptive Boosting」中，介紹Aggregation-Learning Models的Linear Aggregation Type，稱之為AdaBoost，它的特點在於它可以使得每個Predictive Features彼此間可以截長補短。</p>
<p>第九堂課「Decision Tree」中，介紹Aggregation-Learning Models的Non-linear Aggregation Type，稱之為Decision Tree。</p>
<p>第十堂課「Random Forest」中，使用Bagging來做Decision Tree，這叫做Random Forest。</p>
<p>第十一堂課「Gradient Boosted Decision Tree」中，會介紹AdaBoost的Regression版本稱為GradientBoost，並且運用AdaBoost和GradientBoost在Decision Tree上面。</p>
<p><strong>這5堂課，我們將會介紹Aggregation Models，引入綜合、集合Predictive Feature的概念來使我們造出更好的Model。</strong></p>
<p><br/></p>
<h3>Distilling Implicit Features：Extraction Models</h3>
<p>那最後這個部分則是介紹現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，在這裡我們通稱Extraction Models。</p>
<p><strong>Extraction Models的特色在於它「特徵轉換」的方法，使用一層一層神經元來做非線性的特徵轉換，如果具有多層神經元，那就是做了多次的非線性特徵轉換，這就是「深度學習」，藉由Data機器會自行學習出這每一層的特徵轉換，找出隱含的Features。</strong></p>
<p>第十二堂課「Neural Network」中，介紹Neural Network，並介紹Neural Network的演算法—Back-Propagation（反向傳遞法），在概念上Gradient Descent就是Back-Propagation的源頭，另外介紹避免Overfitting的方法—Early Stopping。</p>
<p>第十三堂課「Deep Learning」中，開始介紹「深度學習」，考慮多層神經元的Neural Network就叫做Deep Learning，我們會探討如何在Deep Learning中加入Regularization，並介紹一種叫做Auto-encoder的特殊Deep Learning方法。</p>
<p>第十四堂課「Radial Basis Function Network」中，介紹Radial Basis Function (RBF) Network，並且介紹K-means等非監督分類法。</p>
<p>第十五堂課「Matrix Factorization」中，我們會探討類別的匹配問題，例如：我想要知道用戶喜歡看什麼電影，而我的Data只有用戶的ID和電影的編號。</p>
<p><strong>這4堂課，我們將會介紹Extraction Model，使用神經元的概念來萃取出Data中的Features。</strong></p>
<p><br/></p>
<h3>後話</h3>
<p>最後總結一下《機器學習技法》會講哪些東西？我們會講具有三種不同「特徵轉換」方式的Models。<strong>Kernel Model的「特徵轉換」是將非線性Features擴張到無窮多個；Aggregation Model的「特徵轉換」是產生出有預測能力的Features；Extraction Model的「特徵轉換」是利用神經元的方式來做到萃取出隱含的資訊。</strong></p>
<p><strong>跟《機器學習基石》不一樣的地方，《機器學習技法》中介紹更厲害的「特徵轉換」來產生更厲害的Model，不過因為會有Overfitting的狀況，所以我們還需要介紹相應的配套措施。</strong></p>
<p>在未來一系列的文章，我會帶大家一一的來看這些內容，不過和之前一樣，我不會以課堂當作單位來講，而是以單元式的方式，而且我主要的目的是去點出概念，並盡可能的不去牽涉太多的數學計算，但是數學計算的部分是很重要的，這會影響到你真正的實作，數學的部份可以去看林軒田老師的影片或投影片，裡頭都有很詳細的介紹。</p>


    <div class="comments">
        <div id="disqus_thread"></div>
            <script type="text/javascript">
                var disqus_shortname = 'ycnote-1';
                var disqus_identifier = 'ml-course-techniques_1.html';
                var disqus_url = './ml-course-techniques_1.html';
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            </script>
        <noscript>Please enable JavaScript to view the comments.</noscript>
    </div>
        
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="./archives.html">Archives</a></li>
                            <li><a href="./tags.html">Tags</a></li>
                            <li><a href="YCNote/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">Atom Feed</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Contact Me</div>
                        <ul class="list-unstyled">
                            <li><a href="./about-me.html" target="_blank">About Me</a></li>
                            <li><a href="https://github.com/GitYCC" target="_blank">Github</a></li>
                            <li><a href="mailto:ycc.tw.email@gmail.com" target="_blank">Email</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; YC Note 2018</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>