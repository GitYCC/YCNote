
<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="True" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="index, follow" name="robots"/>
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&amp;family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&amp;display=swap" rel="stylesheet"/>
<link href="https://ycc.idv.tw/theme/stylesheet/style.less" rel="stylesheet/less" type="text/css"/>
<script src="//cdnjs.cloudflare.com/ajax/libs/less.js/2.5.1/less.min.js" type="text/javascript"></script>
<link href="https://ycc.idv.tw/theme/pygments/default.min.css" id="pygments-light-theme" rel="stylesheet" type="text/css"/>
<link href="https://ycc.idv.tw/theme/stork/stork.css" rel="stylesheet" type="text/css">
<link href="https://ycc.idv.tw/theme/font-awesome/css/fontawesome.css" rel="stylesheet" type="text/css"/>
<link href="https://ycc.idv.tw/theme/font-awesome/css/brands.css" rel="stylesheet" type="text/css"/>
<link href="https://ycc.idv.tw/theme/font-awesome/css/solid.css" rel="stylesheet" type="text/css"/>
<link href="/images/favicon.png" rel="shortcut icon" type="image/x-icon"/>
<link href="/images/favicon.png" rel="icon" type="image/x-icon"/>
<!-- Chrome, Firefox OS and Opera -->
<meta content="#FFFFFF" name="theme-color"/>
<!-- Windows Phone -->
<meta content="#FFFFFF" name="msapplication-navbutton-color"/>
<!-- iOS Safari -->
<meta content="yes" name="apple-mobile-web-app-capable"/>
<meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"/>
<!-- Microsoft EDGE -->
<meta content="#FFFFFF" name="msapplication-TileColor"/>
<link href="https://ycc.idv.tw/feeds/all.atom.xml" rel="alternate" title="YC Note Atom" type="application/atom+xml"/>
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68393177-2', 'auto');
  ga('send', 'pageview');
</script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-LXDD9FZFX2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LXDD9FZFX2');
</script>
<meta content="YC Chen" name="author">
<meta content="Word2Vec觀念解析 / Word2Vec的架構 / Word2Vec的兩種常用方法：Skip-Gram和CBOW / 準備文本語料庫 / 實作Skip-Gram / 實作CBOW (Continuous Bag of Words)" name="description">
<meta content="Tensorflow" name="keywords"/>
<meta content="YC Note" property="og:site_name">
<meta content="實作Tensorflow (5)：Word2Vec" property="og:title">
<meta content="Word2Vec觀念解析 / Word2Vec的架構 / Word2Vec的兩種常用方法：Skip-Gram和CBOW / 準備文本語料庫 / 實作Skip-Gram / 實作CBOW (Continuous Bag of Words)" property="og:description">
<meta content="en_US" property="og:locale">
<meta content="https://ycc.idv.tw/tensorflow-tutorial_5.html" property="og:url"/>
<meta content="article" property="og:type"/>
<meta content="2017-11-19 12:00:00+08:00" property="article:published_time"/>
<meta content="" property="article:modified_time"/>
<meta content="https://ycc.idv.tw/author/yc-chen.html" property="article:author"/>
<meta content="AI.ML" property="article:section">
<meta content="Tensorflow" property="article:tag"/>
<meta content="" property="og:image"/>
<title>YC Note – 實作Tensorflow (5)：Word2Vec</title>
<script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
    (adsbygoogle = window.adsbygoogle || []).push({
      google_ad_client: "ca-pub-5639899546876072",
      enable_page_level_ads: true
    });
  </script>
</meta></meta></meta></meta></meta></meta></meta></link><link href="https://ycc.idv.tw/tensorflow-tutorial_5.html" rel="canonical"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "BreadcrumbList", "itemListElement": [{"@type": "ListItem", "position": 1, "name": "YC Note", "item": "https://ycc.idv.tw"}, {"@type": "ListItem", "position": 2, "name": "Tensorflow tutorial_5", "item": "https://ycc.idv.tw/tensorflow-tutorial_5.html"}]}</script><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "author": {"@type": "Person", "name": "YC Chen"}, "publisher": {"@type": "Organization", "name": "YC Note"}, "headline": "實作Tensorflow (5)：Word2Vec", "about": "AI.ML", "datePublished": "2017-11-19 12:00"}</script></head>
<body class="light-theme">
<aside>
<div>
<a href="https://ycc.idv.tw/">
<img alt="YC Note" src="https://ycc.idv.tw/theme/img/profile.png" title="YC Note"/>
</a>
<h1>
<a href="https://ycc.idv.tw/">YC Note</a>
</h1>
<p style="text-align: center;">ML/DL Tech Blog (Total Views: 522,267) </p>
<div class="stork">
<input autocomplete="off" class="stork-input" data-stork="sitesearch" name="q" onclick="loadStorkIndex(this); this.onclick=null;" placeholder="Search (beta feature) ..." type="text"/>
<div class="stork-output" data-stork="sitesearch-output"></div>
</div>
<!-- <script>
      window.addEventListener('load', 
        function() { 
          loadStorkIndex();
        }, false);
    </script> -->
<p>Hello, I am YC, an ML engineer/researcher with experience in CV, NLP/NLU, and Recommender. I also have experience in high-QPS ML systems. In my spare time, I'm a blogger and guitar singer. <a href="https://ycc.idv.tw/about-me.html#anchor" style="color:yellow">More about me.</a></p>
<p>This blog is a resource for anyone interested in data science and machine learning, featuring tutorials, research papers, and the latest industry technologies.</p>
<ul class="social">
<li>
<a class="sc-facebook" href="https://www.facebook.com/yc.note" target="_blank">
<i class="fa-brands fa-facebook"></i>
</a>
</li>
<li>
<a class="sc-github" href="https://github.com/GitYCC" target="_blank">
<i class="fa-brands fa-github"></i>
</a>
</li>
<li>
<a class="sc-linkedin" href="https://www.linkedin.com/in/yi-chang-chen-aba1b6114/" target="_blank">
<i class="fa-brands fa-linkedin"></i>
</a>
</li>
</ul>
</div>
</aside>
<main>
<nav id="anchor">
<a href="https://ycc.idv.tw/">Home</a>
<a href="/about-me.html#anchor">About Me</a>
<a href="/category/aiml.html#anchor">AI.ML</a>
<a href="/category/cs.html#anchor">CS</a>
<a href="/categories.html#anchor">Categories</a>
<a href="/tags.html#anchor">Tags</a>
<a href="https://ycc.idv.tw/feeds/all.atom.xml">Atom</a>
</nav>
<article class="single">
<header>
<h1 id="tensorflow-tutorial_5">實作Tensorflow (5)：Word2Vec</h1>
<p>
      Posted on November 19, 2017 in <a href="https://ycc.idv.tw/category/aiml.html">AI.ML</a>. View: 5,997

    </p>
</header>
<div class="tag-cloud">
<p>
<a href="https://ycc.idv.tw/tag/tensorflow.html">Tensorflow</a>
</p>
</div>
<script async="" src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle ads-responsive" data-ad-client="ca-pub-5639899546876072" data-ad-slot="5718861428"></ins>
<script>
      (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
<div class="main-contents">
<p>機器有辦法自行從文本中觀察出詞彙間的相似度嗎？是可以的，word2vec是"word to vector"的縮寫，代表的正是將每個字轉換成向量，而一旦兩個字的向量越是靠近，就代表它的相似度越高，我們究竟要如何得到這些向量呢？方法簡單但出奇有效，文章的最後會向大家呈現它的精彩的結果。</p>
<p>本單元程式碼Skip-Gram Word2Vec部分可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/05_1_word2vec_SkipGram.py">Github</a>下載，CBOW Word2Vec部分可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/05_2_word2vec_CBOW.py">Github</a>下載。</p>
<h3 id="word2vec">Word2Vec觀念解析</h3>
<p>Word2Vec的形式和Autoencoder有點像，一樣是從高維度的空間轉換到低維度的空間，再轉換回去原本的維度，只是這一次轉回去的東西不再是原本一模一樣的東西了。</p>
<p>Word2Vec的Input和Output這次變成是上下文的文字組合，舉個例子，"by the way"這個用法如果多次被機器看過的話，機器是有辦法去學習到這樣的規律的，此時"by"與"the"和"way"便會產生一個上下文的關聯性，為了將這樣的關聯性建立起來，我們希望當我輸入"by"時，機器有辦法預測並輸出"the"或"way"，這代表在機器內部它已經學習到了上下文的關聯性。</p>
<p>那如果今天這個機器也同時看到很多次的"on the way"這種用法，所以當我輸入"on"時，機器要有辦法預測並輸出"the"或"way"，但是我們不希望"on"和"by"兩個詞在學習時是分開學習的，我們希望機器可以因為"by the way"和"on the way"的結構很相似，所以有辦法抓出"on"和"by"是彼此相似的結論。</p>
<p>如何做到呢？答案就是限縮這個上下文的關聯性的儲存維度，如果我的字彙量有1000個，這1000個字彙彼此有上下文的關聯性，最完整表示上下文關聯性的方法就是設置一個1000x1000或者更大的表格，把所有字彙間的上下文關聯性全部存起來，但我們不想要這麼做，我要求機器用更小的表格來儲存上下文的關聯性，此時機器被迫將一些詞彙使用同樣的表格位置，同樣的轉換。一旦限縮了上下文關聯性的儲存維度，"on the way"和"by the way"中的"on"和"by"就會被迫分為同一類，因此我們成功的建立了字詞間的相似性關係。</p>
<h3 id="word2vec_1">Word2Vec的架構</h3>
<p><img alt="word2vec" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.008.jpeg"/></p>
<p>實作上如上圖所示，我們輸入一個字詞，譬如"cat"，通常會將他轉成One-hot encoding表示，但要注意喔！文本的字彙量是非常龐大的，所以當我們使用One-hot encoding表示時，將會出現一個非常長但Sparse的向量，相同的輸出層也同樣是一個很長的One-hot encoding，它的維度會和輸入層一樣大，因為我們要分析的字彙在輸入和輸出是一樣多的。</p>
<p>然後，和Autoencoder使用一樣的手法，中間的Hidden Layer放置低維度、少神經元的一層，但不同於Autoencoder，Word2Vec所有的轉換都是線性的，沒有非線性的Activation Function夾在其中，為什麼呢？因為我們的輸入是Sparse的而且只有0和1的差別，所以每一條通路就變成只有導通或不導通的差別，Activation Function有加等於沒加，使用線性就足夠了。</p>
<p>這個中間的Hidden Layer被稱為Embedding Matrix，它做了一個線性的Dimension Reduction，將原本高維度的One-hot encoding降低成低維度，然後再透過一個線性模型轉換回去原本的維度。假設字彙的數量有N個，所以輸入矩陣X是一個1xN的矩陣，輸出的矩陣同樣也是1xN的矩陣，當我先做一個線性的Dimension Reduction，將維度降到d維，此時Embedding Matrix會是一個Nxd的矩陣V，然後再由線性模型轉換回去原本的維度，這個轉換矩陣W是一個Nxd矩陣，因此綜合上述，可用一個簡潔的表示式表示：<span class="math">\(Y=W^T VX\)</span>，我們的目標就是找出這個W和V矩陣的每個元素。</p>
<p>你會想說線性模型很簡單啊！就是仿照Autoencoder的作法，然後把Activation Function拿掉不就了事了，並且因為輸出是One-hot Encoding所以最後套用Softmax，那不就輕鬆完成！但是真正的大魔王就出在字彙量，字彙量一旦很大，事情就變得不可收拾了，而且字彙量是一定小不得的，那怎麼辦？</p>
<p>在Dimension Reduction我們可以採取一個快速的方法，因為除了我要表示的字的位置是1以外其他都是0，所以其他都可以不看，我們就直接看是在第幾個位置上是1，然後再到Embedding Matrix上找到相應的行直接取出就是答案了，這樣查詢的動作，在Tensorflow中可以使用<code>tf.nn.embedding_lookup</code>來辦到。</p>
<p>再接下來最後的Cross-Entropy Loss計算也非常龐大，因為有幾個字彙就需要累加幾組數字，我們有一招偷吃步的方法叫做「Sampled Softmax」，作法是這樣的，我們不去計算全部詞彙的Cross-Entropy，而是選擇幾組詞彙來評估Cross-Entropy，在選擇上我們會隨機挑選一些Labels和預測結果差異度很大的詞彙(稱為Negative Examples)來算Cross-Entropy，我們在Tensorflow可以使用<code>tf.nn.sampled_softmax_loss</code>來辦到「Sampled Softmax」。</p>
<p>我們先不管輸入和輸出究竟怎麼取得，如果我們成功的建立了輸入和輸出的上下文關係，此時中間的Embedding空間正是精華的所在，經過剛剛推論，我們預期在這個空間當中，相似的詞彙會彼此靠近，我們評估兩個向量的相似性可以使用Cosine來評估，當兩向量的夾角越小代表它們越是相似，待會的實作當中我們將會利用Cosine來建立Similarity的大小，藉此來找到前幾個和它很靠近的詞彙。</p>
<p>另外，經研究指出這個Embedding空間的效果不只是可以算出詞彙間的相似性，還可以顯示詞彙間的比較關係，例如：北京之於中國，等同於台北之於台灣，這樣的比較關係也顯示在這個Embedding空間裡頭，所以在這空間裡會有以下的向量關係式：<span class="math">\(V_{北京} - V_{中國}+V_{台灣}=V_{台北}\)</span>，是不是很神奇啊！</p>
<h3 id="word2vecskip-gramcbow">Word2Vec的兩種常用方法：Skip-Gram和CBOW</h3>
<p><img alt="Skip-Gram和CBOW" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.009.jpeg"/></p>
<p>剛剛一直在講的是中間的結構應該怎麼建立，現在來看看我們可以輸入和輸出哪些詞彙來建立起上下文的關係，有兩種常用的類別：Skip-Gram和CBOW。</p>
<p>Skip-Gram如上圖所示，當我輸入一個<span class="math">\(word(t)\)</span>時，我希望它能輸出它的前文和後文，這是相當直覺的建立上下文的方法，所以如果我希望用前一個字和後一個字來訓練我的Word2Vec，我就會有兩組數據：<span class="math">\((w(t),w(t-1))\)</span>和<span class="math">\((w(t),w(t+1))\)</span>，相當好理解。</p>
<p>而CBOW(Continuous Bag of Words)使用另外一種方法來建立上下文關係，它將一排字挖掉中間一個字，然後希望由上下文的關係有辦法猜出中間那個字，就像是填空題，此時輸入層就變成會有多於1個字，那該怎麼處理，答案是轉換到Embedding空間後再相加平均，因為是線性轉換，所以直接線性累加就可以了。</p>
<h3 id="_1">準備文本語料庫</h3>
<p>先帶入一些待會會用到的函式庫，並且決定我們要取用多少<code>VOCABULARY_SIZE</code>個詞彙量來做訓練。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">VOCABULARY_SIZE</span> <span class="o">=</span> <span class="mi">100000</span>
</code></pre></div></td></tr></table></div>
<p>接下來下載Dataset，並做一些前處理。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">maybe_download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">expected_bytes</span><span class="p">):</span>
    <span class="sd">"""Download a file if not present, and make sure it's the right size."""</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
        <span class="n">filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">stat</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span> <span class="o">==</span> <span class="n">expected_bytes</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'Found and verified </span><span class="si">%s</span><span class="s1">'</span> <span class="o">%</span> <span class="n">filename</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
          <span class="s1">'Failed to verify '</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s1">'. Can you get to it with a browser?'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">filename</span>


<span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="sd">"""Extract the first file enclosed in a zip file as a list of words"""</span>
    <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">namelist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">data</span>


<span class="k">def</span> <span class="nf">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'UNK'</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="n">count</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">vocabulary_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">count</span><span class="p">:</span>
        <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">unk_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># dictionary['UNK']</span>
            <span class="n">unk_count</span> <span class="o">=</span> <span class="n">unk_count</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">unk_count</span>
    <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span>


<span class="nb">print</span><span class="p">(</span><span class="s1">'Downloading text8.zip'</span><span class="p">)</span>
<span class="n">filename</span> <span class="o">=</span> <span class="n">maybe_download</span><span class="p">(</span><span class="s1">'http://mattmahoney.net/dc/text8.zip'</span><span class="p">,</span> <span class="s1">'./text8.zip'</span><span class="p">,</span> <span class="mi">31344016</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'====='</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Data size </span><span class="si">%d</span><span class="s1">'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'First 10 words: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">10</span><span class="p">]))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'====='</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span>
                                                                <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">)</span>
<span class="k">del</span> <span class="n">words</span>  <span class="c1"># Hint to reduce memory.</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Most common words (+UNK)'</span><span class="p">,</span> <span class="n">count</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Sample data'</span><span class="p">,</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="l l-Scalar l-Scalar-Plain">Downloading text8.zip</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Found and verified ./text8.zip</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">=====</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Data size 17005207</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">First 10 words</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'anarchism'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'abuse'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'first'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'used'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'against'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">=====</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Most common words (+UNK) [['UNK', 189230], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]</span><span class="w"></span>
</code></pre></div></td></tr></table></div>
<p>我們取用<code>VOCABULARY_SIZE = 100000</code>，也是說我們將文本中的詞彙按出現次數的多寡來排列，取前面<code>VOCABULARY_SIZE</code>個保留，其餘詞彙皆歸類到「UNK Token」裡頭，UNK代表UNKnown的縮寫。</p>
<p>我們文本的字詞數量總共有17005207個字，開頭前十個字的句子是'anarchism originated as a term of abuse first used against'。所有的這17005207個字會依照<code>dictionary</code>給予每個字Index，而文本會被表示為一個由整數所構成的List，這會放在<code>data</code>裡頭，而這個Index也就直接當作One-hot Encoding中代表這個詞彙的維度位置。當我想要把Index轉換回去我們看得懂的字的時候，就需要<code>reverse_dictionary</code>的幫忙，有了這些，我們的語料庫就已經建立完成了。</p>
<h3 id="skip-gram">實作Skip-Gram</h3>
<p>有了語料庫，我們就可以產生出我想要的輸入和輸出，在Skip-Gram方法，如果我的輸入是<code>target word</code>，我會先從<code>target word</code>向前、向後看出去<code>skip_window</code>的大小，所以可以選擇當作輸出的字有<code>skip_window*2</code>個，接下來我從這<code>skip_window*2</code>個中選擇<code>num_skips</code>個當作輸出，所以一個<code>target word</code>會產生<code>num_skips</code>筆數據，如果我一個batch需要<code>batch_size</code>筆數據，我就必須有<code>batch_size//num_skips</code>個<code>target word</code>，依照這樣的規則下面建立一個Generator來掃描文本，並輸出要訓練使用的Batch Data。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">skip_gram_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">num_skips</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">num_skips</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span>

    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># [ skip_window target skip_window ]</span>
    <span class="n">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>

    <span class="c1"># initialization</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">):</span>
        <span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># generate</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">skip_window</span>  <span class="c1"># target label at the center of the buffer</span>
        <span class="n">targets_to_avoid</span> <span class="o">=</span> <span class="p">[</span><span class="n">target</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_skips</span><span class="p">):</span>
            <span class="k">while</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets_to_avoid</span><span class="p">:</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">span</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">targets_to_avoid</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">buffer</span><span class="p">[</span><span class="n">skip_window</span><span class="p">]</span>
            <span class="n">labels</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">buffer</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
            <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Recycle</span>
        <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># scan data</span>
        <span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="c1"># Enough num to output</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>


<span class="c1"># demonstrate generator</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'data:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">di</span><span class="p">]</span> <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">]])</span>

<span class="k">for</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]:</span>
    <span class="n">batch_generator</span> <span class="o">=</span> <span class="n">skip_gram_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_skips</span><span class="o">=</span><span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="o">=</span><span class="n">skip_window</span><span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">with num_skips = </span><span class="si">%d</span><span class="s1"> and skip_window = </span><span class="si">%d</span><span class="s1">:'</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'    batch:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="k">for</span> <span class="n">bi</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'    labels:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">li</span><span class="p">]</span> <span class="k">for</span> <span class="n">li</span> <span class="ow">in</span> <span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">)])</span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span>
<span class="normal">9</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nt">data</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'anarchism'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'abuse'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'first'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'used'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'against'</span><span class="p p-Indicator">]</span><span class="w"></span>

<span class="nt">with num_skips = 2 and skip_window = 1</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">batch</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">    </span><span class="nt">labels</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'anarchism'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">]</span><span class="w"></span>

<span class="nt">with num_skips = 4 and skip_window = 2</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">batch</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">    </span><span class="nt">labels</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'anarchism'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">]</span><span class="w"></span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SkipGram</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_vocabulary</span><span class="p">,</span> <span class="n">n_embedding</span><span class="p">,</span> <span class="n">reverse_dictionary</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span> <span class="o">=</span> <span class="n">n_vocabulary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span> <span class="o">=</span> <span class="n">n_embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">reverse_dictionary</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>  <span class="c1"># initialize new grap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>  <span class="c1"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>  <span class="c1"># create session by the graph</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c1">### Input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

            <span class="c1">### Optimalization</span>
            <span class="c1"># build neurel network structure and get their loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span>
                <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">,</span>
                <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># normalize embeddings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">'embeddings'</span><span class="p">]),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">'embeddings'</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span>

            <span class="c1"># define training operation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1">### Prediction</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span>
                <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">,</span>
                <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># similarity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span><span class="p">,</span>
                                            <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">))</span>

            <span class="c1">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="c1">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">'embeddings'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                                <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span>
                                                  <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span>
                <span class="s1">'softmax'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                             <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span>
                                                 <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">)))</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">'softmax'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">]))</span>
            <span class="p">}</span>

        <span class="c1">### Structure</span>
        <span class="c1"># Look up embeddings for inputs.</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">'embeddings'</span><span class="p">],</span> <span class="n">dataset</span><span class="p">)</span>

        <span class="c1"># Compute the softmax loss, using a sample of the negative labels each time.</span>
        <span class="n">num_softmax_sampled</span> <span class="o">=</span> <span class="mi">64</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                 <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sampled_softmax_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">'softmax'</span><span class="p">],</span>
                                            <span class="n">biases</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">'softmax'</span><span class="p">],</span>
                                            <span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span>
                                            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                                            <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_softmax_sampled</span><span class="p">,</span>
                                            <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">online_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">}</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">nearest_words</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">top_nearest</span><span class="p">):</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span><span class="p">,</span>
                                   <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>
        <span class="n">X_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">valid_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">nearests</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_size</span><span class="p">):</span>
            <span class="n">valid_word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">valid_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">valid_word</span><span class="p">)</span>

            <span class="c1"># select highest similarity word</span>
            <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">similarity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_nearest</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">nearests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">nearest</span><span class="p">)))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">valid_words</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">nearests</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">embedding_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">find_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
<p>以上就是我建立的Model，這裡我採取<code>online_fit</code>的方法，不同於之前的<code>fit</code>，<code>online_fit</code>可以不用事先將所有Data一次餵進去，而是可以陸續的餵入Data，所以我會從上面的Generator陸續產生Batch Data並餵入Model裡來做訓練。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># build skip-gram batch generator</span>
<span class="n">batch_generator</span> <span class="o">=</span> <span class="n">skip_gram_batch_generator</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">num_skips</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">skip_window</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="c1"># build skip-gram model</span>
<span class="n">model_SkipGram</span> <span class="o">=</span> <span class="n">SkipGram</span><span class="p">(</span>
    <span class="n">n_vocabulary</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">,</span>
    <span class="n">n_embedding</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">reverse_dictionary</span><span class="o">=</span><span class="n">reverse_dictionary</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>

<span class="c1"># initial model</span>
<span class="n">model_SkipGram</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

<span class="c1"># online training</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_batchs_in_epoch</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batchs_in_epoch</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model_SkipGram</span><span class="o">.</span><span class="n">online_fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">loss</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">avg_loss</span> <span class="o">/</span> <span class="n">num_batchs_in_epoch</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Epoch </span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">: </span><span class="si">%d</span><span class="s1">s loss = </span><span class="si">%9.4f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span><span class="p">,</span> <span class="n">avg_loss</span> <span class="p">))</span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nt">Epoch 1/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">17s loss =    4.2150</span><span class="w"></span>
<span class="nt">Epoch 2/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.7561</span><span class="w"></span>
<span class="nt">Epoch 3/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.6276</span><span class="w"></span>
<span class="nt">Epoch 4/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.5098</span><span class="w"></span>
<span class="nt">Epoch 5/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.5123</span><span class="w"></span>
<span class="nt">Epoch 6/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.5000</span><span class="w"></span>
<span class="nt">Epoch 7/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.5155</span><span class="w"></span>
<span class="nt">Epoch 8/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.3983</span><span class="w"></span>
<span class="nt">Epoch 9/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.4418</span><span class="w"></span>
<span class="nt">Epoch 10/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.4118</span><span class="w"></span>
<span class="nt">Epoch 11/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3993</span><span class="w"></span>
<span class="nt">Epoch 12/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.4074</span><span class="w"></span>
<span class="nt">Epoch 13/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.3243</span><span class="w"></span>
<span class="nt">Epoch 14/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.3448</span><span class="w"></span>
<span class="nt">Epoch 15/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3607</span><span class="w"></span>
<span class="nt">Epoch 16/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3408</span><span class="w"></span>
<span class="nt">Epoch 17/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3705</span><span class="w"></span>
<span class="nt">Epoch 18/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3894</span><span class="w"></span>
<span class="nt">Epoch 19/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3536</span><span class="w"></span>
<span class="nt">Epoch 20/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3123</span><span class="w"></span>
<span class="nt">Epoch 21/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3046</span><span class="w"></span>
<span class="nt">Epoch 22/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3117</span><span class="w"></span>
<span class="nt">Epoch 23/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3023</span><span class="w"></span>
<span class="nt">Epoch 24/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2623</span><span class="w"></span>
<span class="nt">Epoch 25/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.3197</span><span class="w"></span>
<span class="nt">Epoch 26/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2833</span><span class="w"></span>
<span class="nt">Epoch 27/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2456</span><span class="w"></span>
<span class="nt">Epoch 28/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2272</span><span class="w"></span>
<span class="nt">Epoch 29/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2663</span><span class="w"></span>
<span class="nt">Epoch 30/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2274</span><span class="w"></span>
<span class="nt">Epoch 31/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2335</span><span class="w"></span>
<span class="nt">Epoch 32/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.3003</span><span class="w"></span>
<span class="nt">Epoch 33/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.2507</span><span class="w"></span>
<span class="nt">Epoch 34/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2486</span><span class="w"></span>
<span class="nt">Epoch 35/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2382</span><span class="w"></span>
<span class="nt">Epoch 36/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2687</span><span class="w"></span>
<span class="nt">Epoch 37/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2145</span><span class="w"></span>
<span class="nt">Epoch 38/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2437</span><span class="w"></span>
<span class="nt">Epoch 39/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2171</span><span class="w"></span>
<span class="nt">Epoch 40/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.0492</span><span class="w"></span>
<span class="nt">Epoch 41/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.9380</span><span class="w"></span>
<span class="nt">Epoch 42/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.1556</span><span class="w"></span>
<span class="nt">Epoch 43/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.1804</span><span class="w"></span>
<span class="nt">Epoch 44/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">16s loss =    3.2800</span><span class="w"></span>
<span class="nt">Epoch 45/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.1366</span><span class="w"></span>
<span class="nt">Epoch 46/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2190</span><span class="w"></span>
<span class="nt">Epoch 47/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2381</span><span class="w"></span>
<span class="nt">Epoch 48/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2419</span><span class="w"></span>
<span class="nt">Epoch 49/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.0127</span><span class="w"></span>
<span class="nt">Epoch 50/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.1232</span><span class="w"></span>
</code></pre></div></td></tr></table></div>
<p>我們來看看效果如何，我們使用Embedding Vectors彼此間的Cosine來定義出字詞間的相關性，並且列出8個最為靠近的字詞。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">valid_words_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">210</span><span class="p">,</span> <span class="mi">239</span><span class="p">,</span> <span class="mi">392</span><span class="p">,</span> <span class="mi">396</span><span class="p">])</span>

<span class="n">valid_words</span><span class="p">,</span> <span class="n">nearests</span> <span class="o">=</span> <span class="n">model_SkipGram</span><span class="o">.</span><span class="n">nearest_words</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">valid_words_index</span><span class="p">,</span> <span class="n">top_nearest</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_words</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Nearest to </span><span class="se">\'</span><span class="si">{}</span><span class="se">\'</span><span class="s1">: '</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">valid_words</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">nearests</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="l l-Scalar l-Scalar-Plain">Nearest to 'two'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'three'</span><span class="w"> </span><span class="s">'four'</span><span class="w"> </span><span class="s">'five'</span><span class="w"> </span><span class="s">'eight'</span><span class="w"> </span><span class="s">'six'</span><span class="w"> </span><span class="s">'one'</span><span class="w"> </span><span class="s">'seven'</span><span class="w"> </span><span class="s">'zero'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'that'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'which'</span><span class="w"> </span><span class="s">'however'</span><span class="w"> </span><span class="s">'thus'</span><span class="w"> </span><span class="s">'what'</span><span class="w"> </span><span class="s">'sepulchres'</span><span class="w"> </span><span class="s">'dancewriting'</span><span class="w"> </span><span class="s">'tatars'</span><span class="w"></span>
<span class="w"> </span><span class="s">'resent'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'his'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'her'</span><span class="w"> </span><span class="s">'their'</span><span class="w"> </span><span class="s">'your'</span><span class="w"> </span><span class="s">'my'</span><span class="w"> </span><span class="s">'its'</span><span class="w"> </span><span class="s">'our'</span><span class="w"> </span><span class="s">'othniel'</span><span class="w"> </span><span class="s">'personal'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'were'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'are'</span><span class="w"> </span><span class="s">'was'</span><span class="w"> </span><span class="s">'have'</span><span class="w"> </span><span class="s">'remain'</span><span class="w"> </span><span class="s">'junkanoo'</span><span class="w"> </span><span class="s">'those'</span><span class="w"> </span><span class="s">'include'</span><span class="w"> </span><span class="s">'had'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'all'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'both'</span><span class="w"> </span><span class="s">'various'</span><span class="w"> </span><span class="s">'many'</span><span class="w"> </span><span class="s">'several'</span><span class="w"> </span><span class="s">'every'</span><span class="w"> </span><span class="s">'these'</span><span class="w"> </span><span class="s">'some'</span><span class="w"> </span><span class="s">'obtaining'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'area'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'areas'</span><span class="w"> </span><span class="s">'region'</span><span class="w"> </span><span class="s">'territory'</span><span class="w"> </span><span class="s">'location'</span><span class="w"> </span><span class="s">'xylophone'</span><span class="w"> </span><span class="s">'stadium'</span><span class="w"> </span><span class="s">'city'</span><span class="w"></span>
<span class="w"> </span><span class="s">'island'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'east'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'west'</span><span class="w"> </span><span class="s">'south'</span><span class="w"> </span><span class="s">'southeast'</span><span class="w"> </span><span class="s">'north'</span><span class="w"> </span><span class="s">'eastern'</span><span class="w"> </span><span class="s">'southwest'</span><span class="w"> </span><span class="s">'central'</span><span class="w"></span>
<span class="w"> </span><span class="s">'mainland'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'himself'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'him'</span><span class="w"> </span><span class="s">'themselves'</span><span class="w"> </span><span class="s">'herself'</span><span class="w"> </span><span class="s">'them'</span><span class="w"> </span><span class="s">'itself'</span><span class="w"> </span><span class="s">'wignacourt'</span><span class="w"> </span><span class="s">'majored'</span><span class="w"></span>
<span class="w"> </span><span class="s">'mankiewicz'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'white'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'black'</span><span class="w"> </span><span class="s">'red'</span><span class="w"> </span><span class="s">'blue'</span><span class="w"> </span><span class="s">'green'</span><span class="w"> </span><span class="s">'yellow'</span><span class="w"> </span><span class="s">'dark'</span><span class="w"> </span><span class="s">'papyri'</span><span class="w"> </span><span class="s">'kemal'</span><span class="p p-Indicator">]</span><span class="w"></span>
</code></pre></div></td></tr></table></div>
<p>結果相當驚人，與'two'靠近的真的都是數字類型的文字，與'that'靠近的都是文法功能性的詞彙，與'his'靠近的都是所有格代名詞，與'were'靠近的是be動詞，與'all'最靠近的是'both'，與'east'靠近的都是一些代表方向的詞彙，與'white'靠近的都是一些顏色的詞彙，真的是太神奇了！</p>
<p>接下來直接來觀察Embedding空間，以下使用t-SNE來圖像化Embedding空間。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s1">'More labels than embeddings'</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>  <span class="c1"># in inches</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">'offset points'</span><span class="p">,</span>
                   <span class="n">ha</span><span class="o">=</span><span class="s1">'right'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">)</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">visualization_words</span> <span class="o">=</span> <span class="mi">800</span>
<span class="c1"># transform embeddings to 2D by t-SNE</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">model_SkipGram</span><span class="o">.</span><span class="n">embedding_matrix</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">'pca'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">'exact'</span><span class="p">)</span>
<span class="n">two_d_embed</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
<span class="c1"># list labels</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_SkipGram</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="c1"># plot</span>
<span class="n">plot</span><span class="p">(</span><span class="n">two_d_embed</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/05_output_13_0.png"/></p>
<p>如此一來你將可以簡單的看出，哪些詞彙彼此相似而靠近。</p>
<h3 id="cbow-continuous-bag-of-words">實作CBOW (Continuous Bag of Words)</h3>
<p>接著看CBOW的方法，如果我預期輸出的字是<code>target word</code>，從<code>target word</code>向前向後看出去<code>context_window</code>的大小，看到的字都當作我的輸入，所以我輸入的字總共需要<code>context_window*2</code>個，一個<code>target word</code>只會產生一筆數據，如果我一個batch需要<code>batch_size</code>筆數據，我就必須有<code>batch_size</code>個<code>target word</code>，依照這樣的規則下面建立一個Generator來掃描文本，並輸出要訓練使用的Batch Data。</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">cbow_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">context_window</span><span class="p">):</span>
    <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">context_window</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># [ context_window target context_window ]</span>
    <span class="n">num_bow</span> <span class="o">=</span> <span class="n">span</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_bow</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>

    <span class="c1"># initialization</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">):</span>
        <span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># generate</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">context_window</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">bow</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">bow</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bow</span><span class="p">):</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span>
        <span class="n">labels</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">buffer</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Recycle</span>
        <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># scan data</span>
        <span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="c1"># Enough num to output</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>


<span class="c1"># demonstrate generator</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'data:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">di</span><span class="p">]</span> <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">]])</span>

<span class="k">for</span> <span class="n">context_window</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">batch_generator</span> <span class="o">=</span> <span class="n">cbow_batch_generator</span><span class="p">(</span>
        <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">context_window</span><span class="o">=</span><span class="n">context_window</span>
    <span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">with context_window = </span><span class="si">%d</span><span class="s1">:'</span> <span class="o">%</span> <span class="p">(</span><span class="n">context_window</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'batch:'</span><span class="p">)</span>
    <span class="n">show_batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">tmp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]])</span>
        <span class="n">show_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">show_batch</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'labels:'</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">li</span><span class="p">]</span> <span class="k">for</span> <span class="n">li</span> <span class="ow">in</span> <span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">)])</span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nt">data</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'anarchism'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'abuse'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'first'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'used'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'against'</span><span class="p p-Indicator">]</span><span class="w"></span>

<span class="nt">with context_window = 1</span><span class="p">:</span><span class="w"></span>
<span class="nt">batch</span><span class="p">:</span><span class="w"></span>
<span class="p p-Indicator">[[</span><span class="s">'anarchism'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'abuse'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'of'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'first'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'abuse'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'used'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'first'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'against'</span><span class="p p-Indicator">]]</span><span class="w"></span>
<span class="nt">labels</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'abuse'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'first'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'used'</span><span class="p p-Indicator">]</span><span class="w"></span>

<span class="nt">with context_window = 2</span><span class="p">:</span><span class="w"></span>
<span class="nt">batch</span><span class="p">:</span><span class="w"></span>
<span class="p p-Indicator">[[</span><span class="s">'anarchism'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'originated'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'abuse'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'abuse'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'first'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'first'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'used'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'of'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'abuse'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'used'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'against'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'abuse'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'first'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'against'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'early'</span><span class="p p-Indicator">],</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'first'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'used'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'early'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'working'</span><span class="p p-Indicator">]]</span><span class="w"></span>
<span class="nt">labels</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">'as'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'a'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'term'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'of'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'abuse'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'first'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'used'</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">'against'</span><span class="p p-Indicator">]</span><span class="w"></span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CBOW</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_vocabulary</span><span class="p">,</span> <span class="n">n_embedding</span><span class="p">,</span>
                 <span class="n">context_window</span><span class="p">,</span> <span class="n">reverse_dictionary</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span> <span class="o">=</span> <span class="n">n_vocabulary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span> <span class="o">=</span> <span class="n">n_embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_window</span> <span class="o">=</span> <span class="n">context_window</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">reverse_dictionary</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>  <span class="c1"># initialize new grap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>  <span class="c1"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>  <span class="c1"># create session by the graph</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c1">### Input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_window</span><span class="o">*</span><span class="mi">2</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

            <span class="c1">### Optimalization</span>
            <span class="c1"># build neurel network structure and get their predictions and loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span>
                <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">,</span>
                <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># normalize embeddings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">'embeddings'</span><span class="p">]),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">'embeddings'</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span>

            <span class="c1"># define training operation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1">### Prediction</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

            <span class="c1"># similarity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span><span class="p">,</span>
                                            <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">))</span>

            <span class="c1">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="c1">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">'embeddings'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                                <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span>
                                                  <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span>
                <span class="s1">'softmax'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span>
                                                <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">)))</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">'softmax'</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">]))</span>
            <span class="p">}</span>

        <span class="c1">### Structure</span>
        <span class="c1"># Look up embeddings for inputs.</span>
        <span class="n">embed_bow</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">'embeddings'</span><span class="p">],</span> <span class="n">dataset</span><span class="p">)</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">embed_bow</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Compute the softmax loss, using a sample of the negative labels each time.</span>
        <span class="n">num_softmax_sampled</span> <span class="o">=</span> <span class="mi">64</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                 <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sampled_softmax_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">'softmax'</span><span class="p">],</span>
                                            <span class="n">biases</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">'softmax'</span><span class="p">],</span>
                                            <span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span>
                                            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                                            <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_softmax_sampled</span><span class="p">,</span>
                                            <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">online_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">}</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">nearest_words</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">top_nearest</span><span class="p">):</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>
        <span class="n">X_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">valid_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">nearests</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_size</span><span class="p">):</span>
            <span class="n">valid_word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">valid_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">valid_word</span><span class="p">)</span>

            <span class="c1"># select highest similarity word</span>
            <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">similarity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_nearest</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">nearests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">nearest</span><span class="p">)))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">valid_words</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">nearests</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">embedding_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">find_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">context_window</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># build CBOW batch generator</span>
<span class="n">batch_generator</span> <span class="o">=</span> <span class="n">cbow_batch_generator</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">context_window</span><span class="o">=</span><span class="n">context_window</span>
<span class="p">)</span>

<span class="c1"># build CBOW model</span>
<span class="n">model_CBOW</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span>
    <span class="n">n_vocabulary</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">,</span>
    <span class="n">n_embedding</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">context_window</span><span class="o">=</span><span class="n">context_window</span><span class="p">,</span>
    <span class="n">reverse_dictionary</span><span class="o">=</span><span class="n">reverse_dictionary</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>

<span class="c1"># initialize model</span>
<span class="n">model_CBOW</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

<span class="c1"># online training</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_batchs_in_epoch</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batchs_in_epoch</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model_CBOW</span><span class="o">.</span><span class="n">online_fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">loss</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">avg_loss</span> <span class="o">/</span> <span class="n">num_batchs_in_epoch</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Epoch </span><span class="si">%d</span><span class="s1">/</span><span class="si">%d</span><span class="s1">: </span><span class="si">%d</span><span class="s1">s loss = </span><span class="si">%9.4f</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span><span class="p">,</span> <span class="n">avg_loss</span> <span class="p">))</span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nt">Epoch 1/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.8700</span><span class="w"></span>
<span class="nt">Epoch 2/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.2961</span><span class="w"></span>
<span class="nt">Epoch 3/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.1988</span><span class="w"></span>
<span class="nt">Epoch 4/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.1201</span><span class="w"></span>
<span class="nt">Epoch 5/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.0734</span><span class="w"></span>
<span class="nt">Epoch 6/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    3.0239</span><span class="w"></span>
<span class="nt">Epoch 7/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.9378</span><span class="w"></span>
<span class="nt">Epoch 8/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.9549</span><span class="w"></span>
<span class="nt">Epoch 9/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.9651</span><span class="w"></span>
<span class="nt">Epoch 10/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.9028</span><span class="w"></span>
<span class="nt">Epoch 11/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.8770</span><span class="w"></span>
<span class="nt">Epoch 12/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.8298</span><span class="w"></span>
<span class="nt">Epoch 13/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.8437</span><span class="w"></span>
<span class="nt">Epoch 14/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.7681</span><span class="w"></span>
<span class="nt">Epoch 15/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.7823</span><span class="w"></span>
<span class="nt">Epoch 16/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.7867</span><span class="w"></span>
<span class="nt">Epoch 17/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.7540</span><span class="w"></span>
<span class="nt">Epoch 18/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.7567</span><span class="w"></span>
<span class="nt">Epoch 19/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.7340</span><span class="w"></span>
<span class="nt">Epoch 20/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.6212</span><span class="w"></span>
<span class="nt">Epoch 21/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5187</span><span class="w"></span>
<span class="nt">Epoch 22/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.7150</span><span class="w"></span>
<span class="nt">Epoch 23/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.6647</span><span class="w"></span>
<span class="nt">Epoch 24/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.7381</span><span class="w"></span>
<span class="nt">Epoch 25/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5337</span><span class="w"></span>
<span class="nt">Epoch 26/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.6587</span><span class="w"></span>
<span class="nt">Epoch 27/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.6648</span><span class="w"></span>
<span class="nt">Epoch 28/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5963</span><span class="w"></span>
<span class="nt">Epoch 29/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5418</span><span class="w"></span>
<span class="nt">Epoch 30/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.6041</span><span class="w"></span>
<span class="nt">Epoch 31/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5535</span><span class="w"></span>
<span class="nt">Epoch 32/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5928</span><span class="w"></span>
<span class="nt">Epoch 33/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5535</span><span class="w"></span>
<span class="nt">Epoch 34/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5233</span><span class="w"></span>
<span class="nt">Epoch 35/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5658</span><span class="w"></span>
<span class="nt">Epoch 36/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5966</span><span class="w"></span>
<span class="nt">Epoch 37/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5422</span><span class="w"></span>
<span class="nt">Epoch 38/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5673</span><span class="w"></span>
<span class="nt">Epoch 39/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5142</span><span class="w"></span>
<span class="nt">Epoch 40/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5175</span><span class="w"></span>
<span class="nt">Epoch 41/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.4909</span><span class="w"></span>
<span class="nt">Epoch 42/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.4872</span><span class="w"></span>
<span class="nt">Epoch 43/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5513</span><span class="w"></span>
<span class="nt">Epoch 44/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.4917</span><span class="w"></span>
<span class="nt">Epoch 45/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5198</span><span class="w"></span>
<span class="nt">Epoch 46/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.5007</span><span class="w"></span>
<span class="nt">Epoch 47/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.2530</span><span class="w"></span>
<span class="nt">Epoch 48/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.4154</span><span class="w"></span>
<span class="nt">Epoch 49/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.4927</span><span class="w"></span>
<span class="nt">Epoch 50/50</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15s loss =    2.4948</span><span class="w"></span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">valid_words_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">210</span><span class="p">,</span> <span class="mi">239</span><span class="p">,</span> <span class="mi">392</span><span class="p">,</span> <span class="mi">396</span><span class="p">])</span>

<span class="n">valid_words</span><span class="p">,</span> <span class="n">nearests</span> <span class="o">=</span> <span class="n">model_CBOW</span><span class="o">.</span><span class="n">nearest_words</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">valid_words_index</span><span class="p">,</span> <span class="n">top_nearest</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_words</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Nearest to </span><span class="se">\'</span><span class="si">{}</span><span class="se">\'</span><span class="s1">: '</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">valid_words</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">nearests</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="l l-Scalar l-Scalar-Plain">Nearest to 'two'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'three'</span><span class="w"> </span><span class="s">'four'</span><span class="w"> </span><span class="s">'five'</span><span class="w"> </span><span class="s">'six'</span><span class="w"> </span><span class="s">'seven'</span><span class="w"> </span><span class="s">'eight'</span><span class="w"> </span><span class="s">'nine'</span><span class="w"> </span><span class="s">'zero'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'that'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'which'</span><span class="w"> </span><span class="s">'what'</span><span class="w"> </span><span class="s">'furthermore'</span><span class="w"> </span><span class="s">'however'</span><span class="w"> </span><span class="s">'talmudic'</span><span class="w"> </span><span class="s">'endress'</span><span class="w"> </span><span class="s">'tonight'</span><span class="w"></span>
<span class="w"> </span><span class="s">'how'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'his'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'her'</span><span class="w"> </span><span class="s">'their'</span><span class="w"> </span><span class="s">'my'</span><span class="w"> </span><span class="s">'your'</span><span class="w"> </span><span class="s">'its'</span><span class="w"> </span><span class="s">'our'</span><span class="w"> </span><span class="s">'the'</span><span class="w"> </span><span class="s">'photographs'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'were'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'are'</span><span class="w"> </span><span class="s">'have'</span><span class="w"> </span><span class="s">'include'</span><span class="w"> </span><span class="s">'contain'</span><span class="w"> </span><span class="s">'was'</span><span class="w"> </span><span class="s">'vigorous'</span><span class="w"> </span><span class="s">'tend'</span><span class="w"> </span><span class="s">'substituting'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'all'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'various'</span><span class="w"> </span><span class="s">'both'</span><span class="w"> </span><span class="s">'many'</span><span class="w"> </span><span class="s">'every'</span><span class="w"> </span><span class="s">'shamed'</span><span class="w"> </span><span class="s">'everyone'</span><span class="w"> </span><span class="s">'those'</span><span class="w"> </span><span class="s">'wiccan'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'area'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'areas'</span><span class="w"> </span><span class="s">'region'</span><span class="w"> </span><span class="s">'regions'</span><span class="w"> </span><span class="s">'taipan'</span><span class="w"> </span><span class="s">'northeast'</span><span class="w"> </span><span class="s">'boundaries'</span><span class="w"> </span><span class="s">'hattin'</span><span class="w"></span>
<span class="w"> </span><span class="s">'surface'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'east'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'west'</span><span class="w"> </span><span class="s">'southeast'</span><span class="w"> </span><span class="s">'south'</span><span class="w"> </span><span class="s">'northwest'</span><span class="w"> </span><span class="s">'southwest'</span><span class="w"> </span><span class="s">'eastern'</span><span class="w"> </span><span class="s">'northeast'</span><span class="w"></span>
<span class="w"> </span><span class="s">'north'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'himself'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'him'</span><span class="w"> </span><span class="s">'themselves'</span><span class="w"> </span><span class="s">'herself'</span><span class="w"> </span><span class="s">'itself'</span><span class="w"> </span><span class="s">'them'</span><span class="w"> </span><span class="s">'donal'</span><span class="w"> </span><span class="s">'activex'</span><span class="w"> </span><span class="s">'carnaval'</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">Nearest to 'white'</span><span class="p p-Indicator">:</span><span class="w">  </span><span class="p p-Indicator">[</span><span class="s">'black'</span><span class="w"> </span><span class="s">'red'</span><span class="w"> </span><span class="s">'morel'</span><span class="w"> </span><span class="s">'green'</span><span class="w"> </span><span class="s">'bluish'</span><span class="w"> </span><span class="s">'dead'</span><span class="w"> </span><span class="s">'blue'</span><span class="w"> </span><span class="s">'lessig'</span><span class="p p-Indicator">]</span><span class="w"></span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s1">'More labels than embeddings'</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>  <span class="c1"># in inches</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">'offset points'</span><span class="p">,</span>
                   <span class="n">ha</span><span class="o">=</span><span class="s1">'right'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">)</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">visualization_words</span> <span class="o">=</span> <span class="mi">800</span>
<span class="c1"># transform embeddings to 2D by t-SNE</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">model_CBOW</span><span class="o">.</span><span class="n">embedding_matrix</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">'pca'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">'exact'</span><span class="p">)</span>
<span class="n">two_d_embed</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
<span class="c1"># list labels</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_CBOW</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="c1"># plot</span>
<span class="n">plot</span><span class="p">(</span><span class="n">two_d_embed</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/05_output_20_0.png"/></p>
<h3 id="reference">Reference</h3>
<ul>
<li>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "1em",
        linebreak = "true";

    if (true) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</div>
<div class="center social-share">
<p>Like this article? Share it with your friends!</p>
<div class="addthis_native_toolbox"></div>
<div class="addthis_sharing_toolbox"></div>
<div class="addthis_inline_share_toolbox"></div>
</div>
<div class="neighbors">
<a class="btn float-left" href="https://ycc.idv.tw/tensorflow-tutorial_4.html#anchor" title="實作Tensorflow (4)：Autoencoder">
<i class="fa fa-angle-left"></i> Previous Post
    </a>
<a class="btn float-right" href="https://ycc.idv.tw/tensorflow-tutorial_6.html#anchor" title="實作Tensorflow (6)：Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)">
      Next Post <i class="fa fa-angle-right"></i>
</a>
</div>
<div class="addthis_relatedposts_inline"></div>
<div class="related-posts">
<h4>You might enjoy</h4>
<ul class="related-posts">
<li><a href="https://ycc.idv.tw/ml-course-techniques_6.html">機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)</a></li>
<li><a href="https://ycc.idv.tw/tensorflow-tutorial_2.html">實作Tensorflow (2)：Build First Deep Neurel Network (DNN)</a></li>
<li><a href="https://ycc.idv.tw/tensorflow-tutorial_3.html">實作Tensorflow (3)：Build First Convolutional Neurel Network (CNN)</a></li>
<li><a href="https://ycc.idv.tw/tensorflow-tutorial_4.html">實作Tensorflow (4)：Autoencoder</a></li>
<li><a href="https://ycc.idv.tw/tensorflow-tutorial_6.html">實作Tensorflow (6)：Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)</a></li>
</ul>
</div>
<!-- Disqus -->
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'ycnote-1';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>
    Please enable JavaScript to view comments.
</noscript>
<!-- End Disqus -->
</article>
<footer>
<p>
  © 2023  - This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" rel="license" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p><p>
<a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license" target="_blank">
<img alt="Creative Commons License" height="15" src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png" style="border-width:0" title="Creative Commons License" width="80"/>
</a>
</p></footer> </main>
<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " YC Note ",
  "url" : "https://ycc.idv.tw",
  "image": "",
  "description": "[ YC Note - ML/DL Tech Blog ] Hello, I am YC, an ML engineer/researcher with experience in CV, NLP/NLU, and Recommender. I built this blog for anyone interested in data science and machine learning."
}
</script><script async="async" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-63b4eabb5e84e9fb" type="text/javascript"></script>
<script>
    window.loadStorkIndex = async (input_obj) => {
      input_obj.disabled = true;
      input_obj.placeholder = 'Downloading index file, please wait ...'
      await stork.register("sitesearch", "https://ycc.idv.tw/search-index.st", { showProgress: false });
      input_obj.placeholder = 'Search ...'
      input_obj.disabled = false;
    }
  </script>
<script src="https://files.stork-search.net/releases/v1.5.0/stork.js"></script>
</body>
</html>