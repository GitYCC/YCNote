<!DOCTYPE html>
<html lang="zh">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="機器有辦法自行從文本中觀察出詞彙間的相似度嗎？是可以的，word2vec是&#34;word to vector&#34;的縮寫，代表的正是將每個字轉換成向量，而一旦兩個字的向量越是靠近，就代表它的相似度越高，我們究竟要如何得到這些向量呢？方法簡單但出奇有效，文章的最後會向大家呈現它的精彩的結果。 本單元程式碼Skip-Gram Word2Vec部分可於Github下載，CBOW ...">
        <meta name="keywords" content="Tensorflow">
        <link rel="icon" href="https://www.ycc.idv.tw/static/img/favicon.png">

        <title>實作Tensorflow (5)：Word2Vec - YC Note</title>

        <!-- Stylesheets -->
        <link href="https://www.ycc.idv.tw/theme/css/all.min.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script type="text/x-mathjax-config"> 
            MathJax.Hub.Config({ 
                "HTML-CSS": { scale: 90, linebreaks: { automatic: true } }, 
                SVG: { linebreaks: { automatic:true } } 
                });
        </script>


        <!-- Google Analytics -->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-68393177-2', 'auto');
          ga('send', 'pageview');
        </script>
        <!-- /Google Analytics -->


    </head>

    <body>

        <!-- Header -->
    <div class="header-container" style="background: linear-gradient(rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0.2)), url('https://www.ycc.idv.tw/images/tensorflow-logo.jpg'); background-position: center; background-size: cover;">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="https://www.ycc.idv.tw/"><img class="logo" src="https://www.ycc.idv.tw/static/img/favicon.png" alt="logo">YC Note</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="https://www.ycc.idv.tw/category/coding.html">Coding</a>
                                <a href="https://www.ycc.idv.tw/category/aiml.html">AI.ML</a>
                                <a href="https://www.ycc.idv.tw/category/reading.html">Reading</a>
                                <a href="https://www.ycc.idv.tw/category/recording.html">Recording</a>
                                <a href="https://www.ycc.idv.tw/about-me.html">About Me</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">實作Tensorflow (5)：Word2Vec</h1>
                      <p class="header-date">By <a href="https://www.ycc.idv.tw/author/yc-chen.html">YC Chen</a>, 2017 / 11月 19, in category <a href="https://www.ycc.idv.tw/category/aiml.html">AI.ML</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="https://www.ycc.idv.tw/tag/tensorflow.html">Tensorflow</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <p>機器有辦法自行從文本中觀察出詞彙間的相似度嗎？是可以的，word2vec是"word to vector"的縮寫，代表的正是將每個字轉換成向量，而一旦兩個字的向量越是靠近，就代表它的相似度越高，我們究竟要如何得到這些向量呢？方法簡單但出奇有效，文章的最後會向大家呈現它的精彩的結果。</p>
<p>本單元程式碼Skip-Gram Word2Vec部分可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/05_1_word2vec_SkipGram.py">Github</a>下載，CBOW Word2Vec部分可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/05_2_word2vec_CBOW.py">Github</a>下載。</p>
<p><br/></p>
<h3>Word2Vec觀念解析</h3>
<p>Word2Vec的形式和Autoencoder有點像，一樣是從高維度的空間轉換到低維度的空間，再轉換回去原本的維度，只是這一次轉回去的東西不再是原本一模一樣的東西了。</p>
<p>Word2Vec的Input和Output這次變成是上下文的文字組合，舉個例子，"by the way"這個用法如果多次被機器看過的話，機器是有辦法去學習到這樣的規律的，此時"by"與"the"和"way"便會產生一個上下文的關聯性，為了將這樣的關聯性建立起來，我們希望當我輸入"by"時，機器有辦法預測並輸出"the"或"way"，這代表在機器內部它已經學習到了上下文的關聯性。</p>
<p>那如果今天這個機器也同時看到很多次的"on the way"這種用法，所以當我輸入"on"時，機器要有辦法預測並輸出"the"或"way"，但是我們不希望"on"和"by"兩個詞在學習時是分開學習的，我們希望機器可以因為"by the way"和"on the way"的結構很相似，所以有辦法抓出"on"和"by"是彼此相似的結論。</p>
<p>如何做到呢？答案就是限縮這個上下文的關聯性的儲存維度，如果我的字彙量有1000個，這1000個字彙彼此有上下文的關聯性，最完整表示上下文關聯性的方法就是設置一個1000x1000或者更大的表格，把所有字彙間的上下文關聯性全部存起來，但我們不想要這麼做，我要求機器用更小的表格來儲存上下文的關聯性，此時機器被迫將一些詞彙使用同樣的表格位置，同樣的轉換。一旦限縮了上下文關聯性的儲存維度，"on the way"和"by the way"中的"on"和"by"就會被迫分為同一類，因此我們成功的建立了字詞間的相似性關係。</p>
<h3>Word2Vec的架構</h3>
<p><img alt="word2vec" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.008.jpeg"></p>
<p>實作上如上圖所示，我們輸入一個字詞，譬如"cat"，通常會將他轉成One-hot encoding表示，但要注意喔！文本的字彙量是非常龐大的，所以當我們使用One-hot encoding表示時，將會出現一個非常長但Sparse的向量，相同的輸出層也同樣是一個很長的One-hot encoding，它的維度會和輸入層一樣大，因為我們要分析的字彙在輸入和輸出是一樣多的。</p>
<p>然後，和Autoencoder使用一樣的手法，中間的Hidden Layer放置低維度、少神經元的一層，但不同於Autoencoder，Word2Vec所有的轉換都是線性的，沒有非線性的Activation Function夾在其中，為什麼呢？因為我們的輸入是Sparse的而且只有0和1的差別，所以每一條通路就變成只有導通或不導通的差別，Activation Function有加等於沒加，使用線性就足夠了。</p>
<p>這個中間的Hidden Layer被稱為Embedding Matrix，它做了一個線性的Dimension Reduction，將原本高維度的One-hot encoding降低成低維度，然後再透過一個線性模型轉換回去原本的維度。假設字彙的數量有N個，所以輸入矩陣X是一個1xN的矩陣，輸出的矩陣同樣也是1xN的矩陣，當我先做一個線性的Dimension Reduction，將維度降到d維，此時Embedding Matrix會是一個Nxd的矩陣V，然後再由線性模型轉換回去原本的維度，這個轉換矩陣W是一個Nxd矩陣，因此綜合上述，可用一個簡潔的表示式表示：<span class="math">\(Y=W^T VX\)</span>，我們的目標就是找出這個W和V矩陣的每個元素。</p>
<p>你會想說線性模型很簡單啊！就是仿照Autoencoder的作法，然後把Activation Function拿掉不就了事了，並且因為輸出是One-hot Encoding所以最後套用Softmax，那不就輕鬆完成！但是真正的大魔王就出在字彙量，字彙量一旦很大，事情就變得不可收拾了，而且字彙量是一定小不得的，那怎麼辦？</p>
<p>在Dimension Reduction我們可以採取一個快速的方法，因為除了我要表示的字的位置是1以外其他都是0，所以其他都可以不看，我們就直接看是在第幾個位置上是1，然後再到Embedding Matrix上找到相應的行直接取出就是答案了，這樣查詢的動作，在Tensorflow中可以使用<code>tf.nn.embedding_lookup</code>來辦到。</p>
<p>再接下來最後的Cross-Entropy Loss計算也非常龐大，因為有幾個字彙就需要累加幾組數字，我們有一招偷吃步的方法叫做「Sampled Softmax」，作法是這樣的，我們不去計算全部詞彙的Cross-Entropy，而是選擇幾組詞彙來評估Cross-Entropy，在選擇上我們會隨機挑選一些Labels和預測結果差異度很大的詞彙(稱為Negative Examples)來算Cross-Entropy，我們在Tensorflow可以使用<code>tf.nn.sampled_softmax_loss</code>來辦到「Sampled Softmax」。</p>
<p>我們先不管輸入和輸出究竟怎麼取得，如果我們成功的建立了輸入和輸出的上下文關係，此時中間的Embedding空間正是精華的所在，經過剛剛推論，我們預期在這個空間當中，相似的詞彙會彼此靠近，我們評估兩個向量的相似性可以使用Cosine來評估，當兩向量的夾角越小代表它們越是相似，待會的實作當中我們將會利用Cosine來建立Similarity的大小，藉此來找到前幾個和它很靠近的詞彙。</p>
<p>另外，經研究指出這個Embedding空間的效果不只是可以算出詞彙間的相似性，還可以顯示詞彙間的比較關係，例如：北京之於中國，等同於台北之於台灣，這樣的比較關係也顯示在這個Embedding空間裡頭，所以在這空間裡會有以下的向量關係式：<span class="math">\(V_{北京} - V_{中國}+V_{台灣}=V_{台北}\)</span>，是不是很神奇啊！</p>
<p><br/></p>
<h3>Word2Vec的兩種常用方法：Skip-Gram和CBOW</h3>
<p><img alt="Skip-Gram和CBOW" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.009.jpeg"></p>
<p>剛剛一直在講的是中間的結構應該怎麼建立，現在來看看我們可以輸入和輸出哪些詞彙來建立起上下文的關係，有兩種常用的類別：Skip-Gram和CBOW。</p>
<p>Skip-Gram如上圖所示，當我輸入一個<span class="math">\(word(t)\)</span>時，我希望它能輸出它的前文和後文，這是相當直覺的建立上下文的方法，所以如果我希望用前一個字和後一個字來訓練我的Word2Vec，我就會有兩組數據：<span class="math">\((w(t),w(t-1))\)</span>和<span class="math">\((w(t),w(t+1))\)</span>，相當好理解。</p>
<p>而CBOW(Continuous Bag of Words)使用另外一種方法來建立上下文關係，它將一排字挖掉中間一個字，然後希望由上下文的關係有辦法猜出中間那個字，就像是填空題，此時輸入層就變成會有多於1個字，那該怎麼處理，答案是轉換到Embedding空間後再相加平均，因為是線性轉換，所以直接線性累加就可以了。</p>
<p><br/></p>
<h3>準備文本語料庫</h3>
<p>先帶入一些待會會用到的函式庫，並且決定我們要取用多少<code>VOCABULARY_SIZE</code>個詞彙量來做訓練。</p>
<div class="highlight"><pre><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">generators</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">six.moves.urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">VOCABULARY_SIZE</span> <span class="o">=</span> <span class="mi">100000</span>
</pre></div>


<p>接下來下載Dataset，並做一些前處理。</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">maybe_download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span><span class="n">filename</span><span class="p">,</span> <span class="n">expected_bytes</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Download a file if not present, and make sure it&#39;s the right size.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
  <span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">stat</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span> <span class="o">==</span> <span class="n">expected_bytes</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Found and verified </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">filename</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
      <span class="s">&#39;Failed to verify &#39;</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s">&#39;. Can you get to it with a browser?&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">filename</span>

<span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Extract the first file enclosed in a zip file as a list of words&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">namelist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">data</span>

<span class="k">def</span> <span class="nf">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span><span class="n">vocabulary_size</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">):</span>
  <span class="n">count</span> <span class="o">=</span> <span class="p">[[</span><span class="s">&#39;UNK&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
  <span class="n">count</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">vocabulary_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">count</span><span class="p">:</span>
    <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>
  <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="n">unk_count</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c"># dictionary[&#39;UNK&#39;]</span>
      <span class="n">unk_count</span> <span class="o">=</span> <span class="n">unk_count</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
  <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">unk_count</span>
  <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span> 
  <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;Downloading text8.zip&quot;</span><span class="p">)</span>
<span class="n">filename</span> <span class="o">=</span> <span class="n">maybe_download</span><span class="p">(</span><span class="s">&#39;http://mattmahoney.net/dc/text8.zip&#39;</span><span class="p">,</span><span class="s">&#39;./text8.zip&#39;</span><span class="p">,</span> <span class="mi">31344016</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;=====&quot;</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;Data size </span><span class="si">%d</span><span class="s">&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;First 10 words: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">10</span><span class="p">]))</span>

<span class="k">print</span><span class="p">(</span><span class="s">&quot;=====&quot;</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span>
                                                            <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">)</span>
<span class="k">del</span> <span class="n">words</span>  <span class="c"># Hint to reduce memory.</span>

<span class="k">print</span><span class="p">(</span><span class="s">&#39;Most common words (+UNK)&#39;</span><span class="p">,</span> <span class="n">count</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;Sample data&#39;</span><span class="p">,</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span class="n">Downloading</span> <span class="n">text8</span><span class="p">.</span><span class="n">zip</span>
<span class="n">Found</span> <span class="n">and</span> <span class="n">verified</span> <span class="p">.</span><span class="o">/</span><span class="n">text8</span><span class="p">.</span><span class="n">zip</span>
<span class="o">=====</span>
<span class="n">Data</span> <span class="n">size</span> <span class="mi">17005207</span>
<span class="n">First</span> <span class="mi">10</span> <span class="n">words</span><span class="o">:</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">anarchism</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">used</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">against</span><span class="err">&#39;</span><span class="p">]</span>
<span class="o">=====</span>
<span class="n">Most</span> <span class="n">common</span> <span class="n">words</span> <span class="p">(</span><span class="o">+</span><span class="n">UNK</span><span class="p">)</span> <span class="p">[[</span><span class="err">&#39;</span><span class="n">UNK</span><span class="err">&#39;</span><span class="p">,</span> <span class="mi">189230</span><span class="p">],</span> <span class="p">(</span><span class="err">&#39;</span><span class="n">the</span><span class="err">&#39;</span><span class="p">,</span> <span class="mi">1061396</span><span class="p">),</span> <span class="p">(</span><span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">,</span> <span class="mi">593677</span><span class="p">),</span> <span class="p">(</span><span class="err">&#39;</span><span class="n">and</span><span class="err">&#39;</span><span class="p">,</span> <span class="mi">416629</span><span class="p">),</span> <span class="p">(</span><span class="err">&#39;</span><span class="n">one</span><span class="err">&#39;</span><span class="p">,</span> <span class="mi">411764</span><span class="p">)]</span>
<span class="n">Sample</span> <span class="n">data</span> <span class="p">[</span><span class="mi">5234</span><span class="p">,</span> <span class="mi">3081</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">195</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3134</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">59</span><span class="p">,</span> <span class="mi">156</span><span class="p">]</span>
</pre></div>


<p>我們取用<code>VOCABULARY_SIZE = 100000</code>，也是說我們將文本中的詞彙按出現次數的多寡來排列，取前面<code>VOCABULARY_SIZE</code>個保留，其餘詞彙皆歸類到「UNK Token」裡頭，UNK代表UNKnown的縮寫。</p>
<p>我們文本的字詞數量總共有17005207個字，開頭前十個字的句子是'anarchism originated as a term of abuse first used against'。所有的這17005207個字會依照<code>dictionary</code>給予每個字Index，而文本會被表示為一個由整數所構成的List，這會放在<code>data</code>裡頭，而這個Index也就直接當作One-hot Encoding中代表這個詞彙的維度位置。當我想要把Index轉換回去我們看得懂的字的時候，就需要<code>reverse_dictionary</code>的幫忙，有了這些，我們的語料庫就已經建立完成了。</p>
<p><br/></p>
<h3>實作Skip-Gram</h3>
<p>有了語料庫，我們就可以產生出我想要的輸入和輸出，在Skip-Gram方法，如果我的輸入是<code>target word</code>，我會先從<code>target word</code>向前、向後看出去<code>skip_window</code>的大小，所以可以選擇當作輸出的字有<code>skip_window*2</code>個，接下來我從這<code>skip_window*2</code>個中選擇<code>num_skips</code>個當作輸出，所以一個<code>target word</code>會產生<code>num_skips</code>筆數據，如果我一個batch需要<code>batch_size</code>筆數據，我就必須有<code>batch_size//num_skips</code>個<code>target word</code>，依照這樣的規則下面建立一個Generator來掃描文本，並輸出要訓練使用的Batch Data。</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">skip_gram_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">num_skips</span><span class="p">,</span><span class="n">skip_window</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">num_skips</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">num_skips</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span>

    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span> <span class="o">+</span> <span class="mi">1</span> <span class="c"># [ skip_window target skip_window ]</span>
    <span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>

    <span class="c"># initialization</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">):</span>
        <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c"># generate</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">skip_window</span>  <span class="c"># target label at the center of the buffer</span>
        <span class="n">targets_to_avoid</span> <span class="o">=</span> <span class="p">[</span> <span class="n">target</span> <span class="p">]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_skips</span><span class="p">):</span>
            <span class="k">while</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets_to_avoid</span><span class="p">:</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">span</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">targets_to_avoid</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">skip_window</span><span class="p">]</span>
            <span class="n">labels</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
            <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c"># Recycle </span>
        <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span> <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c"># scan data</span>
        <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="c"># Enough num to output</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

<span class="c"># demonstrate generator</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;data:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">di</span><span class="p">]</span> <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">]])</span>

<span class="k">for</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]:</span>
    <span class="n">batch_generator</span> <span class="o">=</span> <span class="n">skip_gram_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span><span class="n">num_skips</span><span class="o">=</span><span class="n">num_skips</span><span class="p">,</span><span class="n">skip_window</span><span class="o">=</span><span class="n">skip_window</span><span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\n</span><span class="s">with num_skips = </span><span class="si">%d</span><span class="s"> and skip_window = </span><span class="si">%d</span><span class="s">:&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;    batch:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="k">for</span> <span class="n">bi</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;    labels:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">li</span><span class="p">]</span> <span class="k">for</span> <span class="n">li</span> <span class="ow">in</span> <span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">)])</span>
</pre></div>


<div class="highlight"><pre><span class="nl">data:</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">anarchism</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">used</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">against</span><span class="err">&#39;</span><span class="p">]</span>

<span class="n">with</span> <span class="n">num_skips</span> <span class="o">=</span> <span class="mi">2</span> <span class="n">and</span> <span class="n">skip_window</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span>
    <span class="nl">batch:</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">]</span>
    <span class="nl">labels:</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">anarchism</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">]</span>

<span class="n">with</span> <span class="n">num_skips</span> <span class="o">=</span> <span class="mi">4</span> <span class="n">and</span> <span class="n">skip_window</span> <span class="o">=</span> <span class="mi">2</span><span class="o">:</span>
    <span class="nl">batch:</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">]</span>
    <span class="nl">labels:</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">anarchism</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span class="k">class</span> <span class="nc">SkipGram</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_vocabulary</span><span class="p">,</span><span class="n">n_embedding</span><span class="p">,</span><span class="n">reverse_dictionary</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span> <span class="o">=</span> <span class="n">n_vocabulary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span> <span class="o">=</span> <span class="n">n_embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">reverse_dictionary</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span> <span class="c"># initialize new grap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span> <span class="c"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span> <span class="c"># create session by the graph </span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c">### Input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

            <span class="c">### Optimalization</span>
            <span class="c"># build neurel network structure and get their loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span> <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">,</span>
                                        <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">,</span>
                                      <span class="p">)</span>

            <span class="c"># normalize embeddings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s">&#39;embeddings&#39;</span><span class="p">]),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s">&#39;embeddings&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span>

            <span class="c"># define training operation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="c">### Prediction</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span> <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">,</span>
                                            <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">,</span>
                                          <span class="p">)</span>

            <span class="c"># similarity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span><span class="p">,</span> 
                                            <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">))</span>

            <span class="c">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>  

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dataset</span><span class="p">,</span><span class="n">labels</span><span class="p">):</span>
        <span class="c">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">&#39;embeddings&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                                <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span> 
                                                  <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span>
                <span class="s">&#39;softmax&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                             <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span>
                               <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">)))</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">&#39;softmax&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">]))</span>
            <span class="p">}</span>


        <span class="c">### Structure</span>
        <span class="c"># Look up embeddings for inputs.</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s">&#39;embeddings&#39;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">)</span>

        <span class="c"># Compute the softmax loss, using a sample of the negative labels each time.</span>
        <span class="n">num_softmax_sampled</span> <span class="o">=</span> <span class="mi">64</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                 <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sampled_softmax_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s">&#39;softmax&#39;</span><span class="p">],</span> 
                                            <span class="n">biases</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s">&#39;softmax&#39;</span><span class="p">],</span> 
                                            <span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span>
                                            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
                                            <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_softmax_sampled</span><span class="p">,</span> 
                                            <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>


    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">online_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>      
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">}</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">nearest_words</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">top_nearest</span><span class="p">):</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span><span class="p">,</span>
                                   <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>
        <span class="n">X_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">valid_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">nearests</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_size</span><span class="p">):</span>
            <span class="n">valid_word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">valid_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">valid_word</span><span class="p">)</span>    

            <span class="c"># select highest similarity word</span>
            <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">similarity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_nearest</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>  
            <span class="n">nearests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">nearest</span><span class="p">)))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">valid_words</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">nearests</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">embedding_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">find_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</pre></div>


<p>以上就是我建立的Model，這裡我採取<code>online_fit</code>的方法，不同於之前的<code>fit</code>，<code>online_fit</code>可以不用事先將所有Data一次餵進去，而是可以陸續的餵入Data，所以我會從上面的Generator陸續產生Batch Data並餵入Model裡來做訓練。</p>
<div class="highlight"><pre><span class="c"># build skip-gram batch generator</span>
<span class="n">batch_generator</span> <span class="o">=</span> <span class="n">skip_gram_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                                            <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                                            <span class="n">num_skips</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                            <span class="n">skip_window</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># build skip-gram model</span>
<span class="n">model_SkipGram</span> <span class="o">=</span> <span class="n">SkipGram</span><span class="p">(</span><span class="n">n_vocabulary</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">,</span>
                          <span class="n">n_embedding</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                          <span class="n">reverse_dictionary</span><span class="o">=</span><span class="n">reverse_dictionary</span><span class="p">,</span>
                          <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="c"># initial model</span>
<span class="n">model_SkipGram</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

<span class="c"># online training</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_batchs_in_epoch</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batchs_in_epoch</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model_SkipGram</span><span class="o">.</span><span class="n">online_fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
                                         <span class="n">Y</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">loss</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">avg_loss</span> <span class="o">/</span> <span class="n">num_batchs_in_epoch</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Epoch </span><span class="si">%d</span><span class="s">/</span><span class="si">%d</span><span class="s">: </span><span class="si">%d</span><span class="s">s loss = </span><span class="si">%9.4f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span><span class="p">,</span>
                                                   <span class="n">avg_loss</span> <span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span class="n">Epoch</span> <span class="mi">1</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">18</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">4.2115</span>
<span class="n">Epoch</span> <span class="mi">2</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">17</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.7554</span>
<span class="n">Epoch</span> <span class="mi">3</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.6211</span>
<span class="n">Epoch</span> <span class="mi">4</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.5072</span>
<span class="n">Epoch</span> <span class="mi">5</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.5084</span>
<span class="n">Epoch</span> <span class="mi">6</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.4988</span>
<span class="n">Epoch</span> <span class="mi">7</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.5165</span>
<span class="n">Epoch</span> <span class="mi">8</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3949</span>
<span class="n">Epoch</span> <span class="mi">9</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.4382</span>
<span class="n">Epoch</span> <span class="mi">10</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.4121</span>
<span class="n">Epoch</span> <span class="mi">11</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.4027</span>
<span class="n">Epoch</span> <span class="mi">12</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.4074</span>
<span class="n">Epoch</span> <span class="mi">13</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3222</span>
<span class="n">Epoch</span> <span class="mi">14</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3448</span>
<span class="n">Epoch</span> <span class="mi">15</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">16</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3616</span>
<span class="n">Epoch</span> <span class="mi">16</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3389</span>
<span class="n">Epoch</span> <span class="mi">17</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3729</span>
<span class="n">Epoch</span> <span class="mi">18</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3911</span>
<span class="n">Epoch</span> <span class="mi">19</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3512</span>
<span class="n">Epoch</span> <span class="mi">20</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3107</span>
<span class="n">Epoch</span> <span class="mi">21</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">16</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3046</span>
<span class="n">Epoch</span> <span class="mi">22</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3103</span>
<span class="n">Epoch</span> <span class="mi">23</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3042</span>
<span class="n">Epoch</span> <span class="mi">24</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2634</span>
<span class="n">Epoch</span> <span class="mi">25</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3181</span>
<span class="n">Epoch</span> <span class="mi">26</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2808</span>
<span class="n">Epoch</span> <span class="mi">27</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2464</span>
<span class="n">Epoch</span> <span class="mi">28</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2246</span>
<span class="n">Epoch</span> <span class="mi">29</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2666</span>
<span class="n">Epoch</span> <span class="mi">30</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2275</span>
<span class="n">Epoch</span> <span class="mi">31</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2312</span>
<span class="n">Epoch</span> <span class="mi">32</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.3022</span>
<span class="n">Epoch</span> <span class="mi">33</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2504</span>
<span class="n">Epoch</span> <span class="mi">34</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2484</span>
<span class="n">Epoch</span> <span class="mi">35</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2368</span>
<span class="n">Epoch</span> <span class="mi">36</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2693</span>
<span class="n">Epoch</span> <span class="mi">37</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2177</span>
<span class="n">Epoch</span> <span class="mi">38</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2395</span>
<span class="n">Epoch</span> <span class="mi">39</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2151</span>
<span class="n">Epoch</span> <span class="mi">40</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.0505</span>
<span class="n">Epoch</span> <span class="mi">41</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.9364</span>
<span class="n">Epoch</span> <span class="mi">42</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.1546</span>
<span class="n">Epoch</span> <span class="mi">43</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.1810</span>
<span class="n">Epoch</span> <span class="mi">44</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2778</span>
<span class="n">Epoch</span> <span class="mi">45</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.1340</span>
<span class="n">Epoch</span> <span class="mi">46</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2218</span>
<span class="n">Epoch</span> <span class="mi">47</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2395</span>
<span class="n">Epoch</span> <span class="mi">48</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2422</span>
<span class="n">Epoch</span> <span class="mi">49</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.0131</span>
<span class="n">Epoch</span> <span class="mi">50</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.1287</span>
</pre></div>


<p>我們來看看效果如何，我們使用Embedding Vectors彼此間的Cosine來定義出字詞間的相關性，並且列出8個最為靠近的字詞。</p>
<div class="highlight"><pre><span class="n">valid_words_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">210</span><span class="p">,</span><span class="mi">239</span><span class="p">,</span><span class="mi">392</span><span class="p">,</span><span class="mi">396</span><span class="p">])</span>

<span class="n">valid_words</span><span class="p">,</span> <span class="n">nearests</span> <span class="o">=</span> <span class="n">model_SkipGram</span><span class="o">.</span><span class="n">nearest_words</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">valid_words_index</span><span class="p">,</span><span class="n">top_nearest</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_words</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Nearest to &#39;{}&#39;: &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">valid_words</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">nearests</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">two</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">three</span><span class="sc">&#39; &#39;</span><span class="n">four</span><span class="sc">&#39; &#39;</span><span class="n">five</span><span class="sc">&#39; &#39;</span><span class="n">eight</span><span class="sc">&#39; &#39;</span><span class="n">six</span><span class="sc">&#39; &#39;</span><span class="n">seven</span><span class="sc">&#39; &#39;</span><span class="n">nine</span><span class="sc">&#39; &#39;</span><span class="n">one</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">that</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">which</span><span class="sc">&#39; &#39;</span><span class="n">however</span><span class="sc">&#39; &#39;</span><span class="n">eophona</span><span class="sc">&#39; &#39;</span><span class="n">clemency</span><span class="sc">&#39; &#39;</span><span class="n">invariants</span><span class="sc">&#39; &#39;</span><span class="n">ratchet</span><span class="sc">&#39; &#39;</span><span class="n">what</span><span class="err">&#39;</span>
 <span class="err">&#39;</span><span class="n">fiona</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">his</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">her</span><span class="sc">&#39; &#39;</span><span class="n">their</span><span class="sc">&#39; &#39;</span><span class="n">my</span><span class="sc">&#39; &#39;</span><span class="n">your</span><span class="sc">&#39; &#39;</span><span class="n">its</span><span class="sc">&#39; &#39;</span><span class="n">our</span><span class="sc">&#39; &#39;</span><span class="n">thy</span><span class="sc">&#39; &#39;</span><span class="n">witchcraft</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">were</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">are</span><span class="sc">&#39; &#39;</span><span class="n">was</span><span class="sc">&#39; &#39;</span><span class="n">include</span><span class="sc">&#39; &#39;</span><span class="n">have</span><span class="sc">&#39; &#39;</span><span class="n">cyanobacterial</span><span class="sc">&#39; &#39;</span><span class="n">seem</span><span class="sc">&#39; &#39;</span><span class="n">be</span><span class="sc">&#39; &#39;</span><span class="n">those</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">all</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">both</span><span class="sc">&#39; &#39;</span><span class="n">various</span><span class="sc">&#39; &#39;</span><span class="n">many</span><span class="sc">&#39; &#39;</span><span class="n">counting</span><span class="sc">&#39; &#39;</span><span class="n">some</span><span class="sc">&#39; &#39;</span><span class="n">every</span><span class="sc">&#39; &#39;</span><span class="n">several</span><span class="sc">&#39; &#39;</span><span class="n">risked</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">area</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">region</span><span class="sc">&#39; &#39;</span><span class="n">areas</span><span class="sc">&#39; &#39;</span><span class="n">suctoria</span><span class="sc">&#39; &#39;</span><span class="n">regions</span><span class="sc">&#39; &#39;</span><span class="n">island</span><span class="sc">&#39; &#39;</span><span class="n">pwned</span><span class="sc">&#39; &#39;</span><span class="n">territory</span><span class="err">&#39;</span>
 <span class="err">&#39;</span><span class="n">plains</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">east</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">west</span><span class="sc">&#39; &#39;</span><span class="n">eastern</span><span class="sc">&#39; &#39;</span><span class="n">southeast</span><span class="sc">&#39; &#39;</span><span class="n">south</span><span class="sc">&#39; &#39;</span><span class="n">southwest</span><span class="sc">&#39; &#39;</span><span class="n">curable</span><span class="sc">&#39; &#39;</span><span class="n">north</span><span class="err">&#39;</span>
 <span class="err">&#39;</span><span class="n">hispaniolan</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">himself</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">him</span><span class="sc">&#39; &#39;</span><span class="n">themselves</span><span class="sc">&#39; &#39;</span><span class="n">them</span><span class="sc">&#39; &#39;</span><span class="n">itself</span><span class="sc">&#39; &#39;</span><span class="n">megalith</span><span class="sc">&#39; &#39;</span><span class="n">herself</span><span class="sc">&#39; &#39;</span><span class="n">successfully</span><span class="err">&#39;</span>
 <span class="err">&#39;</span><span class="n">armas</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">white</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">red</span><span class="sc">&#39; &#39;</span><span class="n">black</span><span class="sc">&#39; &#39;</span><span class="n">blue</span><span class="sc">&#39; &#39;</span><span class="n">yellow</span><span class="sc">&#39; &#39;</span><span class="n">green</span><span class="sc">&#39; &#39;</span><span class="n">overdraft</span><span class="sc">&#39; &#39;</span><span class="n">horse</span><span class="sc">&#39; &#39;</span><span class="n">dark</span><span class="err">&#39;</span><span class="p">]</span>
</pre></div>


<p>結果相當驚人，與'two'靠近的真的都是數字類型的文字，與'that'靠近的都是文法功能性的詞彙，與'his'靠近的都是所有格代名詞，與'were'靠近的是be動詞，與'all'最靠近的是'both'，與'east'靠近的都是一些代表方向的詞彙，與'white'靠近的都是一些顏色的詞彙，真的是太神奇了！</p>
<p>接下來直接來觀察Embedding空間，以下使用t-SNE來圖像化Embedding空間。</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s">&#39;More labels than embeddings&#39;</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>  <span class="c"># in inches</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s">&#39;offset points&#39;</span><span class="p">,</span>
                   <span class="n">ha</span><span class="o">=</span><span class="s">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">&#39;bottom&#39;</span><span class="p">)</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">visualization_words</span> <span class="o">=</span> <span class="mi">800</span>
<span class="c"># transform embeddings to 2D by t-SNE</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">model_SkipGram</span><span class="o">.</span><span class="n">embedding_matrix</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;pca&#39;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">&#39;exact&#39;</span><span class="p">)</span>
<span class="n">two_d_embed</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
<span class="c"># list labels</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_SkipGram</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="c"># plot</span>
<span class="n">plot</span><span class="p">(</span><span class="n">two_d_embed</span><span class="p">,</span><span class="n">words</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/05_output_13_0.png"></p>
<p>如此一來你將可以簡單的看出，哪些詞彙彼此相似而靠近。</p>
<p><br/></p>
<h3>實作CBOW (Continuous Bag of Words)</h3>
<p>接著看CBOW的方法，如果我預期輸出的字是<code>target word</code>，從<code>target word</code>向前向後看出去<code>context_window</code>的大小，看到的字都當作我的輸入，所以我輸入的字總共需要<code>context_window*2</code>個，一個<code>target word</code>只會產生一筆數據，如果我一個batch需要<code>batch_size</code>筆數據，我就必須有<code>batch_size</code>個<code>target word</code>，依照這樣的規則下面建立一個Generator來掃描文本，並輸出要訓練使用的Batch Data。</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">cbow_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">context_window</span><span class="p">):</span>
    <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">context_window</span> <span class="o">+</span> <span class="mi">1</span> <span class="c"># [ context_window target context_window ]</span>
    <span class="n">num_bow</span> <span class="o">=</span> <span class="n">span</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_bow</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>

    <span class="c"># initialization</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">):</span>
        <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c"># generate</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">context_window</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>        
        <span class="n">bow</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">bow</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bow</span><span class="p">):</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span>
        <span class="n">labels</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c"># Recycle </span>
        <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span> <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c"># scan data</span>
        <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="c"># Enough num to output</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>



<span class="c"># demonstrate generator</span>
<span class="k">print</span><span class="p">(</span><span class="s">&#39;data:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">di</span><span class="p">]</span> <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">]])</span>

<span class="k">for</span> <span class="n">context_window</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">batch_generator</span> <span class="o">=</span> <span class="n">cbow_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                           <span class="n">context_window</span><span class="o">=</span><span class="n">context_window</span><span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\n</span><span class="s">with context_window = </span><span class="si">%d</span><span class="s">:&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">context_window</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;batch:&#39;</span><span class="p">)</span>
    <span class="n">show_batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">tmp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]])</span>
        <span class="n">show_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">show_batch</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;labels:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">li</span><span class="p">]</span> <span class="k">for</span> <span class="n">li</span> <span class="ow">in</span> <span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">)])</span>
</pre></div>


<div class="highlight"><pre><span class="nl">data:</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">anarchism</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">used</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">against</span><span class="err">&#39;</span><span class="p">]</span>

<span class="n">with</span> <span class="n">context_window</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span>
<span class="nl">batch:</span>
<span class="p">[[</span><span class="err">&#39;</span><span class="n">anarchism</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">used</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">against</span><span class="err">&#39;</span><span class="p">]]</span>
<span class="nl">labels:</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">used</span><span class="err">&#39;</span><span class="p">]</span>

<span class="n">with</span> <span class="n">context_window</span> <span class="o">=</span> <span class="mi">2</span><span class="o">:</span>
<span class="nl">batch:</span>
<span class="p">[[</span><span class="err">&#39;</span><span class="n">anarchism</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">originated</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">used</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">used</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">against</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">against</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">early</span><span class="err">&#39;</span><span class="p">],</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">used</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">early</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">working</span><span class="err">&#39;</span><span class="p">]]</span>
<span class="nl">labels:</span> <span class="p">[</span><span class="err">&#39;</span><span class="n">as</span><span class="err">&#39;</span><span class="p">,</span> <span class="sc">&#39;a&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">term</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">of</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">abuse</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">first</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">used</span><span class="err">&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">against</span><span class="err">&#39;</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span class="k">class</span> <span class="nc">CBOW</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_vocabulary</span><span class="p">,</span><span class="n">n_embedding</span><span class="p">,</span><span class="n">context_window</span><span class="p">,</span><span class="n">reverse_dictionary</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span> <span class="o">=</span> <span class="n">n_vocabulary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span> <span class="o">=</span> <span class="n">n_embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_window</span> <span class="o">=</span> <span class="n">context_window</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">reverse_dictionary</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span> <span class="c"># initialize new grap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span> <span class="c"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span> <span class="c"># create session by the graph </span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c">### Input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_window</span><span class="o">*</span><span class="mi">2</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

            <span class="c">### Optimalization</span>
            <span class="c"># build neurel network structure and get their predictions and loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span> <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">,</span>
                                        <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">,</span>
                                      <span class="p">)</span>

            <span class="c"># normalize embeddings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s">&#39;embeddings&#39;</span><span class="p">]),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s">&#39;embeddings&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span>

            <span class="c"># define training operation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="c">### Prediction</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

            <span class="c"># similarity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span><span class="p">,</span> 
                                            <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">))</span>

            <span class="c">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>  

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dataset</span><span class="p">,</span><span class="n">labels</span><span class="p">):</span>
        <span class="c">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">&#39;embeddings&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                                <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span>
                                                  <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span>
                <span class="s">&#39;softmax&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                             <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span>
                               <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">)))</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s">&#39;softmax&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">]))</span>
            <span class="p">}</span>


        <span class="c">### Structure</span>
        <span class="c"># Look up embeddings for inputs.</span>
        <span class="n">embed_bow</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s">&#39;embeddings&#39;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">)</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">embed_bow</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c"># Compute the softmax loss, using a sample of the negative labels each time.</span>
        <span class="n">num_softmax_sampled</span> <span class="o">=</span> <span class="mi">64</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                 <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sampled_softmax_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s">&#39;softmax&#39;</span><span class="p">],</span> 
                                            <span class="n">biases</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s">&#39;softmax&#39;</span><span class="p">],</span> 
                                            <span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span>
                                            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
                                            <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_softmax_sampled</span><span class="p">,</span> 
                                            <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>


    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">online_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>      
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">}</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">nearest_words</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">top_nearest</span><span class="p">):</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>
        <span class="n">X_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">valid_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">nearests</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_size</span><span class="p">):</span>
            <span class="n">valid_word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">valid_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">valid_word</span><span class="p">)</span>    

            <span class="c"># select highest similarity word</span>
            <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">similarity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_nearest</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">nearests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">nearest</span><span class="p">)))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">valid_words</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">nearests</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">embedding_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">find_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span class="n">context_window</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c"># build CBOW batch generator</span>
<span class="n">batch_generator</span> <span class="o">=</span> <span class="n">cbow_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                                       <span class="n">context_window</span><span class="o">=</span><span class="n">context_window</span><span class="p">)</span>

<span class="c"># build CBOW model</span>
<span class="n">model_CBOW</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">n_vocabulary</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">,</span>
                  <span class="n">n_embedding</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                  <span class="n">context_window</span><span class="o">=</span><span class="n">context_window</span><span class="p">,</span>
                  <span class="n">reverse_dictionary</span><span class="o">=</span><span class="n">reverse_dictionary</span><span class="p">,</span>
                  <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c"># initialize model</span>
<span class="n">model_CBOW</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

<span class="c"># online training</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_batchs_in_epoch</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batchs_in_epoch</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model_CBOW</span><span class="o">.</span><span class="n">online_fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
                                     <span class="n">Y</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">loss</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">avg_loss</span> <span class="o">/</span> <span class="n">num_batchs_in_epoch</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Epoch </span><span class="si">%d</span><span class="s">/</span><span class="si">%d</span><span class="s">: </span><span class="si">%d</span><span class="s">s loss = </span><span class="si">%9.4f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span><span class="p">,</span>
                                                   <span class="n">avg_loss</span> <span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span class="n">Epoch</span> <span class="mi">1</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.8643</span>
<span class="n">Epoch</span> <span class="mi">2</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.2952</span>
<span class="n">Epoch</span> <span class="mi">3</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.1950</span>
<span class="n">Epoch</span> <span class="mi">4</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.1204</span>
<span class="n">Epoch</span> <span class="mi">5</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.0737</span>
<span class="n">Epoch</span> <span class="mi">6</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">3.0243</span>
<span class="n">Epoch</span> <span class="mi">7</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.9382</span>
<span class="n">Epoch</span> <span class="mi">8</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.9539</span>
<span class="n">Epoch</span> <span class="mi">9</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.9690</span>
<span class="n">Epoch</span> <span class="mi">10</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.9003</span>
<span class="n">Epoch</span> <span class="mi">11</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.8737</span>
<span class="n">Epoch</span> <span class="mi">12</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.8308</span>
<span class="n">Epoch</span> <span class="mi">13</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.8444</span>
<span class="n">Epoch</span> <span class="mi">14</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.7676</span>
<span class="n">Epoch</span> <span class="mi">15</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.7811</span>
<span class="n">Epoch</span> <span class="mi">16</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.7926</span>
<span class="n">Epoch</span> <span class="mi">17</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.7528</span>
<span class="n">Epoch</span> <span class="mi">18</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.7552</span>
<span class="n">Epoch</span> <span class="mi">19</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.7353</span>
<span class="n">Epoch</span> <span class="mi">20</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.6232</span>
<span class="n">Epoch</span> <span class="mi">21</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5206</span>
<span class="n">Epoch</span> <span class="mi">22</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.7120</span>
<span class="n">Epoch</span> <span class="mi">23</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.6625</span>
<span class="n">Epoch</span> <span class="mi">24</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.7351</span>
<span class="n">Epoch</span> <span class="mi">25</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5335</span>
<span class="n">Epoch</span> <span class="mi">26</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.6600</span>
<span class="n">Epoch</span> <span class="mi">27</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.6636</span>
<span class="n">Epoch</span> <span class="mi">28</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5972</span>
<span class="n">Epoch</span> <span class="mi">29</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5400</span>
<span class="n">Epoch</span> <span class="mi">30</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.6047</span>
<span class="n">Epoch</span> <span class="mi">31</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5544</span>
<span class="n">Epoch</span> <span class="mi">32</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5932</span>
<span class="n">Epoch</span> <span class="mi">33</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5554</span>
<span class="n">Epoch</span> <span class="mi">34</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5256</span>
<span class="n">Epoch</span> <span class="mi">35</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5664</span>
<span class="n">Epoch</span> <span class="mi">36</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5977</span>
<span class="n">Epoch</span> <span class="mi">37</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5392</span>
<span class="n">Epoch</span> <span class="mi">38</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5666</span>
<span class="n">Epoch</span> <span class="mi">39</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5123</span>
<span class="n">Epoch</span> <span class="mi">40</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5169</span>
<span class="n">Epoch</span> <span class="mi">41</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.4920</span>
<span class="n">Epoch</span> <span class="mi">42</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.4872</span>
<span class="n">Epoch</span> <span class="mi">43</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5512</span>
<span class="n">Epoch</span> <span class="mi">44</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.4895</span>
<span class="n">Epoch</span> <span class="mi">45</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5202</span>
<span class="n">Epoch</span> <span class="mi">46</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.5011</span>
<span class="n">Epoch</span> <span class="mi">47</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.2540</span>
<span class="n">Epoch</span> <span class="mi">48</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.4145</span>
<span class="n">Epoch</span> <span class="mi">49</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.4916</span>
<span class="n">Epoch</span> <span class="mi">50</span><span class="o">/</span><span class="mi">50</span><span class="o">:</span> <span class="mi">14</span><span class="n">s</span> <span class="n">loss</span> <span class="o">=</span>    <span class="mf">2.4924</span>
</pre></div>


<div class="highlight"><pre><span class="n">valid_words_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">210</span><span class="p">,</span><span class="mi">239</span><span class="p">,</span><span class="mi">392</span><span class="p">,</span><span class="mi">396</span><span class="p">])</span>

<span class="n">valid_words</span><span class="p">,</span> <span class="n">nearests</span> <span class="o">=</span> <span class="n">model_CBOW</span><span class="o">.</span><span class="n">nearest_words</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">valid_words_index</span><span class="p">,</span><span class="n">top_nearest</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_words</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Nearest to &#39;{}&#39;: &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">valid_words</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">nearests</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">two</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">three</span><span class="sc">&#39; &#39;</span><span class="n">four</span><span class="sc">&#39; &#39;</span><span class="n">five</span><span class="sc">&#39; &#39;</span><span class="n">six</span><span class="sc">&#39; &#39;</span><span class="n">seven</span><span class="sc">&#39; &#39;</span><span class="n">eight</span><span class="sc">&#39; &#39;</span><span class="n">one</span><span class="sc">&#39; &#39;</span><span class="n">xx</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">that</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">which</span><span class="sc">&#39; &#39;</span><span class="n">however</span><span class="sc">&#39; &#39;</span><span class="n">furthermore</span><span class="sc">&#39; &#39;</span><span class="n">what</span><span class="sc">&#39; &#39;</span><span class="n">nevertheless</span><span class="sc">&#39; &#39;</span><span class="n">imaginable</span><span class="err">&#39;</span>
 <span class="err">&#39;</span><span class="n">assemblage</span><span class="sc">&#39; &#39;</span><span class="n">where</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">his</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">her</span><span class="sc">&#39; &#39;</span><span class="n">their</span><span class="sc">&#39; &#39;</span><span class="n">my</span><span class="sc">&#39; &#39;</span><span class="n">your</span><span class="sc">&#39; &#39;</span><span class="n">its</span><span class="sc">&#39; &#39;</span><span class="n">whose</span><span class="sc">&#39; &#39;</span><span class="n">our</span><span class="sc">&#39; &#39;</span><span class="n">dufay</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">were</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">are</span><span class="sc">&#39; &#39;</span><span class="n">remain</span><span class="sc">&#39; &#39;</span><span class="n">include</span><span class="sc">&#39; &#39;</span><span class="n">have</span><span class="sc">&#39; &#39;</span><span class="n">was</span><span class="sc">&#39; &#39;</span><span class="n">tend</span><span class="sc">&#39; &#39;</span><span class="n">those</span><span class="sc">&#39; &#39;</span><span class="n">appear</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">all</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">both</span><span class="sc">&#39; &#39;</span><span class="n">various</span><span class="sc">&#39; &#39;</span><span class="n">unacknowledged</span><span class="sc">&#39; &#39;</span><span class="n">every</span><span class="sc">&#39; &#39;</span><span class="n">faked</span><span class="sc">&#39; &#39;</span><span class="n">aurangazeb</span><span class="sc">&#39; &#39;</span><span class="n">some</span><span class="err">&#39;</span>
 <span class="err">&#39;</span><span class="n">many</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">area</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">region</span><span class="sc">&#39; &#39;</span><span class="n">areas</span><span class="sc">&#39; &#39;</span><span class="n">regions</span><span class="sc">&#39; &#39;</span><span class="n">land</span><span class="sc">&#39; &#39;</span><span class="n">campus</span><span class="sc">&#39; &#39;</span><span class="n">streets</span><span class="sc">&#39; &#39;</span><span class="n">harbour</span><span class="sc">&#39; &#39;</span><span class="n">tacos</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">east</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">west</span><span class="sc">&#39; &#39;</span><span class="n">south</span><span class="sc">&#39; &#39;</span><span class="n">southwest</span><span class="sc">&#39; &#39;</span><span class="n">north</span><span class="sc">&#39; &#39;</span><span class="n">northeast</span><span class="sc">&#39; &#39;</span><span class="n">eastern</span><span class="sc">&#39; &#39;</span><span class="n">southeast</span><span class="err">&#39;</span>
 <span class="err">&#39;</span><span class="n">highlights</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">himself</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">him</span><span class="sc">&#39; &#39;</span><span class="n">themselves</span><span class="sc">&#39; &#39;</span><span class="n">them</span><span class="sc">&#39; &#39;</span><span class="n">itself</span><span class="sc">&#39; &#39;</span><span class="n">herself</span><span class="sc">&#39; &#39;</span><span class="n">papp</span><span class="sc">&#39; &#39;</span><span class="n">aafk</span><span class="sc">&#39; &#39;</span><span class="n">heartbroken</span><span class="err">&#39;</span><span class="p">]</span>
<span class="n">Nearest</span> <span class="n">to</span> <span class="err">&#39;</span><span class="n">white</span><span class="err">&#39;</span><span class="o">:</span>  <span class="p">[</span><span class="err">&#39;</span><span class="n">red</span><span class="sc">&#39; &#39;</span><span class="n">black</span><span class="sc">&#39; &#39;</span><span class="n">blue</span><span class="sc">&#39; &#39;</span><span class="n">dark</span><span class="sc">&#39; &#39;</span><span class="n">yellow</span><span class="sc">&#39; &#39;</span><span class="n">culturally</span><span class="sc">&#39; &#39;</span><span class="n">dead</span><span class="sc">&#39; &#39;</span><span class="n">angelman</span><span class="err">&#39;</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s">&#39;More labels than embeddings&#39;</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>  <span class="c"># in inches</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s">&#39;offset points&#39;</span><span class="p">,</span>
                   <span class="n">ha</span><span class="o">=</span><span class="s">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">&#39;bottom&#39;</span><span class="p">)</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">visualization_words</span> <span class="o">=</span> <span class="mi">800</span>
<span class="c"># transform embeddings to 2D by t-SNE</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">model_CBOW</span><span class="o">.</span><span class="n">embedding_matrix</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;pca&#39;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">&#39;exact&#39;</span><span class="p">)</span>
<span class="n">two_d_embed</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
<span class="c"># list labels</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_CBOW</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="c"># plot</span>
<span class="n">plot</span><span class="p">(</span><span class="n">two_d_embed</span><span class="p">,</span><span class="n">words</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/05_output_20_0.png"></p>
<p><br/></p>
<h3>Reference</h3>
<ul>
<li>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

            <h3>Related Posts</h3>
            <dl class="dl-horizontal">
                <dt>2017 / 4月 17</dt>
                <dd><a href="https://www.ycc.idv.tw/ml-course-techniques_6.html">機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)</a></dd>
                <dt>2017 / 11月 07</dt>
                <dd><a href="https://www.ycc.idv.tw/tensorflow-tutorial_2.html">實作Tensorflow (2)：Build First Deep Neurel Network (DNN)</a></dd>
                <dt>2017 / 11月 12</dt>
                <dd><a href="https://www.ycc.idv.tw/tensorflow-tutorial_3.html">實作Tensorflow (3)：Build First Convolutional Neurel Network (CNN)</a></dd>
                <dt>2017 / 11月 18</dt>
                <dd><a href="https://www.ycc.idv.tw/tensorflow-tutorial_4.html">實作Tensorflow (4)：Autoencoder</a></dd>
                <dt>2017 / 11月 25</dt>
                <dd><a href="https://www.ycc.idv.tw/tensorflow-tutorial_6.html">實作Tensorflow (6)：Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)</a></dd>
            </dl>

        <br/><br/>

<div id="disqus_thread"></div>
<script type="text/javascript">
/* <![CDATA[ */

    var disqus_shortname = 'ycnote-1';
    var disqus_identifier = "tensorflow-tutorial_5.html";

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
/* ]]> */
</script>
<noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



        
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="https://www.ycc.idv.tw/archives.html">Archives</a></li>
                            <li><a href="https://www.ycc.idv.tw/tags.html">Tags</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Contact Me</div>
                        <ul class="list-unstyled">
                            <li><a href="./about-me.html" target="_blank">About Me</a></li>
                            <li><a href="https://github.com/GitYCC" target="_blank">Github</a></li>
                            <li><a href="mailto:ycc.tw.email@gmail.com" target="_blank">Email</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; YC Note 2018</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>