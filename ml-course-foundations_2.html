<!DOCTYPE html>
<html lang="zh">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="前言 在上一回當中，我們初探了機器學習，了解了什麼時候適合使用機器學習，而不是一般的Hard Coding，那今天這篇文章要繼續問下去。 為什麼機器可以學習(Why Can Machines Learn?)，本篇會介紹學理上機器學習（ML）必須要有哪些條件才可行，這些理論有非常多的數學，但卻是了解機器學習非常重要的內功，我會盡量避開繁複的數學運算，而帶大家直接的了解式子所要告訴我們的觀念。...">
        <meta name="keywords" content="機器學習基石">
        <link rel="icon" href="./static/img/favicon.png">

        <title>機器學習基石 學習筆記 (2)：為什麼機器可以學習? - YC Note</title>

        <!-- Stylesheets -->
        <link href="./theme/css/all.min.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script type="text/x-mathjax-config"> 
            MathJax.Hub.Config({ 
                "HTML-CSS": { scale: 90, linebreaks: { automatic: true } }, 
                SVG: { linebreaks: { automatic:true } } 
                });
        </script>




    </head>

    <body>

        <!-- Header -->
    <div class="header-container" style="background: linear-gradient(rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0.2)), url('./images/ai_front_board.jpg'); background-position: center; background-size: cover;">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="./"><img class="logo" src="./static/img/favicon.png" alt="logo">YC Note</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="./category/coding.html">Coding</a>
                                <a href="./category/aiml.html">AI.ML</a>
                                <a href="./category/reading.html">Reading</a>
                                <a href="./category/recording.html">Recording</a>
                                <a href="./about-me.html">About Me</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title">機器學習基石 學習筆記 (2)：為什麼機器可以學習?</h1>
                      <p class="header-date">By <a href="./author/yc-chen.html">YC Chen</a>, 2016 / 6月 26, in category <a href="./category/aiml.html">AI.ML</a></p>
                      <div class="header-underline"></div>
                      <div class="clearfix"></div>
                      <p class="pull-right header-tags">
                          <span class="glyphicon glyphicon-tags mr5" aria-hidden="true"></span>
<a href="./tag/ji-qi-xue-xi-ji-shi.html">機器學習基石</a>                      </p>
                  </div>
              </div>
        </div>
    </div>
    <!-- /Header -->
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="container content">
        <h3>前言</h3>
<p>在上一回當中，我們初探了機器學習，了解了什麼時候適合使用機器學習，而不是一般的Hard Coding，那今天這篇文章要繼續問下去。</p>
<p><strong>為什麼機器可以學習(Why Can Machines Learn?)</strong>，本篇會介紹學理上機器學習（ML）必須要有哪些條件才可行，這些理論有非常多的數學，但卻是了解機器學習非常重要的內功，我會盡量避開繁複的數學運算，而帶大家直接的了解式子所要告訴我們的觀念。</p>
<h3>機器可以學習嗎?</h3>
<p><img alt="MachineLearningFoundations.001" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.001.jpeg"></p>
<p>還記得上面這張圖嗎? 上次帶大家初探了Machine Learning(ML)的基本架構，可以把整個概念總結成上面這張圖。</p>
<p>我們來複習一下，先從最上面的盆子開始看起，我們用Target Function代表你想要學習的技能，在非常理想的情況下，也就是沒有noise的情況，每組輸入變數 <span class="math">\(X_n\)</span>都會找到一組精確的輸出 <span class="math">\(y_n\)</span>，而這個Target Function能產生多個Data，圖中那些小球就是代表由Target Function產生的Data，今天我從中隨機抽取出<span class="math">\(N\)</span>組Data來做機器學習，接下來Learning Algorithm會利用這些取出的Data去找出最吻合的Hypothesis，那這組Hypothesis就成了我們學習出來的結果，我們可以利用這個結果來預測新的問題。</p>
<p>那麼上面這張圖真的合理嗎? 我們真的有辦法用上面的方法讓機器學習嗎? </p>
<p>先介紹幾個名詞，我們會稱<strong>抽樣的Data為In-sample Data</strong>，並且稱<strong>Hypothesis預測In-sample Data的誤差為In-sample Error，記作<span class="math">\(E_{in}\)</span></strong>，因此Learning Algorithm的目的就是找出那組Hypothesis使得<span class="math">\(E_{in}\)</span>最小。</p>
<p>回想一下二元分類問題，在上一篇當中我們使用PLA來挑選Hypothesis Set，還記得我們做了什麼事來確保我們可以得到最佳解嗎? 那就是Pocket的方法，Pocket的目的就是去留住一組能預測最好的Hypothesis，也就是能保留一組最佳參數使得<span class="math">\(E_{in}\)</span>最小。</p>
<p>但如果<span class="math">\(E_{in}\)</span>真的已經可以壓到0了，我們就可以說機器學習已經完成了嗎？</p>
<p>並不是這樣的，回到目的，我們真正希望的是機器有辦法預測新的問題，所以真正的目標是能將「沒有看過的Data」也可以預測好，而不是單單將取樣的Data預測好就夠了。</p>
<p>我們會稱<strong>未被取樣的Data為Out-sample Data</strong>，並且稱<strong>Hypothesis預測Out-sample Data的誤差為Out-sample Error，記作<span class="math">\(E_{out}\)</span>，我們最終目的就是把<span class="math">\(E_{out}\)</span>壓下來，也就代表可以預測新的問題</strong>。</p>
<p>但遺憾的是我們不會真正知道<span class="math">\(E_{out}\)</span>，除非我們知道Target Function，所以我們只能評估<span class="math">\(E_{in}\)</span>來選取Model參數，因此重要的是需要<span class="math">\(E_{in} \approx E_{out}\)</span>這個條件要成立，否則一切的學習都是無效的。</p>
<p><strong>總結一下機器學習的條件，我們必須建立一個 Learning Model可以確保<span class="math">\(E_{in}\approx E_{out}\)</span>，所以在Learning Algorithm選出最小<span class="math">\(E_{in}\)</span>的Hypothesis，同時這組Hypothesis也可以很好的預測Out-sample，我們就可以說機器已經會學習了。</strong></p>
<h3><span class="math">\(E_{in}\)</span>和<span class="math">\(E_{out}\)</span>的差異</h3>
<p><img alt="image" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.005.jpeg"></p>
<p>剛剛我們已經提到了如果機器能學習，那就必須先確保<span class="math">\(E_{in} \approx E_{out}​\)</span>，下面我會引入Hoeffding不等式來說明這個條件怎麼成立。</p>
<p>先想像一下我有一個桶子，這個桶子裝了兩種顏色的小球，分別為橘色和綠色，今天如果桶子內橘色球佔的比例為<span class="math">\(μ\)</span>，而今天我們從中隨機抽樣出<span class="math">\(N\)</span>顆小球，並且計算出這<span class="math">\(N\)</span>顆小球中橘色佔的比例為<span class="math">\(ν\)</span>，此時我們可以想像的到，<span class="math">\(μ=ν\)</span>不一定會成立，但<span class="math">\(μ\)</span>也不至於離<span class="math">\(ν\)</span>太遠，所以Hoeffding不等式就告訴我們<span class="math">\(|μ-ν|\)</span>會被限制在一個範圍內，表示為：
</p>
<div class="math">$$
\mathbb{P}[|ν-μ|&gt;ε] \leq 2 exp(-2ε^2N)
$$</div>
<p>
當<span class="math">\(ε\)</span>越大，出現的機率就越低。</p>
<p>接下來我們再把橘球和綠球的意義換成是，一組Hypothesis預測每筆Data的好或壞，預測正確的是綠球，預測失敗的是橘球，所以對於In-Sample來說，<span class="math">\(μ\)</span> 就是 <span class="math">\(E_{in}\)</span>
</p>
<div class="math">$$
μ = (1/N) \sum_{n=1}^N ⟦h(x)\neq y_n⟧ = E_{in}(h)
$$</div>
<p>
對於Out-Sample來說，<span class="math">\(ν\)</span> 就是 <span class="math">\(E_{out}\)</span>
</p>
<div class="math">$$
ν =  ε_{x \sim P} ⟦h(x)\neq f(x)⟧ = E_{out}(h)
$$</div>
<p>
如果有部分Data，<span class="math">\(E_{in}\)</span>和<span class="math">\(E_{out}\)</span>差異非常大，我們會稱為Bad Data(不好的數據)，這就代表<span class="math">\(E_{in}\approx E_{out}\)</span>這個條件漸漸被侵蝕了，但還好！Hoeffding不等式告訴我們，<span class="math">\(E_{in}\)</span>和<span class="math">\(E_{out}\)</span>差異大於 <span class="math">\(ε\)</span> 的機率會被限制在一個範圍內，如下：
</p>
<div class="math">$$
\mathbb{P}[|E_{in}(h)-E_{out}(h)|&gt;ε] \leq 2 exp(-2ε^2N)
$$</div>
<p>
因此上述式子保證的是，出現Bad Data的機率將被一個定值給限制住，所以只要出現Bad Data的機率不是太大，基本上我們還是可以說<span class="math">\(E_{in} \approx E_{out}\)</span>。</p>
<p><img alt="image" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.006.jpeg"></p>
<p>而事實上，我們的hypothesis不會只有一個，所以接下來來考慮如果有M個Hypotheses的情況下我們的<span class="math">\(E_{in}\)</span>和<span class="math">\(E_{out}\)</span>的差異會怎麼被參數影響。</p>
<p>如果我們考慮M組Hypotheses，就會發現每種Hypothesis出現Bad Data的地方可能不一樣，因此大大的減少能使用的Data，如上圖左側所示。</p>
<p>今天如果我有1000份從Target Function取<span class="math">\(N\)</span>個Data的情形，然後只用一個Hypothesis來衡量，根據Hoeffding's Inequality，1000份裡面假設大概5份會出現Bad Data，但今天我再增加一組Hypothesis來衡量，對於這個Hypothesis也可能有自己的5份Bad Data，如果很不幸的，剛剛好這5份Bad Data和前5份沒有重疊，因此用這兩個hypotheses來評估的話，1000份裡頭將會出現10份的Bad Data，由此類推，如果有<span class="math">\(M\)</span>組Hypotheses，最差的情況會發生在什麼時候呢? 那就是<span class="math">\(M\)</span>個Hypotheses的每份Bad Data彼此都沒有交集，夠慘吧! 所以把這些出現Bad Data的機率取聯集得到以下式子：
</p>
<div class="math">$$
\mathbb{P}[\exists h\in \mathbb{H}\ s.t.\ |E_{in}(h)-E_{out}(h)|&gt;ε] \leq 2M exp(-2ε^2N)
$$</div>
<p>
大家現在回想一下上一篇所提到的Perceptron Hypothesis Set就會發現，糟糕了! Perceptron Hypothesis Set 裡有無限多組的Hypotheses，也就是<span class="math">\(M→∞\)</span>，那我們不就需要無限多的Data才能做到<span class="math">\(E_{in} \approx E_{out}\)</span>，否則機器根本不會學習，所以前一篇的內容都在亂講，PLA根本無法學習，因為<span class="math">\(E_{in} \approx E_{out}\)</span>，就算<span class="math">\(E_{in}\)</span>很小也不代表學習成立，機器學習是不可能的。等一下！先沉住氣，聽我接下來慢慢解釋，你就會發現還有一線生機。</p>
<h3>VC Generalization Bound</h3>
<p><img alt="image" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.007.jpeg"></p>
<p>問題出在這裡，我們在Multi-Bin Hoeffding’s Inequality中採用了一個假設，就是假設每組Hypotheses的Bad Data彼此間都沒有重疊，所以在<span class="math">\(M→∞\)</span>的情況下，當然會有一個無限大的上限值，但如果考慮了Bad Data重疊的情形，縱使<span class="math">\(M→∞\)</span>的情況下還是有機會把Bad Data的出現機率壓在一個有限的定值之下。</p>
<p>我們回到二元分類問題，看一下上圖中左側的圖例，如果今天在二維平面上做二元分類，當<span class="math">\(n=1\)</span>時，就算你的Hypotheses有無限多組，對於一組Data來說就只有兩種而已，再來看<span class="math">\(n=2\)</span>的情況，一樣的無限多組的Hypotheses也只能分類成4種。</p>
<p>因此Hypotheses彼此之間因為Data數量的關係，而出現重疊的狀況。但聰明的你一定想到，如果今天n的數量不斷的增加，則Hypotheses被分類的數量就會增加，Hypotheses彼此之間的重疊就會漸漸減少，我們還是無法限制住Bad Data出現的機率。</p>
<p>我們繼續看下去，當<span class="math">\(n=3\)</span>，沒有意外的Hypotheses會被分類為8種，那接下來<span class="math">\(n=4\)</span>時，你就會發現一個有趣的現象，開始有一些情況是不會出現在這一組Hypothesis Set的，因此我們擔心因為Data數量增加而造成Hypotheses的種類暴增的情形被排除了，有一些狀況是不會出現的。</p>
<p>剛剛所提到的分類方式的數量又稱為Dichotomy。在<span class="math">\(n=1\)</span>、<span class="math">\(n=2\)</span>到<span class="math">\(n=3\)</span>的情形，所有列得出來的方式都可被完整分類開來，我們稱這情形為Shatter，但是到了n=4的時候，有些不可能被分類的情形出現了，稱為不可被Shatter，另外又稱此情形開始發生的那點為Break Point，這邊注意一下喔! 會不會有Break Point取決於你的Hypothesis Set長怎麼樣，現在是因為線性二元分類的Hypothesis Set，所以Break Point才會在<span class="math">\(n=4\)</span>，其他的Hypothesis Set就不一定了。</p>
<p>Break Point的出現非常重要，他所代表的是Bad Data的出現機率不會無所限制的大下去，因此把這概念帶入Multi-Bin Hoeffding’s Inequality，經過繁複的計算，就可以得到上圖右側的公式，原本的M消失了，取而代之的是Growth Function，Growth Function與Data數量<span class="math">\(N\)</span>有關，這就是我們剛剛解說的，決定Hypothesis Set的種類的其實是 Data的數量<span class="math">\(N\)</span>。</p>
<p>那麼Growth Function要怎麼和Break Point連結起來呢？</p>
<p>先定義一下VC Dimension：<span class="math">\(d_{VC}= Break Point-1\)</span>，Break Point代表首次出現不Shatter的情況，那比它小一級代表的正是最大可以Shatter的點，上面的例子中<span class="math">\(d_{VC}=3\)</span>。而這個VC Dimension就可以和我們在意的Growth Function連接起來，經過數學推倒可以得到上圖右側下方的關係式。</p>
<p>所以我們就知道啦！<strong>只要有Break Point存在，VC Dimension就是一個有限的值，也因此Growth Function是一個有限的值，VC Bound就產生了，就可以確保Bad Data出現的機率被壓在一個定值之下，所以一樣的只要資料量<span class="math">\(N\)</span>夠多就可以確保<span class="math">\(E_{in} \approx E_{out}\)</span>，機器將可以學習。</strong></p>
<p>另外一件重要的事，VC Dimension在數學上是有意義的，<strong><span class="math">\(d_{VC} \approx 可調控變數的個數\)</span></strong>，像是上述的二維二元分類問題，它的可調控變數有<span class="math">\(w_0\)</span>, <span class="math">\(w_1\)</span> 和 <span class="math">\(w_2\)</span>，總共3個，所以<span class="math">\(d_{VC}=3\)</span>。<strong>也就是說Hypothesis Set的可調變參數如果是有限，大部分都可以做機器學習。</strong></p>
<h3>機器要能學習的三要素</h3>
<p>前面拉哩拉雜的講了一堆，終於要推出我們的結論了! 所以如果剛剛的數學讓你感到很挫敗，沒關係，讀懂這段那就足夠了。</p>
<p>從VC Generalization Bound，我們可以知道機器學習是可能的，只要它具備三點要素：</p>
<ol>
<li><strong>Good Hypothesis Set: Hypothesis Set 必須有Break Point的存在，也意味著VC Dimension是有限的，而且越小越好，在意義上代表可以調控的變數不要太多。</strong></li>
<li><strong>Good Data: 數據量越大越好，可以壓低VC Generalization Bound</strong></li>
<li><strong>Good Learning Algorithm: 以上兩點可以確定的是<span class="math">\(E_{in} \approx E_{out}\)</span>，接下來好的Learning Algorithm要有能力找到<span class="math">\(E_{in}\)</span> 最小的參數。很直觀的，當我們可以調控的變數越多，我們的選擇就越多，也就是我們可以找到更小<span class="math">\(E_{in}\)</span> 的機會變多了，所以可以調控的變數不可以太少。</strong></li>
</ol>
<p>眼尖的你有沒有發現矛盾啊! 可以調控的變數很少，我們能確保<span class="math">\(E_{in} \approx E_{out}​\)</span>，但是如果我想要找到更小的<span class="math">\(E_{in}​\)</span> 又必須有更多的調控變數，這個矛盾是機器學習上一個重要的課題，<strong>解法是我們必須要能找到適當的調控變數數量，也就是適當大小的<span class="math">\(d_{VC}\)</span> </strong>。</p>
<p><img alt="" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.000.02.jpeg"></p>
<p>from: <a href="https://d396qusza40orc.cloudfront.net/ntumlone/lecture_slides/07_handout.pdf">https://d396qusza40orc.cloudfront.net/ntumlone/lecture_slides/07_handout.pdf</a></p>
<p>上圖中，我們把VC Generalization Bound公式帶入Growth Function和<span class="math">\(d_{VC}\)</span>的關係式，並且設<span class="math">\(δ\)</span> 為最大可以容忍的Bad Data出現機率，把它帶入取代掉<span class="math">\(ε\)</span>，整理一下，就可以推出上圖的公式，後面帶根號的紅字稱為Model Complexity，這一項代表的是Hypothesis Set造成的模型複雜度，我們可以看到它隨著<span class="math">\(d_{VC}\)</span>增加而增加。Model Complexity越大代表Bad Data更容易出現，所以<span class="math">\(E_{in}\)</span>和<span class="math">\(E_{out}\)</span>開始被帶開了。</p>
<p><strong>這個現象有一個很常見的名字叫做Overfitting，指的是使用非常複雜的Model來Fitting，雖然可以把手頭上的數據Fit的很漂亮，但是拿到其他的數據來看就會發現這Model的預測性非常的差，原因就是因為Model Complexity造成<span class="math">\(E_{in}\)</span>和<span class="math">\(E_{out}\)</span>脫鉤了，所以選擇一個複雜度適中的Model是很重要的。</strong></p>
<h3>機器學習架構一般化</h3>
<p><img alt="image" src="http://www.ycc.idv.tw/media/MachineLearningFoundations/MachineLearningFoundations.008.jpeg"></p>
<p>最後我們來總結一下機器學習的流程，上圖中是之前提到的機器學習的架構，額外的我們還需要考慮到一些真實情形，</p>
<ol>
<li>每筆Data出現的機會不一定，同樣的採樣結果也是會受機率的影響，所以上圖中標示為<span class="math">\(\mathbb{P} (x)\)</span>，這個修改並不會影響機器學習的流程和結果。</li>
<li>Data可能會受到Noise的影響，所以給定<span class="math">\(X_n​\)</span>並不一定會百分之一百得到<span class="math">\(y_n​\)</span>，他存在著可能會出錯，上圖標示為<span class="math">\(\mathbb{P}(y|x)​\)</span>，我們可以增大我們採樣的數量<span class="math">\(N​\)</span>來減少Noise的影響。</li>
<li>我們是採用<span class="math">\(E_{in}\)</span>來當作選擇Model參數的指標，因此我們需要訂出Error的評估方式，常見的有<span class="math">\(E_{squared} = (y_n - y_{prediction})^2\)</span>。</li>
</ol>
<p>跟著架構我們就有一套機器學習的<strong>標準流程</strong>，</p>
<ol>
<li><strong>準備好足夠的數據</strong></li>
<li><strong>把Model建立好，<span class="math">\(d_{VC}\)</span>必須要是有限的，而且大小要適中</strong></li>
<li><strong>定義好評估<span class="math">\(E_{in}\)</span>的Error Measurement</strong></li>
<li><strong>使用演算法找出最佳參數把<span class="math">\(E_{in}\)</span>降低</strong></li>
<li><strong>最後評估一下是否有Overfitting的狀況，確保<span class="math">\(E_{in} \approx E_{out}\)</span></strong>（未來會講怎麼做）</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


        <br/><br/>

<div id="disqus_thread"></div>
<script type="text/javascript">
/* <![CDATA[ */

    var disqus_shortname = 'ycnote-1';
    var disqus_identifier = "ml-course-foundations_2.html";

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
/* ]]> */
</script>
<noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



        
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="./archives.html">Archives</a></li>
                            <li><a href="./tags.html">Tags</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Contact Me</div>
                        <ul class="list-unstyled">
                            <li><a href="./about-me.html" target="_blank">About Me</a></li>
                            <li><a href="https://github.com/GitYCC" target="_blank">Github</a></li>
                            <li><a href="mailto:ycc.tw.email@gmail.com" target="_blank">Email</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; YC Note 2018</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>