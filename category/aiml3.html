<!DOCTYPE html>
<html lang="zh">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Ai.ml">
        <meta name="keywords" content="">
        <link rel="icon" href="../static/img/favicon.png">

        <title>AI.ML - YC Note</title>

        <!-- Stylesheets -->
        <link href="../theme/css/all.min.css" rel="stylesheet">
        <!-- /Stylesheets -->

        <!-- RSS Feeds -->
        <link href="YCNote/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="YC Note Full Atom Feed" />
        <link href="YCNote/feeds/aiml.atom.xml" type="application/atom+xml" rel="alternate" title="YC Note Categories Atom Feed" />
        <!-- /RSS Feeds -->

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->



    </head>

    <body>

        <!-- Header -->
    <div class="header-container" style="background: linear-gradient(rgba(0, 0, 0, 0.2), rgba(0, 0, 0, 0.2)), url('../images/welcome_front_board.jpg'); background-position: center; background-size: cover;">

            <!-- Static navbar -->
            <div class="container">
                <div class="header-nav">
                    <div class="header-logo">
                        <a class="pull-left" href="../"><img class="logo" src="../static/img/favicon.png" alt="logo">YC Note</a>
                    </div>
                    <div class="nav pull-right">
                                <a href="../category/coding.html">Coding</a>
                                <a href="../category/aiml.html">AI.ML</a>
                                <a href="../category/reading.html">Reading</a>
                                <a href="../category/recording.html">Recording</a>
                                <a href="../about-me.html">About Me</a>
                    </div>
                </div>
            </div>
            <!-- /Static navbar -->

            <!-- Header -->
    <div class="container header-wrapper">
        <div class="row">
              <div class="col-lg-12">
                  <div class="header-content">
                      <h1 class="header-title text-uppercase">Ai.ml</h1>
                      <div class="header-underline"></div>
                      <p class="header-subtitle header-subtitle-homepage">www.ycc.idv.tw</p>
                  </div>
              </div>
        </div>
    </div>
            <!-- /Header -->

        </div>
        <!-- /Header -->


        <!-- Content -->
    <div class="archive-container">
        <div class="container content archive">
            <h2><a href="../category/aiml.html">AI.ML : All Articles</u> </a></h2>
            <dl class="dl-horizontal">
            	<dt>2017 / 11月 25</dt>
            	<dd><a href="../tensorflow-tutorial_6.html">實作Tensorflow (6)：RNN and LSTM</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><p>如果我們想要處理的問題是具有時序性的，該怎麼辦呢？本章將會介紹有時序性的Neurel Network。</p>
<p>本單元程式碼LSTM部分可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/06_LSTM.py">Github</a>下載。</p>
<h5><u>概論RNN</u></h5>
<p>當我們想使得Neurel Network具有時序性，我們的Neurel Network就必須有記憶的功能，然後在我不斷的輸入新資訊時，也能同時保有歷史資訊的影響，最簡單的作法就是將Output的結果保留，等到新資訊進來時，將新的資訊和舊的Output一起考量來訓練Neurel Network。</p>
<p><img alt="unrolling" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.010.jpeg"></p>
<p>這種將舊有資訊保留的Neurel Network統稱為Recurrent Neural Networks (RNN)，這種不斷回饋的網路可以攤開來處理，如上圖，如果我有5筆數據，拿訓練一個RNN 5個回合並做了5次更新，其實就等效於攤開來一次處理5筆數據並做1次更新，這樣的手法叫做Unrolling，我們實作上會使用Unrolling的手法來增加計算效率。</p>
<p><img alt="RNN" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.011.jpeg"></p>
<p>接下來來看RNN內部怎麼實現的，上圖是最簡單的RNN形式，我們將上一回產生的Output和這一回的Input一起評估出這一回的Output，詳細式子如下：</p>
<p>new_o = tanh( i*W<sub>i</sub> + o*W<sub>o</sub> + B )</p>
<p>如此一來RNN就具有時序性了，舊的歷史資料將可以被「記憶」起來，你可以把RNN的「記憶」看成是「短期記憶」，因為它只會記得上一回的Output而已。</p>
<h5><u>梯度消失與梯度爆炸</u></h5>
<p>但這種形式的RNN在實作上會遇到很大的問題，還記得第二章當中，我們有講過像是tanh這類有飽和區的函數，會造成梯度消失的問題，而我們如果使用Unrolling的觀點來看RNN，將會發現這是一個超級深的網路，Backpapagation必須一路通到t0的RNN，想當然爾，有些梯度將會消失，部分權重就更新不到了，那有一些聰明的讀者一定會想到，那就使用Relu就好啦！不過其實還有一個重要的因素造成梯度消失，同時也造成梯度爆炸。</p>
<p>注意喔！雖然我們使用Unrolling的觀點，把網路看成是一個Deep網路的連接，但是和之前DNN不同之處，這些RNN彼此間是共享同一組權重的，這會造成梯度消失和梯度爆炸兩個問題，在RNN的結構裡頭，一個權重會隨著時間不斷的加強影響一個單一特徵，因為不同時間之下的RNN Cell共用同一個權重，這麼一來若是權重大於1，影響將會隨時間放大到梯度爆炸，若是權重小於1，影響將會隨時間縮小到梯度消失，就像是蝴蝶效應一般，微小的差異因為回饋的機制，而不合理的放大或是消失，因此RNN的Error Surface將會崎嶇不平，這會造成我們無法穩定的找到最佳解，難以收斂。這才是RNN難以使用的重要原因，把Activation Function換成Relu不會解決問題，文獻上反而告訴我們會變更差。</p>
<p>解決梯度爆炸有一個聽起來很廢但廣為人們使用的方法，叫做Gradient Clipping，也就是只要在更新過程梯度超過一個值，我就切掉讓梯度維持在這個上限，這樣就不會爆炸啦，待會會講到的LSTM只能夠解決梯度消失問題，但不能解決梯度爆炸問題，因此我們還是需要Gradient Clipping方法的幫忙。</p>
<p>在Tensorflow怎麼做到Gradient Clipping呢？作法是這樣的，以往我們使用<code>optimizer.minimize(loss)</code>來進行更新，事實上我們可以把這一步驟拆成兩部分，第一部分計算所有參數的梯度，第二部分使用這些梯度進行更新。因此我們可以從中作梗，把gradients偷天換日一番，一開始使用<code>optimizer.compute_gradients(loss)</code>來計算出個別的梯度，然後使用<code>tf.clip_by_global_norm(gradients, clip_norm)</code>來切梯度，最後再使用<code>optimizer.apply_gradients</code>把新的梯度餵入進行更新。</p>
<h5><u>Long Short-Term Memory (LSTM)</u></h5>
<p>LSTM是現今RNN的主流，它可以解決梯度消失的問題，我們先來看看結構，先預告一下，LSTM是迄今為止這系列課程當中看過最複雜的Neurel Network。</p>
<p><img alt="LSTM" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.012.jpeg"></p>
<p>最一開始和RNN一樣，Input會和上一回的Output一起評估一個「短期記憶」，</p>
<p>f<sub>m</sub> = tanh( i*W<sub>mi</sub> + o*W<sub>mo</sub> + B<sub>m</sub> )</p>
<p>但接下來不同於RNN直接輸出，LSTM做了一個類似於轉換成「長期記憶」的機制，「長期記憶」在這裡稱為State，State的狀態由三道門所控制，Input Gate負責控管哪些「短期記憶」可以進到「長期記憶」，Forget Gate負責調配哪一些「長期記憶」需要被遺忘，Output Gate則負責去決定需要從「長期記憶」中輸出怎樣的內容，先不要管這些Gate怎麼來，我們可以把這樣的記憶機制寫成以下的式子，假設State為f<sub>state</sub>、Input Gate為G<sub>i</sub>、Forget Gate為G<sub>f</sub>和Output Gate為G<sub>o</sub>。</p>
<p>new_f<sub>state</sub> = G<sub>i</sub> * f<sub>m</sub> + G<sub>f</sub> * f<sub>state</sub></p>
<p>new_o = G<sub>o</sub>*tanh( new_f<sub>state</sub> )</p>
<p>如果我們要使得上面中Gates的部分具有開關的功能的話，我們會希望Gates可以是0到1的值，0代表全關，1代表全開，sigmoid正可以幫我們做到這件事，那哪些因素會決定Gates的關閉與否呢？不妨考慮所有可能的因素，也就是所有輸入這個Cell的資訊都考慮進去，但上一回的State必須被剔除於外，因為上一回的State來決定下一個State的操作是不合理的，因此我們就可以寫下所有Gates的表示式了。</p>
<p>G<sub>i</sub> = Sigmoid(i*W<sub>ii</sub> + o*W<sub>io</sub> + B<sub>i</sub>)</p>
<p>G<sub>f</sub> = Sigmoid(i*W<sub>fi</sub> + o*W<sub>fo</sub> + B<sub>f</sub>)</p>
<p>G<sub>o</sub> = Sigmoid(i*W<sub>oi</sub> + o*W<sub>oo</sub> + B<sub>o</sub>)</p>
<p>這就是LSTM，「長期記憶」的出現可以解決掉梯度消失的問題，RNN只有「短期記憶」，所以一旦認為一個特徵不重要，經過幾回連乘，這個特徵的梯度就會消失殆盡，但是LSTM保留State，並且使用「加」的方法更新State，所以有一些重要的State得以留下來持續影響著Output，解決了梯度消失的問題。但是，不幸的LSTM還是免不了梯度爆炸，為什麼呢？如果一個特徵真的很重要，State會記住，Input也會強調，所以幾輪下來還是有可能出現爆炸的情況，這時候我們就需要Gradient Clipping的幫忙。</p>
<h5><u>使用LSTM實作文章產生器</u></h5>
<p>接下來我們來實作LSTM，目標是做一個文章產生器，我們希望機器可以不斷的根據前文猜測下一個「字母」(Letters)應該要下什麼，如此一來我只要給個開頭字母，LSTM就可以幫我腦補成一篇文章。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">range</span>
<span class="kn">from</span> <span class="nn">six.moves.urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
<span class="kn">import</span> <span class="nn">time</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">LETTER_SIZE</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># [a-z] + &#39; &#39;</span>
<span class="n">FIRST_LETTER_ASCII</span> <span class="o">=</span> <span class="nb">ord</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">maybe_download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span><span class="n">filename</span><span class="p">,</span> <span class="n">expected_bytes</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Download a file if not present, and make sure it&#39;s the right size.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
        <span class="n">filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
    <span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">stat</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span> <span class="o">==</span> <span class="n">expected_bytes</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Found and verified </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">filename</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
          <span class="s1">&#39;Failed to verify &#39;</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s1">&#39;. Can you get to it with a browser?&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">filename</span>

<span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">namelist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="k">def</span> <span class="nf">char2id</span><span class="p">(</span><span class="n">char</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">string</span><span class="o">.</span><span class="n">ascii_lowercase</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">ord</span><span class="p">(</span><span class="n">char</span><span class="p">)</span> <span class="o">-</span> <span class="n">FIRST_LETTER_ASCII</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">char</span> <span class="o">==</span> <span class="s1">&#39; &#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Unexpected character: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">char</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">id2char</span><span class="p">(</span><span class="n">dictid</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dictid</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">chr</span><span class="p">(</span><span class="n">dictid</span> <span class="o">+</span> <span class="n">FIRST_LETTER_ASCII</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">&#39; &#39;</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Downloading text8.zip&quot;</span><span class="p">)</span>
<span class="n">filename</span> <span class="o">=</span> <span class="n">maybe_download</span><span class="p">(</span><span class="s1">&#39;http://mattmahoney.net/dc/text8.zip&#39;</span><span class="p">,</span><span class="s1">&#39;./text8.zip&#39;</span><span class="p">,</span> <span class="mi">31344016</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;=====&quot;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Data size </span><span class="si">%d</span><span class="s1"> letters&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;=====&quot;</span><span class="p">)</span>
<span class="n">valid_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">valid_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[:</span><span class="n">valid_size</span><span class="p">]</span>
<span class="n">train_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="n">valid_size</span><span class="p">:]</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_text</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Train Dataset: size:&#39;</span><span class="p">,</span><span class="n">train_size</span><span class="p">,</span><span class="s1">&#39;letters,</span><span class="se">\n</span><span class="s1">  first 64:&#39;</span><span class="p">,</span><span class="n">train_text</span><span class="p">[:</span><span class="mi">64</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Validation Dataset: size:&#39;</span><span class="p">,</span><span class="n">valid_size</span><span class="p">,</span><span class="s1">&#39;letters,</span><span class="se">\n</span><span class="s1">  first 64:&#39;</span><span class="p">,</span><span class="n">valid_text</span><span class="p">[:</span><span class="mi">64</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>Downloading text8.zip
Found and verified ./text8.zip
=====
Data size 100000000 letters
=====
Train Dataset: size: 99999000 letters,
  first 64: ons anarchists advocate social relations based upon voluntary as
Validation Dataset: size: 1000 letters,
  first 64:  anarchism originated as a term of abuse first used against earl
</pre></div>


<p>上面操作我們建制完成了字母庫，接下來就可以產生我們訓練所需要的Batch Data，所以我們來看看究竟要產生怎樣格式的資料。</p>
<p><img alt="LSTM Implement" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.013.jpeg"></p>
<p>如上圖所示，有點小複雜，假設我要設計一個LSTM Model，它的Unrolling Number為3，Batch Size為2，然後遇到的字串是"abcde fghij klmno pqrst"，接下來就開始產生每個Round要用的Data，產生的結果如上圖所示，你會發現產生的Data第0軸表示的是考慮unrolling需要取樣的資料，總共應該會有(Unrolling Number+1)筆，如上圖例，共有4筆，3筆當作輸入而3筆當作Labels，中間有2筆重疊使用，另外還有一點，我們會保留最後一筆Data當作下一個回合的第一筆，這是為了不浪費使用每一個字母前後的組合。而第1軸則是餵入單一LSTM需要的資料，我們一次可以餵多組不相干的字母進去，如上圖例，Batch Size=2所以餵2個字母進去，那這些不相干的字母在取樣的時候，我們會盡量讓它平均分配在文字庫，才能確保彼此之間不相干，以增加LSTM的訓練效率和效果。</p>
<p>因此，先產生Batch Data吧！</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">characters</span><span class="p">(</span><span class="n">probabilities</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Turn a 1-hot encoding or a probability distribution over the possible</span>
<span class="sd">    characters back into its (most likely) character representation.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">id2char</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">batches2string</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert a sequence of batches back into their (most likely) string</span>
<span class="sd">    representation.&quot;&quot;&quot;</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">batches</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">characters</span><span class="p">(</span><span class="n">b</span><span class="p">))]</span>
    <span class="k">return</span> <span class="n">s</span>

<span class="k">def</span> <span class="nf">rnn_batch_generator</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_unrollings</span><span class="p">):</span>
    <span class="n">text_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1">### initialization</span>
    <span class="n">segment</span> <span class="o">=</span> <span class="n">text_size</span> <span class="o">//</span> <span class="n">batch_size</span>
    <span class="n">cursors</span> <span class="o">=</span> <span class="p">[</span> <span class="n">offset</span> <span class="o">*</span> <span class="n">segment</span> <span class="k">for</span> <span class="n">offset</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>

    <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_initial</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">LETTER_SIZE</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">cursor</span> <span class="o">=</span> <span class="n">cursors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">id_</span> <span class="o">=</span> <span class="n">char2id</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">cursor</span><span class="p">])</span>
        <span class="n">batch_initial</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">id_</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="c1">#move cursor</span>
        <span class="n">cursors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">cursors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">text_size</span>

    <span class="n">batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_initial</span><span class="p">)</span> 

    <span class="c1">### generate loop</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="p">[</span> <span class="n">batches</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_unrollings</span><span class="p">):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">LETTER_SIZE</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
                <span class="n">cursor</span> <span class="o">=</span> <span class="n">cursors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">id_</span> <span class="o">=</span> <span class="n">char2id</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">cursor</span><span class="p">])</span>
                <span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">id_</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>

                <span class="c1">#move cursor</span>
                <span class="n">cursors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">cursors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">text_size</span>
            <span class="n">batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="k">yield</span> <span class="n">batches</span>  <span class="c1"># [last batch of previous batches] + [unrollings]</span>


<span class="c1"># demonstrate generator</span>
<span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span>
<span class="n">num_unrollings</span><span class="o">=</span><span class="mi">10</span>

<span class="n">train_batches</span> <span class="o">=</span> <span class="n">rnn_batch_generator</span><span class="p">(</span><span class="n">train_text</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_unrollings</span><span class="p">)</span>
<span class="n">valid_batches</span> <span class="o">=</span> <span class="n">rnn_batch_generator</span><span class="p">(</span><span class="n">valid_text</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;*** train_batches:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">train_batches</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">train_batches</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;*** valid_batches:&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">valid_batches</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="n">batches2string</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">valid_batches</span><span class="p">)))</span>
</pre></div>


<div class="highlight"><pre><span></span>*** train_batches:
[&#39;ons anarchi&#39;, &#39;when milita&#39;, &#39;lleria arch&#39;, &#39; abbeys and&#39;, &#39;married urr&#39;, &#39;hel and ric&#39;, &#39;y and litur&#39;, &#39;ay opened f&#39;, &#39;tion from t&#39;, &#39;migration t&#39;, &#39;new york ot&#39;, &#39;he boeing s&#39;, &#39;e listed wi&#39;, &#39;eber has pr&#39;, &#39;o be made t&#39;, &#39;yer who rec&#39;, &#39;ore signifi&#39;, &#39;a fierce cr&#39;, &#39; two six ei&#39;, &#39;aristotle s&#39;, &#39;ity can be &#39;, &#39; and intrac&#39;, &#39;tion of the&#39;, &#39;dy to pass &#39;, &#39;f certain d&#39;, &#39;at it will &#39;, &#39;e convince &#39;, &#39;ent told hi&#39;, &#39;ampaign and&#39;, &#39;rver side s&#39;, &#39;ious texts &#39;, &#39;o capitaliz&#39;, &#39;a duplicate&#39;, &#39;gh ann es d&#39;, &#39;ine january&#39;, &#39;ross zero t&#39;, &#39;cal theorie&#39;, &#39;ast instanc&#39;, &#39; dimensiona&#39;, &#39;most holy m&#39;, &#39;t s support&#39;, &#39;u is still &#39;, &#39;e oscillati&#39;, &#39;o eight sub&#39;, &#39;of italy la&#39;, &#39;s the tower&#39;, &#39;klahoma pre&#39;, &#39;erprise lin&#39;, &#39;ws becomes &#39;, &#39;et in a naz&#39;, &#39;the fabian &#39;, &#39;etchy to re&#39;, &#39; sharman ne&#39;, &#39;ised empero&#39;, &#39;ting in pol&#39;, &#39;d neo latin&#39;, &#39;th risky ri&#39;, &#39;encyclopedi&#39;, &#39;fense the a&#39;, &#39;duating fro&#39;, &#39;treet grid &#39;, &#39;ations more&#39;, &#39;appeal of d&#39;, &#39;si have mad&#39;]
[&#39;ists advoca&#39;, &#39;ary governm&#39;, &#39;hes nationa&#39;, &#39;d monasteri&#39;, &#39;raca prince&#39;, &#39;chard baer &#39;, &#39;rgical lang&#39;, &#39;for passeng&#39;, &#39;the nationa&#39;, &#39;took place &#39;, &#39;ther well k&#39;, &#39;seven six s&#39;, &#39;ith a gloss&#39;, &#39;robably bee&#39;, &#39;to recogniz&#39;, &#39;ceived the &#39;, &#39;icant than &#39;, &#39;ritic of th&#39;, &#39;ight in sig&#39;, &#39;s uncaused &#39;, &#39; lost as in&#39;, &#39;cellular ic&#39;, &#39;e size of t&#39;, &#39; him a stic&#39;, &#39;drugs confu&#39;, &#39; take to co&#39;, &#39; the priest&#39;, &#39;im to name &#39;, &#39;d barred at&#39;, &#39;standard fo&#39;, &#39; such as es&#39;, &#39;ze on the g&#39;, &#39;e of the or&#39;, &#39;d hiver one&#39;, &#39;y eight mar&#39;, &#39;the lead ch&#39;, &#39;es classica&#39;, &#39;ce the non &#39;, &#39;al analysis&#39;, &#39;mormons bel&#39;, &#39;t or at lea&#39;, &#39; disagreed &#39;, &#39;ing system &#39;, &#39;btypes base&#39;, &#39;anguages th&#39;, &#39;r commissio&#39;, &#39;ess one nin&#39;, &#39;nux suse li&#39;, &#39; the first &#39;, &#39;zi concentr&#39;, &#39; society ne&#39;, &#39;elatively s&#39;, &#39;etworks sha&#39;, &#39;or hirohito&#39;, &#39;litical ini&#39;, &#39;n most of t&#39;, &#39;iskerdoo ri&#39;, &#39;ic overview&#39;, &#39;air compone&#39;, &#39;om acnm acc&#39;, &#39; centerline&#39;, &#39;e than any &#39;, &#39;devotional &#39;, &#39;de such dev&#39;]
*** valid_batches:
[&#39; a&#39;]
[&#39;an&#39;]
</pre></div>


<p>定義一下待會會用到的函數。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sample_distribution</span><span class="p">(</span><span class="n">distribution</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sample one element from a distribution assumed to be an array of normalized</span>
<span class="sd">    probabilities.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">distribution</span><span class="p">)):</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">distribution</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">r</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">i</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">distribution</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">prediction</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Turn a (column) prediction into 1-hot encoded samples.&quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">LETTER_SIZE</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">sample_distribution</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="k">return</span> <span class="n">p</span>

<span class="k">def</span> <span class="nf">logprob</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Log-probability of the true labels in a predicted batch.&quot;&quot;&quot;</span>
    <span class="n">predictions</span><span class="p">[</span><span class="n">predictions</span> <span class="o">&lt;</span> <span class="mf">1e-10</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-10</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predictions</span><span class="p">)))</span> <span class="o">/</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<p>開始建制LSTM Model。</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_unrollings</span><span class="p">,</span><span class="n">n_memory</span><span class="p">,</span><span class="n">n_train_batch</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_unrollings</span> <span class="o">=</span> <span class="n">n_unrollings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span> <span class="o">=</span> <span class="n">n_memory</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">saved</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span> <span class="c1"># initialize new grap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">n_train_batch</span><span class="p">)</span> <span class="c1"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span> <span class="c1"># create session by the graph </span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">n_train_batch</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c1">### Input      </span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_unrollings</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">n_train_batch</span><span class="p">,</span><span class="n">LETTER_SIZE</span><span class="p">]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_unrollings</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c1"># labels are inputs shifted by one time step.</span>


            <span class="c1">### Optimalization</span>
            <span class="c1"># build neurel network structure and get their loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span> <span class="n">inputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_inputs</span><span class="p">,</span>
                                                 <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">,</span>
                                                 <span class="n">n_batch</span><span class="o">=</span><span class="n">n_train_batch</span><span class="p">,</span>
                                               <span class="p">)</span>

            <span class="c1"># define training operation</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

            <span class="c1"># gradient clipping</span>
            <span class="n">gradients</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">))</span> <span class="c1"># output gradients one by one</span>
            <span class="n">gradients</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">)</span> <span class="c1"># clip gradient</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span> <span class="c1"># apply clipped gradients</span>


            <span class="c1">### Sampling and validation eval: batch 1, no unrolling.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sample_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">LETTER_SIZE</span><span class="p">])</span>

            <span class="n">saved_sample_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">]))</span>
            <span class="n">saved_sample_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset_sample_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span>     <span class="c1"># reset sample state operator</span>
                <span class="n">saved_sample_output</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">])),</span>
                <span class="n">saved_sample_state</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">])))</span>

            <span class="n">sample_output</span><span class="p">,</span> <span class="n">sample_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_cell</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sample_input</span><span class="p">,</span> <span class="n">saved_sample_output</span><span class="p">,</span> <span class="n">saved_sample_state</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">saved_sample_output</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">sample_output</span><span class="p">),</span>
                                          <span class="n">saved_sample_state</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">sample_state</span><span class="p">)]):</span>
                <span class="c1"># use tf.control_dependencies to make sure &quot;saving&quot; before &quot;prediction&quot;</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">sample_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">xw_plus_b</span><span class="p">(</span><span class="n">sample_output</span><span class="p">,</span> 
                                                                  <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;classifier&#39;</span><span class="p">],</span> 
                                                                  <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;classifier&#39;</span><span class="p">]))</span>

            <span class="c1">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>  

    <span class="k">def</span> <span class="nf">lstm_cell</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="n">o</span><span class="p">,</span><span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf</span>
<span class="sd">        Note that in this formulation, we omit the various connections between the</span>
<span class="sd">        previous state and the gates.&quot;&quot;&quot;</span>
        <span class="c1">## Build Input Gate</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;input_gate_i&#39;</span><span class="p">]</span>
        <span class="n">im</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;input_gate_o&#39;</span><span class="p">]</span>
        <span class="n">ib</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;input_gate&#39;</span><span class="p">]</span>
        <span class="n">input_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ix</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">im</span><span class="p">)</span> <span class="o">+</span> <span class="n">ib</span><span class="p">)</span>
        <span class="c1">## Build Forget Gate</span>
        <span class="n">fx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;forget_gate_i&#39;</span><span class="p">]</span>
        <span class="n">fm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;forget_gate_o&#39;</span><span class="p">]</span>
        <span class="n">fb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;forget_gate&#39;</span><span class="p">]</span>        
        <span class="n">forget_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">fx</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">fm</span><span class="p">)</span> <span class="o">+</span> <span class="n">fb</span><span class="p">)</span>
        <span class="c1">## Memory</span>
        <span class="n">cx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;memory_i&#39;</span><span class="p">]</span>
        <span class="n">cm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;memory_o&#39;</span><span class="p">]</span>
        <span class="n">cb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;memory&#39;</span><span class="p">]</span>
        <span class="n">update</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cx</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">cm</span><span class="p">)</span> <span class="o">+</span> <span class="n">cb</span>
        <span class="c1">## Update State</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">forget_gate</span> <span class="o">*</span> <span class="n">state</span> <span class="o">+</span> <span class="n">input_gate</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">update</span><span class="p">)</span>
        <span class="c1">## Build Output Gate        </span>
        <span class="n">ox</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;output_gate_i&#39;</span><span class="p">]</span>
        <span class="n">om</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;output_gate_o&#39;</span><span class="p">]</span>
        <span class="n">ob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;output_gate&#39;</span><span class="p">]</span>
        <span class="n">output_gate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">ox</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">om</span><span class="p">)</span> <span class="o">+</span> <span class="n">ob</span><span class="p">)</span>
        <span class="c1">## Ouput</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output_gate</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">inputs</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">n_batch</span><span class="p">):</span>
        <span class="c1">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
              <span class="s1">&#39;input_gate_i&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">LETTER_SIZE</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)),</span>
              <span class="s1">&#39;input_gate_o&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)),</span>
              <span class="s1">&#39;forget_gate_i&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">LETTER_SIZE</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)),</span>
              <span class="s1">&#39;forget_gate_o&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)),</span>
              <span class="s1">&#39;output_gate_i&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">LETTER_SIZE</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)),</span>
              <span class="s1">&#39;output_gate_o&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)),</span>
              <span class="s1">&#39;memory_i&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="n">LETTER_SIZE</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)),</span>
              <span class="s1">&#39;memory_o&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)),</span>
              <span class="s1">&#39;classifier&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">,</span> <span class="n">LETTER_SIZE</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)),</span>

            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
              <span class="s1">&#39;input_gate&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">])),</span>
              <span class="s1">&#39;forget_gate&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">])),</span>
              <span class="s1">&#39;output_gate&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">])),</span>
              <span class="s1">&#39;memory&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">])),</span>
              <span class="s1">&#39;classifier&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">LETTER_SIZE</span><span class="p">])),</span>
            <span class="p">}</span>

        <span class="c1"># Variables saving state across unrollings.</span>
        <span class="n">saved_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n_batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">]),</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">saved_state</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n_batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_memory</span><span class="p">]),</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1">### Structure</span>
        <span class="c1"># Unrolled LSTM loop.</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">saved_output</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">saved_state</span>
        <span class="k">for</span> <span class="n">input_</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm_cell</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1"># State saving across unrollings.</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">saved_output</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">output</span><span class="p">),</span>
                                      <span class="n">saved_state</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">state</span><span class="p">)]):</span>
            <span class="c1"># use tf.control_dependencies to make sure &quot;saving&quot; before &quot;calculating loss&quot;</span>

            <span class="c1"># Classifier</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">xw_plus_b</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> 
                                     <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;classifier&#39;</span><span class="p">],</span> 
                                     <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;classifier&#39;</span><span class="p">])</span>
            <span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                    <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span>
                        <span class="n">labels</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">y_</span><span class="p">,</span> <span class="n">loss</span>


    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">online_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>      
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_unrollings</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">feed_dict</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_data</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>    
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">perplexity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="n">sum_logprob</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">sample_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reset_sample_state</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sample_size</span><span class="p">):</span>
                <span class="n">sample_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">],</span><span class="n">newshape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                <span class="n">sample_label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">],</span><span class="n">newshape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_prediction</span><span class="p">,</span>
                                            <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_input</span><span class="p">:</span> <span class="n">sample_input</span><span class="p">})</span>
                <span class="n">sum_logprob</span> <span class="o">+=</span> <span class="n">logprob</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">sample_label</span><span class="p">)</span>
        <span class="n">perplexity</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sum_logprob</span> <span class="o">/</span> <span class="n">batch_size</span> <span class="o">/</span> <span class="n">sample_size</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">perplexity</span>

    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">len_generate</span><span class="p">):</span>
        <span class="n">feed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">id2char</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">==</span><span class="n">c</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">LETTER_SIZE</span><span class="p">)]])</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="n">characters</span><span class="p">(</span><span class="n">feed</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reset_sample_state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_generate</span><span class="p">):</span>
            <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_prediction</span><span class="p">,</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_input</span><span class="p">:</span> <span class="n">feed</span><span class="p">})</span>
            <span class="n">feed</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span>
            <span class="n">sentence</span> <span class="o">+=</span> <span class="n">characters</span><span class="p">(</span><span class="n">feed</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">sentence</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># build training batch generator</span>
<span class="n">batch_generator</span> <span class="o">=</span> <span class="n">rnn_batch_generator</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">train_text</span><span class="p">,</span>
                                      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                      <span class="n">num_unrollings</span><span class="o">=</span><span class="n">num_unrollings</span><span class="p">)</span>

<span class="c1"># build validation data</span>
<span class="n">valid_batches</span> <span class="o">=</span> <span class="n">rnn_batch_generator</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">valid_text</span><span class="p">,</span> 
                                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                                    <span class="n">num_unrollings</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">valid_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">valid_batches</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">valid_size</span><span class="p">)]</span>

<span class="c1"># build LSTM model</span>
<span class="n">model_LSTM</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">n_unrollings</span><span class="o">=</span><span class="n">num_unrollings</span><span class="p">,</span>
                  <span class="n">n_memory</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                  <span class="n">n_train_batch</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                  <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="c1"># initial model</span>
<span class="n">model_LSTM</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

<span class="c1"># online training</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">num_batchs_in_epoch</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">valid_freq</span> <span class="o">=</span> <span class="mi">5</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batchs_in_epoch</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model_LSTM</span><span class="o">.</span><span class="n">online_fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">loss</span>

    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">avg_loss</span> <span class="o">/</span> <span class="n">num_batchs_in_epoch</span>

    <span class="n">train_perplexity</span> <span class="o">=</span> <span class="n">model_LSTM</span><span class="o">.</span><span class="n">perplexity</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">: </span><span class="si">%d</span><span class="s2">s loss = </span><span class="si">%6.4f</span><span class="s2">, perplexity = </span><span class="si">%6.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span><span class="p">,</span>
                                                   <span class="n">avg_loss</span><span class="p">,</span> <span class="n">train_perplexity</span><span class="p">))</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">valid_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;=============== Validation ===============&quot;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;validation perplexity = </span><span class="si">%6.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">model_LSTM</span><span class="o">.</span><span class="n">perplexity</span><span class="p">(</span><span class="n">valid_data</span><span class="p">)))</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Generate From &#39;a&#39;:  &quot;</span><span class="p">,</span><span class="n">model_LSTM</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="n">len_generate</span><span class="o">=</span><span class="mi">50</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Generate From &#39;h&#39;:  &quot;</span><span class="p">,</span><span class="n">model_LSTM</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;h&#39;</span><span class="p">,</span><span class="n">len_generate</span><span class="o">=</span><span class="mi">50</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Generate From &#39;m&#39;:  &quot;</span><span class="p">,</span><span class="n">model_LSTM</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;m&#39;</span><span class="p">,</span><span class="n">len_generate</span><span class="o">=</span><span class="mi">50</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;==========================================&quot;</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Epoch 1/30: 96s loss = 1.8350, perplexity = 6.0744
Epoch 2/30: 93s loss = 1.5473, perplexity = 5.9950
Epoch 3/30: 96s loss = 1.4832, perplexity = 5.7988
Epoch 4/30: 95s loss = 1.4460, perplexity = 5.5873
Epoch 5/30: 93s loss = 1.4268, perplexity = 6.0196

=============== Validation ===============
validation perplexity = 3.7728
Generate From &#39;a&#39;:   a addressed trojp herregore efforts taxothers of fi
Generate From &#39;h&#39;:   h a one nine one s personalt god tranant of genuali
Generate From &#39;m&#39;:   m with the of retrintuutar one five zero and even t
==========================================

Epoch 6/30: 92s loss = 1.4116, perplexity = 5.8374
Epoch 7/30: 92s loss = 1.3958, perplexity = 5.7529
Epoch 8/30: 91s loss = 1.3911, perplexity = 5.8161
Epoch 9/30: 92s loss = 1.3670, perplexity = 5.6386
Epoch 10/30: 92s loss = 1.3871, perplexity = 5.5209

=============== Validation ===============
validation perplexity = 3.6448
Generate From &#39;a&#39;:   as mark but use the church management seniorie othe
Generate From &#39;h&#39;:   h mathum it layor j cape not pac feloghaokurg the a
Generate From &#39;m&#39;:   ment condition christmishem the reasons obaging out
==========================================

Epoch 11/30: 92s loss = 1.3772, perplexity = 5.4907
Epoch 12/30: 92s loss = 1.3782, perplexity = 6.1908
Epoch 13/30: 92s loss = 1.3713, perplexity = 5.7394
Epoch 14/30: 92s loss = 1.3722, perplexity = 6.5244
Epoch 15/30: 92s loss = 1.3665, perplexity = 6.5655

=============== Validation ===============
validation perplexity = 3.6228
Generate From &#39;a&#39;:   ans in the first glds for exclusively assistance es
Generate From &#39;h&#39;:   h south and the w cops and goat right as known the 
Generate From &#39;m&#39;:   m charges has a properties keit was in second state
==========================================

Epoch 16/30: 362s loss = 1.3627, perplexity = 5.3342
Epoch 17/30: 95s loss = 1.3674, perplexity = 5.2295
Epoch 18/30: 93s loss = 1.3513, perplexity = 6.6203
Epoch 19/30: 94s loss = 1.3637, perplexity = 5.9332
Epoch 20/30: 94s loss = 1.3561, perplexity = 6.0590

=============== Validation ===============
validation perplexity = 3.4923
Generate From &#39;a&#39;:   a the problems in mind types in one strieging call 
Generate From &#39;h&#39;:   huragre ray fundament lost knishera claokhen nalony
Generate From &#39;m&#39;:   m for five one nine four zero market hell one nine 
==========================================

Epoch 21/30: 93s loss = 1.3569, perplexity = 5.9601
Epoch 22/30: 93s loss = 1.3516, perplexity = 6.9727
Epoch 23/30: 92s loss = 1.3676, perplexity = 5.5722
Epoch 24/30: 94s loss = 1.3603, perplexity = 6.1140
Epoch 25/30: 92s loss = 1.3649, perplexity = 6.2638

=============== Validation ===============
validation perplexity = 3.5306
Generate From &#39;a&#39;:   an experimenting meaning as dosil smold seven eight
Generate From &#39;h&#39;:   h one nine seven biero shimm in died this theorothy
Generate From &#39;m&#39;:   m to threat loss away a roon b one six four nine fa
==========================================

Epoch 26/30: 95s loss = 1.3533, perplexity = 6.1450
Epoch 27/30: 75s loss = 1.3568, perplexity = 6.3603
Epoch 28/30: 93s loss = 1.3719, perplexity = 5.4497
Epoch 29/30: 96s loss = 1.3620, perplexity = 6.1687
Epoch 30/30: 95s loss = 1.3660, perplexity = 5.9484

=============== Validation ===============
validation perplexity = 3.4477
Generate From &#39;a&#39;:   ates in weaved to has be five six zero song in the 
Generate From &#39;h&#39;:   h a neil and would lockspry short there is attempte
Generate From &#39;m&#39;:   man one nine zero eight moming between language yea
==========================================
</pre></div>


<p>最後來產生一篇以"t"為開頭的1000字文章吧！</p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">model_LSTM</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;t&#39;</span><span class="p">,</span><span class="n">len_generate</span><span class="o">=</span><span class="mi">1000</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>tifician linulation fromsantinated inscriptions have been followne members of gomewhokeno science and direct to player by the xh music the work mercing a completely categories following were now shrries the graduate painters but three limil bp inversing to in show monasteria ziver buriale hollesthea or universities contains one nine five three villes on in wolf from home with alimon del wi tallation austry five he is generate three visitiral spectring greece of many proper six one would frequently to be along two zero zero one aberrieds him hockel alphaliatiss r kabif figant in jock final click hospite michael hetrion as the equations were feature to notably algebraic important but better can requires of the same since the many bag among the mastic five official with the homes abertosiar of game mi romannessas nine pp which based for a secrition in one nine five seven recent issannallies algorithm rigarborsphy inctmm information as provides an enjakitine on moll s bodies fit immeble one
</pre></div>


<p>看得出來LSTM想表達什麼嗎，哈哈！</p></dd>
              
            	<dt>2017 / 11月 19</dt>
            	<dd><a href="../tensorflow-tutorial_5.html">實作Tensorflow (5)：Word2Vec</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><p>機器有辦法自行從文本中觀察出詞彙間的相似度嗎？是可以的，word2vec是"word to vector"的縮寫，代表的正是將每個字轉換成向量，而一旦兩個字的向量越是靠近，就代表它的相似度越高，我們究竟要如何得到這些向量呢？方法簡單但出奇有效，文章的最後會向大家呈現它的精彩的結果。</p>
<p>本單元程式碼Skip-Gram Word2Vec部分可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/05_1_word2vec_SkipGram.py">Github</a>下載，CBOW Word2Vec部分可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/05_2_word2vec_CBOW.py">Github</a>下載。</p>
<p><br/></p>
<h5><u>Word2Vec觀念解析</u></h5>
<p>Word2Vec的形式和Autoencoder有點像，一樣是從高維度的空間轉換到低維度的空間，再轉換回去原本的維度，只是這一次轉回去的東西不再是原本一模一樣的東西了。</p>
<p>Word2Vec的Input和Output這次變成是上下文的文字組合，舉個例子，"by the way"這個用法如果多次被機器看過的話，機器是有辦法去學習到這樣的規律的，此時"by"與"the"和"way"便會產生一個上下文的關聯性，為了將這樣的關聯性建立起來，我們希望當我輸入"by"時，機器有辦法預測並輸出"the"或"way"，這代表在機器內部它已經學習到了上下文的關聯性。</p>
<p>那如果今天這個機器也同時看到很多次的"on the way"這種用法，所以當我輸入"on"時，機器要有辦法預測並輸出"the"或"way"，但是我們不希望"on"和"by"兩個詞在學習時是分開學習的，我們希望機器可以因為"by the way"和"on the way"的結構很相似，所以有辦法抓出"on"和"by"是彼此相似的結論。</p>
<p>如何做到呢？答案就是限縮這個上下文的關聯性的儲存維度，如果我的字彙量有1000個，這1000個字彙彼此有上下文的關聯性，最完整表示上下文關聯性的方法就是設置一個1000x1000或者更大的表格，把所有字彙間的上下文關聯性全部存起來，但我們不想要這麼做，我要求機器用更小的表格來儲存上下文的關聯性，此時機器被迫將一些詞彙使用同樣的表格位置，同樣的轉換。一旦限縮了上下文關聯性的儲存維度，"on the way"和"by the way"中的"on"和"by"就會被迫分為同一類，因此我們成功的建立了字詞間的相似性關係。</p>
<h5><u>Word2Vec的架構</u></h5>
<p><img alt="word2vec" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.008.jpeg"></p>
<p>實作上如上圖所示，我們輸入一個字詞，譬如"cat"，通常會將他轉成One-hot encoding表示，但要注意喔！文本的字彙量是非常龐大的，所以當我們使用One-hot encoding表示時，將會出現一個非常長但Sparse的向量，相同的輸出層也同樣是一個很長的One-hot encoding，它的維度會和輸入層一樣大，因為我們要分析的字彙在輸入和輸出是一樣多的。</p>
<p>然後，和Autoencoder使用一樣的手法，中間的Hidden Layer放置低維度、少神經元的一層，但不同於Autoencoder，Word2Vec所有的轉換都是線性的，沒有非線性的Activation Function夾在其中，為什麼呢？因為我們的輸入是Sparse的而且只有0和1的差別，所以每一條通路就變成只有導通或不導通的差別，Activation Function有加等於沒加，使用線性就足夠了。</p>
<p>這個中間的Hidden Layer被稱為Embedding Matrix，它做了一個線性的Dimension Reduction，將原本高維度的One-hot encoding降低成低維度，然後再透過一個線性模型轉換回去原本的維度。假設字彙的數量有N個，所以輸入矩陣X是一個1xN的矩陣，輸出的矩陣同樣也是1xN的矩陣，當我先做一個線性的Dimension Reduction，將維度降到d維，此時Embedding Matrix會是一個Nxd的矩陣V，然後再由線性模型轉換回去原本的維度，這個轉換矩陣W是一個Nxd矩陣，因此綜合上述，可用一個簡潔的表示式表示：Y=W<sup>T</sup>VX，我們的目標就是找出這個W和V矩陣的每個元素。</p>
<p>你會想說線性模型很簡單啊！就是仿照Autoencoder的作法，然後把Activation Function拿掉不就了事了，並且因為輸出是One-hot Encoding所以最後套用Softmax，那不就輕鬆完成！但是真正的大魔王就出在字彙量，字彙量一旦很大，事情就變得不可收拾了，而且字彙量是一定小不得的，那怎麼辦？</p>
<p>在Dimension Reduction我們可以採取一個快速的方法，因為除了我要表示的字的位置是1以外其他都是0，所以其他都可以不看，我們就直接看是在第幾個位置上是1，然後再到Embedding Matrix上找到相應的行直接取出就是答案了，這樣查詢的動作，在Tensorflow中可以使用<code>tf.nn.embedding_lookup</code>來辦到。</p>
<p>再接下來最後的Cross-Entropy Loss計算也非常龐大，因為有幾個字彙就需要累加幾組數字，我們有一招偷吃步的方法叫做「Sampled Softmax」，作法是這樣的，我們不去計算全部詞彙的Cross-Entropy，而是選擇幾組詞彙來評估Cross-Entropy，在選擇上我們會隨機挑選一些Labels和預測結果差異度很大的詞彙(稱為Negative Examples)來算Cross-Entropy，我們在Tensorflow可以使用<code>tf.nn.sampled_softmax_loss</code>來辦到「Sampled Softmax」。</p>
<p>我們先不管輸入和輸出究竟怎麼取得，如果我們成功的建立了輸入和輸出的上下文關係，此時中間的Embedding空間正是精華的所在，經過剛剛推論，我們預期在這個空間當中，相似的詞彙會彼此靠近，我們評估兩個向量的相似性可以使用Cosine來評估，當兩向量的夾角越小代表它們越是相似，待會的實作當中我們將會利用Cosine來建立Similarity的大小，藉此來找到前幾個和它很靠近的詞彙。</p>
<p>另外，經研究指出這個Embedding空間的效果不只是可以算出詞彙間的相似性，還可以顯示詞彙間的比較關係，例如：北京之於中國，等同於台北之於台灣，這樣的比較關係也顯示在這個Embedding空間裡頭，所以在這空間裡會有以下的向量關係式：V<sub>北京</sub>-V<sub>中國</sub>+V<sub>台灣</sub>=V<sub>台北</sub>，是不是很神奇啊！</p>
<p><br/></p>
<h5><u>Word2Vec的兩種常用方法：Skip-Gram和CBOW</u></h5>
<p><img alt="Skip-Gram和CBOW" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.009.jpeg"></p>
<p>剛剛一直在講的是中間的結構應該怎麼建立，現在來看看我們可以輸入和輸出哪些詞彙來建立起上下文的關係，有兩種常用的類別：Skip-Gram和CBOW。</p>
<p>Skip-Gram如上圖所示，當我輸入一個word(t)時，我希望它能輸出它的前文和後文，這是相當直覺的建立上下文的方法，所以如果我希望用前一個字和後一個字來訓練我的Word2Vec，我就會有兩組數據：(w(t),w(t-1))和(w(t),w(t+1))，相當好理解。</p>
<p>而CBOW(Continuous Bag of Words)使用另外一種方法來建立上下文關係，它將一排字挖掉中間一個字，然後希望由上下文的關係有辦法猜出中間那個字，就像是填空題，此時輸入層就變成會有多於1個字，那該怎麼處理，答案是轉換到Embedding空間後再相加平均，因為是線性轉換，所以直接線性累加就可以了。</p>
<p><br/></p>
<h5><u>準備文本語料庫</u></h5>
<p>先帶入一些待會會用到的函式庫，並且決定我們要取用多少<code>VOCABULARY_SIZE</code>個詞彙量來做訓練。</p>
<div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">generators</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">from</span> <span class="nn">six.moves.urllib.request</span> <span class="kn">import</span> <span class="n">urlretrieve</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">VOCABULARY_SIZE</span> <span class="o">=</span> <span class="mi">100000</span>
</pre></div>


<p>接下來下載Dataset，並做一些前處理。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">maybe_download</span><span class="p">(</span><span class="n">url</span><span class="p">,</span><span class="n">filename</span><span class="p">,</span> <span class="n">expected_bytes</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Download a file if not present, and make sure it&#39;s the right size.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">filename</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
  <span class="n">statinfo</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">stat</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span> <span class="o">==</span> <span class="n">expected_bytes</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Found and verified </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">filename</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">statinfo</span><span class="o">.</span><span class="n">st_size</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
      <span class="s1">&#39;Failed to verify &#39;</span> <span class="o">+</span> <span class="n">filename</span> <span class="o">+</span> <span class="s1">&#39;. Can you get to it with a browser?&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">filename</span>

<span class="k">def</span> <span class="nf">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Extract the first file enclosed in a zip file as a list of words&quot;&quot;&quot;</span>
  <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_str</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">namelist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">data</span>

<span class="k">def</span> <span class="nf">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span><span class="n">vocabulary_size</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">):</span>
  <span class="n">count</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;UNK&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
  <span class="n">count</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">vocabulary_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">count</span><span class="p">:</span>
    <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>
  <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
  <span class="n">unk_count</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># dictionary[&#39;UNK&#39;]</span>
      <span class="n">unk_count</span> <span class="o">=</span> <span class="n">unk_count</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
  <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">unk_count</span>
  <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span> 
  <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Downloading text8.zip&quot;</span><span class="p">)</span>
<span class="n">filename</span> <span class="o">=</span> <span class="n">maybe_download</span><span class="p">(</span><span class="s1">&#39;http://mattmahoney.net/dc/text8.zip&#39;</span><span class="p">,</span><span class="s1">&#39;./text8.zip&#39;</span><span class="p">,</span> <span class="mi">31344016</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;=====&quot;</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Data size </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;First 10 words: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">10</span><span class="p">]))</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;=====&quot;</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span>
                                                            <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">)</span>
<span class="k">del</span> <span class="n">words</span>  <span class="c1"># Hint to reduce memory.</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Most common words (+UNK)&#39;</span><span class="p">,</span> <span class="n">count</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Sample data&#39;</span><span class="p">,</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>Downloading text8.zip
Found and verified ./text8.zip
=====
Data size 17005207
First 10 words: [&#39;anarchism&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;a&#39;, &#39;term&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;first&#39;, &#39;used&#39;, &#39;against&#39;]
=====
Most common words (+UNK) [[&#39;UNK&#39;, 189230], (&#39;the&#39;, 1061396), (&#39;of&#39;, 593677), (&#39;and&#39;, 416629), (&#39;one&#39;, 411764)]
Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]
</pre></div>


<p>我們取用<code>VOCABULARY_SIZE = 100000</code>，也是說我們將文本中的詞彙按出現次數的多寡來排列，取前面<code>VOCABULARY_SIZE</code>個保留，其餘詞彙皆歸類到「UNK Token」裡頭，UNK代表UNKnown的縮寫。</p>
<p>我們文本的字詞數量總共有17005207個字，開頭前十個字的句子是'anarchism originated as a term of abuse first used against'。所有的這17005207個字會依照<code>dictionary</code>給予每個字Index，而文本會被表示為一個由整數所構成的List，這會放在<code>data</code>裡頭，而這個Index也就直接當作One-hot Encoding中代表這個詞彙的維度位置。當我想要把Index轉換回去我們看得懂的字的時候，就需要<code>reverse_dictionary</code>的幫忙，有了這些，我們的語料庫就已經建立完成了。</p>
<p><br/></p>
<h5><u>實作Skip-Gram</u></h5>
<p>有了語料庫，我們就可以產生出我想要的輸入和輸出，在Skip-Gram方法，如果我的輸入是<code>target word</code>，我會先從<code>target word</code>向前、向後看出去<code>skip_window</code>的大小，所以可以選擇當作輸出的字有<code>skip_window*2</code>個，接下來我從這<code>skip_window*2</code>個中選擇<code>num_skips</code>個當作輸出，所以一個<code>target word</code>會產生<code>num_skips</code>筆數據，如果我一個batch需要<code>batch_size</code>筆數據，我就必須有<code>batch_size//num_skips</code>個<code>target word</code>，依照這樣的規則下面建立一個Generator來掃描文本，並輸出要訓練使用的Batch Data。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">skip_gram_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">num_skips</span><span class="p">,</span><span class="n">skip_window</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">num_skips</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">num_skips</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span>

    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># [ skip_window target skip_window ]</span>
    <span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>

    <span class="c1"># initialization</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">):</span>
        <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># generate</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">skip_window</span>  <span class="c1"># target label at the center of the buffer</span>
        <span class="n">targets_to_avoid</span> <span class="o">=</span> <span class="p">[</span> <span class="n">target</span> <span class="p">]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_skips</span><span class="p">):</span>
            <span class="k">while</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets_to_avoid</span><span class="p">:</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">span</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">targets_to_avoid</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">skip_window</span><span class="p">]</span>
            <span class="n">labels</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
            <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Recycle </span>
        <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span> <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># scan data</span>
        <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="c1"># Enough num to output</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="n">labels</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

<span class="c1"># demonstrate generator</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;data:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">di</span><span class="p">]</span> <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">]])</span>

<span class="k">for</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]:</span>
    <span class="n">batch_generator</span> <span class="o">=</span> <span class="n">skip_gram_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span><span class="n">num_skips</span><span class="o">=</span><span class="n">num_skips</span><span class="p">,</span><span class="n">skip_window</span><span class="o">=</span><span class="n">skip_window</span><span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">with num_skips = </span><span class="si">%d</span><span class="s1"> and skip_window = </span><span class="si">%d</span><span class="s1">:&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;    batch:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="k">for</span> <span class="n">bi</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;    labels:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">li</span><span class="p">]</span> <span class="k">for</span> <span class="n">li</span> <span class="ow">in</span> <span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">)])</span>
</pre></div>


<div class="highlight"><pre><span></span>data: [&#39;anarchism&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;a&#39;, &#39;term&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;first&#39;, &#39;used&#39;, &#39;against&#39;]

with num_skips = 2 and skip_window = 1:
    batch: [&#39;originated&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;as&#39;, &#39;a&#39;, &#39;a&#39;, &#39;term&#39;, &#39;term&#39;]
    labels: [&#39;as&#39;, &#39;anarchism&#39;, &#39;originated&#39;, &#39;a&#39;, &#39;as&#39;, &#39;term&#39;, &#39;a&#39;, &#39;of&#39;]

with num_skips = 4 and skip_window = 2:
    batch: [&#39;as&#39;, &#39;as&#39;, &#39;as&#39;, &#39;as&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;, &#39;a&#39;]
    labels: [&#39;term&#39;, &#39;anarchism&#39;, &#39;originated&#39;, &#39;a&#39;, &#39;originated&#39;, &#39;term&#39;, &#39;as&#39;, &#39;of&#39;]
</pre></div>


<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SkipGram</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_vocabulary</span><span class="p">,</span><span class="n">n_embedding</span><span class="p">,</span><span class="n">reverse_dictionary</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span> <span class="o">=</span> <span class="n">n_vocabulary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span> <span class="o">=</span> <span class="n">n_embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">reverse_dictionary</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span> <span class="c1"># initialize new grap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span> <span class="c1"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span> <span class="c1"># create session by the graph </span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c1">### Input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

            <span class="c1">### Optimalization</span>
            <span class="c1"># build neurel network structure and get their loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span> <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">,</span>
                                        <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">,</span>
                                      <span class="p">)</span>

            <span class="c1"># normalize embeddings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;embeddings&#39;</span><span class="p">]),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;embeddings&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span>

            <span class="c1"># define training operation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1">### Prediction</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span> <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">,</span>
                                            <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">,</span>
                                          <span class="p">)</span>

            <span class="c1"># similarity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span><span class="p">,</span> 
                                            <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">))</span>

            <span class="c1">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>  

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dataset</span><span class="p">,</span><span class="n">labels</span><span class="p">):</span>
        <span class="c1">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;embeddings&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                                <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span> 
                                                  <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span>
                <span class="s1">&#39;softmax&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                             <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span>
                               <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">)))</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;softmax&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">]))</span>
            <span class="p">}</span>


        <span class="c1">### Structure</span>
        <span class="c1"># Look up embeddings for inputs.</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;embeddings&#39;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">)</span>

        <span class="c1"># Compute the softmax loss, using a sample of the negative labels each time.</span>
        <span class="n">num_softmax_sampled</span> <span class="o">=</span> <span class="mi">64</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                 <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sampled_softmax_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;softmax&#39;</span><span class="p">],</span> 
                                            <span class="n">biases</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;softmax&#39;</span><span class="p">],</span> 
                                            <span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span>
                                            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
                                            <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_softmax_sampled</span><span class="p">,</span> 
                                            <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>


    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">online_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>      
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">}</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">nearest_words</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">top_nearest</span><span class="p">):</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span><span class="p">,</span>
                                   <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>
        <span class="n">X_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">valid_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">nearests</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_size</span><span class="p">):</span>
            <span class="n">valid_word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">valid_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">valid_word</span><span class="p">)</span>    

            <span class="c1"># select highest similarity word</span>
            <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">similarity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_nearest</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>  
            <span class="n">nearests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">nearest</span><span class="p">)))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">valid_words</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">nearests</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">embedding_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">find_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</pre></div>


<p>以上就是我建立的Model，這裡我採取<code>online_fit</code>的方法，不同於之前的<code>fit</code>，<code>online_fit</code>可以不用事先將所有Data一次餵進去，而是可以陸續的餵入Data，所以我會從上面的Generator陸續產生Batch Data並餵入Model裡來做訓練。</p>
<div class="highlight"><pre><span></span><span class="c1"># build skip-gram batch generator</span>
<span class="n">batch_generator</span> <span class="o">=</span> <span class="n">skip_gram_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                                            <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                                            <span class="n">num_skips</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                            <span class="n">skip_window</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># build skip-gram model</span>
<span class="n">model_SkipGram</span> <span class="o">=</span> <span class="n">SkipGram</span><span class="p">(</span><span class="n">n_vocabulary</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">,</span>
                          <span class="n">n_embedding</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                          <span class="n">reverse_dictionary</span><span class="o">=</span><span class="n">reverse_dictionary</span><span class="p">,</span>
                          <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="c1"># initial model</span>
<span class="n">model_SkipGram</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

<span class="c1"># online training</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_batchs_in_epoch</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batchs_in_epoch</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model_SkipGram</span><span class="o">.</span><span class="n">online_fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
                                         <span class="n">Y</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">loss</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">avg_loss</span> <span class="o">/</span> <span class="n">num_batchs_in_epoch</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">: </span><span class="si">%d</span><span class="s2">s loss = </span><span class="si">%9.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span><span class="p">,</span>
                                                   <span class="n">avg_loss</span> <span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Epoch 1/50: 18s loss =    4.2115
Epoch 2/50: 17s loss =    3.7554
Epoch 3/50: 15s loss =    3.6211
Epoch 4/50: 15s loss =    3.5072
Epoch 5/50: 15s loss =    3.5084
Epoch 6/50: 15s loss =    3.4988
Epoch 7/50: 15s loss =    3.5165
Epoch 8/50: 15s loss =    3.3949
Epoch 9/50: 15s loss =    3.4382
Epoch 10/50: 15s loss =    3.4121
Epoch 11/50: 15s loss =    3.4027
Epoch 12/50: 15s loss =    3.4074
Epoch 13/50: 15s loss =    3.3222
Epoch 14/50: 15s loss =    3.3448
Epoch 15/50: 16s loss =    3.3616
Epoch 16/50: 15s loss =    3.3389
Epoch 17/50: 15s loss =    3.3729
Epoch 18/50: 15s loss =    3.3911
Epoch 19/50: 15s loss =    3.3512
Epoch 20/50: 15s loss =    3.3107
Epoch 21/50: 16s loss =    3.3046
Epoch 22/50: 15s loss =    3.3103
Epoch 23/50: 15s loss =    3.3042
Epoch 24/50: 15s loss =    3.2634
Epoch 25/50: 15s loss =    3.3181
Epoch 26/50: 15s loss =    3.2808
Epoch 27/50: 15s loss =    3.2464
Epoch 28/50: 15s loss =    3.2246
Epoch 29/50: 15s loss =    3.2666
Epoch 30/50: 15s loss =    3.2275
Epoch 31/50: 15s loss =    3.2312
Epoch 32/50: 15s loss =    3.3022
Epoch 33/50: 15s loss =    3.2504
Epoch 34/50: 15s loss =    3.2484
Epoch 35/50: 15s loss =    3.2368
Epoch 36/50: 15s loss =    3.2693
Epoch 37/50: 15s loss =    3.2177
Epoch 38/50: 15s loss =    3.2395
Epoch 39/50: 15s loss =    3.2151
Epoch 40/50: 15s loss =    3.0505
Epoch 41/50: 15s loss =    2.9364
Epoch 42/50: 15s loss =    3.1546
Epoch 43/50: 15s loss =    3.1810
Epoch 44/50: 15s loss =    3.2778
Epoch 45/50: 15s loss =    3.1340
Epoch 46/50: 15s loss =    3.2218
Epoch 47/50: 15s loss =    3.2395
Epoch 48/50: 15s loss =    3.2422
Epoch 49/50: 15s loss =    3.0131
Epoch 50/50: 15s loss =    3.1287
</pre></div>


<p>我們來看看效果如何，我們使用Embedding Vectors彼此間的Cosine來定義出字詞間的相關性，並且列出8個最為靠近的字詞。</p>
<div class="highlight"><pre><span></span><span class="n">valid_words_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">210</span><span class="p">,</span><span class="mi">239</span><span class="p">,</span><span class="mi">392</span><span class="p">,</span><span class="mi">396</span><span class="p">])</span>

<span class="n">valid_words</span><span class="p">,</span> <span class="n">nearests</span> <span class="o">=</span> <span class="n">model_SkipGram</span><span class="o">.</span><span class="n">nearest_words</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">valid_words_index</span><span class="p">,</span><span class="n">top_nearest</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_words</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Nearest to &#39;{}&#39;: &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">valid_words</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">nearests</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>Nearest to &#39;two&#39;:  [&#39;three&#39; &#39;four&#39; &#39;five&#39; &#39;eight&#39; &#39;six&#39; &#39;seven&#39; &#39;nine&#39; &#39;one&#39;]
Nearest to &#39;that&#39;:  [&#39;which&#39; &#39;however&#39; &#39;eophona&#39; &#39;clemency&#39; &#39;invariants&#39; &#39;ratchet&#39; &#39;what&#39;
 &#39;fiona&#39;]
Nearest to &#39;his&#39;:  [&#39;her&#39; &#39;their&#39; &#39;my&#39; &#39;your&#39; &#39;its&#39; &#39;our&#39; &#39;thy&#39; &#39;witchcraft&#39;]
Nearest to &#39;were&#39;:  [&#39;are&#39; &#39;was&#39; &#39;include&#39; &#39;have&#39; &#39;cyanobacterial&#39; &#39;seem&#39; &#39;be&#39; &#39;those&#39;]
Nearest to &#39;all&#39;:  [&#39;both&#39; &#39;various&#39; &#39;many&#39; &#39;counting&#39; &#39;some&#39; &#39;every&#39; &#39;several&#39; &#39;risked&#39;]
Nearest to &#39;area&#39;:  [&#39;region&#39; &#39;areas&#39; &#39;suctoria&#39; &#39;regions&#39; &#39;island&#39; &#39;pwned&#39; &#39;territory&#39;
 &#39;plains&#39;]
Nearest to &#39;east&#39;:  [&#39;west&#39; &#39;eastern&#39; &#39;southeast&#39; &#39;south&#39; &#39;southwest&#39; &#39;curable&#39; &#39;north&#39;
 &#39;hispaniolan&#39;]
Nearest to &#39;himself&#39;:  [&#39;him&#39; &#39;themselves&#39; &#39;them&#39; &#39;itself&#39; &#39;megalith&#39; &#39;herself&#39; &#39;successfully&#39;
 &#39;armas&#39;]
Nearest to &#39;white&#39;:  [&#39;red&#39; &#39;black&#39; &#39;blue&#39; &#39;yellow&#39; &#39;green&#39; &#39;overdraft&#39; &#39;horse&#39; &#39;dark&#39;]
</pre></div>


<p>結果相當驚人，與'two'靠近的真的都是數字類型的文字，與'that'靠近的都是文法功能性的詞彙，與'his'靠近的都是所有格代名詞，與'were'靠近的是be動詞，與'all'最靠近的是'both'，與'east'靠近的都是一些代表方向的詞彙，與'white'靠近的都是一些顏色的詞彙，真的是太神奇了！</p>
<p>接下來直接來觀察Embedding空間，以下使用t-SNE來圖像化Embedding空間。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s1">&#39;More labels than embeddings&#39;</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>  <span class="c1"># in inches</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span>
                   <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">visualization_words</span> <span class="o">=</span> <span class="mi">800</span>
<span class="c1"># transform embeddings to 2D by t-SNE</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">model_SkipGram</span><span class="o">.</span><span class="n">embedding_matrix</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;pca&#39;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;exact&#39;</span><span class="p">)</span>
<span class="n">two_d_embed</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
<span class="c1"># list labels</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_SkipGram</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="c1"># plot</span>
<span class="n">plot</span><span class="p">(</span><span class="n">two_d_embed</span><span class="p">,</span><span class="n">words</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/05_output_13_0.png"></p>
<p>如此一來你將可以簡單的看出，哪些詞彙彼此相似而靠近。</p>
<p><br/></p>
<h5><u>實作CBOW (Continuous Bag of Words)</u></h5>
<p>接著看CBOW的方法，如果我預期輸出的字是<code>target word</code>，從<code>target word</code>向前向後看出去<code>context_window</code>的大小，看到的字都當作我的輸入，所以我輸入的字總共需要<code>context_window*2</code>個，一個<code>target word</code>只會產生一筆數據，如果我一個batch需要<code>batch_size</code>筆數據，我就必須有<code>batch_size</code>個<code>target word</code>，依照這樣的規則下面建立一個Generator來掃描文本，並輸出要訓練使用的Batch Data。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cbow_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">context_window</span><span class="p">):</span>
    <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">context_window</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># [ context_window target context_window ]</span>
    <span class="n">num_bow</span> <span class="o">=</span> <span class="n">span</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_bow</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>

    <span class="c1"># initialization</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">):</span>
        <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># generate</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">context_window</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>        
        <span class="n">bow</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">bow</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">bow</span><span class="p">):</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span>
        <span class="n">labels</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Recycle </span>
        <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span> <span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># scan data</span>
        <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="c1"># Enough num to output</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>



<span class="c1"># demonstrate generator</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;data:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">di</span><span class="p">]</span> <span class="k">for</span> <span class="n">di</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[:</span><span class="mi">10</span><span class="p">]])</span>

<span class="k">for</span> <span class="n">context_window</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">batch_generator</span> <span class="o">=</span> <span class="n">cbow_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                           <span class="n">context_window</span><span class="o">=</span><span class="n">context_window</span><span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">with context_window = </span><span class="si">%d</span><span class="s1">:&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">context_window</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;batch:&#39;</span><span class="p">)</span>
    <span class="n">show_batch</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">tmp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">batch</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]])</span>
        <span class="n">show_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">show_batch</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;labels:&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">li</span><span class="p">]</span> <span class="k">for</span> <span class="n">li</span> <span class="ow">in</span> <span class="n">labels</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">)])</span>
</pre></div>


<div class="highlight"><pre><span></span>data: [&#39;anarchism&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;a&#39;, &#39;term&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;first&#39;, &#39;used&#39;, &#39;against&#39;]

with context_window = 1:
batch:
[[&#39;anarchism&#39;, &#39;as&#39;], [&#39;originated&#39;, &#39;a&#39;], [&#39;as&#39;, &#39;term&#39;], [&#39;a&#39;, &#39;of&#39;], [&#39;term&#39;, &#39;abuse&#39;], [&#39;of&#39;, &#39;first&#39;], [&#39;abuse&#39;, &#39;used&#39;], [&#39;first&#39;, &#39;against&#39;]]
labels: [&#39;originated&#39;, &#39;as&#39;, &#39;a&#39;, &#39;term&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;first&#39;, &#39;used&#39;]

with context_window = 2:
batch:
[[&#39;anarchism&#39;, &#39;originated&#39;, &#39;a&#39;, &#39;term&#39;], [&#39;originated&#39;, &#39;as&#39;, &#39;term&#39;, &#39;of&#39;], [&#39;as&#39;, &#39;a&#39;, &#39;of&#39;, &#39;abuse&#39;], [&#39;a&#39;, &#39;term&#39;, &#39;abuse&#39;, &#39;first&#39;], [&#39;term&#39;, &#39;of&#39;, &#39;first&#39;, &#39;used&#39;], [&#39;of&#39;, &#39;abuse&#39;, &#39;used&#39;, &#39;against&#39;], [&#39;abuse&#39;, &#39;first&#39;, &#39;against&#39;, &#39;early&#39;], [&#39;first&#39;, &#39;used&#39;, &#39;early&#39;, &#39;working&#39;]]
labels: [&#39;as&#39;, &#39;a&#39;, &#39;term&#39;, &#39;of&#39;, &#39;abuse&#39;, &#39;first&#39;, &#39;used&#39;, &#39;against&#39;]
</pre></div>


<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CBOW</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_vocabulary</span><span class="p">,</span><span class="n">n_embedding</span><span class="p">,</span><span class="n">context_window</span><span class="p">,</span><span class="n">reverse_dictionary</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span> <span class="o">=</span> <span class="n">n_vocabulary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span> <span class="o">=</span> <span class="n">n_embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_window</span> <span class="o">=</span> <span class="n">context_window</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="n">reverse_dictionary</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span> <span class="c1"># initialize new grap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span> <span class="c1"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span> <span class="c1"># create session by the graph </span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c1">### Input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_window</span><span class="o">*</span><span class="mi">2</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

            <span class="c1">### Optimalization</span>
            <span class="c1"># build neurel network structure and get their predictions and loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span> <span class="n">dataset</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">,</span>
                                        <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">,</span>
                                      <span class="p">)</span>

            <span class="c1"># normalize embeddings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                          <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                            <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;embeddings&#39;</span><span class="p">]),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;embeddings&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span>

            <span class="c1"># define training operation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdagradOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1">### Prediction</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

            <span class="c1"># similarity</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_embed</span><span class="p">,</span> 
                                            <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">))</span>

            <span class="c1">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>  

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dataset</span><span class="p">,</span><span class="n">labels</span><span class="p">):</span>
        <span class="c1">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;embeddings&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                                <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span>
                                                  <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span>
                <span class="s1">&#39;softmax&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span>
                             <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">],</span>
                               <span class="n">stddev</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_embedding</span><span class="p">)))</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;softmax&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">]))</span>
            <span class="p">}</span>


        <span class="c1">### Structure</span>
        <span class="c1"># Look up embeddings for inputs.</span>
        <span class="n">embed_bow</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;embeddings&#39;</span><span class="p">],</span> <span class="n">dataset</span><span class="p">)</span>
        <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">embed_bow</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Compute the softmax loss, using a sample of the negative labels each time.</span>
        <span class="n">num_softmax_sampled</span> <span class="o">=</span> <span class="mi">64</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                 <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sampled_softmax_loss</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;softmax&#39;</span><span class="p">],</span> 
                                            <span class="n">biases</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;softmax&#39;</span><span class="p">],</span> 
                                            <span class="n">inputs</span><span class="o">=</span><span class="n">embed</span><span class="p">,</span>
                                            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
                                            <span class="n">num_sampled</span><span class="o">=</span><span class="n">num_softmax_sampled</span><span class="p">,</span> 
                                            <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_vocabulary</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span>


    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">online_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>      
        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">train_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                     <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">}</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">nearest_words</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">top_nearest</span><span class="p">):</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_similarity</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>
        <span class="n">X_size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">valid_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">nearests</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_size</span><span class="p">):</span>
            <span class="n">valid_word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">valid_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">valid_word</span><span class="p">)</span>    

            <span class="c1"># select highest similarity word</span>
            <span class="n">nearest</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">similarity</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">top_nearest</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">nearests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">find_word</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="n">nearest</span><span class="p">)))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">valid_words</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">nearests</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_dataset</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">:</span> <span class="n">Y</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">embedding_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_embeddings</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">find_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">context_window</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># build CBOW batch generator</span>
<span class="n">batch_generator</span> <span class="o">=</span> <span class="n">cbow_batch_generator</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                                       <span class="n">context_window</span><span class="o">=</span><span class="n">context_window</span><span class="p">)</span>

<span class="c1"># build CBOW model</span>
<span class="n">model_CBOW</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">n_vocabulary</span><span class="o">=</span><span class="n">VOCABULARY_SIZE</span><span class="p">,</span>
                  <span class="n">n_embedding</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                  <span class="n">context_window</span><span class="o">=</span><span class="n">context_window</span><span class="p">,</span>
                  <span class="n">reverse_dictionary</span><span class="o">=</span><span class="n">reverse_dictionary</span><span class="p">,</span>
                  <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="c1"># initialize model</span>
<span class="n">model_CBOW</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

<span class="c1"># online training</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_batchs_in_epoch</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batchs_in_epoch</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model_CBOW</span><span class="o">.</span><span class="n">online_fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
                                     <span class="n">Y</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">loss</span>
    <span class="n">avg_loss</span> <span class="o">=</span> <span class="n">avg_loss</span> <span class="o">/</span> <span class="n">num_batchs_in_epoch</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">: </span><span class="si">%d</span><span class="s2">s loss = </span><span class="si">%9.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span><span class="p">,</span>
                                                   <span class="n">avg_loss</span> <span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Epoch 1/50: 14s loss =    3.8643
Epoch 2/50: 14s loss =    3.2952
Epoch 3/50: 14s loss =    3.1950
Epoch 4/50: 14s loss =    3.1204
Epoch 5/50: 14s loss =    3.0737
Epoch 6/50: 14s loss =    3.0243
Epoch 7/50: 14s loss =    2.9382
Epoch 8/50: 14s loss =    2.9539
Epoch 9/50: 14s loss =    2.9690
Epoch 10/50: 14s loss =    2.9003
Epoch 11/50: 14s loss =    2.8737
Epoch 12/50: 14s loss =    2.8308
Epoch 13/50: 14s loss =    2.8444
Epoch 14/50: 14s loss =    2.7676
Epoch 15/50: 14s loss =    2.7811
Epoch 16/50: 14s loss =    2.7926
Epoch 17/50: 14s loss =    2.7528
Epoch 18/50: 14s loss =    2.7552
Epoch 19/50: 14s loss =    2.7353
Epoch 20/50: 14s loss =    2.6232
Epoch 21/50: 14s loss =    2.5206
Epoch 22/50: 14s loss =    2.7120
Epoch 23/50: 14s loss =    2.6625
Epoch 24/50: 14s loss =    2.7351
Epoch 25/50: 14s loss =    2.5335
Epoch 26/50: 14s loss =    2.6600
Epoch 27/50: 14s loss =    2.6636
Epoch 28/50: 14s loss =    2.5972
Epoch 29/50: 14s loss =    2.5400
Epoch 30/50: 14s loss =    2.6047
Epoch 31/50: 14s loss =    2.5544
Epoch 32/50: 14s loss =    2.5932
Epoch 33/50: 14s loss =    2.5554
Epoch 34/50: 14s loss =    2.5256
Epoch 35/50: 14s loss =    2.5664
Epoch 36/50: 14s loss =    2.5977
Epoch 37/50: 14s loss =    2.5392
Epoch 38/50: 14s loss =    2.5666
Epoch 39/50: 14s loss =    2.5123
Epoch 40/50: 14s loss =    2.5169
Epoch 41/50: 14s loss =    2.4920
Epoch 42/50: 14s loss =    2.4872
Epoch 43/50: 14s loss =    2.5512
Epoch 44/50: 14s loss =    2.4895
Epoch 45/50: 14s loss =    2.5202
Epoch 46/50: 14s loss =    2.5011
Epoch 47/50: 14s loss =    2.2540
Epoch 48/50: 14s loss =    2.4145
Epoch 49/50: 14s loss =    2.4916
Epoch 50/50: 14s loss =    2.4924
</pre></div>


<div class="highlight"><pre><span></span><span class="n">valid_words_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">210</span><span class="p">,</span><span class="mi">239</span><span class="p">,</span><span class="mi">392</span><span class="p">,</span><span class="mi">396</span><span class="p">])</span>

<span class="n">valid_words</span><span class="p">,</span> <span class="n">nearests</span> <span class="o">=</span> <span class="n">model_CBOW</span><span class="o">.</span><span class="n">nearest_words</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">valid_words_index</span><span class="p">,</span><span class="n">top_nearest</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_words</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Nearest to &#39;{}&#39;: &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">valid_words</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">nearests</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>Nearest to &#39;two&#39;:  [&#39;three&#39; &#39;four&#39; &#39;five&#39; &#39;six&#39; &#39;seven&#39; &#39;eight&#39; &#39;one&#39; &#39;xx&#39;]
Nearest to &#39;that&#39;:  [&#39;which&#39; &#39;however&#39; &#39;furthermore&#39; &#39;what&#39; &#39;nevertheless&#39; &#39;imaginable&#39;
 &#39;assemblage&#39; &#39;where&#39;]
Nearest to &#39;his&#39;:  [&#39;her&#39; &#39;their&#39; &#39;my&#39; &#39;your&#39; &#39;its&#39; &#39;whose&#39; &#39;our&#39; &#39;dufay&#39;]
Nearest to &#39;were&#39;:  [&#39;are&#39; &#39;remain&#39; &#39;include&#39; &#39;have&#39; &#39;was&#39; &#39;tend&#39; &#39;those&#39; &#39;appear&#39;]
Nearest to &#39;all&#39;:  [&#39;both&#39; &#39;various&#39; &#39;unacknowledged&#39; &#39;every&#39; &#39;faked&#39; &#39;aurangazeb&#39; &#39;some&#39;
 &#39;many&#39;]
Nearest to &#39;area&#39;:  [&#39;region&#39; &#39;areas&#39; &#39;regions&#39; &#39;land&#39; &#39;campus&#39; &#39;streets&#39; &#39;harbour&#39; &#39;tacos&#39;]
Nearest to &#39;east&#39;:  [&#39;west&#39; &#39;south&#39; &#39;southwest&#39; &#39;north&#39; &#39;northeast&#39; &#39;eastern&#39; &#39;southeast&#39;
 &#39;highlights&#39;]
Nearest to &#39;himself&#39;:  [&#39;him&#39; &#39;themselves&#39; &#39;them&#39; &#39;itself&#39; &#39;herself&#39; &#39;papp&#39; &#39;aafk&#39; &#39;heartbroken&#39;]
Nearest to &#39;white&#39;:  [&#39;red&#39; &#39;black&#39; &#39;blue&#39; &#39;dark&#39; &#39;yellow&#39; &#39;culturally&#39; &#39;dead&#39; &#39;angelman&#39;]
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pylab</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s1">&#39;More labels than embeddings&#39;</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>  <span class="c1"># in inches</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
        <span class="n">pylab</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span>
                   <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
    <span class="n">pylab</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">visualization_words</span> <span class="o">=</span> <span class="mi">800</span>
<span class="c1"># transform embeddings to 2D by t-SNE</span>
<span class="n">embed</span> <span class="o">=</span> <span class="n">model_CBOW</span><span class="o">.</span><span class="n">embedding_matrix</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">,:]</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;pca&#39;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;exact&#39;</span><span class="p">)</span>
<span class="n">two_d_embed</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embed</span><span class="p">)</span>
<span class="c1"># list labels</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_CBOW</span><span class="o">.</span><span class="n">reverse_dictionary</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">visualization_words</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
<span class="c1"># plot</span>
<span class="n">plot</span><span class="p">(</span><span class="n">two_d_embed</span><span class="p">,</span><span class="n">words</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/05_output_20_0.png"></p>
<p><br/></p>
<h5><u>Reference</u></h5>
<ul>
<li>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb</li>
</ul></dd>
              
            	<dt>2017 / 11月 18</dt>
            	<dd><a href="../tensorflow-tutorial_4.html">實作Tensorflow (4)：Autoencoder</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><p>Autoencoder是一個Neurel Network重要的工具，我個人認為它還漂亮的呈現Neurel Network的強大。</p>
<p>本單元程式碼Autoencoder部分可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/04_1_Autoencoder_on_MNIST.py">Github</a>下載，De-noise Autoencoder部分可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/04_2_DenoiseAutoencoder_on_MNIST.py">Github</a>下載。</p>
<p><br/></p>
<h5><u>Autoencoder觀念解析</u></h5>
<p>在「機器學習技法」的系列文章，我也<a href="http://www.ycc.idv.tw/YCNote/post/35">曾經介紹過Autoencoder</a>，可以搭配這篇服用。</p>
<p>Autoencoder概念很簡單，就是做資訊的壓縮，概念是這樣的，當我在一層當中使用神經元愈多，可以儲存的資訊量也就愈多，相反的神經元越少，可以儲存的資訊量越少，如果我要使用Neurel Network作資料壓縮的話，我希望的是可以使用比原本更少的資訊量來儲存，如果原本是一張MNIST的圖，有28x28=784個Pixels，所以可以想知，如果我要作壓縮就要使得壓縮後的神經元可以比784個更少。</p>
<p>但是什麼都不做我們就可以平白無故的做到壓縮？當然不行，我們還得從資料中找到一些規律，套用這些規律把多餘的東西去除，留下精髓，我們才可以把資料作壓縮，所以在實作上我們會建立一個神經元由大到小的Neurel Network，逐步的轉換，逐步的壓縮資訊。</p>
<p>那麼壓縮的目的是為了什麼？當然是有辦法還原回去原本狀態，這樣的壓縮才是有意義的，例如：將文檔打包成RAR，檔案大小會變小，但如果實際要再使用這個檔案，那就必須先做解壓縮，然後還原回去原本的檔案，這裡的還原率必須是百分之一百的，Autoencoder一樣的有一個機制可以還原，在實作上我們會建立一個神經元由小到大的Neurel Network，逐步的還原回去原本的狀態。</p>
<p>因此一個Autoencoder的圖像就出現了，我們需要有一組「Encoder」來逐步的壓縮，最後留下非常精簡的「Embedding Code」，而這組「Embedding Code」可以再經由「Decoder」還原回去原本的樣子，那我們怎麼讓他自己產生「Encoder」和「Decoder」呢？把原本的Input當作Output的目標答案去訓練Neurel Network就可以了，這就是Autoencoder巧妙的地方。</p>
<p>不管是「Encoder」還是「Decoder」他們的權重是可以調整的，所以如果你將Encoder+Decoder的結構建立好並搭配Input當作Output的目標答案，它在Training的過程，Autoencoder會試著找出最好的權重來使得資訊可以盡量完整還原回去，所以Autoencoder可以自行找出了Encoder和Decoder。</p>
<p>Encoder的效果等同於做Dimension Reduction，Encoder轉換原本數據到一個新的空間，這個空間可以比原本Features描述的空間更能精簡的描述這群數據，而中間這層Layer的數值Embedding Code就是新空間裡頭的座標，有些時候我們會用這個新空間來判斷每筆Data之間彼此的接近程度。</p>
<p><img alt="autoencoder" src="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/img/TensorflowTutorial.007.jpeg?raw=true"></p>
<p><br/></p>
<h5><u>Autoencoder程式碼</u></h5>
<p>實現Autoencoder和之前DNN並沒有太大的差異，只有兩點要特別提醒一下。</p>
<p>第一點，以下我會特別把<code>encoder</code>額外的在<code>structure</code>裡頭輸出出來，並且增加新的函數<code>encode</code>，讓使用者可以使用Train好的Encoder來做Encode。</p>
<p>第二點，以下的Regularizer不是採用單純的L2 Regularizer，我將會使用Weight-Elimination L2 Regularizer，這個Regularizer的好處是會使得權重接近Sparse，也就是說權重會留下比較多的0，這有一個好處，就是每個神經元彼此之間的依賴減少了，因為內積(評估相依性)時有0的那個維度將不會有所貢獻。</p>
<p>Weight-Elimination L2 Regularizer有這樣的效果原因是這樣的，L2 Regularizer在抑制W的方法是，如果W的分量大的話就抑制多一點，如果分量小就抑制少一點（因為W<sup>2</sup>微分為一次），所以最後會留下很多不為0的微小分量，不夠Sparse，這樣的Regularization顯然不夠好，L1 Regularizer可以解決這個問題（因為在大部分位置微分為常數），但不幸的是它無法微分，沒辦法作Backpropagation，所以就有了L2 Regularizer的衍生版本，</p>
<p>Weight-elimination L2 regularizer: 𝚺[(W<sub>jk</sub>(ℓ))<sup>2</sup>]/[1+(W<sub>jk</sub>(ℓ))<sup>2</sup>]</p>
<p>這麼一來不管W大或小，它受到抑制的值大小接近的 (Weight-elimination L2 regularizer微分為 -1次方)，因此就可以使得部分W可以為0，達成Sparse的目的。</p>
<p>那為什麼我要特別在Autoencoder講究Sparse特性呢？原因是我們現在正在做的事是Dimension Reduction，做這件事就好像是替原本空間找出新的軸，而這個軸的數量比原本空間軸的數量來得小，達到Dimension Reduction的效果，所以我們會希望這個新的軸彼此間可以不要太多的依賴，什麼是不依賴呢？直角座標就是最不依賴的座標系，X軸和Y軸內積為0，這樣的軸展開的效率是最好的，所以我們希望在做Regularization的同時可以減少新軸的彼此間的依賴性。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">random</span><span class="o">,</span> <span class="nn">time</span>

<span class="c1"># Config the matplotlib backend as plotting inline in IPython</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_features</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">n_hidden</span><span class="o">=</span><span class="p">[</span><span class="mi">1000</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">250</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span> <span class="c1"># initialize new grap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">alpha</span><span class="p">)</span> <span class="c1"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span> <span class="c1"># create session by the graph </span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_features</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c1">### Input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="n">n_features</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_targets</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="n">n_features</span><span class="p">))</span>

            <span class="c1">### Optimalization</span>
            <span class="c1"># build neurel network structure and get their predictions and loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span>
                                               <span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_features</span><span class="p">,</span>
                                               <span class="n">targets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_targets</span><span class="p">,</span>
                                               <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">)</span>

            <span class="c1"># regularization loss</span>
            <span class="c1"># weight elimination L2 regularizer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">regularizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span> \
                    <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                     <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">out_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>

            <span class="c1"># total loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_loss</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularizer</span>

            <span class="c1"># define training operation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1">### Prediction</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="n">n_features</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_targets</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="n">n_features</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_y_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_original_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span>
                                                          <span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_features</span><span class="p">,</span>
                                                          <span class="n">targets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_targets</span><span class="p">,</span>
                                                          <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">)</span>  
            <span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_original_loss</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularizer</span>

            <span class="c1">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>  

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">features</span><span class="p">,</span><span class="n">targets</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">):</span>
        <span class="c1">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="n">n_encoder</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">]</span><span class="o">+</span><span class="n">n_hidden</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_encoder</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;encode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> \
                    <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span>
                        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">n_encoder</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]),</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;encode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> \
                    <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_encoder</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

            <span class="n">n_decoder</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">))</span><span class="o">+</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_decoder</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;decode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> \
                    <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span>
                        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">n_decoder</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]),</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;decode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> \
                    <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_decoder</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="p">),</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>                    

        <span class="c1">### Structure</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span>

        <span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">features</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;encode1&#39;</span><span class="p">],</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;encode1&#39;</span><span class="p">],</span>
                                     <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;encode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;encode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span>
                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>   

        <span class="n">encoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;encode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">))],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;encode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">))])</span> 

        <span class="n">decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;decode1&#39;</span><span class="p">],</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;decode1&#39;</span><span class="p">],</span>
                                     <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">decoder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">decoder</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;decode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;decode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span>
                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span> 

        <span class="n">y_</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">decoder</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;decode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">))],</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;decode{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">))],</span>
                        <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>      

        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">targets</span> <span class="o">-</span> <span class="n">y_</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">encoder</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">getDenseLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_layer</span><span class="p">,</span><span class="n">weight</span><span class="p">,</span><span class="n">bias</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span><span class="n">weight</span><span class="p">),</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">activation</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">test_data</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

        <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">9000</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_size</span><span class="p">:</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">N</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">%2d</span><span class="s2">/</span><span class="si">%2d</span><span class="s2">: &quot;</span><span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="p">))</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

            <span class="c1"># mini-batch gradient descent</span>
            <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
            <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
                <span class="n">index_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
                <span class="n">batch_index</span> <span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span><span class="n">index_size</span><span class="p">))]</span>     

                <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">train_features</span><span class="p">:</span> <span class="n">X</span><span class="p">[</span><span class="n">batch_index</span><span class="p">,:],</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">train_targets</span><span class="p">:</span> <span class="n">Y</span><span class="p">[</span><span class="n">batch_index</span><span class="p">,:]}</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;[</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">] loss = </span><span class="si">%9.4f</span><span class="s2">     &quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">N</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">),</span> <span class="n">N</span><span class="p">,</span> <span class="n">loss</span> <span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">&#39;</span><span class="p">)</span>


            <span class="c1"># evaluate at the end of this epoch</span>
            <span class="n">msg_valid</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="k">if</span> <span class="n">validation_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">val_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">msg_valid</span> <span class="o">=</span> <span class="s2">&quot;, val_loss = </span><span class="si">%9.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">val_loss</span> <span class="p">)</span>

            <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;[</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">] </span><span class="si">%d</span><span class="s2">s loss = </span><span class="si">%9.4f</span><span class="s2"> </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_time</span><span class="p">,</span>
                                                   <span class="n">train_loss</span><span class="p">,</span> <span class="n">msg_valid</span> <span class="p">))</span>

        <span class="k">if</span> <span class="n">test_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">test_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">test_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;test_loss = </span><span class="si">%9.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">test_loss</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_encoder</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_features</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_y_</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_features</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_features</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">new_targets</span><span class="p">:</span> <span class="n">Y</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">_check_array</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ndarray</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ndarray</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span> <span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ndarray</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="n">ndarray</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">ndarray</span>
</pre></div>


<p><br/></p>
<h5><u>測試Autoencoder</u></h5>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s2">&quot;MNIST_data/&quot;</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span>
<span class="n">valid_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">validation</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model_1</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">(</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span>
                     <span class="n">learning_rate</span><span class="o">=</span> <span class="mf">0.0005</span><span class="p">,</span>
                     <span class="n">n_hidden</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span>
                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                    <span class="p">)</span>
<span class="n">model_1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span>
           <span class="n">Y</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span>
           <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
           <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">valid_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">valid_data</span><span class="o">.</span><span class="n">images</span><span class="p">),</span>
           <span class="n">test_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">),</span>
           <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
          <span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">15</span><span class="p">):</span>
    <span class="n">img_original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_original</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">model_1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]),(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>Epoch  1/20: 
[55000/55000] 98s loss =    0.0335 , val_loss =    0.0330
Epoch  2/20: 
[55000/55000] 98s loss =    0.0307 , val_loss =    0.0305
Epoch  3/20: 
[55000/55000] 99s loss =    0.0293 , val_loss =    0.0291
Epoch  4/20: 
[55000/55000] 96s loss =    0.0283 , val_loss =    0.0282
Epoch  5/20: 
[55000/55000] 96s loss =    0.0277 , val_loss =    0.0278
Epoch  6/20: 
[55000/55000] 98s loss =    0.0271 , val_loss =    0.0273

...略...

Epoch 15/20: 
[55000/55000] 99s loss =    0.0250 , val_loss =    0.0258
Epoch 16/20: 
[55000/55000] 98s loss =    0.0246 , val_loss =    0.0255
Epoch 17/20: 
[55000/55000] 99s loss =    0.0246 , val_loss =    0.0256
Epoch 18/20: 
[55000/55000] 97s loss =    0.0247 , val_loss =    0.0256
Epoch 19/20: 
[55000/55000] 98s loss =    0.0246 , val_loss =    0.0256
Epoch 20/20: 
[55000/55000] 99s loss =    0.0244 , val_loss =    0.0255
test_loss =    0.0261
</pre></div>


<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/04_output_5_1.png"></p>
<p>上面圖中上排是進去Autoencoder之前的圖片，下排是經過Autoencoder後的圖片，效果是不是很驚人！大致都有辦法還原回去原圖。唯一可能有缺陷的地方是，這個Autoencoder似乎會把5看成是6，做個Regularization看看能不能解決這個問題。</p>
<div class="highlight"><pre><span></span><span class="n">model_2</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">(</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span>
                     <span class="n">learning_rate</span><span class="o">=</span> <span class="mf">0.0005</span><span class="p">,</span>
                     <span class="n">n_hidden</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span>
                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
                    <span class="p">)</span>
<span class="n">model_2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span>
           <span class="n">Y</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span>
           <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
           <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">valid_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">valid_data</span><span class="o">.</span><span class="n">images</span><span class="p">),</span>
           <span class="n">test_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">),</span>
           <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
          <span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">15</span><span class="p">):</span>
    <span class="n">img_original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_original</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">model_2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">]),(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>Epoch  1/20: 
[55000/55000] 88s loss =    0.0332 , val_loss =    0.0328
Epoch  2/20: 
[55000/55000] 90s loss =    0.0301 , val_loss =    0.0299
Epoch  3/20: 
[55000/55000] 90s loss =    0.0288 , val_loss =    0.0287
Epoch  4/20: 
[55000/55000] 88s loss =    0.0279 , val_loss =    0.0278
Epoch  5/20: 
[55000/55000] 89s loss =    0.0274 , val_loss =    0.0275
Epoch  6/20: 
[55000/55000] 91s loss =    0.0270 , val_loss =    0.0272

...略...

Epoch 15/20: 
[55000/55000] 89s loss =    0.0247 , val_loss =    0.0255
Epoch 16/20: 
[55000/55000] 91s loss =    0.0245 , val_loss =    0.0253
Epoch 17/20: 
[55000/55000] 92s loss =    0.0244 , val_loss =    0.0253
Epoch 18/20: 
[55000/55000] 90s loss =    0.0244 , val_loss =    0.0253
Epoch 19/20: 
[55000/55000] 91s loss =    0.0244 , val_loss =    0.0254
Epoch 20/20: 
[55000/55000] 91s loss =    0.0242 , val_loss =    0.0252
test_loss =    0.0258
</pre></div>


<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/04_output_7_1.png"></p>
<p>似乎看起來是有效果的，現在5會被完整還原了。</p>
<p><br/></p>
<h5><u>壓縮碼Code與視覺化</u></h5>
<p>剛剛提到在Autoencoder前半段是一個Encoder，所以我們可以利用這個Encoder來做壓縮，會得到一個Code，在上面的這個例子，這個Code總共有4個值，因為中間層有4個神經元，可以把這個Code看成Dimension Reduction的結果，原本一張圖代表的是28x28=784個維度下的一個點，現在經過轉換後變成是4個維度下的一個點，而我們會直覺的認為同樣一群的數字圖形應該會有較高的相似度，所以在4個維度之下，同樣的數字圖片應該會彼此靠近的比較近，甚至聚成一團。</p>
<p>我想要驗證一下這件事，我們需要先圖像化，不過卻卡在維度太高的問題，人類無法想像高於3個維度以上的空間，也沒辦法將它視覺化，這個時候我們需要再做一次的Dimension Reduction，將維度降到低於3才可以視覺化，那一般手法是使用PCA來做這件事，有關於PCA我之前已經介紹過，請參考<a href="http://www.ycc.idv.tw/YCNote/post/35">這篇</a>，如此一來就可以在4個維度中切一個重要的截面來視覺化這些數據。不過記得喔！4個維度才是真正可以表示這群資料，做PCA只是為了畫圖而做的粗略轉換而已。</p>
<div class="highlight"><pre><span></span><span class="c1"># get code</span>
<span class="n">encode</span> <span class="o">=</span> <span class="n">model_2</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>

<span class="c1"># PCA 2D visualization</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">encode</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/04_output_9_0.png"></p>
<p>上面我以不同顏色當作不同的數字圖形，我們可以看到同樣的數字圖形會彼此聚成一團，所以的確同樣的數字的族群會被歸類到具有相似的特性，因此在code裏頭距離是彼此靠近的，還記得一開始我們沒加Regularization時。Model會把5看成是6，在這張圖你就會到原因，因為5號藍綠色和6號黃色靠的很近，很容易誤判。</p>
<p>這張圖同時揭露了Autoencoder的一個強大特性，注意喔!我們一開始Train這個Autoencoder的時候是沒有給它看任何Labels的，但他卻可以在壓縮資訊的同時找出規律，這個規律可以想成是我們人類在辨認每個不同數字的方法，所以Autoencoder可以在沒有Labels的情況下做歸納和學習，因此Autoencoder常常會被用在Unsupervised Learning (非監督式學習)。</p>
<p>另外介紹一種也是很流行的方法叫做t-SNE (讀作"tee-snee") ，這裡不多著墨這個方法的原理，但是它卻是目前2D Visualization最流行的作法，PCA只用線性的方式去做座標轉換，也就是從一個橫切面去看數據，這樣粗略的轉換並不能讓我們在視覺化時看出資料和資料間彼此的距離，尤其是從高維度轉換過來，經常會失真，而t-SNE是針對數據和數據間的距離去做轉換，最後被攤成2維時正是顯示數據點的距離關係，更能描述群聚的現象。</p>
<p>來看看t-SNE做起來效果如何。</p>
<div class="highlight"><pre><span></span><span class="c1"># get code</span>
<span class="n">encode</span> <span class="o">=</span> <span class="n">model_2</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>

<span class="c1"># TSNE 2D visualization</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_embedded</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">encode</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_embedded</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_embedded</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/04_output_11_0.png"></p>
<p><br/></p>
<h5><u>去雜訊(De-noise) Autoencoder</u></h5>
<p>我們巧妙的利用一下Autoencoder，我們將原本Autoencoder的前面加了一道人工雜訊的流程，但是最終又要讓Autoencoder試著去還原出原來沒有加入雜訊的資訊，這麼一來我們將可以找到一個Autoencoder是可以自行消除雜訊的，把這個Denoising Autoencoder加到正常Neural Network的前面，那這個Neural Network就擁有了抑制雜訊的功用，所以可以當作一種Regularization的方法。</p>
<p>先將圖片加上雜訊。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">add_noise</span><span class="p">(</span><span class="n">ndarr</span><span class="p">):</span>
    <span class="n">noise_factor</span> <span class="o">=</span> <span class="mf">0.3</span>
    <span class="n">noisy_ndarr</span> <span class="o">=</span> <span class="n">ndarr</span> <span class="o">+</span> <span class="n">noise_factor</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                                                          <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                                                          <span class="n">size</span><span class="o">=</span><span class="n">ndarr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">noisy_ndarr</span>

<span class="n">noisy_train_img</span> <span class="o">=</span> <span class="n">add_noise</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>
<span class="n">noisy_valid_img</span> <span class="o">=</span> <span class="n">add_noise</span><span class="p">(</span><span class="n">valid_data</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>
<span class="n">noisy_test_img</span> <span class="o">=</span> <span class="n">add_noise</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">15</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">noisy_train_img</span><span class="p">[</span><span class="n">i</span><span class="p">],(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/04_output_13_0.png"></p>
<p>圖片現在看起來非常的髒。</p>
<p>用這些髒圖片當作Input，正常圖當作Output的目標，我們就可以自然而然的Train出可以消除雜訊的Autoencoder。</p>
<div class="highlight"><pre><span></span><span class="n">denoise_model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">(</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span>
                     <span class="n">learning_rate</span><span class="o">=</span> <span class="mf">0.0003</span><span class="p">,</span>
                     <span class="n">n_hidden</span><span class="o">=</span><span class="p">[</span><span class="mi">512</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span>
                     <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                    <span class="p">)</span>
<span class="n">denoise_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">noisy_train_img</span><span class="p">,</span>
                  <span class="n">Y</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span>
                  <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                  <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">noisy_valid_img</span><span class="p">,</span><span class="n">valid_data</span><span class="o">.</span><span class="n">images</span><span class="p">),</span>
                  <span class="n">test_data</span><span class="o">=</span><span class="p">(</span><span class="n">noisy_test_img</span><span class="p">,</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">),</span>
                  <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
                 <span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">15</span><span class="p">):</span>
    <span class="n">img_original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_original</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">img_noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">noisy_test_img</span><span class="p">[</span><span class="n">i</span><span class="p">],(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_noisy</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">denoise_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">noisy_test_img</span><span class="p">[</span><span class="n">i</span><span class="p">]),(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>Epoch  1/20: 
[55000/55000] 50s loss =    0.0402 , val_loss =    0.0398
Epoch  2/20: 
[55000/55000] 50s loss =    0.0361 , val_loss =    0.0360
Epoch  3/20: 
[55000/55000] 51s loss =    0.0341 , val_loss =    0.0341
Epoch  4/20: 
[55000/55000] 51s loss =    0.0328 , val_loss =    0.0330
Epoch  5/20: 
[55000/55000] 51s loss =    0.0319 , val_loss =    0.0322
Epoch  6/20: 
[55000/55000] 51s loss =    0.0313 , val_loss =    0.0319

...略...

Epoch 15/20: 
[55000/55000] 52s loss =    0.0288 , val_loss =    0.0305
Epoch 16/20: 
[55000/55000] 51s loss =    0.0287 , val_loss =    0.0303
Epoch 17/20: 
[55000/55000] 51s loss =    0.0285 , val_loss =    0.0303
Epoch 18/20: 
[55000/55000] 51s loss =    0.0285 , val_loss =    0.0304
Epoch 19/20: 
[55000/55000] 52s loss =    0.0286 , val_loss =    0.0306
Epoch 20/20: 
[55000/55000] 52s loss =    0.0283 , val_loss =    0.0303
test_loss =    0.0307
</pre></div>


<p><img alt="png" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/04_output_15_1.png"></p>
<p>上面圖片第一排為原圖，第二排是加完雜訊後的結果，第三排是經過Autoencoder後的圖，傑克真的是太神奇啦！所有的雜訊都被消除掉了，特別注意，這裡我的Regularization下的特別重，原因是雜訊增多了，也更容易Overfitting，所以要下更多的Regularization才能抑制它。</p></dd>
              
            	<dt>2017 / 11月 12</dt>
            	<dd><a href="../tensorflow-tutorial_3.html">實作Tensorflow (3)：Build First Convolutional Neurel Network (CNN)</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><p>這一章我們終於要討論到影像辨識的重頭戲啦！通常，處理影像類別我們會用Convolutional Neurel Network，聽起來很難很厲害，不過只要了解背後概念你就知道為什麼要這麼做了，讓我們看下去。</p>
<p>本單元程式碼可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/03_CNN_classification_on_MNIST.py">Github</a>下載。</p>
<h5><u>影像有什麼特性</u></h5>
<p>來想一下，影像具備了什麼特性？</p>
<p>(1) 局域性：通常物件只在一個局域的範圍裡有效，而與太遠的距離無關，譬如我要找一張圖的鳥嘴，鳥嘴的呈現在一張圖當中只會出現在一個小範圍內，所以其實只需要評估這小範圍就可以判斷這是不是鳥嘴了。</p>
<p>(2) 平移性：通常一張圖任意平移並不影響它的意義，一隻鳥不管是放在圖片的左上角還是右下角，牠都是一隻鳥。</p>
<p>(3) 縮放性：通常一張圖我把它等比例的放大縮小是不影響它的意義的。</p>
<p><img alt="影像特性" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.004.jpeg"></p>
<h5><u>DNN用在影像上的侷限</u></h5>
<p>我們剛剛看過了影像具有的三種特性：「局域性」、「平移性」和「縮放性」，那我們就拿這三種特性來檢驗上一回的DNN Classification。</p>
<p>DNN有「局域性」嗎？沒有，因為我們把圖片攤平處理，原本應該是相鄰的關係就被打壞了，事實上DNN的結構會造成每個Input都會同時影響下一層的「每個」神經元，所以相不相鄰根本沒關係，因為每個Pixels的影響是全域的。</p>
<p>DNN有「平移性」嗎？沒有，沒有局域性就沒有平移性。</p>
<p>DNN有「縮放性」嗎？我們沒有一層試著去縮放，而且圖片已經被攤平了，難以做到縮放的效果。</p>
<p>所以其實使用DNN並不能好好的詮釋影像。</p>
<h5><u>Convolutional Neurel Network (CNN)</u></h5>
<p>我們需要引入新的架構來處理影像，讓它可以擁有以上三種特性，Convolutional Neurel Network (CNN)此時就登場了，CNN有兩大新要素：Convolution Layer和Pooling Layer，Convolution Layer為我們的Model添加了局域性和平移性，而Pooling Layer則讓Model擁有縮放的特性。</p>
<p><img alt="Convolution Layer和Pooling Layer" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.005.jpeg"></p>
<p>Convolution Layer是由Filters所構成的，Filters可以想像是一張小圖，以上面的例子，Filter是一個鳥嘴的小圖，這張小圖要怎麼去過濾原圖呢？答案是使用Convolution(卷積)，把小圖疊到大圖的任意位置，接下來將大圖小圖對到的相應元素相乘起來再加總一起，然後在另外一張表格中填入這個加總值，接下來移動Filter，重複的動作再做一次，如此循環並將值陸續填入表格中，這表格最後就會像是另外一張圖一樣，而這張圖可以繼續串另外的Neurel Network，這就是Convolution Layer的計算方法。</p>
<p>舉個例子，假設我今天有矩陣A：</p>
<p>[[1, 2, 3, 4],<br/>
 [4, 5, 6, 7],<br/>
 [7, 8, 9,10],<br/>
 [1, 3, 5, 7]]</p>
<p>然後再有一個Filter：</p>
<p>[[1, 0, 0],<br/>
 [0, 1, 0],<br/>
 [0, 0, 0]]</p>
<p>使用Filter對A做Convolution得到B為：</p>
<p>[[6, 8 ],<br/>
 [12,14]]</p>
<p>原本4x4的矩陣做了Convolution後變成了2x2的矩陣，原因在於邊界限制了Filter的移動，那如果我想要讓Convolution玩的矩陣維持在4x4，怎麼做？我們可以在邊緣的地方鋪上0就可以達到這樣的效果，來試試看，先將矩陣A擴張成矩陣C：</p>
<p>[[0, 0, 0, 0, 0, 0],<br/>
 [0, 1, 2, 3, 4, 0],<br/>
 [0, 4, 5, 6, 7, 0],<br/>
 [0, 7, 8, 9,10, 0],<br/>
 [0, 1, 3, 5, 7, 0],<br/>
 [0, 0, 0, 0, 0, 0]]</p>
<p>再使用Filter對C做Convolution會得到D為：</p>
<p>[[1, 2, 3, 4],<br/>
 [4, 6, 8,10],<br/>
 [7,12,14,16],<br/>
 [1,10,13,16]]</p>
<p>而此時D就是一個4x4的矩陣。</p>
<p>Convolution的過程造成怎麼樣的效果呢？當Filter是一個鳥嘴的小圖，一旦遇到與鳥嘴相似的局部，此時加總的值會是一個大的值，如果是一個和鳥嘴無關的局部此時的值會是一個小的值，所以這個Filter具有將特定特徵篩選出來的能力，符合特徵分數高不符合則分數低，因此局部的特徵變得是有意義的，此時我的Model就具有局域性，而且藉由Filter的平移掃視，這個特徵就具有可平移的特性。</p>
<p>Convolution Layer不同於Fully-connected Layer有兩點，第一，每個Pixels間有相對的距離關係，擁有上下左右的關係才有辦法構成一張圖，第二，除了有距離上的關係以外，它還能在有限範圍內抓出一種特徵模式，所以我們將可以使用影像的語言來做特徵轉換和抓取特徵。</p>
<p>實際情況下，Filter上的Weights是會自行調整的，Model Fitting的時候，Model會根據數據自行訓練出Filter，也就是說機器可以自行學習出圖片的特徵。通常這樣的Filters會有好幾個，讓機器可以有更多維度可以學習。</p>
<p>接下來來看Pooling Layer如何讓Model擁有檢視縮放的特性？</p>
<p>先來看在影像上如何做到放大縮小，以Pixels的觀點來看，最簡單的放大方法是，在每個既有的Pixels附近增加一些與它們相似的新Pixel，這樣做就像是將原本的小圖直接拉成大圖，畫質雖然很差，不過這是最簡單的放大方法。那麼縮小就相反操作，把一群附近的Pixels濃縮成一個Pixel當作代表，就可以達到縮小圖片的效果。</p>
<p>所以回到Pooling Layer的討論，Pooling做的事情就是在縮小圖片，例如我使用2x2來做Pooling，它會在原圖上以2x2來掃描，所以會有四個元素被檢視，然後從這四個值當中產生一個代表值，把原本2x2的Pixels減少成這個1x1的代表值，如果是Max Pooling就是從四個中選最大的那個，如果是Average Pooling則是平均四個值得到平均值，通常如果是2x2的Pooling我們會以每2格一跳的方式掃視，如果是3x3則會以每3格一跳，以此類推。</p>
<h5><u>Convolution Layer</u></h5>
<p>來看一下Tensorflow如何實作Convolution Layer。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># Config the matplotlib backend as plotting inline in IPython</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">]],</span>
                       <span class="p">[[</span><span class="mi">3</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">]],</span>
                       <span class="p">[[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">3</span><span class="p">],[</span><span class="mi">0</span><span class="p">]],</span>
                       <span class="p">[[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]],</span>
                       <span class="p">[[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">]]]],</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># shape of img: [batch, in_height, in_width, in_channels]</span>

    <span class="n">filter_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">]],[[</span><span class="mi">2</span><span class="p">]]],</span>
                           <span class="p">[[[</span><span class="mi">3</span><span class="p">]],[[</span><span class="mi">0</span><span class="p">]]]],</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># shape of filter: [filter_height, filter_width, in_channels, out_channels]</span>

    <span class="n">conv_strides</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">padding_method</span> <span class="o">=</span> <span class="s1">&#39;VALID&#39;</span>

    <span class="n">conv</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">filter_</span><span class="p">,</span> 
                        <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">conv_strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">conv_strides</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">],</span> 
                        <span class="n">padding</span><span class="o">=</span><span class="n">padding_method</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Shape of conv:&quot;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Conv:&quot;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span>Shape of conv:
(1, 4, 4, 1)
Conv:
[[[[ 14.]
   [  2.]
   [  0.]
   [  3.]]

  [[  3.]
   [  0.]
   [  2.]
   [ 14.]]

  [[  3.]
   [  0.]
   [  6.]
   [  6.]]

  [[  1.]
   [  0.]
   [  2.]
   [  1.]]]]
</pre></div>


<p>首先一開始是圖片<code>img</code>的部分，Rank為4，每個維度分別為<code>[batch, in_height, in_width, in_channels]</code>，in_channels的部分一般是RGB，這邊我採用和MNIST相同的灰階表示，所以in_channels只有1個維度。</p>
<p><code>filter_</code>的部分Rank為4，每個維度分別為<code>[filter_height, filter_width, in_channels, out_channels]</code>，當如果我想要使用多個filters的時候，我的out_channels就不只1而已，如果有RGB的話，in_channels則會是3。</p>
<p>接下來來看一下<code>tf.nn.conv2d</code>裡頭的參數<code>strides</code>，這可能會讓人感到困惑，它的設定值是<code>[1,conv_strides[0],conv_strides[1],1]</code>，我特別把第二、三項額外用<code>conv_strides</code>來表示，因為這兩個值才是真正代表在圖片上平移每步的距離，那第一項和最後一項的1代表什麼意義呢？是這樣的，Tensorflow是站在維度的角度看平移這件事情，這四個維度分別表示<code>[batch, in_height, in_width, in_channels]</code>的移動量，所以一般情況下只有<code>in_height</code>和<code>in_width</code>需要指定平移的距離。</p>
<p>最後一個參數就是<code>padding</code>，有兩種可以選擇，分別為<code>VALID</code>和<code>SAME</code>，<code>VALID</code>指的就是沒有額外鋪上0的邊界的情形，<code>SAME</code>則是額外鋪上0的邊界，並且使得輸出的維度和輸入一樣。</p>
<h5><u>Pooling Layer</u></h5>
<p>接下來來看Tensorflow如何實作Pooling Layer。</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">]],</span>
                       <span class="p">[[</span><span class="mi">3</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">]],</span>
                       <span class="p">[[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">3</span><span class="p">]],</span>
                       <span class="p">[[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">]]]],</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># shape of img: [batch, in_height, in_width, in_channels]</span>

    <span class="n">pooling</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="n">img</span><span class="p">,</span>
                    <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Shape of pooling:&quot;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">pooling</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Pooling:&quot;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">pooling</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span>Shape of pooling:
(1, 2, 2, 1)
Pooling:
[[[[ 3.]
   [ 1.]]

  [[ 1.]
   [ 3.]]]]
</pre></div>


<p><code>tf.nn.max_pool</code>的參數中<code>ksize</code>代表kernel size，也就是要Pooling的大小，一樣依照Input layer的Rank去配置，還有<code>strides</code>決定平移的方法。</p>
<h5><u>最簡單的CNN架構：LeNet5</u></h5>
<p><img alt="LeNet5" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.006.jpeg"></p>
<p>接下來我們就真正的來實作一下CNN網路，和之前兩個單元一樣，我們拿MNIST的分類問題來當作題目。</p>
<p>本單元介紹的是最簡單的CNN Classification的方法—LeNet5，流程如上圖所示。</p>
<p>(1) conv1+relu+pooling2：一開始使用Convolution抓取圖片中的特徵，並且加入Activation Function使得Model具有非線性因子，通常在這種非常深的網路，我們會採用Relu，它的好處是不會出現「梯度消失」(Vanishing Gradient)的問題，如果你使用像是tanh或sigmoid這類在飽和區梯度接近0的函數，則就很有可能在深網路的情形下，造成一開始的幾個Layers梯度太小的問題，也就是前面幾層我們無法訓練到，而Relu在Turn-on的情況下是線性的，不會有飽和的問題，也就不會出現「梯度消失」。做完Convolution後，我們已經對於這個圖片有一點認識了，所以減少一些Pixels來減少一些計算量，所以加入Pooling Layer，注意喔！Pooling Layer結束之後，不需要再做一次Activation，因為Pooling只是用來平均前面的結果而已。</p>
<p>(2) conv3+relu+pooling4：做第二次的圖片特徵抽取。</p>
<p>(3) fatten5：將抽取完的特徵完全打平，為了接下來的Fully-connected Network做準備。</p>
<p>(4) fc6+fc7+fc8+softmax：這個部分就和之前DNN Classification做的事情一樣，全盤考慮每一個擷取來的特徵，並且非線性的轉換成最後可以分為相應的10種類別。</p>
<p>接下來我們看一下程式碼要怎麼寫？</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CNNLogisticClassification</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">shape_picture</span><span class="p">,</span><span class="n">n_labels</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">dropout_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape_picture</span> <span class="o">=</span> <span class="n">shape_picture</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span> <span class="o">=</span> <span class="n">n_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span> <span class="c1"># initialize new grap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">dropout_ratio</span><span class="p">,</span><span class="n">alpha</span><span class="p">)</span> <span class="c1"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span> <span class="c1"># create session by the graph </span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">dropout_ratio</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c1">### Input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_pictures</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> 
                                                 <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">]</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">shape_picture</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span>   <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> 
                                                 <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">))</span>

            <span class="c1">### Optimalization</span>
            <span class="c1"># build neurel network structure and get their predictions and loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">original_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span><span class="n">pictures</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_pictures</span><span class="p">,</span>
                                                        <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">,</span>
                                                        <span class="n">dropout_ratio</span><span class="o">=</span><span class="n">dropout_ratio</span><span class="p">,</span>
                                                        <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="p">)</span>
            <span class="c1"># regularization loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                                   <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">/</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">out_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                                        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>
            <span class="c1"># total loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_loss</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span>

            <span class="c1"># define training operation</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1">### Prediction</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_pictures</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> 
                                               <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">]</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">shape_picture</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span>   <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> 
                                               <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_y_</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span><span class="n">pictures</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_pictures</span><span class="p">,</span>
                                                       <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">,)</span>

            <span class="c1">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">pictures</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">dropout_ratio</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>

        <span class="c1">### Variable</span>
        <span class="c1">## LeNet5 Architecture(http://yann.lecun.com/exdb/lenet/) </span>
        <span class="c1"># input:(batch,28,28,1) =&gt; conv1[5x5,6] =&gt; (batch,24,24,6)</span>
        <span class="c1"># pool2 =&gt; (batch,12,12,6) =&gt; conv2[5x5,16] =&gt; (batch,8,8,16)</span>
        <span class="c1"># pool4 =&gt; fatten5 =&gt; (batch,4x4x16) =&gt; fc6 =&gt; (batch,120)</span>
        <span class="c1"># (batch,120) =&gt; fc7 =&gt; (batch,84)</span>
        <span class="c1"># (batch,84) =&gt; fc8 =&gt; (batch,10) =&gt; softmax</span>

        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;conv1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span>
                                                         <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span> 
                <span class="s1">&#39;conv3&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">16</span><span class="p">),</span>
                                                         <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span>
                <span class="s1">&#39;fc6&#39;</span><span class="p">:</span>   <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="mi">4</span><span class="o">*</span><span class="mi">16</span><span class="p">,</span><span class="mi">120</span><span class="p">),</span>
                                                         <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span>
                <span class="s1">&#39;fc7&#39;</span><span class="p">:</span>   <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span><span class="mi">84</span><span class="p">),</span>
                                                         <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span>    
                <span class="s1">&#39;fc8&#39;</span><span class="p">:</span>   <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">),</span>
                                                         <span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span>                   
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span>  <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;conv1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span> <span class="p">)),</span>
                <span class="s1">&#39;conv3&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span> <span class="p">)),</span>
                <span class="s1">&#39;fc6&#39;</span><span class="p">:</span>   <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">120</span><span class="p">)</span> <span class="p">)),</span>
                <span class="s1">&#39;fc7&#39;</span><span class="p">:</span>   <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">84</span><span class="p">)</span> <span class="p">)),</span>
                <span class="s1">&#39;fc8&#39;</span><span class="p">:</span>   <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">)</span> <span class="p">)),</span>
            <span class="p">}</span> 

        <span class="c1">### Structure</span>
        <span class="n">conv1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getConv2DLayer</span><span class="p">(</span><span class="n">pictures</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">],</span>
                                    <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">pool2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="n">conv1</span><span class="p">,</span>
                               <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">)</span>
        <span class="n">conv3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getConv2DLayer</span><span class="p">(</span><span class="n">pool2</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">],</span>
                                    <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">pool4</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="n">conv3</span><span class="p">,</span>
                               <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">)</span>
        <span class="n">fatten5</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getFlattenLayer</span><span class="p">(</span><span class="n">pool4</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">train</span><span class="p">:</span> <span class="n">fatten5</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">fatten5</span><span class="p">,</span><span class="n">keep_prob</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">dropout_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="n">fc6</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">fatten5</span><span class="p">,</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;fc6&#39;</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;fc6&#39;</span><span class="p">],</span>
                                 <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">train</span><span class="p">:</span> <span class="n">fc6</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">fc6</span><span class="p">,</span><span class="n">keep_prob</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">dropout_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">fc7</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">fc6</span><span class="p">,</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;fc7&#39;</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;fc7&#39;</span><span class="p">],</span>
                                 <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">fc7</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;fc8&#39;</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;fc8&#39;</span><span class="p">])</span>

        <span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                 <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                                                         <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">getDenseLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_layer</span><span class="p">,</span><span class="n">weight</span><span class="p">,</span><span class="n">bias</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span><span class="n">weight</span><span class="p">),</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">activation</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">getConv2DLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_layer</span><span class="p">,</span>
                       <span class="n">weight</span><span class="p">,</span><span class="n">bias</span><span class="p">,</span>
                       <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span>
                           <span class="n">weight</span><span class="p">,</span>
                           <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">],</span>
                           <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">),</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">activation</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">getFlattenLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_layer</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">input_layer</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
        <span class="n">n</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">n</span> <span class="o">*=</span> <span class="n">s</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
            <span class="n">validation_data</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">test_data</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">9000</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_size</span><span class="p">:</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">N</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">%2d</span><span class="s2">/</span><span class="si">%2d</span><span class="s2">: &quot;</span><span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="p">))</span>

            <span class="c1"># mini-batch gradient descent</span>
            <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
            <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
                <span class="n">index_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
                <span class="n">batch_index</span><span class="o">=</span><span class="p">[</span><span class="n">index</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span><span class="n">index_size</span><span class="p">))]</span>    

                <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_pictures</span><span class="p">:</span> <span class="n">X</span><span class="p">[</span><span class="n">batch_index</span><span class="p">,:],</span> 
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">:</span> <span class="n">y</span><span class="p">[</span><span class="n">batch_index</span><span class="p">],</span> 
                <span class="p">}</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span>
                                        <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;[</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">] loss = </span><span class="si">%.4f</span><span class="s2">     &quot;</span><span class="o">%</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">),</span><span class="n">N</span><span class="p">,</span><span class="n">loss</span><span class="p">),</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">&#39;</span><span class="p">)</span>


            <span class="c1"># evaluate at the end of this epoch</span>
            <span class="n">y_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
            <span class="n">train_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;[</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">] loss = </span><span class="si">%8.4f</span><span class="s2">, acc = </span><span class="si">%3.2f%%</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="n">train_loss</span><span class="p">,</span><span class="n">train_acc</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">validation_data</span><span class="p">:</span>
                <span class="n">val_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">val_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">msg</span><span class="o">+=</span><span class="s2">&quot;, val_loss = </span><span class="si">%8.4f</span><span class="s2">, val_acc = </span><span class="si">%3.2f%%</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span><span class="o">*</span><span class="mi">100</span> <span class="p">)</span>

            <span class="k">print</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>


        <span class="k">if</span> <span class="n">test_data</span><span class="p">:</span>
            <span class="n">test_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">test_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;test_acc = </span><span class="si">%3.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">test_acc</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_y_</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_pictures</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_pictures</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> 
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">_check_array</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ndarray</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ndarray</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span> 
            <span class="n">ndarray</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ndarray</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="n">ndarray</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">ndarray</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s2">&quot;MNIST_data/&quot;</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span>
<span class="n">valid_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">validation</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span>
</pre></div>


<div class="highlight"><pre><span></span>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">CNNLogisticClassification</span><span class="p">(</span>   <span class="n">shape_picture</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
                                     <span class="n">n_labels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                     <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.07</span><span class="p">,</span>
                                     <span class="n">dropout_ratio</span><span class="o">=</span><span class="p">[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">],</span>
                                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                 <span class="p">)</span>

<span class="n">train_img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">,[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">valid_img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">valid_data</span><span class="o">.</span><span class="n">images</span><span class="p">,[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">test_img</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">,[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">train_img</span><span class="p">,</span>
          <span class="n">y</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">valid_img</span><span class="p">,</span><span class="n">valid_data</span><span class="o">.</span><span class="n">labels</span><span class="p">),</span>
          <span class="n">test_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_img</span><span class="p">,</span><span class="n">test_data</span><span class="o">.</span><span class="n">labels</span><span class="p">),</span>
          <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
         <span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Epoch  1/10: 
[55000/55000] loss =   0.0781, acc = 97.52%, val_loss =   0.0785, val_acc = 97.50%
Epoch  2/10: 
[55000/55000] loss =   0.0473, acc = 98.50%, val_loss =   0.0545, val_acc = 98.38%
Epoch  3/10: 
[55000/55000] loss =   0.0446, acc = 98.57%, val_loss =   0.0526, val_acc = 98.46%
Epoch  4/10: 
[55000/55000] loss =   0.0364, acc = 98.95%, val_loss =   0.0457, val_acc = 98.64%
Epoch  5/10: 
[55000/55000] loss =   0.0306, acc = 99.05%, val_loss =   0.0400, val_acc = 98.84%
Epoch  6/10: 
[55000/55000] loss =   0.0260, acc = 99.18%, val_loss =   0.0393, val_acc = 98.90%
Epoch  7/10: 
[55000/55000] loss =   0.0220, acc = 99.34%, val_loss =   0.0390, val_acc = 98.78%
Epoch  8/10: 
[55000/55000] loss =   0.0222, acc = 99.35%, val_loss =   0.0362, val_acc = 98.90%
Epoch  9/10: 
[55000/55000] loss =   0.0183, acc = 99.43%, val_loss =   0.0326, val_acc = 99.02%
Epoch 10/10: 
[55000/55000] loss =   0.0176, acc = 99.48%, val_loss =   0.0331, val_acc = 99.04%
test_acc = 99.10%
</pre></div>


<p>太棒了！我們的預測效果如果跟之前的結果比較，在Epoch=3已經達到98.5%了，在Epoch=10更是高達99%！</p>
<h5><u>圖像化</u></h5>
<p>有這麼好的成果，我們不妨就拉進去看，裡面每個Filters究竟是長什麼樣子的？</p>
<p>先來看第一層Convolution的Filters。</p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">])[:,:,:,</span><span class="n">i</span><span class="p">]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAagAAABTCAYAAADKkJOuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz%0AAAALEgAACxIB0t1+/AAADnxJREFUeJzt3X1slHW2B/DvaWfK0AKltIXSAqVUUDAaUIOyGqOuMQuu%0AS4gYvDFm/zAheLNGEjBuvIZETDRGZWOyykriNXuVSLOLsLi1EgwYuFplLbZqeTEsyEuLlNeW0tJp%0Ap+f+0Wc6vcPgc1qfdp72+X6SJvNycubXb6c9nZnnRVQVREREfpOR7gUQERGlwgFFRES+xAFFRES+%0AxAFFRES+xAFFRES+xAFFRES+xAFFRES+xAFFRES+xAFFRES+FErXA+fk5GheXp5r3ahRo0z9Ojs7%0AXWsuXbpk6nXx4kVTHYCzqlpoLb6WcePG6cSJE13rRo8ebep36tQp1xprFtFo1FQHj7IYM2aM6XnR%0A3d1t6nf58mXXmlgsZuqVkWH7f66lpcWTLETEdJiXzMxMUz/L+rOzs029Jk+ebKo7ePCgJ1mMHz9e%0Ai4uLXeus67c8r5uamky9Tp8+baqDR78joVBIw+Gwa11ubq6p39ixYy2PaeplWRcAfPfdd6Ys0jag%0A8vLy8NRTT7nWlZeXm/r99NNPrjWfffaZqdfmzZtNdQCOWQt/zsSJE/Haa6+51t10002mfi+++KJr%0AjTWLY8fM36InWeTl5WHVqlWudW1tbaZ+e/fuda1pbm429bL+s7R9+3ZPsrCy/IEBgJycHNeaW265%0AxdTrueeeM9UtWLDAkyyKi4vx/vvvu9ZZ1295Xr/55pumXq+++qqpDh79joTDYcyYMcO1buHChaZ+%0A9913n2tNQUGBqZf1H5dp06aZsjD9SygivxGRQyJyWET+mOL+USJS4dz/lYhMN61y+CtKvoFZJAQp%0AizNnzmDPnj3YvXs3EPAsqqursWzZMixduhQIeBZJmEU/uQ4oEckE8CaAhQDmAPgPEZmTVPYEgAuq%0Aeh2APwF4xeuF+tQEZtErsFmoKg4cOIBbb70Vd911FxDgLGKxGF5//XWsW7cOH3zwARDgLFJgFv1k%0AeQU1H8BhVT2iqlEAmwAsTqpZDOCvzuW/A/i1iIh3y/St82AWcYHNorm5GdnZ2cjOzo5/zhPYLPbv%0A348pU6agpKQk/nlEYLNIgVn0k2VAlQA40ef6See2lDWq2gWgGUB+ciMRWS4iX4vI15YPr4eBKDzI%0AoqWlZdAXOgQ8yWI4Pi+uXLmCSCTS9yZPshik5Q6qM2fOIGmDH0+yuHDhwiCteEh5koV1o56RYEg3%0AM1fVDap6m6reZvnAdiTrm8W4cePSvZy04vMioW8W6V5LuvXNwrJl50jWNwvrVpsjgWVANQCY2uf6%0AFOe2lDUiEgKQC+CcFwv0uSwwi7jAZhGJRHDlypW+NwU2i8LCwuTNswObRQrMop8sA+pfAGaKSJmI%0AZAF4FMC2pJptAH7vXF4KYKcG41S9E8As4gKbxbhx49DW1oa2trb4/lmBzWL27Nk4ceIEGhsb4/sm%0ABjaLFJhFP7nuB6WqXSLyBwDbAWQC+G9VrReRtQC+VtVtAN4B8J6IHEbPB4GPDuaifeQ8s+gV2Cwy%0AMjIwe/Zs1NTUwPnbEtgsQqEQVq1ahZUrV8aHdWCzSIFZ9JNpR11V/RjAx0m3relz+QqAR/rzwK2t%0Araiurnats344+vzzz7vWzJo1y9Tryy+/NNU1NDT8BPzyLMLhsGkHN+te2padFevr6029LDsEAsCu%0AXbs8yaKrqwtnz551rbv77rtN/e6//37XGuvOyGPGjDHVLVq0yJMssrKyYDl6gjWLBQsWuNbce++9%0Apl5ZWVnXvG/JkiVYsmQJAGDGjBmeZBGLxUxHP/nhhx9M/Sw7N+/YscPUy/q8aG1t9SSL0tJSvP32%0A2651zi4Prqqqqlxr3nvvPVMvrzfg4LH4iIjIlzigiIjIlzigiIjIlzigiIjIlzigiIjIlzigiIjI%0AlzigiIjIlzigiIjIlzigiIjIl9J2yveMjIyf3Rs97tw523ETLXUPPPCAqdfNN99sqmtoSD7u48Dk%0A5OTg9ttvd607evSoqd+PP/7oWvPwww+bet1xxx2mul27dpnq3IRCIYwfP961znqa8+zsbNeaRx6x%0A7cjf2tpqqvNKOBxGSUny2RmuZj269XXXXedac/3115t6OScjHDI5OTmmI6ScOnXK1O+ZZ55xrbH+%0A7bFmVlNTY6pzE4vFTEfYWbw4+dRTqe3cudO1xnqEkRUrVpjq1q9fb6rjKygiIvIlDigiIvIlDigi%0AIvIlDigiIvIlDigiIvIl1634RGQqgP8BMAmAAtigqm8k1dwD4B8A4puZfaiqa71davq1t7fj22+/%0ARUdHB0QEACYm1wQli6amJrz88st9tyYKbBYnT57E8uXL0dTUFPjnxblz57B+/Xo0NzfHbwpsFtFo%0AFEePHkVXV1f8psBmMVCWzcy7AKxS1X0iMhZAjYjsUNX9SXV7VPW33i/RP0QEN9xwA3Jzc9HV1YUd%0AO3ZMFJE5QcwiMzMTTz75JGbNmoW2tjY8+OCDgc0iFArhpZdewty5c3Hp0iUUFxcHNouMjAw89thj%0AKCsrQ3t7O5544onAZiEimDp1KrKzsxGLxVBbWxvYLAbK9S0+VT2lqvucy5cAHADgvnPGCBSJRJCb%0Amwug548SgHYENIv8/PzeMxQ7+xoFNouioiLMnTsXQO/+WYHNIi8vD2VlZQCA0aNHAwHOIhwO9+6H%0A5+yrFtgsBqpfO+qKyHQA8wB8leLuBSJSB6ARwGpVveqc4iKyHMBy5zK2b9/u+pjTpk0zra2iosK1%0AxstTY1+4cAFVVVXZ8CCLyZMn4/vvv3d9zH379rnWALYdSi07sAIw7TTb2NgIAJ5kkZWVZfpZHjly%0AxLUG6MnWzdatW029LLk6b215kkUkEjGdTjw/P9+1BgAuXrzoWvPWW2+Zeu3evdu15vLly4BHWRQU%0AFOCTTz5xfczq6mrXGsC2E641V8up1VtaWlBTU+NJFqFQCKtXr3Z9TMuBEACguLjYtebQoUOmXg89%0A9JCpzso8oERkDIDNAFaqakvS3fsAlKpqq4gsArAVwMzkHqq6AcAGAMjMzNQBrzrNOjo6sGnTJgA4%0A4UUWN95447DNoq2tLf7L4kkWOTk5wzaLaDSKLVu2AB5lkZubO2yz6OzsxOeffw54lEV5efmwzSIa%0AjaKyshLwKItIJDJss+gv01Z8IhJGz3DaqKofJt+vqi2q2upc/hhAWEQKPF2pT8RiMWzatCl+OKSr%0A/iUNUhadnZ1YvXo1Fi5cCAQ8i1gshi1btmDOnDlAwLPo7u7GF198gdLSUiDgWcRiMVRWVsYPhxTo%0ALAbCdUBJz2ZJ7wA4oKrrrlFT5NRBROY7fW0HshpGVBVbt25FYWEh7rzzzpQ1QcrihRdeQFlZGR5/%0A/PGUNUHKoqqqCvn5+Zg/f37KmiBlsXfvXowdO/aax6gLUhaffvopJkyYcM3jCAYli4GyvMV3J4DH%0AAXwnIrXObc8BmAYAqvoXAEsBPCkiXej5IPBRVR1xL0OPHz+Ouro6TJo0Kf5e/RznZXngsqitrUVl%0AZSVmzpyJZcuWAQHOoqGhAfX19SgsLMS7774LBDiLs2fP4tixY8jNzY1/xhzYLBobG3Hw4EHk5+dj%0A48aNQICzGCjXAaWq/wtAXGr+DODPXi3Kr0pLS7F2bWIXhTVr1ux3Xpb3CkoW8+bNwzfffNP3emCz%0AmDJlCp599tne66+88kpgsygsLIz/wwIAqKioCGwWJSUlePrpp3uvv/HGG4HNYqB4JAkiIvIlDigi%0AIvIlDigiIvIlDigiIvKltJ3yPRKJ9B4q5+e0t7eb+n300UeuNefPnzf1cvZlGTJZWVmmI2bs2bPH%0A1K+8vNy1pqioyNTr+PHjpjqvdHd3m37mlqOQALYjTkQiEVOvoX5eiAjC4bBrnfVU9Pv3Jx8C7moT%0AJkww9Zo0aZKpzivNzc2oqqpyraurqzP16+jocK2JH7LJzfTp0011XolGo6bfS+vvuOX7PH36tKlX%0Ad3e3qc6Kr6CIiMiXOKCIiMiXOKCIiMiXOKCIiMiXOKCIiMiXOKCIiMiXOKCIiMiXOKCIiMiXOKCI%0AiMiXJF2nHhGRMwCOJd1cAODsID+0l49RqqqFv7QJs0hIUxZe92cWCcwigVkkmLJI24BKRUS+VtXb%0AhvtjeIFZJAz2OodLDgCz6ItZJIzULPgWHxER+RIHFBER+ZLfBtSGEfIYXmAWCYO9zuGSA8As+mIW%0ACSMyC199BkVERBTnt1dQREREADigiIjIp9IyoETkNyJySEQOi8gfU9w/SkQqnPu/EpHp/eg9VUR2%0Aich+EakXkadT1NwjIs0iUut8rfll39HAMYv/txZmkVgLs0ishVkk1hKsLFR1SL8AZAL4N4AZALIA%0A1AGYk1TznwD+4lx+FEBFP/pPBnCLc3ksgB9S9L8HwD+H+ntnFsyCWTALZmH/SscrqPkADqvqEVWN%0AAtgEYHFSzWIAf3Uu/x3Ar0VELM1V9ZSq7nMuXwJwAECJJyv3HrNIYBYJzCKBWSQELot0DKgSACf6%0AXD+Jq0PorVHVLgDNAPL7+0DOy9t5AL5KcfcCEakTkSoRubG/vT3CLBKYRQKzSGAWCYHLIjSYzdNJ%0ARMYA2Axgpaq2JN29Dz3HgmoVkUUAtgKYOdRrHCrMIoFZJDCLBGaR4Kcs0vEKqgHA1D7Xpzi3pawR%0AkRCAXADnrA8gImH0BLxRVT9Mvl9VW1S11bn8MYCwiBT055vwCLNIYBYJzCKBWSQELot0DKh/AZgp%0AImUikoWeD/K2JdVsA/B75/JSADvV+YTOjfN+6zsADqjqumvUFMXflxWR+ejJwfxD9BCzSGAWCcwi%0AgVkkBC+Lodoao+8XgEXo2ULk3wD+y7ltLYDfOZcjAP4G4DCAvQBm9KP3XQAUwLcAap2vRQBWAFjh%0A1PwBQD16toL5EsCv0pEDs2AWzIJZMItrf/FQR0RE5Es8kgQREfkSBxQREfkSBxQREfkSBxQREfkS%0ABxQREfkSBxQREfkSBxQREfnS/wHRnw01r1VfLAAAAABJRU5ErkJggg==%0A"></p>
<p>再來看第二層Convolution的Filters。</p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">])[:,:,</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
        <span class="n">axis</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">2</span><span class="o">+</span><span class="n">j</span><span class="o">//</span><span class="mi">8</span><span class="p">][</span><span class="n">j</span><span class="o">%</span><span class="mi">8</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAkkAAAKvCAYAAAB+nVurAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A0N/LBB+YAAAAAElFTkSuQmCC%0A"></p>
<p>單看這些Filters似乎很難看出什麼？</p>
<p>我們試著丟幾張圖進去做Convolution，看一下在Filters的拆解之下圖片會變怎樣？</p>
<p>以下圖片的第一行表示原圖片，第二行表示做完第一次Convolution後的結果，第三行表示做完第二次Convolution後的結果。</p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">picture</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_img</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:,:],(</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">conv1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">getConv2DLayer</span><span class="p">(</span><span class="n">picture</span><span class="p">,</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">],</span><span class="n">model</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">],</span>
                         <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="n">pool2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="n">conv1</span><span class="p">,</span>
                         <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">)</span>

    <span class="n">conv3</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">getConv2DLayer</span><span class="p">(</span><span class="n">pool2</span><span class="p">,</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">],</span><span class="n">model</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">],</span>
                         <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="n">eval_conv1</span> <span class="o">=</span> <span class="n">conv1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">eval_conv3</span> <span class="o">=</span> <span class="n">conv3</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">picture</span><span class="p">,(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">eval_conv1</span><span class="p">[:,:,:,</span><span class="n">i</span><span class="p">]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">24</span><span class="p">,</span><span class="mi">24</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">eval_conv3</span><span class="p">[:,:,:,</span><span class="n">i</span><span class="p">]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6IAAADFCAYAAABO4U/4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A"></p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">picture</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_img</span><span class="p">[</span><span class="mi">10</span><span class="p">,:,:,:],(</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">conv1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">getConv2DLayer</span><span class="p">(</span><span class="n">picture</span><span class="p">,</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">],</span><span class="n">model</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">],</span>
                         <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="n">pool2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="n">conv1</span><span class="p">,</span>
                         <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">)</span>

    <span class="n">conv3</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">getConv2DLayer</span><span class="p">(</span><span class="n">pool2</span><span class="p">,</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">],</span><span class="n">model</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">],</span>
                         <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="n">eval_conv1</span> <span class="o">=</span> <span class="n">conv1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">eval_conv3</span> <span class="o">=</span> <span class="n">conv3</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">picture</span><span class="p">,(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">eval_conv1</span><span class="p">[:,:,:,</span><span class="n">i</span><span class="p">]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">24</span><span class="p">,</span><span class="mi">24</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">eval_conv3</span><span class="p">[:,:,:,</span><span class="n">i</span><span class="p">]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6IAAADFCAYAAABO4U/4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0AAAAAAElFTkSuQmCC%0A"></p>
<div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">picture</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_img</span><span class="p">[</span><span class="mi">15</span><span class="p">,:,:,:],(</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">conv1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">getConv2DLayer</span><span class="p">(</span><span class="n">picture</span><span class="p">,</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">],</span><span class="n">model</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;conv1&#39;</span><span class="p">],</span>
                         <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="n">pool2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="n">conv1</span><span class="p">,</span>
                         <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">)</span>

    <span class="n">conv3</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">getConv2DLayer</span><span class="p">(</span><span class="n">pool2</span><span class="p">,</span>
                         <span class="n">model</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">],</span><span class="n">model</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;conv3&#39;</span><span class="p">],</span>
                         <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
    <span class="n">eval_conv1</span> <span class="o">=</span> <span class="n">conv1</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">eval_conv3</span> <span class="o">=</span> <span class="n">conv3</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">picture</span><span class="p">,(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">eval_conv1</span><span class="p">[:,:,:,</span><span class="n">i</span><span class="p">]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">24</span><span class="p">,</span><span class="mi">24</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">eval_conv3</span><span class="p">[:,:,:,</span><span class="n">i</span><span class="p">]</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">axis</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6IAAADFCAYAAABO4U/4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A"></p>
<p>有看出什麼規律嗎？哈哈。</p></dd>
              
            	<dt>2017 / 11月 07</dt>
            	<dd><a href="../tensorflow-tutorial_2.html">實作Tensorflow (2)：Build First Deep Neurel Network (DNN)</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><p>接續著<a href="http://www.ycc.idv.tw/YCNote/post/38">上一回</a>，我們已經有一個單層的Neurel Network，緊接著我們來試著一步一步改造它，讓它成為我們常使用的Deep Neurel Network的形式。</p>
<p>本單元程式碼可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/02_DNN_classification_on_MNIST.py">Github</a>下載。</p>
<p><br/></p>
<h5><u>增加Hidden Layer</u></h5>
<p>在上一回當中，我們只有一層Neurel Network，也就是做完一個線性轉換後，就直接使用Softmax Layer來轉換成機率表示方式，這樣的結構並不夠powerful，我們需要把它的結構弄的又窄又深，這樣效果才會好，詳細原因請參考<a href="http://www.ycc.idv.tw/YCNote/post/35">這一篇的介紹</a>。</p>
<p>因此，我們來試著加入一層Hidden Layer，來打造成兩層的Neurel Network，並在兩層之間加入Activation Function，為我的Model增加非線性因子。</p>
<div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">features</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">activation</span><span class="p">):</span>
        <span class="c1"># build neurel network structure and return their predictions and loss</span>
        <span class="c1">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;fc1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">)</span> <span class="p">)),</span>
                <span class="s1">&#39;fc2&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">)</span> <span class="p">)),</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span>  <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;fc1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span> <span class="p">)),</span>
                <span class="s1">&#39;fc2&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">)</span> <span class="p">)),</span>
            <span class="p">}</span> 
        <span class="c1">### Structure</span>
        <span class="c1"># layer 1</span>
        <span class="n">fc1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;fc1&#39;</span><span class="p">],</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;fc1&#39;</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>

        <span class="c1"># layer 2</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">fc1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;fc2&#39;</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;fc2&#39;</span><span class="p">])</span>

        <span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                 <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>
</pre></div>


<p>首先在變數需要有二層的fully-connect參數，注意這些參數的大小，受到Hidden Layer的神經元數目<code>n_hidden</code>決定。接下來開始建構整個Neurel Network的結構，<code>fc1</code>產生一個fully-connect的結果，並且通過Activation Function再輸出，然後進到下一層，第二層直接使用<code>fc1</code>的結果當作新的輸入，再做一次fully-connect，並且讓它通過Softmax Layer來完成最後的Logistic轉換，它的loss一樣的是使用cross-entropy來評估。</p>
<p><br/></p>
<h5><u>Activation Function的選擇</u></h5>
<p>剛剛提到的Hidden Layer可以採用不同的Activation Function，我列幾個常使用的Activation Function給大家看看。</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">tanh</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">relu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">tanh</span><span class="p">,</span><span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">sigmoid</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">relu</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0A/j+3uXalz1k0gQAAAABJRU5ErkJggg==%0A"></p>
<p><br/></p>
<h5><u>Mini-Batch Gradient Descent</u></h5>
<p>在上一回當中，我們的Gradient Descent採用的是將所有的Data一次全考慮進去，評估完所有的Data在一次性的更新權重參數，這樣的作法好處是比較穩定，因為我考慮的是真正的Training Set的E<sub>in</sub>，但缺點就是計算時間長，因為要考慮所有Training Set的每筆數據，需要做大矩陣的計算，而且通常Training Set的數據量也不可以太小，這麼一來計算時間就會拉的很長。</p>
<p>因此，有另外一種作法一次只考慮「一筆」數據，使用一筆數據來評估並更新權重參數，每一筆雖然更新結果都不怎麼準確，但是當我隨著時間看過整個Training Set後，就會有平均的效果，所以最後只要Learning Rate不要太大，最後的結果還是可以朝向最佳解的，這個手法會使得Gradient Descent具有隨機性，因此又被稱為Stochastic Gradient Descent，它所帶來的優點是計算時間變短了，我們將可以避免去涉及大矩陣的運算，但缺點是一次只評估一筆數據，將會非常的不穩定。</p>
<p>另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Mini-Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，Mini-Batch Gradient Descent一次評估k組數據，並更新參數W，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的架構設計是非常有利於矩陣計算的，因為GPU會利用它強大的平行化將矩陣運算中每個元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Mini-Batch Gradient Descent，這個k值不要超過GPU平行計算所能容納的最大上限，這是個既有效率又更為穩定的作法，順道一提Tensorflow是可以支援GPU的計算的。</p>
<p>實務經驗告訴我們Mini-Batch Gradient Descent雖然穩定性比Gradient Descent差，但是收斂的速度卻一點都不輸給Gradient Descent，原因就出在更新的次數，Mini-Batch Gradient Descent一次看的數據筆數比較少，所以Mini-Batch一個Epoch可以更新參數好幾次，而Gradient Descent卻只能更新一次，Mini-Batch的靈敏性，使得它的收斂速度更為快速。打個比方，就好像是兩艘船在搜尋小島，Gradient Descent像是巡洋艦，它有更好設備可以有更好的觀測能力，但是因為它的笨重造成它反應不夠靈敏，Mini-Batch Gradient Descent就像是小船一樣，雖然觀測設備沒這麼好，但是反應靈敏，卻是可以更容易率先找到小島。</p>
<p>那我們來看看要怎麼做到Mini-Batch Gradient Descent。</p>
<div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">test_data</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

        <span class="o">...</span><span class="err">略</span><span class="o">...</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">%2d</span><span class="s2">/</span><span class="si">%2d</span><span class="s2">: &quot;</span><span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="p">))</span>

            <span class="c1"># mini-batch gradient descent</span>
            <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
            <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
                <span class="n">index_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
                <span class="n">batch_index</span> <span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span><span class="n">index_size</span><span class="p">))]</span>    

                <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_features</span><span class="p">:</span> <span class="n">X</span><span class="p">[</span><span class="n">batch_index</span><span class="p">,:],</span> 
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">:</span> <span class="n">y</span><span class="p">[</span><span class="n">batch_index</span><span class="p">],</span> 
                <span class="p">}</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;[</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">] loss = </span><span class="si">%9.4f</span><span class="s2">     &quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">N</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">),</span> <span class="n">N</span><span class="p">,</span> <span class="n">loss</span> <span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">&#39;</span><span class="p">)</span>

            <span class="o">...</span><span class="err">略</span><span class="o">...</span>
</pre></div>


<p>在每一個<code>epoch</code>都完整的看過數據一遍，而mini-batch gradient descent是隨機取<code>batch_size</code>筆數據來更新權重，所以我採用這樣的作法，先依照數據的筆數<code>N</code>列出可能的Index有哪些，然後再做一個<code>random.shuffle</code>來做到隨機採樣，然後接下來只要簡單的從前面取<code>batch_size</code>筆數據進行更新，直到用盡所有的index為止，就可以做到mini-batch的效果。</p>
<p><br/></p>
<h5><u>Regularization</u></h5>
<p>當你開始加深你的DNN時，就已經在增加Model的複雜度，增加複雜度想當然爾的可以增加對於數據的描述能力，在分類問題中代表可以增加精確度，不過要特別注意Overfitting的出現，當Model越複雜越容易產生Overfitting，Overfitting的結果是有看過的數據描述的很好，但沒看過的數據預測就很差，所以在Training的過程要特別注意Validation Set的表現，如果發現Training Set的表現越來越好，但是Validation Set的表現裹足不前甚至變的更差，那就很有可能已經Overfitting了。</p>
<p>如果你已經看到Overfitting出現了，有什麼方法可以抑制他呢？這個時候就需要Regularization的幫忙，在Neurel Network常見的Regularization有兩種：Weight Regularization和Dropout，待會會一一介紹。</p>
<p>這邊特別注意，不要每次精確度沒有提升就怪Overfitting！如果連你的Training Set都沒辦法有好的表現，這就可能不是Overfitting，反而可能是Underfitting，這個時候不要再增加Regularization，而是試著去調整Learning Rate，或者增加模型的複雜度，加深DNN或增加神經元的數目。</p>
<h5><u>Weight Regularization</u></h5>
<p>我們可以藉著在Loss Function裡頭加入Weight的貢獻，來達到限制W的大小的目標，這樣做可以降低Overfitting，詳細原理請<a href="http://www.ycc.idv.tw/YCNote/post/28">參考這篇的Regularization的部分</a>。</p>
<p>我們來看看應該怎麼做到L2 Regularization。</p>
<div class="highlight"><pre><span></span><span class="c1"># regularization loss</span>
<span class="n">regularization</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">/</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">out_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                                      <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>
<span class="c1"># total loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">original_loss</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">regularization</span>
</pre></div>


<p>在這裡我習慣將Loss對Weights的個數做平均，這有一個好處當我在調整神經元數目時，<code>alpha</code>可以不需要大動作調整。</p>
<p><br/></p>
<h5><u>Dropout</u></h5>
<p>Dropout是Deep Learning常用的Regularization技巧，它的作法是在訓練的時候我先隨機把部份的神經元關閉，使用較少的神經元訓練，來達到Regularization的效果，最後在「推論」的時候再使用全部的神經元，我個人覺得這有Aggregation Model的味道，分別訓練出許多的sub-model再做Aggregation以達到截長補短的效果。</p>
<p>實作上有一些細節必須要注意，當我們關閉一些神經元時，也就是等於減少部份的貢獻量，所以我們需要依照相應比例來給予權重，以抵銷減少的部分。舉個例子，假設今天原本應該要輸出的值有十個，這十個值都是1，然後因為Dropout，變成五個1五個0的輸出，我們看到原本貢獻量因為Dropout一半而少一半，這樣並不合理，會導致我後面的Weights在更新的時候低估更新量，所以我們必須要將「沒被Dropout的部分」權重乘上一倍，才可以解決問題。因此，如果Dropout r倍的神經元，權重就要乘以(1/r)倍，我們來看看Tensorflow怎麼做的。</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># 3 Data, 8 dim. Score</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Original S =&quot;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">S</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
    <span class="n">S_drop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">S</span><span class="p">,</span><span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># dropout ratio = 1 - keep_prob = 0.5</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Dropout S =&quot;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">S_drop</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span>Original S =
[[ 1.  1.  1.  1.  1.  1.  1.  1.]
 [ 3.  3.  3.  3.  3.  3.  3.  3.]
 [ 5.  5.  5.  5.  5.  5.  5.  5.]]
Dropout S =
[[  2.   2.   2.   0.   0.   2.   0.   0.]
 [  6.   0.   6.   0.   6.   6.   0.   0.]
 [ 10.  10.  10.  10.  10.   0.   0.  10.]]
</pre></div>


<p>我們看到因為dropout 0.5倍，所以輸出值權重乘上2倍，另外一提，Tensorflow的Dropout機制是隨機的，所以Drop out的比例會接近我們想要的比例，但不是絕對剛好。</p>
<p>那我要怎麼把Dropout放進去我的Model呢？特別注意，我們並不希望已經Training完的Model還有Dropout這一層，所以我在<code>structure</code>裡頭設計一個<code>train</code>的開關，當我在Training過程就把它打開，Dropout這一層就會被加進去，「推論」的時候就關閉，保持原有的神經元數量。</p>
<div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">features</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">activation</span><span class="p">,</span><span class="n">dropout_ratio</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>

        <span class="o">...</span> <span class="err">略</span> <span class="o">...</span>

        <span class="c1"># layer 1</span>
        <span class="n">fc1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;fc1&#39;</span><span class="p">],</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;fc1&#39;</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="n">fc1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">fc1</span><span class="p">,</span><span class="n">keep_prob</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">dropout_ratio</span><span class="p">)</span>

        <span class="c1"># layer 2</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">fc1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;fc2&#39;</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;fc2&#39;</span><span class="p">])</span>

        <span class="o">...</span> <span class="err">略</span> <span class="o">...</span>
</pre></div>


<p><br/></p>
<h5><u>Optimizer的選擇</u></h5>
<p>我們可以自由的替換我們想要使用的Optimizer。</p>
<div class="highlight"><pre><span></span><span class="c1"># define training operation</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
</pre></div>


<p>在Tensorflow中目前有以下十種Optimizer供我們使用。</p>
<ul>
<li>tf.train.GradientDescentOptimizer</li>
<li>tf.train.AdadeltaOptimizer</li>
<li>tf.train.AdagradOptimizer</li>
<li>tf.train.AdagradDAOptimizer</li>
<li>tf.train.MomentumOptimizer</li>
<li>tf.train.AdamOptimizer</li>
<li>tf.train.FtrlOptimizer</li>
<li>tf.train.ProximalGradientDescentOptimizer</li>
<li>tf.train.ProximalAdagradOptimizer</li>
<li>tf.train.RMSPropOptimizer</li>
</ul>
<p>如果想要了解每個Optimizer的演算法可以參考<a href="http://ruder.io/optimizing-gradient-descent/">這篇有詳細的說明</a>。</p>
<p><br/></p>
<h5><u>來看看程式怎麼寫</u></h5>
<p>講了那麼多，來看看完整的程式怎麼寫？照慣例，先畫個流程圖。</p>
<p><img alt="DNNLogisticClassification" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.003.jpeg"></p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># Config the matplotlib backend as plotting inline in IPython</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DNNLogisticClassification</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_features</span><span class="p">,</span><span class="n">n_labels</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">n_hidden</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                 <span class="n">dropout_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span> <span class="o">=</span> <span class="n">n_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span> <span class="c1"># initialize new graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">activation</span><span class="p">,</span>
                   <span class="n">dropout_ratio</span><span class="p">,</span><span class="n">alpha</span><span class="p">)</span> <span class="c1"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span> <span class="c1"># create session by the graph </span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">activation</span><span class="p">,</span><span class="n">dropout_ratio</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
        <span class="c1"># Building Graph</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c1">### Input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span>   <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span>  <span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">))</span>

            <span class="c1">### Optimalization</span>
            <span class="c1"># build neurel network structure and get their predictions and loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">original_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_features</span><span class="p">,</span>
                                                        <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">,</span>
                                                        <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span>
                                                        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
                                                        <span class="n">dropout_ratio</span><span class="o">=</span><span class="n">dropout_ratio</span><span class="p">,</span>
                                                        <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="c1"># regularization loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                                   <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">/</span><span class="n">tf</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">out_type</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                                        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>
            <span class="c1"># total loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_loss</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularization</span>

            <span class="c1"># define training operation</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1">### Prediction</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span>   <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span>  <span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_y_</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_features</span><span class="p">,</span>
                                                       <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">,</span>
                                                       <span class="n">n_hidden</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">,</span>
                                                       <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span> 

            <span class="c1">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">features</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">,</span><span class="n">activation</span><span class="p">,</span><span class="n">dropout_ratio</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="c1"># build neurel network structure and return their predictions and loss</span>
        <span class="c1">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;fc1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span><span class="n">n_hidden</span><span class="p">)</span> <span class="p">)),</span>
                <span class="s1">&#39;fc2&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">)</span> <span class="p">)),</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span>  <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;fc1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span> <span class="p">)),</span>
                <span class="s1">&#39;fc2&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">)</span> <span class="p">)),</span>
            <span class="p">}</span> 
        <span class="c1">### Structure</span>
        <span class="c1"># layer 1</span>
        <span class="n">fc1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;fc1&#39;</span><span class="p">],</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;fc1&#39;</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="n">fc1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">fc1</span><span class="p">,</span><span class="n">keep_prob</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">dropout_ratio</span><span class="p">)</span>

        <span class="c1"># layer 2</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">fc1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;fc2&#39;</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;fc2&#39;</span><span class="p">])</span>

        <span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                 <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">getDenseLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_layer</span><span class="p">,</span><span class="n">weight</span><span class="p">,</span><span class="n">bias</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># fully connected layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span><span class="n">weight</span><span class="p">),</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">activation</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">test_data</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">9000</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_size</span><span class="p">:</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">N</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">%2d</span><span class="s2">/</span><span class="si">%2d</span><span class="s2">: &quot;</span><span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="p">))</span>

            <span class="c1"># mini-batch gradient descent</span>
            <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
            <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
                <span class="n">index_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
                <span class="n">batch_index</span> <span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span><span class="n">index_size</span><span class="p">))]</span>    

                <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_features</span><span class="p">:</span> <span class="n">X</span><span class="p">[</span><span class="n">batch_index</span><span class="p">,:],</span> 
                    <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">:</span> <span class="n">y</span><span class="p">[</span><span class="n">batch_index</span><span class="p">],</span> 
                <span class="p">}</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;[</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">] loss = </span><span class="si">%9.4f</span><span class="s2">     &quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">N</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">),</span> <span class="n">N</span><span class="p">,</span> <span class="n">loss</span> <span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">&#39;</span><span class="p">)</span>


            <span class="c1"># evaluate at the end of this epoch</span>
            <span class="n">y_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
            <span class="n">train_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;[</span><span class="si">%d</span><span class="s2">/</span><span class="si">%d</span><span class="s2">] loss = </span><span class="si">%8.4f</span><span class="s2">, acc = </span><span class="si">%3.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="o">*</span><span class="mi">100</span> <span class="p">)</span>

            <span class="k">if</span> <span class="n">validation_data</span><span class="p">:</span>
                <span class="n">val_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">val_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot;, val_loss = </span><span class="si">%8.4f</span><span class="s2">, val_acc = </span><span class="si">%3.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span><span class="o">*</span><span class="mi">100</span> <span class="p">)</span>

            <span class="k">print</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>


        <span class="k">if</span> <span class="n">test_data</span><span class="p">:</span>
            <span class="n">test_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">test_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;test_acc = </span><span class="si">%3.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">test_acc</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_y_</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_features</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_features</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> 
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">_check_array</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ndarray</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ndarray</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span> <span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ndarray</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="n">ndarray</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">ndarray</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s2">&quot;MNIST_data/&quot;</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span>
<span class="n">valid_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">validation</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span>
</pre></div>


<div class="highlight"><pre><span></span>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">DNNLogisticClassification</span><span class="p">(</span>   <span class="n">n_features</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span>
                                     <span class="n">n_labels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                     <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                                     <span class="n">n_hidden</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                                     <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                                     <span class="n">dropout_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                                 <span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span>
          <span class="n">y</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">valid_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">valid_data</span><span class="o">.</span><span class="n">labels</span><span class="p">),</span>
          <span class="n">test_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">test_data</span><span class="o">.</span><span class="n">labels</span><span class="p">),</span>
          <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
         <span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Epoch  1/ 3: 
[55000/55000] loss =   0.5235, acc = 88.03%, val_loss =   0.6335, val_acc = 88.22%
Epoch  2/ 3: 
[55000/55000] loss =   0.3493, acc = 91.90%, val_loss =   0.4541, val_acc = 91.62%
Epoch  3/ 3: 
[55000/55000] loss =   0.2653, acc = 93.22%, val_loss =   0.3612, val_acc = 92.68%
test_acc = 92.02%
</pre></div>


<p>跟上次的結果比，你會發現有長足的進步，精確率來到90幾，大家可以<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/02_DNN_classification_on_MNIST.py">下載程式碼</a>，試著調整參數使得DNN Model的精確率可以更高，參數包含：
<em> Hidden Layer的神經元數量
</em> 不同的Activation Function
<em> 不同的Batch Size
</em> 調整Weight Regularization的比例
<em> 調整Dropout Ratio
</em> 選擇不同Optimizer
* 使得DNN更深</p>
<p>調整Model是重要的工作，試著自己動手做做看，你可以讓你的Model有多準呢？</p></dd>
              
            	<dt>2017 / 10月 23</dt>
            	<dd><a href="../tensorflow-tutorial_1.html">實作Tensorflow (1)：Simple Logistic Classification on MNIST</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><p>Tensorflow Tutorial 1: Simple Logistic Classification on MNIST</p>
<p>初次學習Tensorflow最困難的地方莫過於不知道從何下手，已經學會很多的Deep Learning理論，但是要自己使用Tensorflow將Network建起來卻是非常困難的，這篇文章我會先簡單的介紹幾個Tensorflow的概念，最後利用這些概念建立一個簡單的分類模型。</p>
<p>本單元程式碼可於<a href="https://github.com/GitYCC/Tensorflow_Tutorial/blob/master/code/01_simple_logistic_classification_on_MNIST.py">Github</a>下載。</p>
<h5><u>MNIST Dataset</u></h5>
<p>首先，先<code>import</code>一些會用到的function，並且定義<code>summary</code> function以便於觀察ndarray。</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="c1"># Config the matplotlib backend as plotting inline in IPython</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="n">ndarr</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">ndarr</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;* shape: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ndarr</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;* min: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">ndarr</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;* max: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">ndarr</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;* avg: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ndarr</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;* std: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">ndarr</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;* unique: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">ndarr</span><span class="p">)))</span>
</pre></div>


<p>ndarray是numpy的基本元素，它非常便於我們做矩陣的運算。</p>
<p>我們使用MNIST Dataset來當作我們練習的標的，MNIST包含一包手寫數字的圖片，每張圖片大小為28x28，每一張圖片都是一個手寫的阿拉伯數字包含0到9，並且標記上它所對應的數字。我們的目標就是要利用MNIST做到手寫數字辨識。</p>
<p>在Tensorflow你可以很簡單的得到「處理過後的」MNIST，只要利用以下程式碼，</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s2">&quot;MNIST_data/&quot;</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span>
<span class="n">valid_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">validation</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span>
</pre></div>


<div class="highlight"><pre><span></span>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
</pre></div>


<p>每個<code>train_data</code>、<code>valid_data</code>、<code>test_data</code>都包含兩部分：圖片和標籤。</p>
<p>我們來看一下圖片的部分，<code>train_data.images</code>一共有55000張圖，每一張圖原本大小是28x28，不過特別注意這裡的Data已經先做過預先處理了，因此圖片已經被打平成28x28=784的一維矩陣了，另外每個Pixel的值也先做過「Normalization」了，通常會這樣處理，每個值減去128再除以128，所以你可以從以下的<code>summary</code>中看到它的最大最小值落在0到1之間，還有這個Dataset也已經做過亂數重排了。</p>
<div class="highlight"><pre><span></span><span class="n">summary</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">[[ 0.  0.  0. ...,  0.  0.  0.]</span>
 <span class="k">[ 0.  0.  0. ...,  0.  0.  0.]</span>
 <span class="k">[ 0.  0.  0. ...,  0.  0.  0.]</span>
 <span class="na">..., </span>
 <span class="k">[ 0.  0.  0. ...,  0.  0.  0.]</span>
 <span class="k">[ 0.  0.  0. ...,  0.  0.  0.]</span>
 <span class="k">[ 0.  0.  0. ...,  0.  0.  0.]]</span>
<span class="na">* shape: (55000, 784)</span>
<span class="na">* min: 0.0</span>
<span class="na">* max: 1.0</span>
<span class="na">* avg: 0.13070042431354523</span>
<span class="na">* std: 0.30815958976745605</span>
<span class="na">* unique: [ 0.          0.00392157  0.00784314  0.01176471  0.01568628  0.01960784</span>
  <span class="na">0.02352941  0.02745098  0.03137255  0.03529412  0.03921569  0.04313726</span>
  <span class="na">0.04705883  0.0509804   0.05490196  0.05882353  0.0627451   0.06666667</span>
  <span class="na">0.07058824  0.07450981  0.07843138  0.08235294  0.08627451  0.09019608</span>
  <span class="na">0.09411766  0.09803922  0.10196079  0.10588236  0.10980393  0.1137255</span>
  <span class="na">0.11764707  0.12156864  0.1254902   0.12941177  0.13333334  0.13725491</span>
  <span class="na">0.14117648  0.14509805  0.14901961  0.15294118  0.15686275  0.16078432</span>
  <span class="na">0.16470589  0.16862746  0.17254902  0.17647059  0.18039216  0.18431373</span>
  <span class="na">0.18823531  0.19215688  0.19607845  0.20000002  0.20392159  0.20784315</span>
  <span class="na">0.21176472  0.21568629  0.21960786  0.22352943  0.227451    0.23137257</span>
  <span class="na">0.23529413  0.2392157   0.24313727  0.24705884  0.25098041  0.25490198</span>
  <span class="na">0.25882354  0.26274511  0.26666668  0.27058825  0.27450982  0.27843139</span>
  <span class="na">0.28235295  0.28627452  0.29019609  0.29411766  0.29803923  0.3019608</span>
  <span class="na">0.30588236  0.30980393  0.3137255   0.31764707  0.32156864  0.32549021</span>
  <span class="na">0.32941177  0.33333334  0.33725491  0.34117648  0.34509805  0.34901962</span>
  <span class="na">0.35294119  0.35686275  0.36078432  0.36470589  0.36862746  0.37254903</span>
  <span class="na">0.37647063  0.38039219  0.38431376  0.38823533  0.3921569   0.39607847</span>
  <span class="na">0.40000004  0.4039216   0.40784317  0.41176474  0.41568631  0.41960788</span>
  <span class="na">0.42352945  0.42745101  0.43137258  0.43529415  0.43921572  0.44313729</span>
  <span class="na">0.44705886  0.45098042  0.45490199  0.45882356  0.46274513  0.4666667</span>
  <span class="na">0.47058827  0.47450984  0.4784314   0.48235297  0.48627454  0.49019611</span>
  <span class="na">0.49411768  0.49803925  0.50196081  0.50588238  0.50980395  0.51372552</span>
  <span class="na">0.51764709  0.52156866  0.52549022  0.52941179  0.53333336  0.53725493</span>
  <span class="na">0.5411765   0.54509807  0.54901963  0.5529412   0.55686277  0.56078434</span>
  <span class="na">0.56470591  0.56862748  0.57254905  0.57647061  0.58039218  0.58431375</span>
  <span class="na">0.58823532  0.59215689  0.59607846  0.60000002  0.60392159  0.60784316</span>
  <span class="na">0.61176473  0.6156863   0.61960787  0.62352943  0.627451    0.63137257</span>
  <span class="na">0.63529414  0.63921571  0.64313728  0.64705884  0.65098041  0.65490198</span>
  <span class="na">0.65882355  0.66274512  0.66666669  0.67058825  0.67450982  0.67843139</span>
  <span class="na">0.68235296  0.68627453  0.6901961   0.69411767  0.69803923  0.7019608</span>
  <span class="na">0.70588237  0.70980394  0.71372551  0.71764708  0.72156864  0.72549021</span>
  <span class="na">0.72941178  0.73333335  0.73725492  0.74117649  0.74509805  0.74901962</span>
  <span class="na">0.75294125  0.75686282  0.76078439  0.76470596  0.76862752  0.77254909</span>
  <span class="na">0.77647066  0.78039223  0.7843138   0.78823537  0.79215693  0.7960785</span>
  <span class="na">0.80000007  0.80392164  0.80784321  0.81176478  0.81568635  0.81960791</span>
  <span class="na">0.82352948  0.82745105  0.83137262  0.83529419  0.83921576  0.84313732</span>
  <span class="na">0.84705889  0.85098046  0.85490203  0.8588236   0.86274517  0.86666673</span>
  <span class="na">0.8705883   0.87450987  0.87843144  0.88235301  0.88627458  0.89019614</span>
  <span class="na">0.89411771  0.89803928  0.90196085  0.90588242  0.90980399  0.91372555</span>
  <span class="na">0.91764712  0.92156869  0.92549026  0.92941183  0.9333334   0.93725497</span>
  <span class="na">0.94117653  0.9450981   0.94901967  0.95294124  0.95686281  0.96078438</span>
  <span class="na">0.96470594  0.96862751  0.97254908  0.97647065  0.98039222  0.98431379</span>
  <span class="na">0.98823535  0.99215692  0.99607849  1.        ]</span>
</pre></div>


<p>來試著畫圖來看看，我們使用ndarray的index功能來選出第10張圖片，<code>train_data.images[10,:]</code>表示的是選第一軸的第10個和第二軸的全部。</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_fatten_img</span><span class="p">(</span><span class="n">ndarr</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">ndarr</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">img</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">plot_fatten_img</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">10</span><span class="p">,:])</span>
</pre></div>


<p><img alt="png" src="output_8_0.png"></p>
<p>很顯而易見的，這是一個0。</p>
<p>接下來來看標籤的部分，<code>train_data.labels</code>不意外的一樣的也是有相應的55000筆資料，所對應的就是前面的每一張圖片，總共有10種類型:0到9，所以大小為(55000, 10)。</p>
<div class="highlight"><pre><span></span><span class="n">summary</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">[[ 0.  0.  0. ...,  1.  0.  0.]</span>
 <span class="k">[ 0.  0.  0. ...,  0.  0.  0.]</span>
 <span class="k">[ 0.  0.  0. ...,  0.  0.  0.]</span>
 <span class="na">..., </span>
 <span class="k">[ 0.  0.  0. ...,  0.  0.  0.]</span>
 <span class="k">[ 0.  0.  0. ...,  0.  0.  0.]</span>
 <span class="k">[ 0.  0.  0. ...,  0.  1.  0.]]</span>
<span class="na">* shape: (55000, 10)</span>
<span class="na">* min: 0.0</span>
<span class="na">* max: 1.0</span>
<span class="na">* avg: 0.1</span>
<span class="na">* std: 0.30000000000000004</span>
<span class="na">* unique: [ 0.  1.]</span>
</pre></div>


<p>所以我們來看看上面那張圖片的標籤，</p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
</pre></div>


<p>看起來的確沒錯，在0的位置標示1.，而其他地方標示為0.，因此這是一個標示為0的label沒有錯，這種表示方法稱為One-Hot Encoding，它具有機率的涵義，所代表的是有100%的機會落在0的類別上。</p>
<h5><u>Softmax</u></h5>
<p>通常One-Hot Encoding會搭配Softmax一同服用，最後的Output結果如果是機率分布，那我也需要讓我的Neurel Network可以輸出機率分布。</p>
<p><img alt="softmax" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.001.jpeg"></p>
<p>通過Softmax這一層，我們就可以將輸出轉變為以「機率」表示。</p>
<p>我們可以來手刻一個Softmax Function，不過直接套用Tensorflow中函數的也是可以的。</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># avoid exp function go to too large,</span>
    <span class="c1"># pre-reduce before applying exp function</span>
    <span class="n">max_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">max_score</span>

    <span class="n">exp_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sum_exp_s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_s</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">softmax</span> <span class="o">=</span> <span class="n">exp_s</span> <span class="o">/</span> <span class="n">sum_exp_s</span>
    <span class="k">return</span> <span class="n">softmax</span>

<span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>[ 0.8360188   0.11314284  0.05083836]
</pre></div>


<h5><u>Cross-Entropy Loss</u></h5>
<p>一旦我們要處理機率預測的問題，就不可以使用單純的「平方誤差」，而必須使用Cross-Entropy Loss，是這樣計算的：</p>
<p>Cross-Entropy Loss = -𝚺<sub>i</sub> y<sub>i</sub> ln[ s<sub>i</sub> ]</p>
<p>其中，y<sub>i</sub>為目標Label，s<sub>i</sub>為經過Softmax產生的預測值。</p>
<p>至於如果你想要了解為何需要使用Cross-Entropy Loss？這我在機器學習基石的筆記中已經有提及過，請看<a href="http://www.ycc.idv.tw/YCNote/post/27">介紹Logistic Regression的部分</a>。</p>
<h5><u>分離數據的重要性</u></h5>
<p>在MNIST Dataset中，你會發現分為Training Dataset、Validation Dataset和Testing Dataset，這樣的作法在Machine Learning中是常見且必要的。</p>
<p>流程是這樣的，我們會先使用Training Dataset來訓練Model，並且使用Validation Dataset來檢驗Model的好壞，我們會依據Validation Dataset的檢驗調整Model上的參數，試著盡可能的壓低Validation Dataset的Error，記住！在過程中所產生的所有Models都要保留下來，因為最後選擇的Model並不是Validation Dataset的Error最小的，而是要再由Testing Dataset來做最後的挑選，挑選出能使Testing Dataset的Error最小的Model。</p>
<p>這所有的作法都是為了避免Overfitting的情況發生，也就是機器可能因為看過一筆Data，結果就把這筆Data給完整記了起來，而Data本身含有雜訊，雜訊就這樣滲透到Model裡，確實做到分離是很重要的，讓Model在測試階段時可以使用沒有看過的Data。</p>
<p>因此，Validation Dataset的分離是為了避免讓Model在Training階段看到要驗證的資料，所以更能正確的評估Model的好壞。但這樣是不夠的，人為會根據Validation Dataset來調整Model，這樣無形之中已經將Validation Dataset的資訊間接的經由人傳給了Model，所以還是沒有徹底分離，因此在最後挑選Models時，我們會使用另外一筆從沒看過的資料Testing Dataset來做挑選，一旦挑選完就不能再去調整任何參數了。</p>
<h5><u>Tensorflow工作流程</u></h5>
<p>我們這一篇將會使用Tensorflow實作最簡單的單層Neurel Network，在這之前我們來看看Tensorflow是如何運作的？</p>
<p>深度學習是由一層一層可以微分的神經元所連接而成，數學上可以表示為張量(Tensor)的表示式，我們一般講的矩陣運算是指2x2的矩陣運算，而張量(Tensor)則是拓寬到n維陣列做計算，在Machine Learning當中我們常常需要處理到相當高維度的計算，例如：有五張28x28的彩色圖的表示就必須使用到四維張量，第一維表示第幾張、第二、三維表示圖片的大小、第四維則表示RGB，如果你是物理系的學生應該也對張量不陌生，廣義相對論裡頭大量的使用四維張量運算，三維空間加一維時間。</p>
<p>而在做Neurel Network時，我們會根據需求不同設計不同形式但合理的流程(Flow)，再使用數據來訓練我的Model。所以，這就是Tensorflow命名由來：Tensor+Flow。</p>
<p>因此，一開始要先設計Model的結構，這在Tensorflow裡頭稱為Graph，Graph的作用是事先決定Neurel Network的結構，決定Neuron要怎麼連接？決定哪一些窗口是可以由外部置放數據的？決定哪一些變數是可以被訓練的？哪一些變數是不可以被訓練的？定義將要怎麼樣優化這個系統？...等等。</p>
<div class="highlight"><pre><span></span><span class="n">my_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span> <span class="c1"># Initialize a new graph</span>

<span class="k">with</span> <span class="n">my_graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span> <span class="c1"># Create a scope to build graph</span>
    <span class="c1"># ...</span>
    <span class="c1"># detail of building graph</span>
</pre></div>


<p>Graph只是一個結構，它不具有有效的資訊，而當我們定義完成Graph之後，接下來我們需要創造一個環境叫做Session，Session會將Graph的結構複製一份，然後再放入資訊進行Training或是預測等等，因此Session是具有有效資訊的。</p>
<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">my_graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span> <span class="c1"># Copy graph into session</span>
    <span class="c1"># ...</span>
    <span class="c1"># detail of doing machine learning  </span>
</pre></div>


<p>還有另外一種寫法也是相同作用的，我個人比較喜歡下面這種寫法。</p>
<div class="highlight"><pre><span></span><span class="n">my_session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">my_graph</span><span class="p">)</span>
<span class="n">my_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>


<h5><u>Tensorflow的基本「張量」元素</u></h5>
<p>接下來我們就來看看有哪些構成Graph的基本元素可以使用。</p>
<p>(1) 常數張量：</p>
<p>一開始來看看「常數張量」，常數指的是在Model中不會改變的數值。</p>
<div class="highlight"><pre><span></span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>


<p>(2) 變數張量：</p>
<p>與常數截然不同的就是變數，「變數張量」是指在訓練當中可以改變的值，一般「變數張量」會用作於Machine Learning需要被訓練的參數，如果你沒有特別設定，在最佳化的過程中，Tensorflow會自動調整「變數張量」的數值來最佳化。</p>
<div class="highlight"><pre><span></span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span> <span class="p">)</span>
</pre></div>


<p>因為變數通常是未知且待優化的參數，所以我們一般會使用Initalizer來設定它的初始值，<code>tf.truncated_normal(shape=(3,5))</code>會隨機產生大小3x5的矩陣，它的值呈常態分佈但只取兩個標準差以內的數值。</p>
<p>如果今天你想要有一個「變數張量」但是又不希望它因為最佳化而改變，這時你要特別指定<code>trainable</code>為<code>False</code>。</p>
<div class="highlight"><pre><span></span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>


<p>(3) 置放張量：</p>
<p>另外有一些張量負責擔任輸入窗口的角色，稱為Placeholder。</p>
<div class="highlight"><pre><span></span><span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="mi">1000</span><span class="p">))</span>
</pre></div>


<p>因為我們在訓練之前還尚未知道Data的數量，所以這裡使用None來表示未知。<code>tf.placeholder</code>在Graph階段是沒有數值的，必須等到Session階段才將數值給輸入進去。</p>
<p>(4) 操作型張量：</p>
<p>這類張量並不含有實際數值，而是一種操作，常用的「操作型張量」有兩種，第一種是作為最佳化使用，</p>
<div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>


<p>選擇Optimizer和最佳化的方式來定義最佳化的操作方法，上述的例子是使用learning_rate為0.5的Gradient Descent來降低loss。</p>
<p>另外一種是初始化的操作，</p>
<div class="highlight"><pre><span></span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
</pre></div>


<p>這一個步驟是必要的但常常被忽略，還記得剛剛我們定義「變數張量」時有用到Initalizer，這些Initalizer在Graph完成時還不具有數值，必須使用<code>init_op</code>來給予數值，所以記住一定要放<code>init_op</code>進去Graph裡頭，而且必須先定義完成所有會用到的Initalizer再來設定這個<code>init_op</code>。</p>
<h5><u>Session的操作</u></h5>
<p>「張量」元素具有兩個面向：功能和數值，在Graph階段「張量」只具有功能但不具有數值，只有到了Session階段才開始有數值，那如何將這些數值取出來呢？有兩種方法，以1+1當作範例來看看，</p>
<div class="highlight"><pre><span></span><span class="n">g1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">g1</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1"># add x and y</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">g1</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span> 
    <span class="k">print</span><span class="p">(</span><span class="n">sol</span><span class="p">)</span> <span class="c1"># print tensor, not their value</span>
</pre></div>


<div class="highlight"><pre><span></span>Tensor(&quot;Add:0&quot;, shape=(), dtype=int32)
</pre></div>


<div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">g1</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span> 
    <span class="k">print</span><span class="p">(</span><span class="n">sol</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span> <span class="c1"># evaluate their value</span>
</pre></div>


<div class="highlight"><pre><span></span>2
</pre></div>


<div class="highlight"><pre><span></span><span class="n">s1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">g1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">s1</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">sol</span><span class="p">))</span> <span class="c1"># another way of evaluating value</span>
</pre></div>


<div class="highlight"><pre><span></span>2
</pre></div>


<p>那如果我想使用placeholder來做到x+y呢？</p>
<div class="highlight"><pre><span></span><span class="n">g2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">g2</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">sol</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1"># add x and y</span>

<span class="n">s2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">g2</span><span class="p">)</span>

<span class="c1"># if x = 2 and y = 3</span>
<span class="k">print</span><span class="p">(</span><span class="n">s2</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">sol</span><span class="p">,</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span><span class="n">y</span><span class="p">:</span> <span class="mi">3</span><span class="p">}))</span> 
</pre></div>


<div class="highlight"><pre><span></span>5
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># if x = 5 and y = 7</span>
<span class="k">print</span><span class="p">(</span><span class="n">s2</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">sol</span><span class="p">,</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span><span class="n">y</span><span class="p">:</span> <span class="mi">7</span><span class="p">}))</span> 
</pre></div>


<div class="highlight"><pre><span></span>12
</pre></div>


<p>因為x和y是placeholder，所以必須使用<code>feed_dict</code>來餵入相關資訊，否則會報錯。</p>
<h5><u>第一個Tensorflow Model</u></h5>
<p>有了以上的認識我們就可以來建立我們第一個Model。</p>
<p>以下我會使用物件導向的寫法，讓程式碼更有條理。</p>
<p>Machine Learning在操作上可以整理成三個大步驟：建構(Building)、訓練(Fitting)和推論(Inference)，所以我們將會使用這三大步驟來建製我們的Model。</p>
<p>在<code>SimpleLogisticClassification</code>裡頭，「建構」的動作在<code>__init__</code>中會進行，由<code>build</code>函式來建立Graph，其中我將Neurel Network的結構分離存於<code>structure</code>裡。「訓練」的動作在<code>fit</code>中進行，這裡採用傳統的Gradient Descent的方法，將所有Data全部考慮進去最佳化，未來會再介紹Batch Gradient Descent。最後，「推論」的部分在<code>predict</code>和<code>evaluate</code>中進行。</p>
<p><code>SimpleLogisticClassification</code>將會建構一個只有一層的Neurel Network，也就是說沒有Hidden Layer，畫個圖。</p>
<p><img alt="Simple Logistic Classification" src="https://raw.githubusercontent.com/GitYCC/Tensorflow_Tutorial/master/img/TensorflowTutorial.002.jpeg"></p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleLogisticClassification</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_features</span><span class="p">,</span><span class="n">n_labels</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span> <span class="o">=</span> <span class="n">n_labels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span>  <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span> <span class="c1"># initialize new graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span> <span class="c1"># building graph</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span> <span class="c1"># create session by the graph     </span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">):</span>
        <span class="c1"># Building Graph</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
            <span class="c1">### Input</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span>   <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span>  <span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">))</span>

            <span class="c1">### Optimalization</span>
            <span class="c1"># build neurel network structure and get their predictions and loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_features</span><span class="p">,</span>
                                                        <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">)</span>
            <span class="c1"># define training operation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1">### Prediction</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span>   <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span>  <span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">new_y_</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">structure</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_features</span><span class="p">,</span>
                                                       <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">,)</span>

            <span class="c1">### Initialization</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">structure</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">features</span><span class="p">,</span><span class="n">labels</span><span class="p">):</span>
        <span class="c1"># build neurel network structure and return their predictions and loss</span>
        <span class="c1">### Variable</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;fc1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">)</span> <span class="p">)),</span>
            <span class="p">}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">biases</span>  <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;fc1&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_labels</span><span class="p">)</span> <span class="p">)),</span>
            <span class="p">}</span> 

        <span class="c1">### Structure   </span>
        <span class="c1"># one fully connected layer</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getDenseLayer</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="s1">&#39;fc1&#39;</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="s1">&#39;fc1&#39;</span><span class="p">])</span>

        <span class="c1"># predictions</span>
        <span class="n">y_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

        <span class="c1"># loss: softmax cross entropy</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                 <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">getDenseLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_layer</span><span class="p">,</span><span class="n">weight</span><span class="p">,</span><span class="n">bias</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># fully connected layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_layer</span><span class="p">,</span><span class="n">weight</span><span class="p">),</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">activation</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">test_data</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_op</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">%2d</span><span class="s2">/</span><span class="si">%2d</span><span class="s2">: &quot;</span><span class="o">%</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">epochs</span><span class="p">))</span>

            <span class="c1"># fully gradient descent</span>
            <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">train_features</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_labels</span><span class="p">:</span> <span class="n">y</span><span class="p">}</span>
            <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="n">feed_dict</span><span class="p">)</span>

            <span class="c1"># evaluate at the end of this epoch</span>
            <span class="n">y_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">train_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
            <span class="n">train_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot; loss = </span><span class="si">%8.4f</span><span class="s2">, acc = </span><span class="si">%3.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="o">*</span><span class="mi">100</span> <span class="p">)</span>

            <span class="k">if</span> <span class="n">validation_data</span><span class="p">:</span>
                <span class="n">val_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">val_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">validation_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot;, val_loss = </span><span class="si">%8.4f</span><span class="s2">, val_acc = </span><span class="si">%3.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span><span class="o">*</span><span class="mi">100</span> <span class="p">)</span>

            <span class="k">print</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">test_data</span><span class="p">:</span>
            <span class="n">test_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">test_data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;test_acc = </span><span class="si">%3.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">test_acc</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_y_</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_features</span><span class="p">:</span> <span class="n">X</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_loss</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">new_features</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_labels</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">_check_array</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ndarray</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ndarray</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span> <span class="n">ndarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ndarray</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="n">ndarray</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">ndarray</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleLogisticClassification</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span>
                                     <span class="n">n_labels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                     <span class="n">learning_rate</span><span class="o">=</span> <span class="mf">0.5</span><span class="p">,)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span>
          <span class="n">y</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">valid_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">valid_data</span><span class="o">.</span><span class="n">labels</span><span class="p">),</span>
          <span class="n">test_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">images</span><span class="p">,</span><span class="n">test_data</span><span class="o">.</span><span class="n">labels</span><span class="p">),</span> <span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Epoch  1/10: 
 loss =   9.4080, acc = 12.45%, val_loss =   9.3869, val_acc = 12.78%
Epoch  2/10: 
 loss =   8.2898, acc = 14.57%, val_loss =   8.2863, val_acc = 14.66%
Epoch  3/10: 
 loss =   7.4517, acc = 16.82%, val_loss =   7.4578, val_acc = 16.74%
Epoch  4/10: 
 loss =   6.8298, acc = 19.23%, val_loss =   6.8352, val_acc = 19.10%
Epoch  5/10: 
 loss =   6.3458, acc = 21.83%, val_loss =   6.3448, val_acc = 21.12%
Epoch  6/10: 
 loss =   5.9372, acc = 24.16%, val_loss =   5.9287, val_acc = 23.64%
Epoch  7/10: 
 loss =   5.5760, acc = 26.53%, val_loss =   5.5604, val_acc = 25.98%
Epoch  8/10: 
 loss =   5.2527, acc = 28.88%, val_loss =   5.2306, val_acc = 28.42%
Epoch  9/10: 
 loss =   4.9624, acc = 31.05%, val_loss =   4.9344, val_acc = 30.54%
Epoch 10/10: 
 loss =   4.7012, acc = 33.06%, val_loss =   4.6681, val_acc = 32.52%
test_acc = 32.77%
</pre></div></dd>
              
            	<dt>2017 / 8月 04</dt>
            	<dd><a href="../confusion-matrix.html">如何辨別機器學習模型的好壞？秒懂Confusion Matrix</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><blockquote>
<p>​ 本篇介紹包含Confusion Matrix, True Positive, False Negative, False Positive, True Negative, Type I Error, Type II Error, Prevalence, Accuracy, Precision, Recall, F1 Measure, F Measure, Sensitivity, Specificity, ROC Curve, AUC, TPR, FNR, FPR, TNR, FDR, FOR, PPV, NPV, 算數平均, 幾何平均, 調和平均</p>
</blockquote>
<p>有時要鑑別一個模型的好或壞，並不能簡單的看出來，所以我們需要用一些指標去判定它的好壞，也作為我們挑選模型的依據。如果你稍微查一下有哪些指標，你就會發現指標多到讓人家眼花撩亂，一堆名詞就攤在那邊，讓人無從下手。有一種常用的指標稱之為Confusion Matrix，這個命名很有趣，這個表格的確是很讓人感到很困惑啊！至少在看完這篇之前。Confusion Matrix是用於分類問題的一種常用的指標，它衍生很多不同的指標，下面這張圖我將Confusion Matrix畫出來，並把一些比較重要的衍生指標給標出來。</p>
<p><img alt="mechine_learning_measure.001" src="https://dl.dropboxusercontent.com/s/ty6qfmf3uj63of7/mechine_learning_measure.001.jpeg"></p>
<p>我猜想，你一定看得很模糊吧！沒關係我在這篇文章中會帶大家認識這個圖裡的各個名詞。</p>
<p>一開始我們從下面這個表格開始講起，這個表格就是所謂的Confusion Matrix，這個表格兩欄代表實際情形正向或反向，兩列代表預測情形正向或反向，舉iphone當例子，iphone具有指紋識別解鎖系統，假如iphone判定這個指紋是屬於使用者的，它就會解鎖，如果今天你找個朋友來一起測試這個解鎖系統，如果是由你來按壓，而iphone也順利解鎖了，那這種情形就屬於左上角的情況，稱為True Positive，也就是「正確的正向預測」，如果不幸的你按壓iphone，結果iphone認不得你的指紋，這就是左下角的情況，稱為False Negative，也就是「錯誤的負向預測」，接下來換你朋友來按按看，正常情形你朋友的指紋應該沒辦法解鎖，這是右下角的情況，稱為True Negative，也就是「正確的負向預測」，如果令人意外的是你朋友把你的手機解鎖了，那你最好改成用密碼解鎖...，這種情況就是右上角的狀況，稱為False Positive，也就是「錯誤的正向預測」。</p>
<p><img alt="mechine_learning_measure.002" src="https://dl.dropboxusercontent.com/s/0q5hx5mt9tj10hx/mechine_learning_measure.002.jpeg"></p>
<p>從上面的描述，我們當然希望我們的模型True Positive和True Negative都可以多多出現，而False Positive和False Negative可以盡量不要出現，因此這兩種狀況就稱之為Error，又各自又命名為Type I Error和Type II Error，這兩種錯誤，錯的很不一樣，如果今天指紋辨識不是放在iphone，而是放在你家大門鎖上，那你最不希望發生哪類錯誤？當然是Type I Error，我寧可被關在門外進不去，也不要有陌生人可以進我家囉！但如果今天這個辨別系統是用在Google廣告，Google Ad會預測一個產品的潛在客戶，並做廣告投放，這個時候反而是較不希望Type II Error發生，寧可錯殺一百個也不要放過一個。所以下次在建構你的模型時想想看你最不想要怎樣的錯誤，也許可以藉由放掉另外一種錯誤，來降低這個我們不希望發生的錯誤。</p>
<p>Confusion Matrix還有衍生很多形形色色的指標，我接下來就一一的介紹。</p>
<p><strong>我們把所有正確的情況，也就是True Positive和True Negative，把它加總起來除上所有情形個數，那就是Accuracy，這也是最常用的指標，但是在某些情形下這個指標會失效</strong>，如果今天實際正向的例子很少，譬如有一個聊天機器人和10000個人長談，藉由談話，機器人會預測這些人未來會不會當上CEO，那我只要簡單一步來設計我的模型就可以使它Accuracy達到99%以上，那就是通通預測你不會當上CEO就好了，顯然我們需要別種指標來應對這種情況。</p>
<p>Precision（準確率）和Recall（召回率）這個時候就派上用場了，Precision和Recall同時關注的都是True Positive（都在分子），但是角度不一樣，<strong>Precision看的是在預測正向的情形下，實際的「精準度」是多少，而Recall則是看在實際情形為正向的狀況下，預測「能召回多少」正向的答案</strong>。一樣的，如果是門禁系統，我們希望Precision可以很高，Recall就相較比較不重要，如果是廣告投放，則Recall很重要，越大越好，Precision就顯得沒這麼重要了。<strong>Precision和Recall都不去考慮True Negative</strong>，以上面CEO的例子，大部分的人都不會當上CEO，所以預測這一塊並不是那麼重要，反而我們應該關心的是應該當上CEO的沒被預測到，以及沒當上CEO的卻被預測到了的兩種情形。</p>
<p><img alt="mechine_learning_measure.003" src="https://dl.dropboxusercontent.com/s/c9bpinil22qf4kz/mechine_learning_measure.003.jpeg"></p>
<p>如果今天我覺得Precision和Recall都同等重要，我想要用一個指標來統合標誌它，這就是F1 Score或稱F1 Measure，它是F Measure的一個特例，當belta=1時就是F1 Measure，代表Precision和Recall都同等重要，那如果我希望多看中一點Precision，那belta就可以選擇小一點，當belta=0時，F Measure就是Precision；如果我希望多看中一點Recall，那belta就可以選擇大一點，當belta無限大時，F Measure就是Recall。</p>
<p>如果你仔細看F1 Measure，你會發現它的平均方法是「調和平均」，帶大家go-through三種平均方法，你就能明白為什麼要使用調和平均了。下圖列出了三種平均方法的使用時機，我們要去了解資料或數列的特性，我們才能知道要採取哪種平均方法較為恰當，大多情況算數平均都可以使用，因為我們都假設有線性關係存在，譬如說平均距離；幾何平均常用於人口計算，因為人口增加是成比例增加的；調和平均常用於計算平均速率，在固定距離下，所花時間就是平均速率，這數據成倒數關係，而F1 Measure也同樣是這樣的數據特性，在固定TP的情況下，有不同的分母，所以這裡使用調和平均較為適當。</p>
<p><img alt="mechine_learning_measure.004" src="https://dl.dropboxusercontent.com/s/6cmlwypzla90u1v/mechine_learning_measure.004.jpeg"></p>
<p>下圖的名詞看一下有印象就好。</p>
<p><img alt="mechine_learning_measure.005" src="https://dl.dropboxusercontent.com/s/5w2mgom97wbey9t/mechine_learning_measure.005.jpeg"></p>
<p>最後這頁來講一下醫學上常用的指標，首先是Prevalence（盛行率），如果以人口當作所有的樣本，實際得病的患者所佔的比例就代表這個病的盛行情況。如果今天有一個診斷方法可以判定病人是否有得此病，有兩個指標可以看，那就是Sensitivity和Specificity，Sensitivity就是Recall，它代表的是診斷方法是否夠靈敏可以將真正得病的人診斷出來，而Specificity則是代表診斷方法是否可以指出實際沒的此病的患者。兩種指標都是越高越好。</p>
<p><img alt="mechine_learning_measure.006" src="https://dl.dropboxusercontent.com/s/dgynp8f0vo45kxy/mechine_learning_measure.006.jpeg"></p>
<p>通常在醫學上，會通過一些閥值來斷定病人是否有得此病，而這個閥值就會影響Sensitivity和Specificity，這個不同閥值Sensitivity和Specificity的分布情況可以畫成ROC Curve，而ROC Curve底下的面積稱為AUC，最理想的情況是AUC=1，在這種情況下存在一種閥值，也就是左上角落，使得Sensitivity=1且Specificity=1。而AUC=0.5，則是代表隨機挑選的狀況，沒有預測能力。大部分情形都是落在這兩種之間，透過ROC和AUC我們就可以選出更為強健穩定的模型。</p>
<p>想必這個時候你再回去看第一張圖就更加了解了，有了這些指標，我們就多一把尺來評斷我們的模型好還是不好了。</p></dd>
              
            	<dt>2017 / 4月 22</dt>
            	<dd><a href="../ml-course-techniques_7.html">機器學習技法 學習筆記 (7)：Radial Basis Function Network與Matrix Factorization</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><blockquote>
<p>本篇內容涵蓋Radial Basis Function (RBF) Network、K-Means、One-Hot Encoding和Matrix Factorization。</p>
</blockquote>
<p><br/></p>
<h5><u>Radial Basis Function (RBF) Network</u></h5>
<p>回顧一下Gaussian Kernel SVM，</p>
<blockquote>
<p>W = 𝚺<sub>n=sv</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub>  <br/></p>
<p>G<sub>SVM</sub>   <br/></p>
<p>= sign[WZ+b] <br/></p>
<p>= sign{[𝚺<sub>n=sv</sub>α<sub>n</sub>y<sub>n</sub>K(X<sub>n</sub>,X)]+b} <br/></p>
<p>⇒ G<sub>SVM</sub> = sign{[𝚺<sub>n=sv</sub>α<sub>n</sub>y<sub>n</sub>exp(-γ|X-X<sub>n</sub>|<sup>2</sup>)]+b} <br/></p>
</blockquote>
<p>看到這個式子你想到了什麼？有沒有融會貫通的感覺，你同樣的可以把上面的式子看成是Aggregation，又或者是Network。</p>
<p>先來定義一下RBF Function， 其實就是Gaussian Function，</p>
<p><strong>RBF Function: RBF(X,X<sub>n</sub>)=exp(-γ|X-X<sub>n</sub>|<sup>2</sup>)</strong></p>
<p>所以我們可以仿造SVM的形式來造一個Network，</p>
<p><strong>G=Output{[𝚺<sub>m</sub> β<sub>m</sub>RBF(X,μ<sub>m</sub>)]+b}</strong></p>
<p>當Output為sign Function、β<sub>m</sub>為α<sub>n</sub>y<sub>n</sub>就回到特例SVM了。</p>
<p>我們來細看這個式子傳遞的概念，RBF Network的第一層是先產生M組RBF(X,μ<sub>m</sub>)，意味著以這M個位置μ<sub>m</sub>當作中心點來評估各個X與它的相似程度，RBF是有評估相似度的味道，越接近μ<sub>m</sub>的點，RBF越大，並隨著與μ<sub>m</sub>距離變大，RBF的值也快速遞減，所以這M個μ<sub>m</sub>是有象徵性的，越接近它你越受它的影響。</p>
<p>決定了每一筆數據各是受哪些μ<sub>m</sub>影響，接下來第二層是由這M個代表性的位置來進行投票決定最後的結果，這意味的不同的地方μ<sub>m</sub>對最後結果也有不同的影響力，舉個例子，假設在SVM裡頭，某個μ<sub>m</sub>如果它的y<sub>m</sub>=+1，那它對最後的影響就會是正的；那如果某個μ<sub>m</sub>的y<sub>m</sub>=-1，那它對最後的影響就會是負的，所以一個點進來，先評估一下它和象徵性的幾個點μ<sub>m</sub>的距離，如果相鄰幾點都是正的，這個點最後的結果就會是正的。</p>
<p><img alt="RBF Network" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_05.png"></p>
<p>From: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf</a></p>
<p>RBF Network在歷史上是Neural Network的一個分支，不過從上面的介紹你就會發現，它們的結構是有差異的，演算法也就不一樣。</p>
<p>通常最佳化RBF Network做法是這樣的，我們會先用一些方法將μ<sub>m</sub>決定，如果μ<sub>m</sub>很懶惰的就直接使用所有的Training Data，總共有N個μ<sub>m</sub>，這就叫做<strong>「Full RBF Network」</strong>。<strong>我們也可以使用一些歸納的演算法找出代表資料群體的幾個象徵性的中心點，例如待會會介紹的K-Means的方法</strong>，找出k個μ<sub>m</sub>再做計算，這樣的RBF Network稱為<strong>「k Nearest Neighbor RBF Network」</strong>。</p>
<p>找到了μ<sub>m</sub>就已經決定了所有的RBF Function，接下來就可以線性組合這些RBF Function，我們可以使用Regression的方法來求取β<sub>m</sub>。</p>
<p>而如果你使用「Full RBF Network」，你會發現做完Regression後E<sub>in</sub>=0，這是典型的Overfitting，那這時你可能就要採用有Regularization的Regression啦！譬如說Ridge Regression之類的。</p>
<p><br/></p>
<h5><u>K-Means</u></h5>
<p><img alt="K-Means" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_06.png"></p>
<p>From: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/214_handout.pdf</a></p>
<p>接下來來看怎麼用K-Means找到代表資料群體的幾個象徵性的中心點。</p>
<p>首先，先決定要有幾個「中心點」，這裡假設我要有k個好了，接下來先隨機給這些「中心點」一個初始的位置，接下來根據數據的靠近程度開始歸類，如果一筆數據比較所有的「中心點」後發現離「中心點」A是最近的話，那這筆數據就歸「中心點」A了，就用這樣的規則把所有數據都做分類。</p>
<p>分完類後，接下來平均每一個資料群體裡的數據座標找出新的代表這個群體的「中心點」，然後又拿這個新的「中心點」根據數據的靠近程度再歸類一次，如此循環多次，直到收斂為止。這樣的話，這k個「中心點」收斂後會各自佔據四方，並且代表某個群體的中心點。我們就可以找到代表性的k個點，並拿這些點做「k Nearest Neighbor RBF Network」。</p>
<p><br/></p>
<h5><u>One-Hot Encoding</u></h5>
<p>討論這麼久的ML，我們還沒有討論過假設遇到「類別」要怎麼處理！</p>
<p><strong>通常遇到類別的狀況，我們還是需要把它轉換成數值或向量來處理，常見的方法叫做One-Hot Encoding。</strong></p>
<p>舉個例子，如果要描述血型應該要怎麼做？我們可是無法拿字串下去Regression的啊～此時就需要One-Hot Encoding，假設血型有A, B, AB, O四種，我們可以這樣設定，</p>
<p>A = [1, 0, 0, 0]<sup>T</sup></p>
<p>B = [0, 1, 0, 0]<sup>T</sup></p>
<p>AB = [0, 0, 1, 0]<sup>T</sup></p>
<p>O = [0, 0, 0, 1]<sup>T</sup></p>
<p>就是這麼簡單，這個動作就叫做One-Hot Encoding。</p>
<p><br/></p>
<h5><u>Matrix Factorization</u></h5>
<p><strong>那如果今天我的Input和Output都是類別，而我們想要讓機器自己去找到匹配Input和Output的機制，解決這個問題的方法稱之為Matrix Factorization。</strong></p>
<p><strong>Matrix Factorization精神上有點像是Autoencoder，Autoencoder找出隱含在Data裡的特性，而Matrix Factorization則是找出隱含的匹配關係。</strong></p>
<p>舉個例子，如果Netflix有了一堆用戶和他們曾看過的電影的資料，我們想要從中抽取出用戶與他愛看的電影之間的關係，所以這不單單只是匹配而已，單純匹配就只需要硬碟就做的到了，我們要做的是找出匹配的規律，並且用更少、更精簡的方式表示這個匹配關係，舉個例子，有可能有部分用戶會被歸納到愛看恐怖片的，並且同時這些客戶會被連結到具有恐怖元素的電影，我們預期Matrix Factorization會有自行歸納整理的能力。</p>
<p>可以仿造Autoencoder來設計Matrix Factorization，而你會發現Activation Function只要使用線性就已經足夠了，因為對於One-Hot Encoding的類別來說，只有一條通道是有效的，這已經具有開關的味道了，所以我們不用在Activation Function上面再弄一道開關，所以採用Linear就足夠了。</p>
<p><img alt="Matrix Factorization" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_07.png"></p>
<p>from: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf</a></p>
<p>因為是線性模型的緣故，我們可以很簡單的使用矩陣來描述，</p>
<p>Hypothesis: h(X) = W<sup>T</sup>VX</p>
<p>而如果是某一用戶，則</p>
<p>h(X<sub>n</sub>) = W<sup>T</sup>V<sub>n</sub></p>
<p>對某個用戶而言與他匹配的電影是一個向量，上面紀錄了他看過的電影，假設我再指定一部電影m，此時W<sub>m</sub><sup>T</sup>V<sub>n</sub>就代表這個用戶有沒有看過這部電影。</p>
<p>用這個方法來想問題，假設今天你把用戶和電影填成一個大的表格，或是矩陣，有交集的部分就打個勾，這個矩陣的每個元素表示成r<sub>nm</sub>，有打勾的部分r<sub>nm</sub>=1，沒打勾的部分r<sub>nm</sub>=0，那我們做的轉換W和V最終就是為了讓</p>
<p>W<sub>m</sub><sup>T</sup>V<sub>n</sub>≈r<sub>nm</sub></p>
<p>為了評估匹配的好壞，我們定義Error Function為</p>
<p>E<sub>in</sub>({W<sub>m</sub>},{V<sub>n</sub>}) = (1/𝚺<sub>m</sub> |D<sub>m</sub>|)×𝚺<sub>n,m</sub> (r<sub>nm</sub>-W<sub>m</sub><sup>T</sup>V<sub>n</sub>)<sup>2</sup></p>
<p>最佳化Matrix Factorization有兩個演算法，一個是Alternating Least Squares，另外一個是SGD。</p>
<p><br/></p>
<h5><u>Alternating Least Squares for Matrix Factorization</u></h5>
<p><img alt="Alternating Least Squares for Matrix Factorization" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_08.png"></p>
<p>from: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf</a></p>
<p>第一個方法是利用Linear Regression交互的優化W<sub>m</sub>和V<sub>n</sub>，我們的目標是使得W<sub>m</sub><sup>T</sup>V<sub>n</sub>=r<sub>nm</sub>，這式子可以用兩個角度看，如果固定W<sub>m</sub>，優化V<sub>n</sub>，那就是線性擬合{V<sub>n</sub>, r<sub>nm</sub>}的問題；那如果固定V<sub>n</sub>，優化W<sub>m</sub>，這就是線性擬合{W<sub>m</sub>, r<sub>nm</sub>}的問題。<strong>因此，交替優化W<sub>m</sub>和V<sub>n</sub>就可以使得W<sub>m</sub><sup>T</sup>V<sub>n</sub>越來越接近r<sub>nm</sub>了</strong>。</p>
<p><br/></p>
<h5><u>SGD for Matrix Factorization</u></h5>
<p><img alt="SGD for Matrix Factorization" src="https://www.ycc.idv.tw/media/MachineLearningTechniques/MachineLearningTechniques.000_09.png"></p>
<p>from: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/215_handout.pdf</a></p>
<p>第二個方法則是老招—Gradient Descent，這裡採用隨機的版本SGD，所以過程中我們會隨意的從(n,m)中挑點，然後根據Error Measure</p>
<p>E<sub>in</sub>({W<sub>m</sub>},{V<sub>n</sub>}) = (1/𝚺<sub>m</sub> |D<sub>m</sub>|)×𝚺<sub>n,m</sub> (r<sub>nm</sub>-W<sub>m</sub><sup>T</sup>V<sub>n</sub>)<sup>2</sup></p>
<p>我們就可以得到更新W<sub>m</sub>和V<sub>n</sub>的方法，詳細的方法見上圖所示。</p>
<p><strong>目前，SGD方法是處理大型Matrix Factorization最流行的作法。</strong></p>
<p><br/></p>
<h5><u>結語</u></h5>
<p>本篇介紹類似Neural Network的兩種Network結構，分別為Radial Basis Function (RBF) Network和Matrix Factorization。</p>
<p>在做RBF Network時，我們先找出幾個代表的中心，並評估一筆資料與這些中心的距離，再來再考慮不同中心對於答案的貢獻，加總起來可以預測這筆資料的答案，我們可以使用K-Means的方法來找出k點代表性的中心點來做RBF Network。</p>
<p>Matrix Factorization和Autoencoder有點類似，Autoencoder目標在於找出隱含在Data裡的特性，而Matrix Factorization則是找出隱含的匹配關係，並且介紹了兩種Matrix Factorization的演算法：Alternating Least Squares和SGD方法。</p>
<p>這系列的介紹文章，到這裡算是走到尾聲了，最後跟大家推薦一下老師的最後一堂課的投影片：</p>
<p><a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/216_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/216_handout.pdf</a></p>
<p>這個投影片裡頭林軒田教授用心的彙整了一整個學期的內容，很值得一看。</p></dd>
              
            	<dt>2017 / 4月 17</dt>
            	<dd><a href="../ml-course-techniques_6.html">機器學習技法 學習筆記 (6)：神經網路(Neural Network)與深度學習(Deep Learning)</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><blockquote>
<p>本篇內容涵蓋神經網路(Neural Network, NN)、深度學習(Deep Learning, DL)、反向傳播算法(Backpropagation, BP)、Weight-elimination Regularizer、Early Stop、Autoencoder、Principal Component Analysis (PCA)。</p>
</blockquote>
<p><br/></p>
<h5><u>神經網路(Neural Network)</u></h5>
<p>最後一個主題，我們要來講第三種「特徵轉換」— Extraction Models，其實就是現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，包括下圍棋的AlphaGo、Tesla的自動駕駛都是採用這一類的Machine Learning。</p>
<p>Extraction Models的基本款就是廣為人知的「神經網路」(Neural Network)，它的特色是使用神經元來做非線性的特徵轉換，那如果具有多層神經元，就是做了多次的非線性特徵轉換，這就是所謂的「深度學習」(Deep Learning)。</p>
<p><img alt="Neural Network" src="https://dl.dropbox.com/s/a7divvzh6mzfwvb/MachineLearningTechniques.016.jpeg"></p>
<p>上圖左側就是具有一層神經元的Neural Network，首先我們有一組特徵X，通常我們會加入一個維度X<sub>0</sub>=1，這是為了可以讓結構變得更好看，未來可以與W<sub>0</sub>相乘產生常數項。使用W來給予特徵X權重，最後總和的結果稱之為Score，s = W<sub>0</sub>X<sub>0</sub>+𝚺<sub>i=1</sub>W<sub>i</sub>X<sub>i</sub> = 𝚺<sub>i=0</sub>W<sub>i</sub>X<sub>i</sub>。</p>
<p>這個Score會被輸入到一個Activation Function裡頭，<strong>Activation Function的用意就是開關</strong>，當Score大於某個閥值，就打通線路讓這條路的貢獻可以繼續向後傳遞；當Score小於某個閥值，就關閉線路，所以Activation Function可以是Binary Function，但在實際操作之下不會使用像Binary Function這類不可以微分的Activation Function，所以我們會找具有相似特性但又可以微分的函數，例如tanh或者是ReLU這類比較接近開關效果的函數，經過Activation Function轉換後的輸出表示成g<sub>t</sub> = σ(𝚺<sub>i</sub>W<sub>i</sub>X<sub>i</sub>)，這個g<sub>t</sub>就稱為神經元、σ為Activation Function、𝚺<sub>i</sub> W<sub>i</sub>X<sub>i</sub>是Score。</p>
<p>如果我們有多組權重W就能產生多組神經元g<sub>t</sub>，然後最後把g<sub>t</sub>做線性組合並使用Output Function h(x)來衡量出最後的答案，Output Function可以是Linear Classification的Binary Function h(x)=sign(x)，不過一樣的問題，它不可以微分，通常不會被使用，常見的是使用Linear Regression h(x)=x，或者Logistic Regression h(x)=Θ(x)來當作Output Function，最後的結果可以表示成 y=h(𝚺<sub>t</sub>α<sub>t</sub>g<sub>t</sub>)，看到這個式子有沒有覺得很熟悉，它就像我們上一回講的Aggregation，將特徵X使用特徵轉換轉成使用g<sub>t</sub>表示，再來組合這些g<sub>t</sub>成為最後的Model，所以單層的Neural Network就使用到了Aggregation，它繼承了Aggregation的優點。</p>
<p>有了這個Model的形式了，我們可以使用Gradient Descent的手法來做最佳化，這也就是為什麼要讓操作過程當中所使用的函數都可以微分的原因。Gradient Descent在Neural Network的領域裡面發展出一套方法稱為Backpropagation，我們待會會介紹。<strong>因此實現Backpropagation，我只要餵Data進去，Model就會去尋找可以描述這組Data的特徵轉換g<sub>t</sub>，這就好像是可以從Data中萃取出隱含的Feature一樣，所以這類的Models才會被統稱為Extraction Models</strong>。</p>
<p><br/></p>
<h5><u>深度學習(Deep Learning)</u></h5>
<p>剛剛我們介紹了最基本款的Neural Network，那如果這個Neural Network有好幾層，我還會稱它為Deep Learning，所以基本上Deep Neural Network和Deep Learning是指同一件事，那為什麼會有兩個名字呢？其實是有歷史典故的。</p>
<p>Neural Network的歷史相當悠久，早在1958年就有人提出以Perceptron當作Activation Function的單層Neural Network，大家也知道一層的Neural Network是不Powerful的，所以在1969年，就有人寫了論文叫做「perceptron has limitation」，從那時Neural Network的方法就很少人研究了。</p>
<p>直到1980年代，有人開始使用多層的Neural Network，並在1989年，Yann LeCun博士等人就已經將反向傳播演算法(Backpropagation, BP)應用於Neural Network，當時Neural Network的架構已經和現在的Deep Learning很接近了，不過礙於當時的硬體設備計算力不足，Neural Network無法發揮功效，並且緊接的<strong>有人在1989年證明了只要使用一層Neural Network就可以代表任意函數，那為何還要Deep呢？</strong>所以Deep Neural Network這方法就徹底黑掉了。</p>
<p>一直到了最近，<strong>G. E. Hinton博士為了讓Deep Neural Network起死回生，重新給了它一個新名字「Deep Learning」</strong>，再加上他在2006年提出的RBM初始化方法，這是一個非常複雜的方法，所以在學術界就造成了一股流行，雖然後來被證明RBM是沒有用的，不過卻因為很多人參與研究Deep Learning的關係，也找出了解決Deep Learning痛處的方法，<strong>2009年開始有人發現使用GPU可以大大的加速Deep Learning</strong>，從這一刻起，Deep Learning就開始流行起來，直到去年的2016年3月，圍棋程式Alpha GO運用Deep Learning技術以4:1擊敗世界頂尖棋手李世乭，Deep Learning正式掀起了AI的狂潮。</p>
<p>聽完這個故事我們知道改名字的重要性XDD，不過大家是否還有看到什麼關鍵，「使用一層Neural Network就可以代表任意函數，那為何還要Deep呢？」這句話，這不就否定了我們今天做的事情了嗎？的確，使用一層的Neural Network就可以形成任意函數，但這一層的神經元也同樣需要無窮多個才做的到。<strong>Deep Learning的學習方法和人有點類似，我們在學習一個艱深的理論時，會先單元式的針對幾個簡單的概念學習，然後在整合這些概念去理解更高層次的問題</strong>，Deep Learning透過多層結構學習，雖然第一層的神經元沒有很多，能學到的也只是簡單的概念而已，不過第二層再重組這些簡單概念，第三層再用更高層次的方法看問題，所以同樣的問題使用一層Neural Network可能需要很多神經元才有辦法描述，但是Deep Learning卻可以使用更少的神經元做到一樣的效果。</p>
<p>另外，Deep Learning還有一個很大好處，就是比較不容易Overfitting，你可以想像一下如果我們使用100個神經元來造一個單層Neural Network，跟使用100個神經元來造一個五層的Neural Network，哪一個比容易Overfitting，當然是單層的Neural Network，多層Neural Network是使用一個從簡單到複雜的抽取特徵方法，所以它比較不易受到雜訊的影響，<strong>Deep Learning在建立多層「模組化」的過程可以抑制對於雜訊的過度反應，這等於是一種Regularization</strong>。</p>
<p><strong>因此，Deep Learning中每一層當中做了Aggregation，在增加模型複雜度的同時，也因為平均的效果而做到截長補短，這具有Regularization的效果，並且在採用多層且瘦的結構也同時因為「模組化」而做到Regularization，這就不難想像Deep Learning為何如此強大。</strong></p>
<p><br/></p>
<h5><u>反向傳播算法(Backpropagation, BP)</u></h5>
<p><img alt="Neural Network" src="https://dl.dropbox.com/s/khbpmalswll787f/MachineLearningTechniques.017.jpeg"></p>
<p>我們接下來就來看一下Deep Learning的演算法—反向傳播法，我們來看要怎麼從Gradient Descent來推出這個算法。</p>
<p>看一下上面的圖，我畫出了具有L層深的Deep Learning，每一層都有一個權重W<sub>ij</sub><sup>(ℓ)</sup>，因此我們可以估計出每一層的Score s<sub>j</sub><sup>(ℓ)</sup>= 𝚺<sub>i</sub> W<sub>ij</sub><sup>(ℓ)</sup>X<sub>i</sub><sup>(ℓ-1)</sup>，把Score s<sub>j</sub><sup>(ℓ)</sup>通過Activation Function，就可以得到下一層的Input，如此不斷的疊上去，直到最後一層L為Output Layer，Output最後的結果y，這裡我使用Linear Function來當作Output Function，這就是Deep Learning最簡單的架構。</p>
<p>而我們需要Training的就是這些權重W<sub>ij</sub><sup>(ℓ)</sup>，我們如何一步一步的更新W<sub>ij</sub><sup>(ℓ)</sup>，使得它可以Fit數據呢？回想一下Gradient Descent的流程：</p>
<ol>
<li>定義出Error函數</li>
<li>Error函數讓我們可以去評估E<sub>in</sub></li>
<li>算出它的梯度∇E<sub>in</sub></li>
<li>朝著∇E<sub>in</sub>的反方向更新參數W，而每次只跨出η大小的一步</li>
<li>反覆的計算新參數W的梯度，並一再的更新參數W</li>
</ol>
<p>假設使用平方誤差的話，Error函數在這邊就是</p>
<p>L = (1/2) (y-ŷ)<sup>2</sup>，</p>
<p>因此我們的更新公式可以表示成</p>
<p>W<sub>ij</sub><sup>(ℓ)</sup> ←  W<sub>ij</sub><sup>(ℓ)</sup>-η×∂L/∂W<sub>ij</sub><sup>(ℓ)</sup> </p>
<p>那我們要怎麼解這個式子呢？關鍵就在∂L/∂W<sub>ij</sub><sup>(ℓ)</sup>這項要怎麼計算，這一項在Output Layer (ℓ=L)是很好計算的，</p>
<p>∂L/∂W<sub>ij</sub><sup>(L)</sup></p>
<p>= {∂L/∂s<sub>j</sub><sup>(L)</sup>}×{∂s<sub>j</sub><sup>(L)</sup>/∂W<sub>ij</sub><sup>(L)</sup>}  (連鎖率)</p>
<p>= {δ<sub>j</sub><sup>(L)</sup>}×{X<sub>i</sub><sup>(L-1)</sup>}</p>
<p>上式當中我們使用了微分的連鎖率，並且令</p>
<p><strong>δ<sub>j</sub><sup>(L)</sup> = ∂L/∂s<sub>j</sub><sup>(L)</sup></strong></p>
<p>δ<sub>j</sub><sup>(L)</sup>這一項被稱為Backward Pass Term，而X<sub>i</sub><sup>(L-1)</sup>這項被稱為Forward Pass Term，所以L層權重的更新取決於Forward Pass Term和Backward Pass Term相乘δ<sub>j</sub><sup>(L)</sup>×X<sub>i</sub><sup>(L-1)</sup>。</p>
<p>我們先來看一下L層的Forward Pass Term要怎麼計算，X<sub>i</sub><sup>(L-1)</sup>這項是很容易求的，我們只要讓數據一路從0層傳遞上來就可以自然而然的得到X<sub>i</sub><sup>(L-1)</sup>的值，所以我們會稱X<sub>i</sub><sup>(L-1)</sup>這一項為Forward Pass Term，因為我們必須要往前傳遞才可以得到這個值。</p>
<p>再來看一下L層的Backward Pass Term要怎麼計算，δ<sub>j</sub><sup>(L)</sup>一樣是很容易求得的，</p>
<p>δ<sub>j</sub><sup>(L)</sup> = ∂L/∂s<sub>j</sub><sup>(L)</sup> = ∂[(1/2) (y-ŷ)<sup>2</sup>]/∂y = (y-ŷ)</p>
<p>你會發現這一項的計算需要得到誤差的資訊，而誤差資訊要等到Forward的動作做完才有辦法得到，所以資訊的傳遞方向是從尾巴一路回到頭，是一個Backword的動作。</p>
<p>因此，最後一層也是Output Layer的更新公式如下：</p>
<p>W<sub>ij</sub><sup>(L)</sup> ←  W<sub>ij</sub><sup>(L)</sup>-η×δ<sub>j</sub><sup>(L)</sup>×X<sub>i</sub><sup>(L-1)</sup></p>
<p>權重的更新取決於Input和Error的影響，需要考慮Forward Pass Term和Backward Pass Term。</p>
<p>那除了Output這一層以外的權重應該怎麼更新？來看一下(ℓ)層，</p>
<p>∂L/∂W<sub>ij</sub><sup>(ℓ)</sup></p>
<p>= {∂L/∂s<sub>j</sub><sup>(ℓ)</sup>}×{∂s<sub>j</sub><sup>(ℓ)</sup>/∂W<sub>ij</sub><sup>(ℓ)</sup>} (連鎖率)</p>
<p>= δ<sub>j</sub><sup>(ℓ)</sup>×X<sub>i</sub><sup>(ℓ-1)</sup></p>
<p>一樣是Forward Pass Term和Backword Pass Term相乘，不過δ<sub>j</sub><sup>(ℓ)</sup>這一項的計算有點技巧性，來看一下，</p>
<p>δ<sub>j</sub><sup>(ℓ)</sup></p>
<p>= ∂L/∂s<sub>j</sub><sup>(ℓ)</sup></p>
<p>= 𝚺<sub>k</sub> {∂L/∂s<sub>k</sub><sup>(ℓ+1)</sup>}×{∂s<sub>k</sub><sup>(ℓ+1)</sup>/∂X<sub>jk</sub><sup>(ℓ)</sup>}×{∂X<sub>jk</sub><sup>(ℓ)</sup>/∂s<sub>j</sub><sup>(ℓ)</sup>} (連鎖率)</p>
<p>= 𝚺<sub>k</sub> {δ<sub>k</sub><sup>(ℓ+1)</sup>}×{W<sub>jk</sub><sup>(ℓ)</sup>}×{σ'(s<sub>j</sub><sup>(ℓ)</sup>)}</p>
<p>W<sub>jk</sub><sup>(ℓ)</sup>和σ'(s<sub>j</sub><sup>(ℓ)</sup>)都是Forward之後就會得到的資訊，而δ<sub>k</sub><sup>(ℓ+1)</sup> 而是需要Backward才可以得到，我們已經知道δ<sub>j</sub><sup>(ℓ=L)</sup>的值，就可以從δ<sub>j</sub><sup>(ℓ=L)</sup>開始利用上面的公式，一路Backward把所有的δ<sub>j</sub>都找齊。好！那現在我們已經找到了更新所有Weights的方法了。</p>
<p>看一下上圖中的最下面的Flow，一開始我們Forward，把所有X和s都得到，到了Output Layer，我們得到了δ<sub>j</sub><sup>(ℓ=L)</sup>，再Backward回去找出所有的δ，接下來就可以用Forward Pass Term和Backword Pass Term來Update所有的W了。</p>
<p>總結一下，反向傳播算法(Backpropagation, BP)更新權重的方法為</p>
<blockquote>
<p><strong>W<sub>ij</sub><sup>(ℓ)</sup> ←  W<sub>ij</sub><sup>(ℓ)</sup>-η×δ<sub>j</sub><sup>(ℓ)</sup>×X<sub>i</sub><sup>(ℓ-1)</sup>  <br/></strong></p>
<p><strong>If output layer (ℓ=L), δ<sub>j</sub><sup>(ℓ=L)</sup>=(y-ŷ)  <br/></strong></p>
<p><strong>If other layer, δ<sub>j</sub><sup>(ℓ)</sup>= σ'(s<sub>j</sub><sup>(ℓ)</sup>) × 𝚺<sub>k</sub> δ<sub>k</sub><sup>(ℓ+1)</sup>×W<sub>jk</sub><sup>(ℓ)</sup>  <br/></strong></p>
<p><strong>δ<sub>j</sub><sup>(ℓ)</sup>為Backword Pass Term；X<sub>i</sub><sup>(ℓ-1)</sup>為Forward Pass Term。</strong></p>
</blockquote>
<p><br/></p>
<h5><u>Regularization in Deep Learning</u></h5>
<p>那麼像是Deep Learning這麼複雜的Model，我們要怎麼避免Overfitting呢？有五個方法。</p>
<p>第一個方法，就是我們剛剛提過的<strong>「設計Deep Neural Network的結構」</strong>，藉由限縮一層當中的神經元來達到一種限制，做到Regularization。</p>
<p>第二個方法是<strong>「限制W的大小」</strong>，和標準Regularization作一樣的事情，我們將W的大小加進去Cost裡頭做Fitting，例如使用L2 Regularizer Ω(W)=𝚺(W<sub>jk</sub><sup>(ℓ)</sup>)<sup>2</sup>，但這樣使用有一個問題就是W並不是Sparse的，L2 Regularizer在抑制W的方法是，如果W的分量大的話就抑制多一點，如果分量小就抑制少一點（因為W<sup>2</sup>微分為一次），所以最後會留下很多很小的分量，造成計算量大大增加，尤其像是Deep Learing這麼龐大的Model，這樣的Regularization顯然不夠好，L1 Regularizer顯然可以解決這個問題（因為在大部分位置微分為常數），但不幸的是它無法微分，所以就有了L2 Regularizer的衍生版本，</p>
<p>Weight-elimination L2 regularizer: 𝚺[(W<sub>jk</sub><sup>(ℓ)</sup>)<sup>2</sup>]/[1+(W<sub>jk</sub><sup>(ℓ)</sup>)<sup>2</sup>]</p>
<p>這麼一來不管W大或小，它受到抑制的值大小接近的 (Weight-elimination L2 regularizer微分為 -1次方)，因此就可以使得部分W可以為0，大大便利於我們做計算。</p>
<p>第三種方法是最常使用的<strong>「Early Stopping」</strong>，所謂的Early Stopping就是，在做Backpropagation的過程去觀察Validation Data的Error有沒有脫離Training Data的Error太多，如果開始出現差異，我們就立刻停止計算，這樣就可以確保Model裡的參數沒有使得Model產生Overfitting，是一個很直接的作法。</p>
<p>第四種方法是<strong>「Drop-out」</strong>，在Deep Learing Fitting的過程中，隨機的關閉部分神經元，藉由這樣的作法使得Fitting的過程使用較少的神經元，並且使得結構是瘦長狀的，來達到Regularization。</p>
<p>第五種方法是接下來會用更大篇幅介紹的<strong>「Denoising Autoencoder」</strong>，在Deep Neural Network前面加入這樣的結構有助於抑制雜訊。</p>
<p><br/></p>
<h5><u>Autoencoder</u></h5>
<p><img alt="Regularization in Deep Learning" src="https://dl.dropbox.com/s/qnyy3uyyszq45yx/MachineLearningTechniques.018.jpeg"></p>
<p>Neural Network針對不同需要發展出很多不同的型態，包括CNN, RNN，還有接下來要介紹的Autoencoder，<strong>Autoencoder是一種可以將資料重要資訊保留下來的Neural Network</strong>，效果有點像是資料壓縮，在做資料壓縮時，會有一個稱為Encoder的方法可以將資料壓縮，那當然還要有另外一個方法將它還原回去，這方法稱為Decoder，壓縮的過程就是用更精簡的方式保存了資料。<strong>Autoencoder同樣的有Encoder和Decoder，不過它不像資料壓縮一樣可以百分之一百還原，不過特別之處是Autoencoder會試著從Data中自己學習出Encoder和Decoder，並盡量讓資料在壓縮完了可以還原回去原始數據</strong>。</p>
<p>見上圖中Basic Autoencoder的部分，透過兩層的轉換，我們試著讓Input X可以完整還原回去，通常中間這一層會使用比較少的神經元，因為我們想要將資訊做壓縮，所以第一層的部分就是一個Encoder，而第二層則是Decoder，他們由權重W<sub>jk</sub><sup>(ℓ)</sup>決定，而在Training的過程，Autoencoder會試著找出最好的W<sub>jk</sub><sup>(ℓ)</sup>來使得資訊可以盡量完整還原回去，這也代表Autoencoder可以自行找出了Encoder和Decoder。</p>
<p><strong>Encoder這一段就是在做一個Demension Reduction</strong>，Encoder轉換原本數據到一個新的空間，這個空間可以比原本Features描述的空間更能精準的描述這群數據，而中間這層Layer的數值就是新空間裡頭的座標，有些時候我們會用這個新空間來判斷每筆Data之間彼此的接近程度。</p>
<p>我們也可以讓Encoder和Decoder可以設計的更複雜一點，所以你同樣的可以使用多層結構，稱之為Deep Autoencoder。另外，也有人使用Autoencoder的方法來Pre-train Deep Neural Network的各個權重。</p>
<p>緊接著介紹兩種特殊的例子，第一個是Linear Autoencoder，我們把所有的Activation Function改成線性的，這個方法可以等效於待會要講的Principal Component Analysis (PCA)的方法，PCA是一個全然線性的方法，所以它的效力會比Autoencoder差一點。</p>
<p>第二個是剛剛提到的Denoising Autoencoder，我們在原本Autoencoder的前面加了一道增加人工雜訊的流程，但是又要讓Autoencoder試著去還原出原來沒有加入雜訊的資訊，這麼一來<strong>我們將可以找到一個Autoencoder是可以消除雜訊的</strong>，把這個Denoising Autoencoder加到正常Neural Network的前面，那這個Neural Network就擁有了抑制雜訊的功用，所以可以當作一種Regularization的方法。</p>
<p><br/></p>
<h5><u>Principal Component Analysis (PCA)</u></h5>
<p>最後來講一下Principal Component Analysis (PCA)，它不太算是Deep Learning的範疇，不過它是一個傳統且重要的Dimension Reduction的方法，我們就來看一下。</p>
<p><img alt="PCA" src="https://dl.dropbox.com/s/4ek9k9g8vrwnwia/MachineLearningTechniques.019.jpeg"></p>
<p>PCA的演算法是這樣的，第一步先求出資料Features的平均值，並且將各個Features減掉平均值，令為ζ，第二步求出由ζ<sup>T</sup>ζ產生的矩陣的Eigenvalue和Eigenvector，第三步，從這些Eigenvalue和Eigenvector中挑選前面k個，並組成轉換矩陣W，而最終PCA的轉換就是Φ(x)=W<sup>T</sup>(X-mean(X))，這個轉換做的就是Dimension Reduction，將數據降維到k維。</p>
<p>PCA做的事是這樣的，每一個Eigenvector代表新空間裡頭的一個軸，而Eigenvalue代表站在這個軸上看資料的離散程度，當然我們如果可以描述每筆資料越分離，就代表這樣的描述方法越好，所以Eigenvalue越大的Eigenvector越是重要，<strong>所以取前面k個Eigenvector的用意是在降低維度的過程，還可以盡量的保持對數據的描述力，而且Eigenvector彼此是正交的，也就是說在新空間裡頭的每個軸是彼此垂直，彼此沒有Dependent的軸是最精簡的，所以PCA所做的Dimension Reduction一定是線性模型中最好、最有效率的</strong>。</p>
<p>另外，剛剛有提到的Linear Autoencode幾乎是等效於PCA，大家可以看上圖中的描述，這裡不多贅述，不過不同的是，Linear Autoencoder並沒有限制新空間軸必須是正交的特性，所以它的效率一定會比PCA來的差。</p>
<p><br/></p>
<h5><u>結語</u></h5>
<p>這一篇當中，我們介紹了Neural Network，並且探討多層Neural Network—Deep Neural Network，也等同於Deep Learning，並且說明為什麼需要「Deep」，然後介紹Deep Learning最重要的演算法—反向傳播算法，接著介紹五種常用的Regularization的方法：設計Deep Neural Network的結構、限制W的大小、Early Stopping、Drop-out和Denoising Autoencoder。</p>
<p>介紹完以上內容，我們就已經對於Deep Learning的全貌有了一些認識了，緊接著來看Deep Learning的特殊例子—Autoencoder，Autoencoder可以用來做Dimension Reduction，那既然提到了Dimension Reduction，那就不得不在講一下重要的線性方法PCA。</p>
<p>那在下一回，我們會繼續探討Neural Network還有哪些特殊的分支。</p></dd>
              
            	<dt>2017 / 4月 02</dt>
            	<dd><a href="../ml-course-techniques_5.html">機器學習技法 學習筆記 (5)：Boost Aggregation Models</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><blockquote>
<p>本篇內容涵蓋AdaBoost (Adaptive Boost)、Gradient Boost、AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)。</p>
</blockquote>
<h5><u>Boost的精髓</u></h5>
<p>在上一回當中，我們介紹的Aggregation Models都屬於沒有Boost的，不管是Bagging或Decision Tree都沒有要試著在Training的過程中改善Model，<strong>而這篇將要提到的Boost方法，則是在產生每個g<sub>t</sub>時試圖讓Model整體更完善，更能發揮Aggregation Models中截長補短中的「補短」的效果，也就是說g<sub>t</sub>可以彼此互補不足之處</strong>。</p>
<p>那實際上我們應該怎麼做才能實踐Boost呢？其實方法的道理早就透漏在上一回中的Bagging和Decision Tree裡頭了，不管是Bagging和Decision Tree都是使用變換Data來做到變異度，在這個方法下Model的架構可以本身是不變的，這帶來相當的便利性，而今天我們要講的Boost也同樣的利用「變換Data」來做到變異度，但不同的是Boost的過程中「變換Data」這件事是有目標性的。</p>
<p><strong>Boost方法在「變換Data」時會試著去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的g<sub>t</sub>可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善，這就是Boost的主要精髓。</strong></p>
<p><br/></p>
<h5><u>AdaBoost (Adaptive Boost) for Classification</u></h5>
<p>剛剛上一段的最後我已經揭露了Boost的真正精髓，拿這樣的概念來做分類問題，就是我們接下來要談的AdaBoost，全名稱為Adaptive Boost。</p>
<p>在分類問題中我們怎麼做到「凸顯原先做錯的Data」？簡單的想法是這樣的，我們可以減少原本已經是正確分類的Data的數量，然後增加原本錯誤分類的Data的數量，<strong>增減Data的數量其實是等效於改變每筆Data的權重</strong>，假如我們給每筆資料權重，要做的事是拉低正確分類Data的權重，而且拉高錯誤分類Data的權重。</p>
<p>那我們應該要提升權重或降低權重到什麼程度才是OK的呢？換個方式思考，我們為什麼要去調整權重？目的其實是要去凸顯原先做錯的部分，降低原本做對的部分，也就是想<strong>藉由調整每筆Data的多寡或權重來做到「弭平原先的預測性」，最好可以讓原本的預測方法看起來是隨機分布</strong>，也就是「錯誤率＝正確率」，讓它像是擲銅板一樣，沒有什麼預測能力。</p>
<p><img alt="AdaBoost" src="https://dl.dropbox.com/s/n61vejw6rs6f9nm/MachineLearningTechniques.012.jpeg"></p>
<p>有了概念之後，我們來看實際應該要怎麼做？見上圖說明，首先我們需要先將Data權重u<sup>(1)</sup>先初始化，接下來就可以開始找g<sub>t</sub>了，我們使用任意一個分類問題的Model搭配上Data的權重，求得一組g<sub>t</sub>，接下來計算這組g<sub>t</sub>的<strong>「錯誤率」ε<sub>t</sub></strong>，</p>
<p><strong>ε<sub>t</sub>= 𝚺<sub>n</sub> u<sub>n</sub><sup>(t)</sup> ⟦y<sub>n</sub>≠g<sub>t</sub>(x<sub>n</sub>)⟧ / 𝚺<sub>n</sub> u<sub>n</sub><sup>(t)</sup></strong></p>
<p>有注意到考慮「錯誤率」ε<sub>t</sub>的時候必須要評估u<sub>n</sub><sup>(t)</sup>，要記得會有Data權重是為了表示增加或減少原本的Data的數量，所以依照每筆Data的出現機會不同，會有不同的權重，也就會有對「錯誤率」不同的貢獻程度。</p>
<p>那為了待會要對權重重新分配，我們先定義了β<sub>t</sub>，在未來我會將錯誤的Data的權重乘上β<sub>t</sub>，即u<sub>n</sub><sup>(t+1)</sup>=u<sub>n</sub><sup>(t)</sup>×β<sub>t</sub>，並且把正確的Data權重除以β<sub>t</sub>，即u<sub>n</sub><sup>(t+1)</sup>=u<sub>n</sub><sup>(t)</sup>/β<sub>t</sub>，<strong>而期望的結果是重新分配的Dataset在g<sub>t</sub>的預測下可以表現的像隨機的一樣，於是乎下一次使用這組Dataset訓練出來的g<sub>t+1</sub>將會彌補g<sub>t</sub>的不足</strong>，根據這樣的原則我們來推一下β<sub>t</sub>，</p>
<p>𝚺<sub>n</sub> u<sub>n</sub><sup>(t+1)</sup> ⟦y<sub>n</sub>≠g<sub>t</sub>(x<sub>n</sub>)⟧ / 𝚺<sub>n</sub> u<sub>n</sub><sup>(t+1)</sup>=1/2 (預測能力像隨機分布)</p>
<p>⇒  𝚺<sub>n</sub> u<sub>n</sub><sup>(t+1)</sup> ⟦y<sub>n</sub>≠g<sub>t</sub>(x<sub>n</sub>)⟧ = 𝚺<sub>n</sub> u<sub>n</sub><sup>(t+1)</sup> ⟦y<sub>n</sub>=g<sub>t</sub>(x<sub>n</sub>)⟧ </p>
<p>⇒  𝚺<sub>n</sub> (u<sub>n</sub><sup>(t)</sup>×β<sub>t</sub>)  ⟦y<sub>n</sub>≠g<sub>t</sub>(x<sub>n</sub>)⟧ = 𝚺<sub>n</sub> (u<sub>n</sub><sup>(t)</sup>/β<sub>t</sub>) ⟦y<sub>n</sub>=g<sub>t</sub>(x<sub>n</sub>)⟧ </p>
<p>⇒  β<sub>t</sub><sup>2</sup> = 𝚺<sub>n</sub> u<sub>n</sub><sup>(t)</sup> ⟦y<sub>n</sub>=g<sub>t</sub>(x<sub>n</sub>)⟧ / 𝚺<sub>n</sub> u<sub>n</sub><sup>(t)</sup>  ⟦y<sub>n</sub>≠g<sub>t</sub>(x<sub>n</sub>)⟧</p>
<p>⇒  β<sub>t</sub><sup>2</sup> = [𝚺<sub>n</sub> u<sub>n</sub><sup>(t)</sup> ⟦y<sub>n</sub>=g<sub>t</sub>(x<sub>n</sub>)⟧ /  𝚺<sub>n</sub> u<sub>n</sub><sup>(t)</sup>]/ [𝚺<sub>n</sub> u<sub>n</sub><sup>(t)</sup>  ⟦y<sub>n</sub>≠g<sub>t</sub>(x<sub>n</sub>)⟧ / 𝚺<sub>n</sub> u<sub>n</sub><sup>(t)</sup> ]</p>
<p>⇒  <strong>β<sub>t</sub><sup>2</sup> = 1-ε<sub>t</sub> / ε<sub>t</sub></strong></p>
<p>所以我們就可以利用這個β<sub>t</sub>來更新我的Data權重，並且在多次迭代後，得到很多個g<sub>t</sub>。而將來我們會把所有的g<sub>t</sub>做線性組合，而我們希望<strong>「錯誤率」越低的g<sub>t</sub>可以有更高的貢獻度α<sub>t</sub></strong>，所以使用β<sub>t</sub>緊接著計算「g<sub>t</sub>的權重」α<sub>t</sub>，定義為</p>
<p><strong>α<sub>t</sub> = ln(βt)</strong></p>
<p>所以當一個百分之一百可以完全預測的g<sub>t</sub>出現時，它的ε<sub>t</sub>=0，此時它的β<sub>t</sub> →∞，同時α<sub>t</sub> →∞，所以這樣的g<sub>t</sub>會有完全的貢獻。</p>
<p>如果一個預測效果很差的g<sub>t</sub>出現，它的ε<sub>t</sub>=1/2，此時它的β<sub>t</sub>=1，同時α<sub>t</sub>=0，所以這樣的g<sub>t</sub>並沒有任何參考價值。</p>
<p>那如果出現一個g<sub>t</sub>它的ε<sub>t</sub> &gt; 1/2，那這樣的g<sub>t</sub>並不能說它沒有用處，反而是一個很好的反指標，我們只需要反著看就好了，當ε<sub>t</sub> &gt; 1/2時，β<sub>t</sub> &lt; 1，所以α<sub>t</sub> &lt; 0，這樣的g<sub>t</sub>具有逆向的貢獻。</p>
<p>最後只要把這些訓練好的g<sub>t</sub>乘上各自的α<sub>t</sub>再加總起來，我們就完成了AdaBoost啦！</p>
<p><br/></p>
<h5><u>Gradient Boost for Regression</u></h5>
<p>剛剛我們講了AdaBoost，是個很神奇的方法，當我們做錯了，沒關係！從哪裡跌倒就從哪裡站起來，利用這種精神我們就可以做到Boost的效果，但美中不足的是上面的方法只能用在「分類問題」上，那如果我也想在「Regression問題」也做到Boost呢？這就是接下來要講的GradientBoost的方法。</p>
<p>在課程中林軒田教授是從AdaBoost出發經過推導後，得到一個很像是Gradient Decent的式子，接下來將式子一般化成為可以使用任意Error Measure的形式，我稍微列一下：</p>
<blockquote>
<p>GradientBoost: min<sub>η</sub> min<sub>h</sub> (1/N) 𝚺<sub>n</sub> err[𝚺<sub>τ=1~t-1</sub> α<sub>τ</sub> g<sub>τ</sub>(x<sub>n</sub>) + η h(x<sub>n</sub>), y<sub>n</sub>]</p>
</blockquote>
<p>我們這邊會考慮err為平方誤差(s-y)<sup>2</sup>的結果，詳細的推導這邊就不多加討論，可以到影片中學習，這裡我想要從我觀察出來的觀點，概念性的來看這個GradientBoost的方法。</p>
<p>「從哪裡跌倒就從哪裡站起來」就是Boost的精神，所以今天你有一個Regression問題沒做好，<strong>留下了餘數Residual，怎麼辦？那我就把這個餘數當作另外一個Regression問題來做它</strong>，再把這個結果附到先前的那個就好啦！如果第一次Regression後的Model是g<sub>1</sub>(x)，那剩下的沒做好的餘數就應該是y(x)-g<sub>1</sub>(x)，我們拿這個餘數下去在做一次Regression得到另外一個Model g<sub>2</sub>(x)，此時合併這兩個結果的餘數就變成了y(x)-g<sub>1</sub>(x)-g<sub>2</sub>(x)，就可以使用這個餘數繼續做下去，最後組合所有的g<sub>t</sub>(x)就會得到一個更好的Model。</p>
<p><img alt="Gradient Boost" src="https://dl.dropbox.com/s/dy5xu7ifew5dfn4/MachineLearningTechniques.013.jpeg"></p>
<p>依循這樣的概念我們來看GradientBoost作法，如上圖，一開始我們先初始化每一筆Data的預測值s<sub>n</sub>為0，再接下來開始產生g<sub>t</sub>，我們先把Data的 y<sub>n</sub> 減去每一筆Data當前的預測值s<sub>n</sub>，就會產生餘數(y<sub>n</sub>-s<sub>n</sub>)，當然，在一開始s<sub>n</sub>=0，所以y<sub>n</sub>-s<sub>n</sub>=y<sub>n</sub>，等於是對原問題求解。</p>
<p>接下來因為最後我們要線性組合g<sub>t</sub>(x)，所以需要決定g<sub>t</sub>(x)前面的係數α<sub>t</sub>，也就是貢獻度，這個α<sub>t</sub>的決定方式是去求解一個One-Variable-Linear-Regression (單變數線性迴歸)，目的是<strong>去縮放g<sub>t</sub>(x)使得它更接近剛剛的餘數(y<sub>n</sub>-s<sub>n</sub>)，而找到這個縮放值就是α<sub>t</sub></strong>。所以每一次g<sub>t</sub>(x)的產生都是為了可以把G(x)描述的更好，最後G(x)=𝚺<sub>t</sub> α<sub>t</sub>g<sub>t</sub>(x)。</p>
<p>看到這裡有人一定會認為One-Variable-Linear-Regression求α<sub>t</sub>這一步是多餘的，因為在一開始做{x<sub>n</sub>,y<sub>n</sub>-s<sub>n</sub>}的Regression中我們已經最佳化過g<sub>t</sub>(x)，那為什麼還要把g<sub>t</sub>(x)乘上α<sub>t</sub>再做同樣的事呢？α<sub>t</sub>一定是1的啊！就像我一開始舉的例子一樣啊！其實問題就出在於你把g<sub>t</sub>(x)理所當然的看成是線性模型，你才會覺得這一步是多餘的，如果g<sub>t</sub>(x)不是線性的，求α<sub>t</sub>就很重要的，因為你要使用線性組合來組出G(x)，但是你的g<sub>t</sub>(x)不是線性的，所以你只好在外面再用線性模型來包裝一遍。</p>
<p><br/></p>
<h5><u>AdaBoosted Decision Tree和Gradient Boosted Decision Tree (GBDT)</u></h5>
<p><img alt="AdaBoosted and GrandientBoosted DTree" src="https://dl.dropbox.com/s/zm43nardbpkyr4n/MachineLearningTechniques.014.jpeg"></p>
<p>和Random Forest一樣，我們也可以將AdaBoost和GradientBoost套用到Decision Tree上面，<strong>如果是處理分類問題就使用AdaBoosted Decision Tree；那如果是處理Regression問題可以使用Gradient Boosted Decision Tree</strong>。</p>
<p>但要特別注意的是，這邊的Decision Tree都必須是弱的，也就是Pruning過後的樹，如果直接使用完全長成的樹，你會發現在AdaBoosted Decision Tree中，因為ε<sub>t</sub>=0所以α<sub>t</sub>→∞；在Gradient Boosted Decision Tree中，y<sub>n</sub>-s<sub>n</sub>→0，因為錯誤出現的太少了，所以造成我們不能真正使用到Boost的效果，也就失去做Boost的意義了，<strong>因此在做AdaBoosted Decision Tree或Gradient Boosted Decision Tree時要使用「弱」一點的Decision Tree</strong>。</p>
<p><br/></p>
<h5><u>結語</u></h5>
<p>這一篇當中，我們完整提了Boost的方法，Boost的精神就是從哪裡跌倒就從哪裡站起來，使用變換Data權重的手法去凸顯原先做錯的Data，而降低原本已經做對的Data，藉由這樣的方法訓練出來的g<sub>t</sub>可以補齊前面的不足，所以Boost的過程將會使得Model漸漸的完善。</p>
<p>我們提了兩種Boost的方法，如果是處理分類問題就使用AdaBoost；如果是處理Regression問題可以使用GradientBoost，而且這兩種方法都可以和Decision Tree做結合。</p>
<p>以上兩回，我們已經完成了Aggregation Models了，接下來的下一回將要探討的就是現今很流行的類神經網路和深度學習等等。</p></dd>
              
            	<dt>2017 / 3月 29</dt>
            	<dd><a href="../ml-course-techniques_4.html">機器學習技法 學習筆記 (4)：Basic Aggregation Models</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><blockquote>
<p>本篇內容涵蓋Blending、Bagging、Decision Tree和Random Forest。</p>
</blockquote>
<h5><u>綜觀Aggregation Models</u></h5>
<p>如果今天我有很多支的Model，我有辦法融合他們得到更好的效果嗎？</p>
<p>這就是Aggregation Models的精髓，Aggregation Models藉由類似於投票的方法綜合各個子Models的結果得到效果更好的Model。換個角度看，你可以把整個體系看成一個新的Model，而原本這些子Models當作轉換過後的新Features，<strong>所以Aggregation Model裡頭做了「特徵轉換」，這個特徵轉換產生出許多有預測答案能力的Features，稱為Predictive Features，然後再綜合它們得到最後的Model</strong>。</p>
<p><img alt="Aggregation Models" src="https://dl.dropbox.com/s/ibdowsfjwy0z7zm/MachineLearningTechniques.007.jpeg"></p>
<p>Aggregation Models可以分成兩大類，第一種的作法比較簡單，先Train出一個一個獨立的Predictive Features，然後在綜合它們，<strong>「集合」的動作是發生在得到Train好的Predictive Feature之後，這叫做「Blending Models」</strong>；第二種作法則是，<strong>「集合」的動作和Training同步進行，這叫做「Aggregation-Learning Models」</strong>，Aggregation-Learning Models有一個特殊的例子叫做Boost，翻開字典查Boost的意思是「促進」，在這邊的意義是<strong>假設在Training過程所產生的Predictive Feature朝著改善Model的方向前進就叫做Boost</strong>。</p>
<p>從「集合」的方法上也可以進一步細分三種類型，有票票等值的<strong>「Uniform Aggregation Type」</strong>，有給予Predictive Features不同權重的<strong>「Linear Aggregation Type」</strong>，甚至還可以用條件或任意Model來分配Predictive Features，這叫做<strong>「Non-linear Aggregation Type」</strong>。</p>
<p>所以兩種類型、三種Aggregation Type，交互產生各類的Aggregation Models。有Blending的三種Aggregation Type，Aggregation-Learning的Uniform Type—Bagging，再加上Aggregation-Learning的Linear Type兩種—AdaBoost和GradientBoost，這兩種也亦是Boost的方法，AdaBoost負責處理Classification的問題，而GradientBoost則負責處理Regression的問題，最後介紹Aggregation-Learning的Non-Linear Type—Decision Tree。然後接著，使用Decision Tree結合其他方法再進一步的產生Random Forest、AdaBoost Decision Tree和GradientBoost Decision Tree。</p>
<p>我將會分兩篇來介紹Aggregation Models，一篇介紹沒Boost的部分，就是今天這一篇，另外一篇則是來專攻有Boost的部分。</p>
<p><br/></p>
<h5><u>Blending</u></h5>
<p><strong>Blending是泛指在Training結束之後得到幾個Predictive Features，然後再對這些Predictive Features做集合的方法</strong>。</p>
<p><img alt="Blending" src="https://dl.dropbox.com/s/kotwynmp51p457q/MachineLearningTechniques.008.jpeg"></p>
<p>如上圖，基本流程是這樣的，一開始先把Data切成一部分拿來Training，另外一部分拿來Validation，這部份很重要，因為我們待會要利用Validation的Error來決定每筆Predictive Feature對Model的貢獻分配比重；接下來使用不同的方法來產生不同的Predictive Features g<sub>t</sub>，來源可能是不同的Model形式、不同的參數變化、不同的隨機情形等等；有了各類的g<sub>t</sub>之後，我們就可以選擇使用怎樣的方式來結合它們，如果是Uniform Combination，就直接平均所有g<sub>t</sub>就可以了，那如果是Linear Combination，想當然爾就是使用線性模型來結合，那如果是Non-Linear Combination，你可以使用任意Model來描述也行；決定好結合方式了，也就同時決定了「特徵轉換」的方法，接下來出動Validation Data，使用這個「特徵轉換」來轉化Validation Data並且做Fitting，最後我們會找到一組解最佳的參數來確定結合的方法，如果是Uniform Combination是不需要這一步的，基本上你得到g<sub>t</sub>就直接平均就得到結果了，而Linear Combination則是需要去找出α<sub>t</sub>。</p>
<p><strong>在數學上可以證明Aggregation的效果會比單一一個g<sub>t</sub>的描述的結果還好</strong>，這很像是在做投票選舉，不同方法可能帶有不一樣的偏見，但是綜合所有意見之後可以找到共識，這個共識是具有較少偏見的，你可以想像偏見就像是Overfitting，<strong>所以Aggregation是具有像Regularizaiton一般抑制Overfitting的效果的</strong>，但有些時候特別的看法不一定是偏見，也許這一個方法可以看出其他方法看不出來的規律，此時這個部分也不會被完全忽略掉，<strong>所以Aggregation也可以同時擁有像Feature Transform一樣的複雜度。因此Aggregation的方法可以同時增加Model複雜度又同時防止它Overfitting，這個效果是我們以前沒看過的，所以我們會說Aggregation具有截長補短的效果</strong>。</p>
<p><br/></p>
<h5><u>Bagging</u></h5>
<p><img alt="Bagging" src="https://dl.dropbox.com/s/ht7d8qs8p5744le/MachineLearningTechniques.009.jpeg"></p>
<p><strong>Bagging是一種利用變換原本Data來造出不同g<sub>t</sub>的簡單方法</strong>，Bagging的全名稱為Bootstrap Aggregation，其中<strong>Bootstrap指的是「重新取樣原有Data產生新的Data，取樣的過程是均勻且可以重複取樣的」</strong>，使用Bootstrap我們就可以從一組Data中生出多組Dataset，然後就可以使用這些Dataset來產生多組g<sub>t</sub>，最後再Uniform Combination這些g<sub>t</sub>，就完成了Bagging。</p>
<p><br/></p>
<h5><u>Decision Tree（決策樹）</u></h5>
<p>接下來談Decision Tree這個重要的概念，Decision Tree其實就像是一個多層次的分類，每一次的分類會根據某一個Feature來當作依據判斷它應該繼續往哪一條路走，然後繼續使用可能是另外一個Feature來繼續細分下去。舉個例子好了，假設今天有一個自由式摔跤重量63公斤的女選手Ms. D要參加奧運，所以得透過奧運的分級制度分級，一開始可能根據比賽模式這個Feature下去分類，我查了一下有自由式和古典式兩種，所以Ms. D會被歸類到自由式，再來根據性別這個Feature下去分類，Ms. D是女選手所以分到女選手這一類，再繼續可能會根據體重來細分，體重在奧運分級共有8級，Ms. D可能就被分到62公斤级的那類，這樣的分類精神就是Decision Tree。</p>
<p>所以，Decision Tree的優點是結果所提供的結構非常容易讓人了解，另外在演算法部分也很容易實現，而且因為具有以條件篩選的結構，所以其實很容易可以做到多類別分類。但是Decision Tree也有一些為人詬病的缺點，Decision Tree整體理論是缺乏基礎的，存在很多是前人的巧思，很多作法都是使用起來感覺效果不錯就延續下去了，目前並不了解背後的原因，也因此沒有一個代表性的演算法存在。</p>
<p>在講Decision Tree操作方法之前應該要先來講一下Decision Stump，Decision Stump做的事其實就是上述中提到的對某個Feature做切分的這件事，<strong>可以想知Decision Stump是一個預測效果很差的Model，而Aggregation這些Decision Stump形成Decision Tree卻有很好的效果</strong>，這就是Aggregation的威力。</p>
<p><img alt="Decision Tree" src="https://dl.dropbox.com/s/nafpnsu8icnazic/MachineLearningTechniques.010.jpeg"></p>
<p>見上圖，我們來看一下Decision Tree的流程，Decision Tree最為人所知的演算法是C&amp;RT，C&amp;RT是一整套的套件，我們今天只是提到它整套套件中的一種特例。Decision Tree產生的函式是這樣的，一開始先判斷進來的這筆資料還能不能繼續分支下去，在三個情況下，我們沒辦法繼續分支下去：</p>
<ol>
<li>數據Ɗ只剩一筆數據。</li>
<li>這群數據Ɗ已經最佳化了，我們會說它的Impurity=0，這個時候我們不知道要從哪裡再切一刀。</li>
<li>這群數據Ɗ的Feature X<sub>n</sub>都完全相同。</li>
</ol>
<p><strong>當無法再繼續分支下去時，會回傳一個g<sub>t</sub>(x)=constant，這個常數是一個可以使得這個群體內E<sub>in</sub>最小的數值，在分類問題中這個常數是{y<sub>n</sub>}中佔多數的類別，在Regression問題中這個常數是{y<sub>n</sub>}的平均值。</strong></p>
<p>大家應該會有點驚訝，Decision Tree也有辦法做Regression？其實是可以的，在分類問題中我們可以利用類別來做分類，在Regression問題我們可以利用一個切分數值來區分成兩群或多群，例如：以50當切分數值，大於50的一類，小於等於50的另外一類，當我們切的夠細夠多層的時候就是在做一個Regression問題了。</p>
<p>那接下來來看假如還可以繼續分支下去應該要怎麼做，這邊假設我們只切一刀分為兩個區塊C=2，我們該根據怎樣的條件來切呢？我們剛剛其實有稍微提到，那就是Impurity，我們<strong>可以根據Impurity Function來衡量「一群資料的不相似程度」</strong>。</p>
<p>分類問題的Impurity Function有以下兩種：</p>
<ul>
<li>Impurity(Ɗ) = (1/N) 𝚺<sub>n</sub> ⟦y<sub>n</sub>≠y*⟧，其中y*是Ɗ中佔多數的類別，這個衡量方法就直接的去數出錯誤答案的比例。</li>
<li><strong>Gini Index: Impurity(Ɗ) = 1 - 𝚺<sub>k</sub> [ 𝚺<sub>n</sub>⟦y<sub>n</sub>=k⟧  / N ]<sup>2</sup></strong>，Gini Index是最為流行的作法，它不同於上一個作法，它是在評估所有的類別後才去計算Impurity。</li>
</ul>
<p>而Regression問題有以下方法：</p>
<ul>
<li><strong>Impurity(Ɗ) = (1/N) 𝚺<sub>n</sub> ( y<sub>n</sub> - ȳ )<sup>2</sup></strong>，其中ȳ代表的是{y<sub>n</sub>}的平均值，式子中使用平方誤差來評估資料的離散程度。</li>
</ul>
<p>有了Impurity Function我們就有了指標，找出應該要使用哪個Feature、應該要怎麼切，才能使得Impurity Function總和最小，決定好這一刀後，接下來就從這一刀切下去，把Data一分為二，然後這兩組Data再各自去長出一棵Decision Tree，經過遞迴式的迭代，我們就可以得到一棵完整的Decision Tree了。</p>
<p><img alt="Show C&amp;RT" src="https://dl.dropbox.com/s/sy6xt51dcxfcmz4/MachineLearningTechniques.015.jpeg"></p>
<p>如果我們讓一棵樹完整的長成了，可以想到的後果想當然爾就是Overfitting，所以我們必須要做Regularization，<strong>Decision Tree常用的Regularization的方法是Pruning</strong>，就是砍樹，我們將分支的數量Ω(G)加進去E<sub>in</sub>中做為Regularization，所以我們問題變成是去找到 argmin E<sub>in</sub>(G)+λΩ(G)，其中的λ可以利用Validation Data來做選擇，你會發現如果真正的要去找到argmin E<sub>in</sub>(G)+λΩ(G)的最佳解，這問題會非常的困難，因為你必須要把所有的可能的樹都考慮進去，所以有一個替代方案，<strong>我們可以先將樹整棵長完，然後在一一的去合併分支，看哪兩個分支合併之後可以使E<sub>in</sub>最小就先合併，使用這樣的作法逐步減少分支的數量</strong>。</p>
<p>順道一提，C&amp;RT可以產生許多替代方案，這些替代方案稱為Surrogate Branch，當有一筆Data缺乏某個Feature，我們仍然有辦法使用替代方案來做決策，這是C&amp;RT的一個大大的優點。</p>
<p><br/></p>
<h5><u>Random Forest（隨機森林）</u></h5>
<p>如果我拿Decision Tree來做Bagging這樣可以嗎？當然OK，Aggregation Model的精髓就是可以綜合子Model，那Decision Tree也可以是看成一個子Model，所以我們在做的就是Aggregation of Aggregation，<strong>這種拿Decision Tree來做Bagging的Model叫做Random Forest</strong>，這個名字取的很生動，有很多棵數的地方就是森林啦！</p>
<p><strong>Decision Tree和Bagging其實是有互補的作用</strong>，Decision Tree這種演算法是「變異度」很高的，因為它不像SVM這類的演算法，會去評估與Data之間的距離，空出最大的距離來避免Overfitting，而Bagging正可以拿來減少「變異度」，消除雜訊，所以<strong>Random Forest會比Decision Tree更不易Overfitting</strong>。</p>
<p><img alt="Random Forest" src="https://dl.dropbox.com/s/sw72it5miiczjri/MachineLearningTechniques.011.jpeg"></p>
<p>見上圖，我們來看一下Random Forest的流程，一開始先做和Bagging裡頭一樣做的事Bootstrap，藉此來產生新的Dataset，另外為了讓我們隨機程度變得更高，我也對我們Features來做點變化，將它乘上一個亂數產生的P，如果P<sub>i</sub>=0代表我們完全不取這個Feature，如果P<sub>i</sub>=1代表我們完全取這個Feature，我們更可以以分數來代表我們對某個Feature的重視程度，這個手法叫做Random-subspace。接下來就是把弄的很亂的Dataset放進去長一顆Decision Tree，最後再把所有的Decision Tree平均就是Random Forest的結果。</p>
<p>Random Forest發展出了一套獨特的Validation方法，我們知道Bootstrap的結果會造成有些Data取用而有些Data不使用，而取用的Data會拿來Training，這讓你想到什麼呢？沒錯，沒有用到的Data可以做Validation，我們可以拿那些沒有被取用的Data來評估Training的好壞，我們會稱那些沒被取用的Date叫做Out-of-Bag Data，而利用Out-of-Bag Data來Validation的Error，稱為Out-of-Bag Error，</p>
<blockquote>
<p><strong>Out-of-Bag Error E<sub>oob</sub>=(1/N) 𝚺<sub>n</sub> err(y<sub>n</sub>, G<sub>n</sub><sup>-</sup>(x<sub>n</sub>)) <br/></strong></p>
<p><strong>G<sub>n</sub><sup>-</sup>(x) = Average(沒有取用這筆Data的所有Models)</strong></p>
</blockquote>
<p>Out-of-Bag Error提供一個很方便的Self-validation的方法。</p>
<p>在以前Linear Model中，權重W代表每筆Feature對Model的貢獻度，我們可以由W的分量大小來評估每個Feature的重要程度。Random Forest則是可以利用E<sub>oob</sub>和Random-subspace來標示出每個Feature的重要程度，想法是這樣的，如果今天某一個Feature i 對Model很重要，所以說我只對Feature i 做Random-subspace，也就是只有P<sub>i</sub>是隨機的，可以想知E<sub>oob</sub>會大幅增加，因此利用這個想法我們可以用來定義Feature的重要程度，</p>
<p>important(i) = E<sub>oob</sub>(G) - E<sub>oob</sub>(G with random-subspace at i)</p>
<p><br/></p>
<h5><u>結語</u></h5>
<p>在這一篇我們提了幾個基礎的Aggregation Models，從最簡單的Blending，Blending的方法本身不去產生子Model，而是使用兩階段學習，先自行挑選和訓練來產生很多的子Model，而Blending只在這些結果上做不同方式的結合。</p>
<p>接下來，Learning-Aggregation的方法則化被動為主動，我們先提了Bagging，裡頭使用Bootstrap的技巧來造成資料的隨機性，利用這樣的變異來產生多個g<sub>t</sub>，再接下來我講了Decision Tree，Decision Tree由多個Decision Stump組合而成，每個Decision Stump就是g<sub>t</sub>，Decision Tree做的事就是，產生Decision Stump、切分Dataset、再產生Decision Stump...接續下去，最後綜合全部的Decision Stump成為Decision Tree。</p>
<p>最後，我們結合Decision Tree和Bagging產生了Random Forest，利用彼此的互補，讓效果變得更好可以比單純Decision Tree更好。</p></dd>
              
            	<dt>2017 / 3月 15</dt>
            	<dd><a href="../ml-course-techniques_3.html">機器學習技法 學習筆記 (3)：Kernel Regression</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><blockquote>
<p>本篇內容涵蓋Probabilistic SVM、Kernel Logistic Regression、Kernel Ridge Regression、Support Vector Regression (SVR)。</p>
</blockquote>
<p>在上一篇當中我們看到了Kernel Trick的強大，我們繼續運用這個數學工具在其他的Regression上看看。</p>
<p><br/></p>
<h5><u>Soft-Margin SVM其實很像L2 Regularized Logistic Regression</u></h5>
<p>上一篇中提到的Soft-Margin SVM其實很像<a href="http://www.ycc.idv.tw/tag__筆記：機器學習基石/">《機器學習基石》</a>裡頭提到的L2 Regularized Logistic Regression，如果你還記得的話，Logistic Regression是為了因應雜訊而給予每筆資料的描述賦予「機率」的性質，讓Model在看Data的時候不那麼的非黑及白，那時候有提到這叫做Soft Classification，而這個概念就非常接近於Soft-Margin的概念。</p>
<p>從數學式來看會更清楚，</p>
<blockquote>
<p>Soft-Margin SVM：<br/></p>
<p>min. (W<sup>T</sup>W/2) + C×𝚺<sub>n</sub> ξ<sub>n</sub> s.t. y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b) ≥ 1-ξ<sub>n</sub>且ξ<sub>n</sub> ≥ 0, n=1~N</p>
</blockquote>
<p>上面的式子中，可以將限制條件由max取代掉，轉換成下面的Unbounded的表示方法，</p>
<blockquote>
<p>Soft-Margin SVM：<br></p>
<p>min. C×𝚺<sub>n</sub> Err<sub>hinge,n</sub> + (W<sup>T</sup>W/2)<br/></p>
<p><strong>其中，Err<sub>hinge,n</sub>=max[0,1-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)]，稱之為Hinge Error Measure</strong>。</p>
</blockquote>
<p>接下來比較一下L2 Regularized Logistic Regression，</p>
<blockquote>
<p>L2 Regularized Logistic Regression：<br></p>
<p>min. (1/N)×𝚺<sub>n</sub> Err<sub>ce,n</sub> +  (λ/N)×W<sup>T</sup>W<br/></p>
<p>其中，Err<sub>ce,n</sub>=ln[1+exp(-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>))]，為Cross-Entropy Error Measure。</p>
</blockquote>
<p>你會發現Soft-Margin SVM和L2 Regularized Logistic Regression兩個式子的形式是很接近的，都有W<sup>T</sup>W這一項，只是意義上不同，在Soft-Margin SVM裡頭W<sup>T</sup>W所代表的是反比於空白區大小距離的函式，而在L2 Regularized Logistic Regression裡頭則是指Regularization。</p>
<p>另外，我們來疊一下Err<sub>hinge,n</sub>和Err<sub>ce,n</sub>來看看這兩個函數像不像，</p>
<p><img alt="compare:hinge and ce" src="https://dl.dropbox.com/s/qg2gyf8646cp3jh/MachineLearningTechniques.000_03.png"></p>
<p>from: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf</a></p>
<p><strong>Err<sub>hinge,n</sub>和Err<sub>ce,n</sub>是非常接近的，所以我們可以說做Soft-Margin SVM，很像是在做L2 Regularized Logistic Regression。</strong></p>
<p><strong>雖然說Soft-Margin SVM和L2 Regularized Logistic Regression非常的像，但是我在做完Soft-Margin SVM後，仍然沒辦法像Logistic Regression一樣得到一個具有機率分布的Target Function，以下提供了兩種方法，第一種是間接的方法，使用兩階段學習來達成Logistic的效果；第二種是直接將L2 Regularized Logistic Regression加入有如Soft-Margin SVM的Kernel性質。</strong></p>
<p><br/></p>
<h5><u>使用SVM做Logistic Regression：Probabilistic SVM</u></h5>
<p>要讓Soft-Margin SVM在最後呈現的Target Function時具有機率性質，最簡單的作法就是透過兩階段的學習來達成，第一階段先用Soft-Margin SVM去解出切分資料的平面，第二階段再將Logistic Function套在這個平面上，並做Fitting，最後我們就得到一個以Logistic Function表示的Target Function，這個稱之為Probabilistic SVM。實際操作方法如下：</p>
<blockquote>
<ol>
<li>使用Soft-Margin SVM解出切平面W<sub>SVM</sub><sup>T</sup>Z+b<sub>SVM</sub>=0，並將所有Data進一步的轉換到 Z'<sub>n</sub>=W<sub>SVM</sub><sup>T</sup>Z(X<sub>n</sub>)+b<sub>SVM</sub>。</li>
<li>接下來用轉換後的結果{Z'<sub>n</sub>, y<sub>n</sub>}做Logistic Regression得到係數A和B。</li>
<li>最後的Target Function就是 g(x)=Θ(A∙(W<sub>SVM</sub><sup>T</sup>Z(X<sub>n</sub>)+b<sub>SVM</sub>)+B)，Θ為Logistic Function。</li>
</ol>
</blockquote>
<p>上面的方法有一個缺點，就是如果B的值不接近0時，SVM的切平面就會和Logistic Regression的邊界就會不同，而且一個Model要Fitting兩次也相當的麻煩，以下還有另外一個可以達到一樣的具有機率性質的效果的方法—Kernel Logistic Regression。</p>
<p><br/></p>
<h5><u>Kernel Trick的真正精髓：Representer Theorem</u></h5>
<p>在說明Kernel Logistic Regression之前我們先來複習一下Kernel的概念，並且從中將他的重要觀念萃取出來。</p>
<p>再來看一眼我們怎麼解Kernel Soft-Margin SVM的，</p>
<blockquote>
<p>Kernel Soft-Margin SVM：<br/></p>
<p>在0 ≤ α<sub>n</sub> ≤ C; 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0的限制條件下，求解min. [(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)-𝚺<sub>n</sub> α<sub>n</sub>]</p>
<p><br/></p>
<p>得到α<sub>n</sub>，然後</p>
<p><br/></p>
<p><strong>W = 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub></strong></p>
<p><br/></p>
<p>b=y<sub>sv</sub>-𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>K(X<sub>n</sub>,X<sub>sv</sub>)</p>
</blockquote>
<p>其中W可以想成是由Z<sub>n</sub>所組合而成的，而決定貢獻程度則反應在放在它前面的係數(α<sub>n</sub>y<sub>n</sub>)，y<sub>n</sub>決定貢獻的方向，α<sub>n</sub>決定影響的程度。</p>
<p><strong>數學上，有個理論Representer Theorem可以告訴我們，所有的最佳化問題中，W的最佳解都是由Z<sub>n</sub>所組合而成的，以線性代數的角度，就是W由Z<sub>n</sub>所展開(span)，數學上表示成W*=𝚺<sub>n</sub> β<sub>n</sub>Z<sub>n</sub>。</strong></p>
<p>這個性質為Kernel Trick提供了一個良好的基礎，每次我們只要遇到W*<sup>T</sup>Z的部分，我們就可以使用Representer Theorem把問題轉換成W*<sup>T</sup>Z=𝚺<sub>n</sub> β<sub>n</sub>Z<sub>n</sub>Z=𝚺<sub>n</sub> β<sub>n</sub>K(X<sub>n</sub>,X)，就可以使用Kernel Function了。</p>
<p><img alt="kernel trick" src="https://dl.dropbox.com/s/zba8381572jub0r/MachineLearningTechniques.000_04.png"></p>
<p>from: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/205_handout.pdf</a></p>
<p>上圖是老師在上課時列出來SVM、PLA和Logistic Regression的W的展開式，你會發現都可以表現成Representer Theorem的形式。</p>
<p>有了這個概念，我們就可以把很多問題都利用Representer Theorem來轉換，並且套上Kernel Trick。</p>
<p><br/></p>
<h5><u>Kernel Logistic Regression</u></h5>
<p>那我們有了Representer Theorem就可以直接來轉換L2 Regularized Logistic Regression，讓它有擁有Kernel的效果，</p>
<blockquote>
<p>L2 Regularized Logistic Regression：<br/></p>
<p>min. (1/N)×𝚺<sub>n</sub> ln[1+exp(-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>))] +  (λ/N)×W<sup>T</sup>W</p>
</blockquote>
<p>使用W*=𝚺<sub>n</sub> β<sub>n</sub>Z<sub>n</sub>代入得，</p>
<blockquote>
<p><strong>Kernel Logistic Regression: <br/></strong></p>
<p><strong>min. (1/N)×𝚺<sub>n</sub> ln[ 1+exp(-y<sub>n</sub>×𝚺<sub>n</sub> β<sub>n</sub>K(X<sub>n</sub>,X)) ] +  (λ/N)×𝚺<sub>n</sub>𝚺<sub>m</sub> β<sub>n</sub>β<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)</strong></p>
</blockquote>
<p>上面的式子可以使用Grandient Descent來求解β<sub>n</sub>，進而得到W*=𝚺<sub>n</sub> β<sub>n</sub>Z<sub>n</sub>。而且在Kernel Function的幫助之下，我們更容易可以做到非常高次的特徵轉換。</p>
<p><br/></p>
<h5><u>Kernel Ridge Regression</u></h5>
<p>同理，我們也可以把相同技巧套用到Ridge Regression，</p>
<blockquote>
<p>Ridge Regression：<br/></p>
<p>min. (1/N)×𝚺<sub>n</sub> (y<sub>n</sub>-W<sup>T</sup>Z<sub>n</sub>)<sup>2</sup> +  (λ/N)×W<sup>T</sup>W</p>
</blockquote>
<p>使用W*=𝚺<sub>n</sub> β<sub>n</sub>Z<sub>n</sub>代入得，</p>
<blockquote>
<p><strong>Kernel Ridge Regression：<br/></strong></p>
<p><strong>min. (1/N)×𝚺<sub>n</sub> (y<sub>n</sub>-𝚺<sub>m</sub> β<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>))<sup>2</sup> +  (λ/N)×𝚺<sub>n</sub>𝚺<sub>m</sub> β<sub>n</sub>β<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)</strong></p>
</blockquote>
<p>上面的式子也可以使用Grandient Descent來求解β<sub>n</sub>。</p>
<p>另外，這個式子有辦法推出解析解，先把上式可以寫成矩陣形式，</p>
<blockquote>
<p>Kernel Ridge Regression：<br/></p>
<p>min. E<sub>aug</sub></p>
<p><br/>E<sub>aug</sub>=(1/N)×(β<sup>T</sup>K<sup>T</sup>Kβ-2β<sup>T</sup>K<sup>T</sup>y+y<sup>T</sup>y) +  (λ/N)×β<sup>T</sup>Kβ)</p>
</blockquote>
<p>所以，由∇E<sub>aug</sub>=0就可以得到最小值成立的條件為</p>
<p><strong>β*=(λI+K)<sup>-1</sup>y</strong></p>
<p>其實這個式子非常像之前在線性模型時使用的Pseudo-Inverse，</p>
<p>Pseudo-Inverse：W=(X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y</p>
<p>不過現在更為強大了，可以求得非線性模型+Regularization下的解析解。</p>
<p><strong>我們可以使用Kernel Ridge Regression來做分類問題，稱之為Least-Squares SVM (LSSVM) 。</strong></p>
<p><br/></p>
<h5><u>Support Vector Regression (SVR)</u></h5>
<p>其實，不管是Kernel Logistic Regression還是Kernel Ridge Regression，這種直接套用Representer Theorem在Regression上的都有一個缺點。</p>
<p>那就是它們的<strong>β<sub>n</sub>並不確保大多數是0</strong>，如果Data筆數非常多的話，這在計算上會是一種負荷。在之前我們討論Kernel SVM時有提到只有Support Vector的數據才會對Model最後的結果有所貢獻，Support Vector的α<sub>n</sub>&gt;0；而不是Support Vector的數據則沒有貢獻，Non-Support Vector的α<sub>n</sub>=0。所以你可以想見的是，<strong>α<sub>n</sub>大多數是0除了Support Vector外，我們稱這叫做「Sparse α<sub>n</sub>」性質</strong>，有這樣的性質可以大大的減少計算量。</p>
<p>因此接下來我們打算<strong>讓Regression具有Support Vector的性質，稱之為Support Vector Regression (SVR)</strong>。</p>
<p><img alt="SVR" src="https://dl.dropbox.com/s/76wyl84tdhj9r7a/MachineLearningTechniques.006.jpeg"></p>
<p>見上圖說明，Support Vector Regression簡稱SVR，以往的Linear Regression是求一條擬合直線能使所有數據點到直線的Error最小，而現在我們賦予它Soft-Margin的能力，<strong>SVR將擬合直線向外擴張距離ε，在這個擴張的區域裡頭的數據點不去計算它的Error，只有在超出距離ε外的才去計算Error</strong>，此時這個擬合直線有點像一條水管，水管外我們才計算Error，所以又稱之為Tube Regression。</p>
<p>這個概念和Soft-Margin SVM有點像，都是在邊界給予犯錯的機會，不同的是Soft-Margin SVM因為是分類問題，所以不允許錯誤的數據超過界，所以評估Error的方向是向內的，而SVR是向外評估Error，超出水管之上的Error我們記作ξ<sub>n</sub><sup>⋀</sup>，低於水管之下的Error我們記作ξ<sub>n</sub><sup>⋁</sup>，<strong>所以SVR的目的就是在Regularization之下使得ξ<sub>n</sub><sup>⋀</sup>+ξ<sub>n</sub><sup>⋁</sup>最小，並且調整距離ε和C來決定對Error的容忍程度</strong>。</p>
<p>這個問題同樣的可以化作Dual問題，問題變成只需要最佳化α<sub>n</sub><sup>⋀</sup>和α<sub>n</sub><sup>⋁</sup>，再使用最佳化後的α<sub>n</sub><sup>⋀</sup>和α<sub>n</sub><sup>⋁</sup>就可以得到W和b。其中W=𝚺<sub>n</sub> (α<sub>n</sub><sup>⋀</sup>-α<sub>n</sub><sup>⋁</sup>) Z<sub>n</sub>這式子裡頭隱含著Representer Theorem，每筆數據的貢獻程度β<sub>n</sub>=(α<sub>n</sub><sup>⋀</sup>-α<sub>n</sub><sup>⋁</sup>)，<strong>因此在管子內的α<sub>n</sub><sup>⋀</sup>=0且α<sub>n</sub><sup>⋁</sup>=0，不會有所貢獻，這使得SVR具有Sparse的性質，可以大大的減少計算</strong>。</p>
<p><br/></p>
<h5><u>結語</u></h5>
<p>這一篇中，我們一開始揭露了「Soft-Margin SVM其實很像L2 Regularized Logistic Regression」的這個現象，所以在SVM中最小化W<sup>T</sup>W有點像是Regression中的Regularization，也因為形式上相當的接近，所以在SVM裡頭用到的數學技巧同樣的可以套到這些有Regularized的Regression上。</p>
<p>然後，我們從Kernel Soft-Margin SVM中萃取出Kernel Trick的精華—Representer Theorem，最佳化的W可以由Data的Feature Z<sub>n</sub>所組成，記作W*=𝚺<sub>n</sub> β<sub>n</sub>Z<sub>n</sub>，這提供了Kernel Trick背後的實踐基礎，接下來我們就開始運用Representer Theorem在L2 Regularized Logistic Regression和Ridge Regression上，讓這些Regression可以輕易的做非線性特徵轉換。</p>
<p>最後，我們指出了直接套用Representer Theorem在Regression上的缺點就是參數並不Sparse，所以造成計算量大大增加。因此Support Vector Regression (SVR)參照Soft-Margin SVM的形式重新設計Regression，並且使用Dual Transformation和Kernel Function來轉化問題，最後SVR就具有Sparse的特性了。</p>
<p>上一篇跟這一篇，談的是「Kernel Models」，在這樣的形式下我們可以讓我們的「特徵轉化」變得更為複雜，甚至是無窮多次方還是做得到的。下一篇，我們會進到另外一個主題—Aggregation Models。</p></dd>
              
            	<dt>2017 / 2月 20</dt>
            	<dd><a href="../ml-course-techniques_2.html">機器學習技法 學習筆記 (2)：Support Vector Machine (SVM)</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><blockquote>
<p>本篇內容涵蓋Hard-Margin Support Vector Machine (SVM)、Kernel Function、Kernel Hard-Margin SVM、Soft-Margin SVM、Kernel Soft-Margin SVM、拉格朗日乘子法（Lagrange Multiplier）、Lagrangian Dual Problem。</p>
</blockquote>
<p>在<a href="http://www.ycc.idv.tw/YCNote/post/29">上一篇文章</a>當中，我們掃過了《機器學習技法》 將會包含的內容，今天我們正式來看SVM。</p>
<p>如果我想要使用無窮次高次方的非線性轉換加入我的Model，可以做到嗎？上一篇，我告訴大家，只要使用Dual Transformation加上Kernel Function等數學技巧就可以做到，我們今天就來看一下這是怎麼一回事。</p>
<p>本篇文章分為兩個部分，第一部分我盡量不牽扯太多數學計算，而將數學證明放在第二個部分，數學證明的部分非常複雜，但我並不打算把它們忽略掉，因為這些數學計算是相當重要的，它所帶來的方法和概念是可以重複使用的，也有助於你了解和創造其他演算法，所以有心想要成為專家的你請耐心的把後半段的數學看完。</p>
<p><br/></p>
<h5><u>Hard-Margin Support Vector Machine (SVM)</u></h5>
<p><img alt="Hard-Margin SVM" src="https://dl.dropbox.com/s/tknka2p5a7qcqcn/MachineLearningTechniques.001.jpeg"></p>
<p>回到我們最熟悉的二元分類問題，如果問題的答案是線性可分的話，我們可以找到一條直線把兩類Data給切開來，而在以前PLA的方法，切在哪裡其實是沒辦法決定的，PLA只能幫你找到可以分開兩類的一刀，但不能幫你把這刀切的更好。</p>
<p><strong>我們希望這個切開兩類的邊界可以離兩類Data越遠越好，讓邊界到Data有一個較大的空白區，這就是Hard-Margin SVM做的事</strong>。</p>
<p>我們先來看一下如何計算切平面到任意Data的距離，首先我先假設切平面的方程式為</p>
<p>W<sup>T</sup>X+b = 0 (切平面)</p>
<p>回想一下高中數學，這個平面的法向量是W，垂直於平面，所以垂直於平面的單位法向量是 W/|W|，今天如果我有一點Data Point落在X，另外在平面上任意再找一點X<sub>0</sub>，從X<sub>0</sub>到X的向量表示為X-X<sub>0</sub>，這個向量如果投影到單位法向量上，這個向量的大小正是Data Point到平面的最短距離，表示成</p>
<p>d = |W・(X - X<sub>0</sub>)| / |W|</p>
<p>X<sub>0</sub>符合切平面的方程式W<sup>T</sup>X<sub>0</sub>+b = 0代入，得</p>
<p>d = |W・X + b| / |W|</p>
<p>所以假如我有一群線性可分的二元分類Data，這個切平面我希望可以離兩類Data越遠越好，所以我會有一段全部都沒有Data的空白區，這邊假設這個空白區的邊界為</p>
<p>W<sup>T</sup>X+b = ±1</p>
<p>這個假設是可以做到的，因為我們可以以比例去調整W和b來達到縮放的效果，而不會影響切平面W<sup>T</sup>X+b = 0 。從上面的距離公式，我們知道在這個假設之下，空白區邊界距離切平面為</p>
<p>margin = 1 / |W|</p>
<p>而剛好落在這空白區邊界的Data會符合以下方程式</p>
<p><strong>y<sub>n</sub>×(W<sup>T</sup>X<sub>n</sub>+b) = 1 (Support Vector)</strong></p>
<p>y<sub>n</sub>的正負剛好和(W<sup>T</sup>X<sub>n</sub>+b)相抵消，<strong>這些落在空白區邊界的Data被稱為Support Vector，就字面上的意義就像是空白區由這一些數據給「撐」起來，而切平面只由這些Support Vector的數據點所決定，和其他的數據點無關</strong>。</p>
<p>如果考慮所有Data的話，應該要滿足</p>
<p>y<sub>n</sub>×(W<sup>T</sup>X<sub>n</sub>+b) ≥ 1 (All Data)</p>
<p><strong>綜合上述，Hard-Margin SVM的目標就是，在符合y<sub>n</sub>×(W<sup>T</sup>X<sub>n</sub>+b) ≥ 1 , n=1~N的條件下，求Margin (1 / |W|)最大的情形，也可以等價於求 (W<sup>T</sup>W/2) 最小的情形，這個問題有辦法使用QP Solver來求解，詳見<a href="https://en.wikipedia.org/wiki/Quadratic_programming">這裡</a>，我就不多加介紹這個數學工具。</strong></p>
<p><br/></p>
<h5><u>Kernel Function</u></h5>
<p>Kernel Function是最終可以讓我們有無限多次方特徵的數學工具，但這個工具非常容易理解。</p>
<p>假設考慮一個非線性轉換，將X空間轉換到Z空間，那如果我需要計算轉換過的兩個新Features相乘Z<sub>n</sub>(X<sub>n</sub>)×Z<sub>m</sub>(X<sub>m</sub>)，我有辦法<strong>不需要先做特徵轉換再相乘</strong>，而是直接使用原有的Features X<sub>n</sub>和X<sub>m</sub>求出Z<sub>n</sub>(X<sub>n</sub>)×Z<sub>m</sub>(X<sub>m</sub>)的最後結果？這種情形數學可以表示成K(X<sub>n</sub>,X<sub>m</sub>)=Z<sub>n</sub>(X<sub>n</sub>)×Z<sub>m</sub>(X<sub>m</sub>)，這個函式就叫Kernel Function。</p>
<p><strong>如果有了Kernel Function這樣的數學工具，就可以簡化和優化因為「特徵轉換」所帶來的複雜計算。</strong></p>
<p>我列出以下幾種Kernel Function：</p>
<ul>
<li><strong>Polynomial Kernel：K<sub>Q</sub>(X<sub>n</sub>,X<sub>m</sub>)=(ζ+γ X<sub>n</sub><sup>T</sup>X<sub>m</sub>)<sup>Q</sup>等價於 「Q次方非線性轉換後的兩個新特徵相乘」。</strong></li>
<li><strong>Guassian Kernel：K(X<sub>n</sub>,X<sub>m</sub>)=exp(-γ|X<sub>n</sub>-X<sub>m</sub>|<sup>2</sup>)等價於 「無窮次方非線性轉換後的兩個新特徵相乘」。</strong></li>
</ul>
<p>因此有了Guassian Kernel的幫忙，我們完全不需要管特徵轉換有多複雜，我們可以直接使用原有的Features 來計算「無窮次方的非線性轉換」。</p>
<p><strong>最後給予Kernel Function一個物理解釋，Kernel Function說穿了就是兩個向量轉換到Z空間後的「內積」，「內積」可以約略想成是「相似程度」，當兩個向量同向，內積是正的，相似度高，但當兩個向量反向，內積是負的，相似度極低，所以你會發現Guassian Kernel在X<sub>n</sub>=X<sub>m</sub>會出現最大值，因為代表這兩個位置相似度極高。</strong></p>
<p><br/></p>
<h5><u>Kernel Hard-Margin SVM</u></h5>
<p><img alt="Kernel Hard-Margin SVM" src="https://dl.dropbox.com/s/dpyh8stjm665zxd/MachineLearningTechniques.002.jpeg"></p>
<p>那我們如何使用Kernel Function來使得Hard-Margin SVM更厲害呢？我們必須額外引入另外的數學工具，包括：Lagrange Multiplier和Lagrange Dual Problem，才有辦法把Kernel Function用上，不過這部份的數學有一些複雜，我將這部份的證明放在後面的附錄上，這邊就直接從結果講起。</p>
<p>Kernel Hard-Margin SVM的公式是，在α<sub>n</sub>  ≥ 0; 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0的限制條件下，求解α<sub>n</sub></p>
<p>使得 [(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)-𝚺<sub>n</sub> α<sub>n</sub>]為最小值，</p>
<p>其中K(X<sub>n</sub>,X<sub>m</sub>)就是Kernel Function，由你的特徵轉換方式來決定，這個問題一樣可以使用QP Solver來求解。</p>
<p>當我們已經有了每筆數據點的α<sub>n</sub>了，接下來可以利用α<sub>n</sub>求出切平面的W和b，在那之前來看一下α<sub>n</sub>的意義，<strong>α<sub>n</sub>可以看作是某個數據點對切平面的貢獻程度，α<sub>n</sub>=0的這些數據點為非Support Vector，而α<sub>n</sub>&gt;0的這些數據點是Support Vector，所以對切平面有貢獻的只有Support Vector而已</strong>，這和剛剛的結論相同。因此，W和b可由Support Vector決定，</p>
<p><strong>W = 𝚺<sub>n=sv</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub></strong></p>
<p><strong>b=y<sub>sv</sub>-𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>K(X<sub>n</sub>,X<sub>sv</sub>)</strong></p>
<p>最後提一個非常重要的概念，是什麼原因讓我們不需要管特徵轉換的複雜度？以往我們的作法是這樣的，我們有每筆Data的Features，接下來對每筆Data做特徵轉換，然後在用特徵轉換後的新Features去Train線性模型，這麼一來如果特徵轉換的次方非常高的話，計算的複雜度就會全落在特徵轉換上。<strong>所以我們巧妙的使用數學工具，讓我們可以單單使用Data的Labels來做優化，而將複雜的特徵轉換利用Kernel Function的方式「嵌入」到優化的過程裡頭，此時計算量就只與Data數量有關，所以可以完全不管特徵轉換所帶來的複雜度</strong>。</p>
<p><br/></p>
<h5><u>Kernel Hard-Margin SVM: 無窮次方的特徵轉換效果如何?</u></h5>
<p>終於我們可以使用無窮次方的特徵轉換了，只要使用Kernel Hard-Margin SVM搭配上Guassian Kernel：K(X<sub>n</sub>,X<sub>m</sub>)=exp(-γ|X<sub>n</sub>-X<sub>m</sub>|<sup>2</sup>)就可以辦到，下圖是模擬的結果，是不是看起來很強大，隨著γ的不同會有不一樣的切分方法，<strong>你會發現γ越大時看起來的結果越接近Overfitting，所以必須小心挑選γ的大小。</strong></p>
<p><img alt="Guassian Kernel in Hard-Margin SVM" src="https://dl.dropbox.com/s/nzl29z7z8cveefx/MachineLearningTechniques.000_01.png"></p>
<p>from: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/203_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/203_handout.pdf</a></p>
<p><br/></p>
<h5><u>Soft-Margin SVM</u></h5>
<p><img alt="Soft-Margin SVM" src="https://dl.dropbox.com/s/ipimu7we3zd8vho/MachineLearningTechniques.003.jpeg"></p>
<p>剛剛Hard-Margin SVM會很容易Overfitting的原因在於它的機制無法<strong>容忍雜訊</strong>，所以接下來要講的Soft-Margin SVM可以容忍部份的Data違反規則，讓它們可以超出空白區的邊界。</p>
<p>見上圖，可以發現我們稍微修改了Hard-Margin SVM，加入了參數ξ<sub>n</sub>，ξ<sub>n</sub>代表錯誤的Data離空白區邊界有多遠，而我們將ξ<sub>n</sub>的總和加進去Cost裡面，在優化的過程中將使違反的狀況不會太多和離邊界太遠，<strong>而參數C負責控制ξ<sub>n</sub>總和的影響程度，如果C很大，代表不大能容忍雜訊；如果C很小，則代表對雜訊的容忍很寬鬆</strong>。</p>
<p><strong>因此我們現在有兩種Support Vector，一種是剛好落在空白區邊界的，稱為Free Support Vector；另外一種是違反規則並超出空白區的，稱為Bounded Support Vector，切平面一樣是由這些Support Vector所決定。</strong></p>
<p><br/></p>
<h5><u>Kernel Soft-Margin SVM</u></h5>
<p><img alt="Kernel Soft-Margin SVM" src="https://dl.dropbox.com/s/opndal9c0nhbo9p/MachineLearningTechniques.004.jpeg"></p>
<p>接下來同樣的對Soft-Margin SVM做數學上Lagrange Multiplier和Lagrange Dual Problem的轉換，再將Kernel Function用上，一樣的，我將這部份的證明放在後面的附錄上，這邊就直接從結果講起。</p>
<p>Kernel Soft-Margin SVM的公式是，在0 ≤ <strong>α<sub>n</sub> ≤ C</strong>; 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0的限制條件下，求解α<sub>n</sub></p>
<p>使得 [(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)-𝚺<sub>n</sub> α<sub>n</sub>]為最小值，</p>
<p>你會發現和Kernel Hard-Margin SVM唯一只差在α<sub>n</sub>被C所限制。</p>
<p>當我們已經有了每筆數據點的α<sub>n</sub>了，接下來可以利用α<sub>n</sub>求出切平面的W和b，α<sub>n</sub>一樣的可以看作是某個數據點對切平面的貢獻程度，α<sub>n</sub>=0的這些數據點為非Support Vector，而α<sub>n</sub>&gt;0的這些數據點是Support Vector，可以進一步細分，α<sub>n</sub> &lt; C為Free Support Vector，而α<sub>n</sub>＝C為Bounded Support Vector。相同的，W和b可由Support Vector (Free Support Vector和Bounded Support Vector)決定，跟Kernel Hard-Margin SVM公式一模一樣</p>
<p><strong>W = 𝚺<sub>n=sv</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub></strong></p>
<p><strong>b=y<sub>sv</sub>-𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>K(X<sub>n</sub>,X<sub>sv</sub>)</strong></p>
<p><br/></p>
<h5><u>Kernel Soft-Margin SVM: 容忍雜訊的無窮次方特徵轉換</u></h5>
<p><img alt="Guassian Kernel in Soft-Margin SVM" src="https://dl.dropbox.com/s/aw9v5e2tr9onqfy/MachineLearningTechniques.000_02.png"></p>
<p>from: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/204_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/doc/204_handout.pdf</a></p>
<p>來看看Kernel Soft-Margin SVM搭配上Guassian Kernel的效果如何，上圖是模擬的結果，我們會發現有部分Data違反分類規則，所以Soft-Margin SVM確實可以容忍雜訊，而且C越小，容忍雜訊的能力越強，所以要特別注意C的選取，如果沒有選好還是可能造成Overfitting的。</p>
<p><br/></p>
<h5><u>結語</u></h5>
<p>在這一篇當中，我們介紹了Hard-Margin SVM和Soft-Margin SVM，並且成功的利用數學工具將問題轉換成，可以單單使用Data的Labels來做優化，而將複雜的特徵轉換利用Kernel Function的方式「嵌入」到優化的過程裡頭，此時計算量就只與Data數量有關，所以可以完全不管特徵轉換所帶來的複雜度，因此利用Guassian Kernel就可以做到「無窮多次的特徵轉換」了。最後再次強調數學的部分非常重要，它提供的方法和概念是可以重複使用的，而這部份的數學是少不了的，所以有興趣的可以繼續往下看下去。</p>
<p><br/><br/></p>
<h5><u>[進階] 拉格朗日乘子法（Lagrange Multiplier）</u></h5>
<p>如果是物理系學生修過古典力學，應該對這個數學工具不陌生。<strong>Lagrange Multiplier是用在有限制條件之下的求極值問題</strong>，步驟如下：</p>
<ol>
<li>問題：在限制 g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, i=1~k  之下，求 f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) 的極值</li>
<li>假設Lagrange Function：   L(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>,λ<sub>i</sub>) = f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) + 𝚺<sub>i</sub> λ<sub>i</sub> × g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>)</li>
<li>聯立方程式求解：</li>
<li>找L的極值：∇L = 0  [Stationarity Condition]</li>
<li>g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, i=1~k  [Primal Feasibility Condition]</li>
<li>求解以上聯立方程式得到最佳解 x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub></li>
</ol>
<p>上面的聯立方程式不難理解，Primal Feasibility Condition就是我們的限制式，然後Stationarity Condition就是求極值的方法，非常直觀，滿足上面的式子我們就可以在限制上面找極值。</p>
<p><br/></p>
<p>上面是一般的Lagrange Multiplier，只有考慮到限制式是等式的情形，假如限制條件是不等式呢？我們來看一下加強版的Lagrange Multiplier：</p>
<ol>
<li>問題：在限制 g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, i=1~k 且  h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) ≤ 0, j=1~r 之下，求 f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) 的極值</li>
<li>假設Lagrange Function：   L(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>, λ<sub>i</sub>,μ<sub>j</sub>) = f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) + 𝚺<sub>i</sub> λ<sub>i</sub> × g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) + 𝚺<sub>j</sub> μ<sub>j</sub> × h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>)</li>
<li>聯立方程式求解：</li>
<li><strong>找L的極值：∇L = 0  [Stationarity Condition]</strong></li>
<li><strong>g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, i=1~k 且 h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) ≤ 0, j=1~r  [Primal Feasibility Condition]</strong></li>
<li><strong>μ<sub>j</sub>  × h<sub>j</sub> (x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, j=1~r  [Complementary Slackness Condition]</strong></li>
<li><strong>求L的最小值時 μ<sub>j</sub> ≥ 0, j=1~r；求L的最大值時 μ<sub>j</sub> ≤ 0, j=1~r [Dual Feasibility Condition]</strong></li>
<li><strong>以上的條件包括Stationarity、Primal Feasibility、Complementary Slackness、Dual Feasibility通稱 KKT (Karush-Kuhn-Tucker) Conditions</strong></li>
</ol>
<p>加強版的Lagrange Multiplier和一般版的一樣有Stationarity Condition和Primal Feasibility Condition。唯一增加的是Complementary Slackness Condition和Dual Feasibility Condition。</p>
<p>先來講一下Complementary Slackness Condition怎麼來的，我們來考慮不等式條件h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) ≤ 0，會有兩個情形發生，一個是壓到邊界，也就是h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0，這個時候問題就回到一般版的Lagrange Multiplier，此時μ<sub>j</sub>和λ<sub>i</sub>效果是一樣的，μ<sub>j</sub>可以是任意值；另外一種情況是我沒壓到邊界，也就是h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) &lt; 0，這個時候我可以把這個限制看作不存，最簡易的方法就是令μ<sub>j</sub>=0，他在L(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>, λ<sub>i</sub>,μ<sub>j</sub>) 中就不參與作用了。<strong>所以綜合壓到邊界和不壓到兩種情況，我們可以寫出一個有開關效果的方程式 μ<sub>j</sub> × h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0，這就是Complementary Slackness Condition。</strong></p>
<p>另外一個是Dual Feasibility Condition，這個限制一樣是在不等式條件才會發生，μ<sub>j</sub>的正負號取決於L是要求最大還是求最小值，稍微解釋一下，找極值我們用∇L = 0這個式子來求，代入Lagrange Function後得∇L = ∇f +𝚺<sub>i</sub>λ<sub>i</sub>×∇g<sub>i</sub>+𝚺<sub>j</sub>μ<sub>j</sub>×∇h<sub>j</sub>=0，先定性來看，假設不計∇g<sub>i</sub>的影響，當最後解落在h ≤ 0的邊界上時∇f＝- μ×∇h，因為h ≤ 0的關係，所以∇h是朝向可行區的外面，如果今天是求f的極小值，那們∇f應當朝著可行區才合理，如果不是的話則可行區內部有更小更佳的解，所以求極小值時μ ≥ 0；如果是求f的極大值，那∇f應當朝著可行區的外面，所以μ ≤ 0，這個條件待會會用在對偶問題上面。</p>
<p><br/></p>
<p>其實我們之前在《機器學習基石》裡的Regularization有偷用了Lagrange Multiplier的產物。</p>
<p>Regularization將W的長度限制在一個範圍，表示成</p>
<p>|W|<sup>2</sup> ≤ C</p>
<p>在這個條件下我們要找E<sub>in</sub>的極小值，使用加強版的Lagrange Multiplier：</p>
<ol>
<li>問題：在限制  |W|<sup>2</sup> - C ≤ 0 之下，求 E<sub>in</sub> 的極小值</li>
<li>假設Lagrange Function：   L = E<sub>in</sub> + μ × ( |W|<sup>2</sup> - C)</li>
<li>聯立方程式求解：</li>
<li>𝞉L / 𝞉W = 𝞉E<sub>in</sub> / 𝞉W + 2μ × |W| = 0  [Stationarity Condition]</li>
<li>|W|<sup>2</sup> - C ≤ 0  [Primal Feasibility Condition]</li>
<li>μ × ( C - |W|<sup>2</sup> ) = 0  [Complementary Slackness Condition]</li>
</ol>
<p>Stationarity Condition的結果就是Regularization的結果了，可以<a href="http://www.ycc.idv.tw/YCNote/post/28">回去參照一下</a>。</p>
<p><br/></p>
<h5><u>[進階] Lagrangian Dual Problem</u></h5>
<p>接下來來講對偶問題，這個部分很難，我也是反覆在網路上看了很多篇介紹才弄懂，推薦大家看<a href="http://www.eng.newcastle.edu.au/eecs/cdsc/books/cce/Slides/Duality.pdf">這一篇</a>，這篇介紹的很清楚，應該會對大家理解Lagrangian Dual有幫助。</p>
<p>來考慮一下待會會用到的求極小值問題，</p>
<blockquote>
<p>在限制 g<sub>i</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) = 0, i=1~k 且  h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) ≤ 0, j=1~r 之下，求 f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) 的極小值。</p>
</blockquote>
<p>如果我們利用剛剛的解法，稱之為Lagrangian Primal Problem。</p>
<p><strong>而這個問題可以等效轉換成Lagrangian Dual Problem，利用以下關係式</strong></p>
<p><strong>Minimum Problem ≡ min. L  ≡ min. [max.<sub>μ ≥ 0</sub> L] ≥ max.<sub>μ ≥ 0</sub> [min. L(μ)]</strong></p>
<p>我們在將原本min. L 換成min. [max.<sub>μ ≥ 0</sub> L] 是不影響結果的，因為我們剛剛分析過了在求最小值時μ ≥ 0是合理的，相反的如果μ &lt; 0，則求max.<sub>μ ≥ 0</sub> L時會產生無限大的結果，接下來就是交換min.和max.的部分，數學上可以證明min. [max.<sub>μ ≥ 0</sub> L] ≥ max.<sub>μ ≥ 0</sub> [min. L(μ)]這樣的關係，我們就稱左式轉到右式為Dual轉換。</p>
<p>而上面式子右側的求法，我們可以先求出Θ(λ<sub>i</sub>,μ<sub>j</sub>) = given λ<sub>i</sub>,μ<sub>j</sub> to find min. L(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>, λ<sub>i</sub>,μ<sub>j</sub>) ，作法是使用∇L = 0所產生符合極值的參數代入L(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>, λ<sub>i</sub>,μ<sub>j</sub>)，換成以λ<sub>i</sub>,μ<sub>j</sub>表示的Θ(λ<sub>i</sub>,μ<sub>j</sub>)。然後，再求Θ(λ<sub>i</sub>,μ<sub>j</sub>)的最大值，就可以了。</p>
<p><strong>經過Dual轉換後，我們將原本在x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>的問題轉換到λ<sub>i</sub>,μ<sub>j</sub>的空間上。</strong></p>
<p>這個轉換我們可以使用下面的圖來解釋，</p>
<p><img alt="Lagrangian Dual Geometric Interpretation" src="https://dl.dropbox.com/s/xbham8glnwivfzz/MachineLearningTechniques.005.jpeg"></p>
<p>我們先不管g(x)的部分只看f(x)和h(x)的部分，假設所有的Data x映射到f(x)和h(x)會產生一塊區域G。</p>
<p>在Primal Problem中我們可以很容易的找出h<sub>j</sub>(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) ≤ 0的限制之下f(x<sub>1</sub>,x<sub>2</sub>, … , x<sub>n</sub>) 的最小值，見上圖左側。</p>
<p>見上圖中間，Dual Problem採取另外一個方法，它先去找</p>
<p>Θ(μ) = given μ to find min. L(x,μ)，其中 L(x,μ) = f(x)+μh(x)。</p>
<p>f(x)+μh(x)=α在圖中的平面上是一條直線，而f(x)+μh(x)的值也就是α也正好是它的「截距」，所以在給定μ後要最小化f(x)+μh(x)的方法，就等效於固定直線斜率最小化截距，所以最後這個直線就必須要切於G才能使得截距最小，所以我們得到一條切於G且斜率(-μ)的直線， 因此我們就順利的得到Θ(μ)的關係式了，接下來我要找出Θ(μ)的最大值，所以就必須往上推，這個時候你就發現答案和前面Primal Problem答案一模一樣，這種最佳化答案相同的情況稱為「Strong Duality」，而最佳化答案不相同的情況就叫做「Weak Duality」，見上圖右側，在這種G的形狀下，就會產生最佳化答案不相同的情況。</p>
<p><br/></p>
<h5><u>[進階] Hard-Margin SVM Dual + Kernel Function = Kernel Hard-Margin SVM</u></h5>
<p>那我們現在可以正式的把Lagrangian Dual的東西放到Hard-Margin SVM上面。</p>
<p>回想一下Hard-Margin SVM的問題是：</p>
<blockquote>
<p>在y<sub>n</sub>×(W<sup>T</sup>X<sub>n</sub>+b) ≥ 1 , n=1~N的條件下，求(W<sup>T</sup>W/2) 最小的情形。</p>
</blockquote>
<p>那如果加上非線性轉換，從X空間轉到Z空間，則問題變成</p>
<blockquote>
<p>在y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b) ≥ 1 , n=1~N的條件下，求(W<sup>T</sup>W/2) 最小的情形。</p>
</blockquote>
<p>所以我們可以使用Lagrangian Multiplier來解決問題，依以下步驟：</p>
<ol>
<li>假設Lagrange Function：   L(W,b,α) = (W<sup>T</sup>W/2) +  𝚺<sub>n</sub> α<sub>n</sub> × [1-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)]</li>
<li>考慮Primal Feasibility、Complementary Slackness、Dual Feasibility的限制</li>
<li>Primal Feasibility Condition：1-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b) ≤ 0 [式1-1]</li>
<li>Complementary Slackness Condition：α<sub>n</sub>  × [1-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)] = 0 [式1-2]</li>
<li>Dual Feasibility Condition：α<sub>n</sub>  ≥ 0 [式1-3]</li>
<li>先求出Θ(α) = given α to find min. L(W,b,α)</li>
<li>𝞉L / 𝞉b = - 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0 [式1-4]</li>
<li>𝞉L / 𝞉W<sub>n</sub> =  |W|- 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub> = 0，y<sub>n</sub>Z<sub>n</sub>應該和W同向，所以
     W = 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub> [式1-5]</li>
<li>因此L(W,b,α)只要滿足[式1-4]和[式1-5]就代表是極小值了</li>
<li>所以[式1-4]和[式1-5]代入得Θ(α,β) = (-1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>Z<sub>n</sub>Z<sub>m</sub>+𝚺<sub>n</sub> α<sub>n</sub></li>
<li>求Θ(α)極大值</li>
<li>max.[Θ(α)]＝min.[-Θ(α)]=min.[(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>Z<sub>n</sub>Z<sub>m</sub>-𝚺<sub>n</sub> α<sub>n</sub>] —[式1-6]</li>
<li>綜合上述[式1-3]、[式1-4]、[式1-6]並改寫成Kernel的形式得，min. [(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)-𝚺<sub>n</sub> α<sub>n</sub>], s.t. α<sub>n</sub> ≥ 0 ;  𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0，使用QP Solver可以求出 α<sub>n</sub>。</li>
<li>可以用α<sub>n</sub>來求W和b</li>
<li>α<sub>n</sub>涵義：觀察[式1-2]可得 (1) α<sub>n</sub> = 0 為Non-Support Vector； (2) α<sub>n</sub> &gt; 0 代表y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)=1，為Support Vector。</li>
<li>由[式1-5]得，W = 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub>，從式子中你會發現對W有貢獻的只有Support Vector (α<sub>n</sub>&gt;0)。</li>
<li>假設在某個Support Vector(α<sub>n</sub>&gt;0)上，由[式1-2]可推得，b=y<sub>sv</sub>-𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>K(X<sub>n</sub>,X<sub>sv</sub>)  (at Support Vector)。</li>
</ol>
<p><br/></p>
<h5><u>[進階] Soft-Margin SVM Dual + Kernel Function = Kernel Soft-Margin SVM</u></h5>
<p>考慮Soft-Margin SVM和特徵轉換：</p>
<blockquote>
<p>在y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b) ≥ 1-ξ<sub>n</sub>且ξ<sub>n</sub> ≥ 0, n=1~N的條件下，求(W<sup>T</sup>W/2) + C 𝚺<sub>n</sub> ξ<sub>n</sub>最小的情形。</p>
</blockquote>
<p>所以我們可以使用Lagrangian Dual Problem來解決問題，依以下步驟：</p>
<ol>
<li>假設Lagrange Function：   L(W,b,ξ,α,β) = (W<sup>T</sup>W/2) + C 𝚺<sub>n</sub> ξ<sub>n</sub> +  𝚺<sub>n</sub> α<sub>n</sub> × [1-ξ<sub>n</sub>-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)] + 𝚺<sub>n</sub> β<sub>n</sub> × [-ξ<sub>n</sub>]</li>
<li>考慮Primal Feasibility、Complementary Slackness、Dual Feasibility的限制</li>
<li>Primal Feasibility Condition：1-ξ<sub>n</sub>-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b) ≤ 0 [式2-1]；-ξ<sub>n</sub> ≤ 0 [式2-2]</li>
<li>Complementary Slackness Condition：α<sub>n</sub>  × [1-ξ<sub>n</sub>-y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)] = 0 [式2-3]；β<sub>n</sub> × [-ξ<sub>n</sub>] = 0 [式2-4]</li>
<li>Dual Feasibility Condition：α<sub>n</sub>  ≥ 0 [式2-5]；β<sub>n</sub>  ≥ 0 [式2-6]</li>
<li>先求出Θ(α,β) = given α,β to find min. L(W,b,ξ,α,β)</li>
<li>𝞉L / 𝞉b = - 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0 [式2-7]</li>
<li>𝞉L / 𝞉W<sub>n</sub> =  |W|- 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub> = 0，y<sub>n</sub>Z<sub>n</sub>應該和W同向，所以
     W = 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub> [式2-8]</li>
<li>𝞉L / 𝞉ξ<sub>n</sub> = C - α<sub>n</sub> - β<sub>n</sub> = 0 [式2-9]</li>
<li>因此L(W,b,ξ,α,β)只要滿足[式2-7]、[式2-8]和[式2-9]就代表是極小值了</li>
<li>所以[式2-7]、[式2-8]和[式2-9]代入得Θ(α,β) = (-1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>Z<sub>n</sub>Z<sub>m</sub>+𝚺<sub>n</sub> α<sub>n</sub></li>
<li>求Θ(α,β)極大值</li>
<li>max.[Θ(α,β)]＝min.[-Θ(α,β)]=min.[(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>Z<sub>n</sub>Z<sub>m</sub>-𝚺<sub>n</sub> α<sub>n</sub>] —[式2-10]</li>
<li>綜合上述[式2-5]、[式2-6]、[式2-9]、[式2-10]並改寫成Kernel的形式得，min. [(1/2)𝚺<sub>n</sub>𝚺<sub>m</sub> α<sub>n</sub>α<sub>m</sub>y<sub>n</sub>y<sub>m</sub>K(X<sub>n</sub>,X<sub>m</sub>)-𝚺<sub>n</sub> α<sub>n</sub>], s.t. 0 ≤ α<sub>n</sub> ≤ C;  𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub> = 0，使用QP Solver可以求出 α<sub>n</sub>。</li>
<li>可以用α<sub>n</sub>來求W和b</li>
<li>α<sub>n</sub>涵義：觀察[式2-3]和[式2-4]可得 (1) α<sub>n</sub> = 0 為Non-Support Vector； (2) 0 &lt; α<sub>n</sub> &lt; C 代表y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)=1，為Free Support Vector；(3) α<sub>n</sub> = C 代表y<sub>n</sub>×(W<sup>T</sup>Z<sub>n</sub>+b)=1-ξ<sub>n</sub>，為Bounded Support Vector。</li>
<li>由[式2-8]得，W = 𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>Z<sub>n</sub>，從式子中你會發現對W有貢獻的只有Support Vector (α<sub>n</sub>&gt;0)。</li>
<li>假設在某個Support Vector(α<sub>n</sub>&gt;0且β<sub>n</sub>&gt;0)上，由[式2-3]和[式2-4]可推得，b=y<sub>sv</sub>-𝚺<sub>n</sub> α<sub>n</sub>y<sub>n</sub>K(X<sub>n</sub>,X<sub>sv</sub>)  (at Support Vector)。</li>
</ol></dd>
              
            	<dt>2017 / 1月 12</dt>
            	<dd><a href="../ml-course-techniques_1.html">機器學習技法 學習筆記 (1)：我們將會學到什麼? 先見林再來見樹</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><p>在之前四篇文章中，我總結了台大教授林軒田在Coursera上的《機器學習基石》16堂課程，我覺得這是機器學習初學很重要的基礎課程，接下來我要接續更進階的課程。</p>
<p>林軒田教授的機器學習是兩學期的課，第一學期是《機器學習基石》，第二學期就是接下來這個系列要講的《機器學習技法》，這兩堂課程是有相當大的銜接關係的，所以如果想看這系列的文章，請先看<a href="http://www.ycc.idv.tw/tag__筆記：機器學習基石/">這四篇《機器學習基石》的介紹</a>或者<a href="https://www.coursera.org/learn/ntumlone-mathematicalfoundations">直接到Coursera上學習</a>。</p>
<p>《機器學習技法》課程影片可以到老師的Youtube [ <a href="https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2">https://www.youtube.com/playlist?list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2</a> ]上收看，投影片可以到老師的個人網站上下載 [ <a href="https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/">https://www.csie.ntu.edu.tw/~htlin/course/mltech17spring/</a> ]。</p>
<p>以前，我曾經和實驗室的英國學長聊英國的教育方法，然後我驚人的發現，他的學校在大一就已經學過量子場論（物理上很難的學科XDD）了，我就很好奇量子場論不是需要很深厚的數學基礎嗎？大一是要怎麼教啊？他告訴我，他們大一就會完整走過物理的各大領域，不過是用非常概念的方式來學習，不牽涉到太困難的數學，但這概念的一系列課程卻是四年大學中相當重要的基礎，讓他在開始學細節前就可以知道這些東西未來會用在哪裡？產生了連結讓學習更有效率。</p>
<p>所以，《機器學習技法》中會介紹很多厲害的機器學習的方法，但這一篇我不直接進去看每個方法的細節，我想帶大家坐著直升機來先看看這遊樂園中有哪些遊樂設施，先來見林再來見樹，會更容易了解。</p>
<p><br/></p>
<h5><u>有什麼特徵可以使用？</u></h5>
<p>在之前《機器學習基石》中，我們講到了Features（特徵）的選擇，<strong>Features（特徵）就是我的Model描述Data的方法，也可以說是影響Data的變數</strong>，那在之前我們講過Features（特徵）的選擇可以是線性的，那也可以使用「特徵轉換」來產生非線性。</p>
<p>在這系列文章，我們會看到更多種類的Features，可以分為三類：</p>
<ol>
<li>Embedding Numerous Features（嵌入大量特徵）</li>
<li>Combining Predictive Features（綜合預測結果的特徵）</li>
<li>Distilling Implicit Features（抽取隱含特性的特徵）</li>
</ol>
<p>我已經盡力用我的理解翻譯上面的英文，哈！</p>
<p>這些不同種類的Features就會造成不同的Models，這些Models分別是</p>
<ol>
<li>Embedding Numerous Features ：Kernel Models（Kernel模型）</li>
<li>Combining Predictive Features：Aggregation Models（集合模型）</li>
<li>Distilling Implicit Features：Extraction Models（萃取模型）</li>
</ol>
<p>讓我們依序來看。</p>
<p><br/></p>
<h5><u>Embedding Numerous Features ：Kernel Models</u></h5>
<p>還記得《機器學習基石》中，我們講了哪些Model嗎？我們一開始講了二元分類問題，然後提出了Perceptron Learning Algorithm (PLA)來解決這個問題（<a href="http://www.ycc.idv.tw/YCNote/post/25">詳見《機器學習基石》第一篇</a>），如果數據是線性可分的話，我們就可以使用PLA劃分出一條邊界來區分兩種種類。</p>
<p>接下來提到我們可以使用Regression的方法來做二元分類問題，其中Logistic Regression考慮了雜訊造成每個Label的出現呈機率分布，給予一個較為寬鬆的區分方法，我們會稱PLA為Hard Classification，而Logistic Regression為Soft Classification。（<a href="http://www.ycc.idv.tw/YCNote/post/27">詳見《機器學習基石》第三篇</a>）</p>
<p>最後，我們引入「特徵轉換」將我們原本的線性區分推到非線性區分，讓我的Model有更大的複雜度，也因為如此，我們需要使用Regularization和Validation來避免 Overfitting。（<a href="http://www.ycc.idv.tw/YCNote/post/28">詳見《機器學習基石》第四篇</a>）</p>
<p><strong>那如果我想要使用無窮個高次方的非線性Features來當作我的Model，可以做到嗎？</strong></p>
<p>來看一下之前我們做特徵轉換怎麼做的？其實我們沒有多做什麼功夫，我們只是把高次項先產生出來，然後在把這每一項當作線性模型的Features去處理，我們就用線性模型的方法產生了非線性的效果。</p>
<p>那如果非線性項目的個數無窮多個，顯然這種方法就做不了了啊！</p>
<p>不過，數學總是會拯救我們，<strong>我們可以使用Dual Transformation加上Kernel Function的技巧，帶我們走捷徑，直接用解析解讓我們得出答案，繞過要考慮無窮多個Features後再處理的窘境。</strong></p>
<p>第一堂課「Linear Support Vector Machine」中，提出Hard-Margin Support Vector Machine (SVM)的架構，他和PLA非常相近，屬於Hard Classification，不同的是Hard-Margin SVM還會讓這個切分的邊界落在最佳的位置上。</p>
<p>第二堂課 「Dual Support Vector Machine」中，我們開始使用Dual Transformation，把大部分與Data中Features有關的計算，取代成計算與Data中Labels有關的計算，讓我們朝不需要計算Features邁進一步，但是因為有另外一部分還是需要計算Features，所以一樣的我們還是無法讓Features有無窮多個。</p>
<p>第三堂課「Kernel Support Vector Machine」中，我們引入Kernel Function來幫助我們，現在真的可以不需去列出所有Features也能算出答案，所以我們就可以讓Features有無窮多項，但也因為Model太過複雜，我們不得不去面對Overfitting的問題。</p>
<p>第四堂課「Soft-Margin Support Vector Machine」中，提出Soft-Margin SVM，它是一種Soft Classification，讓我們可以允許部分錯誤發生，並且同樣的使用Dual Transformation加上Kernel Function的技巧，來讓我可以使用無窮多項的Features，而且因為Soft-Margin SVM可以允許錯誤，也就是對雜訊有容忍度，因此可以幫助我們抑制Overfitting的發生。</p>
<p>第五堂課「Kernel Logistic Regression」中，我們將Kernel的方法引入Logistic Regression當中來用不同於Soft-Margin SVM的方式做二元分類。</p>
<p>第六堂課「Support Vector Regression」中，會介紹如何使用Kernel Model來做各類Regression的問題。</p>
<p><strong>這6堂課，主要做的事是把《機器學習基石》裡面學到的東西，全部引入數學工具讓Model的Features可以擴展到無窮多項，產生更強大的Kernel Model。</strong></p>
<p><br/></p>
<h5><u>Combining Predictive Features：Aggregation Models</u></h5>
<p>那如果今天我有很多支的Model，我有辦法融合他們得到更好的效果嗎？</p>
<p><strong>這就是Aggregation Models的精髓，Aggregation Models藉由類似於投票的方法綜合各個子Models的結果得到效果更好的Model。換個角度看，你可以把整個體系看成一個新的Model，而原本這些子Models當作轉換過後的新Features，所以Aggregation Model裡頭做了「特徵轉換」，這個轉換產生出許多有預測答案能力的Features，稱為Predictive Features，然後再綜合它們。</strong></p>
<p>Aggregation Models可以分成兩大類，第一種的作法比較簡單，先Train出一個一個獨立的Predictive Features，然後在綜合它們，<strong>「集合」的動作是發生在得到Train好的Predictive Feature之後，這叫做「Blending Models」</strong>；第二種作法則是，<strong>「集合」的動作和Training同步進行，這叫做「Aggregation-Learning Models」</strong>。</p>
<p>從「集合」的方法上也可以進一步細分三種類型，有票票等值的<strong>「Uniform Aggregation Type」</strong>，有給予Predictive Features不同權重的<strong>「Linear Aggregation Type」</strong>，甚至還可以用條件或任意Model來分配Predictive Features，這叫做<strong>「Non-linear Aggregation Type」</strong>。</p>
<p>所以兩種類型、三種Aggregation Type，交互產生六種Aggregation Models。</p>
<p>第七堂課「Bootstrip Aggregation」中，一開始介紹Blending Models的三種Aggregation Type，第一種是直接平均所有的Predictive Features，第二種則是藉由每個Predictive Feature的預測能力，使用線性模型去調配它們的權重，第三種則是使用任意模型分配權重。接著又介紹了Aggregation-Learning Models的Uniform Aggregation Type，稱之為Bagging，它的特點在於它可以利用變換Dataset來造出很多個Predictive Features，並接著做Aggregation。</p>
<p>第八堂課「Adaptive Boosting」中，介紹Aggregation-Learning Models的Linear Aggregation Type，稱之為AdaBoost，它的特點在於它可以使得每個Predictive Features彼此間可以截長補短。</p>
<p>第九堂課「Decision Tree」中，介紹Aggregation-Learning Models的Non-linear Aggregation Type，稱之為Decision Tree。</p>
<p>第十堂課「Random Forest」中，使用Bagging來做Decision Tree，這叫做Random Forest。</p>
<p>第十一堂課「Gradient Boosted Decision Tree」中，會介紹AdaBoost的Regression版本稱為GradientBoost，並且運用AdaBoost和GradientBoost在Decision Tree上面。</p>
<p><strong>這5堂課，我們將會介紹Aggregation Models，引入綜合、集合Predictive Feature的概念來使我們造出更好的Model。</strong></p>
<p><br/></p>
<h5><u>Distilling Implicit Features：Extraction Models</u></h5>
<p>那最後這個部分則是介紹現今很流行的「類神經網路」(Neural Network) 和「深度學習」(Deep Learning)，在這裡我們通稱Extraction Models。</p>
<p><strong>Extraction Models的特色在於它「特徵轉換」的方法，使用一層一層神經元來做非線性的特徵轉換，如果具有多層神經元，那就是做了多次的非線性特徵轉換，這就是「深度學習」，藉由Data機器會自行學習出這每一層的特徵轉換，找出隱含的Features。</strong></p>
<p>第十二堂課「Neural Network」中，介紹Neural Network，並介紹Neural Network的演算法—Back-Propagation（反向傳遞法），在概念上Gradient Descent就是Back-Propagation的源頭，另外介紹避免Overfitting的方法—Early Stopping。</p>
<p>第十三堂課「Deep Learning」中，開始介紹「深度學習」，考慮多層神經元的Neural Network就叫做Deep Learning，我們會探討如何在Deep Learning中加入Regularization，並介紹一種叫做Auto-encoder的特殊Deep Learning方法。</p>
<p>第十四堂課「Radial Basis Function Network」中，介紹Radial Basis Function (RBF) Network，並且介紹K-means等非監督分類法。</p>
<p>第十五堂課「Matrix Factorization」中，我們會探討類別的匹配問題，例如：我想要知道用戶喜歡看什麼電影，而我的Data只有用戶的ID和電影的編號。</p>
<p><strong>這4堂課，我們將會介紹Extraction Model，使用神經元的概念來萃取出Data中的Features。</strong></p>
<p><br/></p>
<h5><u>後話</u></h5>
<p>最後總結一下《機器學習技法》會講哪些東西？我們會講具有三種不同「特徵轉換」方式的Models。<strong>Kernel Model的「特徵轉換」是將非線性Features擴張到無窮多個；Aggregation Model的「特徵轉換」是產生出有預測能力的Features；Extraction Model的「特徵轉換」是利用神經元的方式來做到萃取出隱含的資訊。</strong></p>
<p><strong>跟《機器學習基石》不一樣的地方，《機器學習技法》中介紹更厲害的「特徵轉換」來產生更厲害的Model，不過因為會有Overfitting的狀況，所以我們還需要介紹相應的配套措施。</strong></p>
<p>在未來一系列的文章，我會帶大家一一的來看這些內容，不過和之前一樣，我不會以課堂當作單位來講，而是以單元式的方式，而且我主要的目的是去點出概念，並盡可能的不去牽涉太多的數學計算，但是數學計算的部分是很重要的，這會影響到你真正的實作，數學的部份可以去看林軒田老師的影片或投影片，裡頭都有很詳細的介紹。</p></dd>
              
            	<dt>2016 / 9月 18</dt>
            	<dd><a href="../ml-course-foundations_4.html">機器學習基石 學習筆記 (4)：機器可以怎麼學得更好?</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><h5><u>前言</u></h5>
<p>在上一回中，我們已經了解了機器學習基本的操作該怎麼做。而這一篇中，我們來看<strong>機器可以怎麼學得更好?</strong> 基本上有三招：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），我們來看看。</p>
<p><br/></p>
<h5><u>Feature Transformation（特徵轉換）</u></h5>
<p><img alt="ML" src="https://dl.dropbox.com/s/vutayryjaw27ckp/MachineLearningFoundations.013.jpeg"></p>
<p>在上一回當中我們講了很多的線性模型，大家有沒有懷疑說，數據呈現的方式一定可以用線性描述嗎？我的答案是通常線性描述會表現不錯，但不是絕對，<strong>那我們怎麼用非線性的方法來描述我們的數據，這邊提供一個方法叫做「非線性轉換」，或者又稱為「特徵轉換」（還記得變數x又可以稱為特徵Features）</strong>，聽起來有點困難齁～其實不會啦！</p>
<p>假設今天你的Data分布是圓圈狀的分布，顯而易見的你很難用一條線去區分他們，那我們應該怎麼做呢？假設今天有一個轉換可以把這個圓圈狀分布的空間轉換到另外一個空間，在這個新的空間可以做到線性可分，這樣的問題不就解決了嗎，我們會做線性可分的問題啊！</p>
<p>這個轉換就叫做「非線性轉換」，那這個轉換要怎麼得到呢？可以用人為定義，譬如你知道這個空間的分布狀況是圓圈分布，記作 </p>
<p>H(x<sub>1</sub>, x<sub>2</sub>) = sign(-A*x<sub>1</sub><sup>2</sup>-B*x<sub>2</sub><sup>2</sup>+C)</p>
<p>，那只要做一件事我就可以把它轉換成線性可描述的，令 z<sub>1</sub>=-x<sub>1</sub><sup>2</sup>; z<sub>2</sub>=-x<sub>2</sub><sup>2</sup>，所以問題就變成</p>
<p>H(z<sub>1</sub>, z<sub>2</sub>) = sign(A*z<sub>1</sub>+B*z<sub>2</sub>+C)</p>
<p>此時這個問題就變成一個線性問題啦！</p>
<p><strong>藉由人為觀察數據並給予適當的特徵轉換是特徵工程（Feature Engineering）中一件重要的事。</strong></p>
<p>但如果我們需要去人為定義這個「非線性轉換」，這就很弱啦！我們當然希望機器可以自行從Data中學習到這個轉換，作法是這樣的，我們先把變數x做個變化和擴充，讓它們互相的相乘創造出高次項，再把這些項等價的放到Linear Model裡，所以我們就用了線性的作法來做到Non-linear Model，而因為有權重W在非線性項前面的關係，所以機器會針對Data自行去調配非線性項，這效果就等同於機器自行學習到「非線性轉換」。</p>
<p><strong>機器自己學習特徵轉換的這個概念應該是現今ML最重要的概念之一，最近很夯的深度學習甚至不只做一次性的特徵轉換，而是做了多層的特徵轉換，而這些轉換都是機器自動從Data中學來的。</strong></p>
<p><strong>特徵轉換讓ML變得很強大，但要特別注意，因為我們增加了非線性項，所以等於是增加了模型的複雜度，這麼做的確可以壓低E<sub>in</sub>沒有錯，但也可能使得E<sub>in</sub> ≈ E<sub>out</sub>不再成立，也就是Overfitting，所以建議要逐步的增加非線性項，從低次方的項開始加起，避免Overfitting。</strong></p>
<p><br /></p>
<h5><u>Overfitting</u></h5>
<p>Overfitting是一個大怪獸，在學習怎麼對付牠之前，我們先來好好的了解牠！</p>
<p><img alt="Overfitting" src="https://dl.dropbox.com/s/jet3ocknucywtlz/MachineLearningFoundations.000.03.png"></p>
<p>From: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf</a></p>
<p>上面這張圖用很簡單的方法說明了Overfitting是怎麼一回事，假設藍色的線是Target，也就是我們抽樣的母群體，因為雜訊的關係，抽樣出來的點可能會稍微偏離Target，而如果這個時候我們用二次式來描述這些抽樣出來的Data（上圖中的左側）會發現E<sub>in</sub>不能壓到0，所以這個時候可能有人想說加進去更高次項來試試看（上圖中的右側），此時會發現E<sub>in</sub>=0，所有數據都可以被完整描述了，但是你會發現Fit的曲線已經完全偏離了Target，反而是使用低次項還描述的比較好，所以結論是<strong>如果我們把「隨機雜訊」（Stochastic Noise）Fit進去Model裡面就會因此產生Overfitting</strong>。</p>
<p><img alt="Overfitting2" src="https://dl.dropbox.com/s/wzv2dxk0m310wuy/MachineLearningFoundations.000.04.png"></p>
<p>From: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf</a></p>
<p>但可別以為沒有「隨機雜訊」鬧場就不會出現Overfitting，上圖假設一個沒有「隨機雜訊」的情形，但是Target Function的複雜度很高（上圖右側），當我們從中採樣一些Data來進行Fitting，如上圖左側，我們分別使用2次和10次來做Fitting，這個時候你會發現雖然2次和10次都和Target曲線差很遠，但是小次方的還是Fit的比較好一點，造成Overfitting的原因是因為當Target很複雜的情況下，如果採樣的數據不大，根本無法反應Target本身，所以就算使用了和Target一樣複雜的Model，也只是在瞎猜而已。<strong>這種因為Target本身的複雜度所帶來的雜訊，我們稱為「決定性雜訊」(Deterministic Noise)</strong>。</p>
<p><img alt="Noise" src="https://dl.dropbox.com/s/vgur2f9qjmonlm0/MachineLearningFoundations.000.05.png"></p>
<p>From: <a href="https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf">https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/doc/13_handout.pdf</a></p>
<p>我們來看一下「隨機雜訊」（Stochastic Noise）和「決定性雜訊」（Deterministic Noise）怎麼造成Overfitting的，上圖中的兩張漸層圖表示的是Overfitting的程度，越接近紅色代表Overfitting越嚴重；反之，越接近藍色則Overfitting越輕微。左邊的漸層圖是考慮「隨機誤差」的影響，右邊的漸層圖則是考慮「決定性雜訊」的影響。從這兩張圖我們可以觀察出下面四點，</p>
<ol>
<li>Data數量N越少，越容易Overfitting</li>
<li>「隨機雜訊」越多，越容易Overfitting</li>
<li>「決定性雜訊」越多，越容易Overfitting</li>
<li>Model本身越複雜，越容易Overfitting</li>
</ol>
<p>那有什麼方法可以防止Overfitting嗎？有的，有一些之前提過，而有一些我接下來會講，我們來看一下：</p>
<ol>
<li><strong>從簡單的模型開始做起，從低次模型開始做起，在慢慢加入高次項</strong></li>
<li><strong>提升資料的正確性：Data Cleaning/Pruning（資料清洗）將錯誤的Data修正或刪除</strong></li>
<li><strong>Data Hinting（製造資料），使用合理的方法擴增原有的資料，例如：在圖形辨識問題中，可以用平移和旋轉來擴增出更多Data</strong></li>
<li><strong>Regularization（正規化）：限制權重W的大小以控制高次的影響。</strong>（接下來會詳述...）</li>
<li><strong>Validation（驗證）：將部分Data保留不進去Fitting，然後用這個Validation Data來檢驗Overfitting的程度。</strong>（接下來會詳述...）</li>
</ol>
<p><br /></p>
<h5><u>Regularization（正規化）</u></h5>
<p><img alt="regularation" src="https://dl.dropbox.com/s/3aulwfr8gj2pr14/MachineLearningFoundations.014.jpeg"></p>
<p>剛剛我們提到了Overfitting所造成的影響很大一部分是因為Model複雜度所造成的，但是為了可以把E<sub>in</sub>給壓下去，我們又的確需要去增加高次項，所以依照建議需要從低次項開始慢慢的加，這樣感覺很麻煩啊！<strong>有沒有辦法讓機器自己去限制高次項的出現呢？有的，這就是Regularization（正規化）</strong>。</p>
<p>還記得剛剛在講「特徵轉換」時，有提到一點，ML有辦法自行學習「特徵轉換」的關鍵是因為高次項前面有一個可調控的權重，而機器會針對Data來調整權重大小，那其實就是等價於機器自己學習到了「特徵轉換」，同理可知，<strong>我們只要限制權重W的大小就等同於限制了機器無所忌憚的使用高次項</strong>。</p>
<p>經數學證明，<strong>限制權重W的大小可以等價於在E<sub>in</sub>上面加上「W大小的平方」乘上定值λ，λ越大代表W大小限制越緊；λ越小代表W大小限制越鬆</strong>，這也非常容易想像，訓練Model的方法是去降低E<sub>in</sub>，但是如果使用了大的W，就會使得E<sub>in</sub>增大，自然而然在訓練的過程中，機器會去尋找小一點的W，也就等同於限制了W的大小。</p>
<p>見上圖左側，我們修改了Gradient Descent讓它受到Regularization的限制。</p>
<p>而上圖左側下方，顯示了在λ增大的同時，限制W的大小會越來越緊，所以Fitting的結果從原本的Overfitting變成Underfitting。</p>
<p><strong>Underfitting所代表的是Model本身的複雜度不足以使得E<sub>in</sub>減小，如果你經過Validation（待會會講）後發現沒有Overfitting的現象，但是你的E<sub>in</sub>始終壓不下來，那就有可能是Underfitting，那你可以考慮增加Model複雜度或者放寬Regularization。</strong></p>
<p><strong>Regularizer的選擇常見的有兩種L2和L1，L2使用「W大小的平方」，L1則使用「W大小的絕對值」。</strong></p>
<p>當Linear Regression使用Regularization限制，統計上有一個名稱稱為Ridge Regression，你可以使用Gradient Descent來做，又或者使用解析解的方法。</p>
<p>最後提一個Regularization的細節，你會發現因為高次項是彼此兩兩相乘的結果，所以項目的個數會隨著次方增加而增加，這麼一來在做Regularization時可能會過度懲罰高次項，因此，我們可以將Feature轉換成Legendre Polynomials來避免這個問題。</p>
<p><br /></p>
<h5><u>Validation（驗證）</u></h5>
<p><img alt="validation" src="https://dl.dropbox.com/s/ytuv7ns8s39ocvd/MachineLearningFoundations.015.jpeg"></p>
<p>講了這麼多Overfitting，但到底要怎麼去量化Overfitting呢？Overfitting就是E<sub>in</sub> ≈ E<sub>out</sub>不成立，但是E<sub>out</sub>我們不會知道啊！因為我們不會知道Target Function是什麼，那該怎麼得到量化Overfitting的值呢？</p>
<p><strong>有一個方法叫做Validation可以拿來量化Overfitting的值，這個方法是先將採樣的數據做分離，一部分將會拿來做Model Fitting（Model Training），另外一部分保留起來評估訓練完畢的Model，因為保留的這一部分源自於母群體，而且又沒有被Model給看過，所以它可以很客觀的反應出E<sub>out</sub>的大小。</strong></p>
<p>我們的Model和Algorithm從以前講到現在已經是越來越複雜了，來複習一下Model和Algorithm受哪些參數影響，Algorithm的選擇就有很多了，包括：PLA、Linear Regression、Logistic Regression；Learning Rate η也需要去選擇大小決定學習速率；Feature Transformation中Feature的決定和次方大小的決定；Regularization也有L2、L1 Regularizer的選擇；還有Regularization的λ值也必須被決定。</p>
<p>這些條件彼此交互搭配會產生很多組的Model，那該如何挑選Model呢？我們就可以使用Validation來當作一個依據來選擇Model，選擇出E<sub>val</sub>最小的Model，如上圖所示。</p>
<p>另外實作上有一些方法：Leave-One-Out Cross Validation和V-Fold Cross Validation，他們的精髓就是保留k筆Data當作未來Validation用，另外一些拿下去Train Model，然後再用這k筆去評估並得到E<sub>val</sub>1，還沒結束，為了讓E<sub>val</sub>盡可能的正確，所以我們會在把Data作一個迴轉，這次使用另外一組k組Data來Validation，其餘的再拿去Train Model，然後在評估出E<sub>val</sub>2， … 以此類推，當轉完一輪之後，在把這些E<sub>val</sub>1, E<sub>val</sub>2, ...做平均得到一個較為精確E<sub>val</sub>。那Leave-One-Out Cross Validation顧名思義就是k=1，但這樣做要付出的代價就是計算量太大了，所以V-Fold Cross Validation則使用k=V來做。實務上，我常常做Validation時根本不會去Cross它們，我大都只是保留一部分的Data來驗證而已，給大家參考。</p>
<p><br /></p>
<h5><u>總結</u></h5>
<p>來到了這四篇有關於林軒田教授機器學習基石學習筆記的尾聲了，讓我們重溫看看我們學會了什麼？</p>
<p>一開始我帶大家初探ML的基本架構，建立Model、使用Data訓練、最後達到描述Target Function的目的，也帶大家認識各種機器學習的類型。</p>
<p>接下來，我們用理論告訴大家，ML是不是真的可以做到，那在什麼時候可以做到？要符合哪些條件？我們知道要有好的Model，VC Dimension越小越好，也就是可調控的參數越少越好，才會使得E<sub>in</sub> ≈ E<sub>out</sub>成立；要有足夠的Data；要有好的Learning Algorithm能把E<sub>in</sub>壓低，這三種條件成立後，如此一來Model在描述訓練數據很好的同時也可以很好的去預測母群體，但我們發現E<sub>in</sub>壓低和可調控的參數越少越好兩者是Trade-off，所以我們必須取適當的VC Dimension。</p>
<p>再接下來我們開始看實際上ML該怎麼做，引入相當重要的Learning Algorithm，也就是Gradient Descent，並且說明了Linear Regression和Logistic Regression，而且還可以使用這兩種Regression來做分類問題。</p>
<p>那最後就真正亮出ML的三大絕招啦：Feature Transformation（特徵轉換）、Regularization（正規化）和Validation（驗證），Feature Transformation使得Model更為強大，所以E<sub>in</sub>更能夠壓低，但是為了避免Overfitting我們必須去限制它，Regularization可以限制高次項的貢獻，另外，Validation可以量化Overfitting的程度，有了這個我們就可以去選出體質健康而且E<sub>in</sub>又小的Model。</p>
<p>機器學習基石的這些概念都很重要，往後如果你開始學習其他的ML技巧，例如：深度學習，這些知識都是你強大的基礎，所以多看幾次吧！</p></dd>
              
            	<dt>2016 / 8月 07</dt>
            	<dd><a href="../ml-course-foundations_3.html">機器學習基石 學習筆記 (3)：機器可以怎麼樣學習?</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><h5><u>前言</u></h5>
<p>在上一回中，我們已經了解了機器學習在理論上有怎樣的條件才可以達成，所以接下來我們就可以正式的來看有哪一些機器學習的方法。</p>
<p>在這一篇中，我會帶大家初探：<strong>機器可以怎麼樣學習?</strong> 內容包括：Gradient Descent、Linear Regression、Logistic Regression、使用迴歸法做二元分類問題等等。</p>
<p><br/></p>
<h5><u>Gradient Descent（梯度下降）</u></h5>
<p><img alt="ML" src="https://dl.dropbox.com/s/9wwibe1ix3cs1od/MachineLearningFoundations.009.jpeg"></p>
<p>還記得上一回我們歸納出了一套ML的流程，複習一下</p>
<ol>
<li>準備好足夠的數據</li>
<li>把Model建立好，d<sub>VC</sub>必須要是有限的，而且大小要適中</li>
<li>定義好評估E<sub>in</sub>的Error Measurement</li>
<li>使用演算法找出最佳參數把E<sub>in</sub>降低</li>
<li>最後評估一下是否有Overfitting的狀況，確保E<sub>in</sub> ≈ E<sub>out</sub></li>
</ol>
<p>請容許我先不管Model這部份該怎麼建立，我們先來看如何找到最佳參數這部份，<strong>假設今天我知道E<sub>in</sub>的評估方法，我該如何找到最佳的參數來使得E<sub>in</sub>更小？有一套普遍的方法叫做Gradient Descent</strong>，很強大，甚至連現今流行的「深度學習」找最佳解的機制也是從Gradient Descent衍生出來的。</p>
<p>想像一下你是一位登山客，你在爬一座由E<sub>in</sub>所決定的高山，你的目標是去這座山最低的山谷，也就是E<sub>in</sub>最小的地方，因為村莊正在那裡，但是很不幸的你沒有地圖，這個時候有什麼方法可以知道低谷在哪裡呢？答案是就一直下坡吧！反正我知道村莊在山谷裡，那我就一路下山應該就可以找到村莊了，這就是Gradient Descent的精髓。</p>
<p>在數學上有一個衡量函數變化的東西，這就是Gradient（梯度），Gradient是一個向量，它的「方向」指向函數值增加量最大的方向，而它的「大小」反應這個變化有多大，其實就是一次微分啦！只不過Gradient推廣到高維度而已。所以我們和這個登山客做一樣的事情，我們朝著下降最多的方向前進，這就是Gradient Descent（梯度下降法），我剛剛說了，梯度是指向函數值增加量最大的方向，那顯然我們往反方向走就可以達到最大下降，所以如果我們有一個Error函數E<sub>in</sub>，它的Gradient就是∇E<sub>in</sub>，那我們的下降方向就是-∇E<sub>in</sub>。</p>
<p>來看一下上圖中Gradient Descent的流程，</p>
<ol>
<li>定義出Error函數</li>
<li>Error函數讓我們可以去評估E<sub>in</sub></li>
<li>算出它的梯度∇E<sub>in</sub></li>
<li>朝著∇E<sub>in</sub>的反方向更新參數W，而每次只跨出η大小的一步</li>
<li>反覆的計算新參數W的梯度，並一再的更新參數W</li>
</ol>
<p>這邊要特別注意，流程中的第四項中，有提到η，<strong>η稱為Learning Rate，它影響的是更新步伐的大小</strong>，η的選擇要適當，如果η太小的時候，我們可能要花很多時間才可以走到低點，但如果η太大的話，又可能導致我們在兩個山腰間跳來跳去，甚至越更新越往高處跑，<strong>所以選擇適當的η相當的重要，所以下次如果你發現E<sub>in</sub>一直降不下來甚至在增大，試著將η減小看看</strong>。另外η也可以是變動的值，我們可以直接設η＝|∇E<sub>in</sub>|，這麼一來遇到陡坡的時候它就會跨大一點的步伐，遇到緩坡的時候就會跨小步一點，隨狀況調整η的值。</p>
<p>Gradient Descent (GD, 梯度下降) 有兩個變形，分別為Stochastic Gradient Descent (SGD, 隨機梯度下降) 和 Batch Gradient Descent (BGD, 批次梯度下降)，這差別只在於評估∇E<sub>in</sub>的時候所考慮的Data數量，正常來說必須要考慮所有的Data，我們才會得到真正的E<sub>in</sub>，才有辦法算出正確的∇E<sub>in</sub>，但這樣所要付出的代價就是較大的計算量。</p>
<p>所以<strong>Stochastic Gradient Descent的作法是一次只拿一筆Data來求E<sub>in</sub>'，並且更新參數W</strong>，這樣的更新方法顯然會比較不穩定，但我們假設，經過好幾輪的更新後，已經完整看過整個數據了，所以平均來說效果和一般的Gradient Descent一樣。</p>
<p>另外還有一種介於Gradient Descent和Stochastic Gradient Descent之間的作法，稱之為Batch Gradient Descent，它不像Stochastic Gradient Descent那麼極端，一次只評估一組Data，<strong>Batch Gradient Descent一次評估k組數據，並更新參數W</strong>，這是相當好的折衷方案，平衡計算時間和更新穩定度，而且在某些情形下，計算時間還比Stochastic Gradient Descent還快，為什麼呢？GPU的計算方法你可以想像成在做矩陣計算，矩陣元素在計算的時候往往是可以拆開計算的，此時GPU利用它強大的平行化運算將這些元素平行計算，可以大大增進效率，所以如果一次只算一筆資料，反而是沒有利用到GPU的效率，<strong>所以如果你用GPU計算的話，依照你的GPU去設計適當的k值做Batch Gradient Descent，是既有效率又穩定的作法</strong>。</p>
<p>Gradient Descent求最佳解其實是會產生問題的，還記得我們的目標嗎？我們希望可以走到最低點的山谷裡，所以我們採取的策略是不斷的下降，這個時候如果遇到兩種情形就會動彈不得，</p>
<ol>
<li>小山谷，數學上稱為<strong>Local Minimum</strong>，雖然在那點看起來，那邊的確是低點，但卻不是整個E<sub>in</sub>的最低點</li>
<li>平原，數學上稱為<strong>Saddle Point（鞍點）</strong>，在一片很平的區域，∇E<sub>in</sub>=0，所以就停止不動了</li>
</ol>
<p>針對這些問題有一些改良後的演算法，在這裡不詳述，請參考<a href="http://ruder.io/optimizing-gradient-descent/">S. Ruder的整理</a>。</p>
<p>好！我們已經了解了怎麼使用Gradient Descent去找到E<sub>in</sub>最小的最佳參數，那我們可以回頭看Model有哪一些？Error Measure該怎麼定？</p>
<p><br/></p>
<h5><u>Linear Regression</u></h5>
<p><img alt="ML" src="https://dl.dropbox.com/s/prx1u719y743s56/MachineLearningFoundations.010.jpeg"></p>
<p>先從最簡單的看起，那就是線性迴歸（Linear Regression），假設今天我要用三種變數(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>)來建立一個簡單的線性模型，那就是</p>
<p>w<sub>0</sub>+w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>+w<sub>3</sub>x<sub>3</sub>，</p>
<p>這個又稱為Score，標為s，為了方便起見，我們會額外增加x<sub>0</sub>=1的參數，這麼一來Score就可以寫成矩陣形式</p>
<p>s = w<sub>0</sub>x<sub>0</sub>+w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>+w<sub>3</sub>x<sub>3</sub>=W<sup>T</sup>x</p>
<p>W = [w<sub>0</sub>, w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>]</p>
<p>x = [x<sub>0</sub>=1, x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>]</p>
<p>在線性模型中，這個 s 就正好是我們Model預測的 y，通常我們會把預測得來的 y 記作 ŷ (y hat)，如果今天這個 y 和 ŷ 是實數的話，那這就是一個標準的Linear Regression問題，那如何去衡量預測的好或不好呢？<strong>我們可以使用Squared Error來衡量，err(ŷ,y)=(ŷ-y)<sup>2</sup></strong>，所以 ŷ 和 y 越靠近Error就越小。</p>
<p>Squared Error的E<sub>in</sub>平面比較簡單，就是一個單純的開口向上的拋物線，所以它的最低點其實是有解析解的，我們可以靠著數學上的<strong>Pseudo-Inverse方法</strong>在評估完全部的Data之後把最佳參數給算出來，這麼簡單的E<sub>in</sub>平面是很難見到的，我們之前介紹的Gradient Descent則是靠著逐步更新的方式去尋找近似解，這個方法是不管E<sub>in</sub>平面有多麼複雜都可以處理，但是需要特別注意別卡在Local Minimum和Saddle Point。</p>
<p><br/></p>
<h5><u>Logistic Regression</u></h5>
<p><img alt="ML" src="https://dl.dropbox.com/s/ugchv7yzd1bcm1a/MachineLearningFoundations.011.jpeg"></p>
<p>在上一回討論二元分類問題時，我們考慮的狀況是「沒有雜訊」的情形，不過在實際情況下，「雜訊」是一定需要考慮的。在「沒有雜訊」的情形下，一筆Data只會有一個確定的答案，<strong>如果考慮「雜訊」，一筆Data有可能有多個答案，呈現機率分布</strong>，對於正確答案的機率也許會高一點，但因為雜訊的干擾的原因並非能百分之一百的出現正確答案。</p>
<p>在二元分類的答案因為雜訊出現了機率分布，可能會產生像下面一樣的情況，</p>
<p>ℙ(◯|X<sup>1</sup>) = 0.9 ;   ℙ(✕|X<sup>1</sup>) = 0.1</p>
<p>而之前PLA的分類方法是屬於非黑及白的，這種分類法我們稱為Hard Classification，並不能描述這種機率分布，所以我們來考慮另外一種分類法，稱之為Soft Classification。</p>
<p><strong>Soft Classification看待每個答案不是非黑及白的，而是去評估每個答案出現的機會有多大，以此作為分類</strong>，我們打算使用Regression的連續特性來產生Soft Classification，我們需要引入一個重要的函數—Logistic Function，這個函數可以將所有實數映射到0到1之間，如上圖下方中間的圖示所示，<strong>Logistic Function會將極大的值映射成1，而將極小值映射成0，這個0到1的值剛剛好可以拿來當作機率的大小</strong>。</p>
<p>所以我們就可以來建立一個有機率概念的模型，這個Model的預測值是一個機率，一樣的先給予輸入變數x權重W求出Score s，再把 s 放到Logistic Function當中，我們就可以映射出在一個機率空間，我們藉由調整W來改變Model以描述我們的Data，有了這個新的Model，我們就可以用機率的方式來描述二元分類，</p>
<p>ℙ(◯|X<sup>1</sup>) = Θ(s) ;   ℙ(✕|X<sup>1</sup>) = 1 - Θ(s) = Θ(-s)</p>
<p>OK! 決定好Model，我們就可以來定義它的Error Measurement的方式了，這個時候如果使用Squared Error來作為Error Measurement你會發現這種評估方式有一點失焦了，我們並不是要將雜訊給放進去Model之中，而是要在考慮雜訊之下盡可能的去描述數據背後真正的機制。</p>
<p>所以我們來探討一下「可能性」，在考慮採樣數據過程因為雜訊造成的機率分布的前提下，我們去看會採樣到這組Data的可能性，我們應該合理的認為採樣出來的這組Data應該具有最大的「可能性」，這個「可能性」可以表示成</p>
<p>Assume ◯ ≡ (y=+1) and ✕ ≡ (y=-1)</p>
<p>ℙ(likelihood of ◯) = ℙ(x<sup>1</sup>)Θ(y<sup>1</sup>×s<sup>1</sup>) × ℙ(x<sup>2</sup>)Θ(y<sup>2</sup>×s<sup>2</sup>) × … × ℙ(x<sup>N</sup>)Θ(y<sup>N</sup>×s<sup>N</sup>)</p>
<p><strong>所以我們需要設計一組Error Measurement，使得Error降低的同時可以使得ℙ of likelihood可以增大，這個Error Measurement就是Cross-Entropy，Error<sub>ce</sub>=ln[1+exp(-ys)]。</strong></p>
<p>來推導一下Cross-Entropy怎麼來的，</p>
<p>Max. ℙ(likelihood of ◯) </p>
<p>= Max. Θ(y<sup>1</sup>×s<sup>1</sup>) × Θ(y<sup>2</sup>×s<sup>2</sup>) × … × Θ(y<sup>N</sup>×s<sup>N</sup>)</p>
<p>= Min. 𝚺 -ln[Θ(y<sup>n</sup>×s<sup>n</sup>)]</p>
<p>= Min. 𝚺 ln[1+exp(-y<sup>n</sup>×s<sup>n</sup>)]</p>
<p>= Min. 𝚺 Error<sub>ce, n</sub></p>
<p><strong>我們可以使用Gradient Descent來降低Cross-Entropy，這又稱為Logistic Regression，在這個問題中就沒有簡單的解析解可以直接算，只能使用近似解來處理。</strong></p>
<p><br/></p>
<h5><u>使用迴歸法做二元分類問題</u></h5>
<p><img alt="ML" src="https://dl.dropbox.com/s/01zyuaqal2achqu/MachineLearningFoundations.012.jpeg"></p>
<p>剛剛介紹了Logistic Regression，我們可以使用Regression方式來做二元分類問題，我們來看一下實際上該怎麼做？</p>
<p>線性模型的標準方法，我們會將變數x做線性組合得到Linear Scoring Function — s，線性組合的係數和Threshold稱為權重W，我們可以調整權重W來改變Model，那針對看待s的不同方式就衍生出不同的方法。那為了可以將Regression問題轉換成二元分類問題，所以通常我們會假設(y=+1)為◯，(y=-1)為✕。</p>
<p>先回顧一下之前<a href="http://www.ycc.idv.tw/YCNote/post/25">PLA的作法</a>，我們把 <code>s&gt;0</code> 的狀況視為◯，也就是(y=+1)；然後把 <code>s&lt;0</code> 的狀況視為✕，也就是(y=-1)，把這個概念畫成上圖右側的圖，圖中藍色的階梯函數就是PLA的Error Measurement，正是因為它是一個階梯函數，所以我們不能使用Gradient Descent等Regression方法來處理，<strong>因為在階梯的每一點∇E<sub>in</sub>都是0（除了原點外），也就是如此PLA在更新的過程才無法確保趨近於最佳解，而需要使用Pocket PLA來解決這個問題</strong>。</p>
<p>那如果我們用Linear Regression來做這件事呢？我們把Squared Error畫在上圖右側小圖的紅線，你會發現它的低點會落在ys=1的地方，這應該不是我們要的結果，雖然它一樣可以把錯誤的判斷修正回正確，但是面對過度確定的正確答案，它反而會去修正它往錯誤的方向，很顯然這不是我們想要的。</p>
<p>最好的方式就是Logistic Regression了，我們將s做Logistic Function的轉換，轉換成機率，並在評估最大化Likelihood的條件下定義出Cross-Entropy來當作Error Measurement，在上圖右側的小圖，我們稍微調整Cross-Entropy，使得它的Error Function可以在ys=0的地方和Squared Error相切，<strong>這張圖告訴我們的是隨著Grandient Descent每次的更新，Logistic Regression會把分類做的越來越好，把◯和✕拉的更遠</strong>。</p>
<p><br /></p>
<h5><u>後話</u></h5>
<p>在這一篇當中，我們介紹了Grandient Descent這一個相當重要的演算法，並且運用在兩種Regression上：Linear Regression和Logistic Regression，Linear Regression是最簡單的Regression方法，甚至它還可以使用Pseudo-Inverse的方法直接算出最佳解，Logistic Regression考慮了有雜訊的Data產生的機率分布，我們可以用Logistic Regression做Soft Binary Classification，而且我們也說明了Logistic Regression為何適合拿來用在二元分類上。本篇我們對於ML的實際作法有了基本認識，在下一篇，我們繼續討論還有沒有什麼方式可以讓ML做的更好。</p></dd>
              
            	<dt>2016 / 6月 26</dt>
            	<dd><a href="../ml-course-foundations_2.html">機器學習基石 學習筆記 (2)：為什麼機器可以學習?</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><h5><u>前言</u></h5>
<p>在上一回當中，我們初探了機器學習，了解了什麼時候適合使用機器學習，而不是一般的Hard Coding，那今天這篇文章要繼續問下去。</p>
<p><strong>為什麼機器可以學習(Why Can Machines Learn?)</strong>，本篇會介紹學理上機器學習（ML）必須要有哪些條件才可行，這些理論有非常多的數學，但卻是了解機器學習非常重要的內功，我會盡量避開繁複的數學運算，而帶大家直接的了解式子所要告訴我們的觀念。</p>
<h5><u>機器可以學習嗎?</u></h5>
<p><img alt="MachineLearningFoundations.001" src="https://dl.dropbox.com/s/rjxzcwyfabb02ae/MachineLearningFoundations.001.jpeg"></p>
<p>還記得上面這張圖嗎? 上次帶大家初探了Machine Learning(ML)的基本架構，可以把整個概念總結成上面這張圖。</p>
<p>我們來複習一下，先從最上面的盆子開始看起，我們用Target Function代表你想要學習的技能，在非常理想的情況下，也就是沒有noise的情況，每組輸入變數 Xn都會找到一組精確的輸出 yn，而這個Target Function能產生多個Data，圖中那些小球就是代表各個單筆的Data，今天我從中隨機抽取出N組Data來做機器學習，接下來Learning Algorithm會利用這些取出的Data去從Hypothesis Set中找出最像Target Function的Hypothesis，那這組Hypothesis就成了我們學習出來的結果，我們可以利用這個結果來預測新的問題。</p>
<p>那麼上面這張圖真的合理嗎? 我們真的有辦法用上面的方法讓機器學習嗎? </p>
<p>先介紹幾個名詞，我們會稱<strong>抽樣的Data為In-sample Data</strong>，並且稱<strong>Hypothesis預測In-sample Data的誤差為In-sample Error，表示為E<sub>in</sub></strong>，因此Learning Algorithm的目的就是找出那組Hypothesis使得E<sub>in</sub>最小。</p>
<p>回想一下二元分類問題，在上一篇當中我們使用PLA來挑選Hypothesis Set，還記得我們做了什麼事來確保我們可以得到最佳解嗎? 那就是Pocket的方法，Pocket的目的就是去留住一組能預測最好的Hypothesis，也就是能保留一組參數使得E<sub>in</sub>最小。</p>
<p>但如果E<sub>in</sub>真的已經可以壓到0了，我們就可以說機器學習已經完成了嗎？</p>
<p>並不是這樣的，回到目的，我們真正希望的是機器有辦法預測新的問題，所以真正的目標是將取樣前的母群體給預測好。</p>
<p>我們會稱<strong>抽樣前的母群體為Out-sample Data</strong>，並且稱<strong>Hypothesis預測Out-sample Data的誤差為Out-sample Error，表示為E<sub>out</sub>，我們最終目的就是把E<sub>out</sub>壓下來</strong>。</p>
<p>但遺憾的是我們不會真正知道E<sub>out</sub>的大小，所以我們只能評估E<sub>in</sub>來選取Model參數，因此重要的是需要E<sub>in</sub> ≈ E<sub>out</sub>這個條件成立，否則一切的學習都是無效的。</p>
<p><strong>總結一下機器學習的條件，我們必須建立一個 Learning Model在N筆資料輸入的情況下可以確保E<sub>in </sub>≈ E<sub>out</sub>，所以在Learning Algorithm選出最小E<sub>in</sub>的Hypothesis，同時這組Hypothesis也可以很好的預測Out-sample，我們就可以說機器已經會學習了。</strong></p>
<h5><u>E<sub>in</sub>和E<sub>out</sub>的差異</u></h5>
<p><img alt="image" src="https://dl.dropbox.com/s/z6s7d0wsq00c5nn/MachineLearningFoundations.005.jpeg"></p>
<p>剛剛我們已經提到了如果機器能學習，那就必須先確保E<sub>in</sub> ≈ E<sub>out</sub>，下面我會引入Hoeffding不等式來說明這個條件怎麼成立。</p>
<p>我們先想像一下我有一個桶子，這個桶子裝了兩種顏色的很多顆小球，分別為橘色和綠色，今天如果桶子內橘色球佔的比例為μ，而今天我們從中隨機抽樣出N顆小球，並且計算出這N顆小球中橘色佔的比例為ν，此時我們可以想像的到，μ=ν不一定會成立，但μ也不至於離ν太遠，所以Hoeffding不等式就告訴我們|μ-ν|會被限制在一個範圍內，大家可以看一下上圖中左側的圖例。</p>
<p>接下來我們再把橘球和綠球的意義換成是，一組Hypothesis預測每筆Data的好或壞，預測正確的是綠球，預測失敗的是橘球，所以橘球的比例正是一組Hypothesis的預測誤差，所以在Out-sample就是E<sub>out</sub>，在In-sample就是E<sub>in</sub>，也就得到上圖右側的公式。</p>
<p>如果我們定義E<sub>in</sub>和E<sub>out</sub>差異大於 ε 的情形為Bad Data(不好的數據)，則上述式子保證的是出現這樣Bad Data的機率將被一個定值給限制住，所以只要出現Bad Data的機率不是太大，基本上我們就可以說E<sub>in</sub> ≈ E<sub>out</sub>。</p>
<p><img alt="image" src="https://dl.dropbox.com/s/yq5l9mz9y5ulh4h/MachineLearningFoundations.006.jpeg"></p>
<p>而事實上，我們的hypothesis不會只有一個，所以接下來來考慮如果有M個Hypotheses的情況下我們的E<sub>in</sub>和E<sub>out</sub>的差異會怎麼被參數影響。</p>
<p>如果我們考慮M組Hypotheses，就會發現每種Hypothesis出現Bad Data的地方可能不一樣，因此大大的減少能使用的Data，如上圖左側所示。</p>
<p>今天如果我有1000份從Target Function取N個Data的情形，然後只用一個Hypothesis來衡量，根據Hoeffding's Inequality，1000份裡面假設大概5份會出現Bad Data，但今天我再增加一組Hypothesis來衡量，對於這個Hypothesis也可能有自己的5份Bad Data，如果很不幸的，剛剛好這5份Bad Data和前5份沒有重疊，因此用這兩個hypotheses來評估的話，1000份裡頭將會出現10份的Bad Data，由此類推，如果有M組Hypotheses，最差的情況會發生在什麼時候呢? 那就是M個Hypotheses的每份Bad Data彼此都沒有交集，夠慘吧! 所以把這些出現Bad Data的機率取聯集得到上圖右側的公式。</p>
<p>大家現在回想一下上一篇所提到的Perceptron Hypothesis Set就會發現，糟糕了! Perceptron Hypothesis Set 裡有無限多組的Hypotheses，也就是M→∞，那我們不就需要無限多的Data才能做到E<sub>in</sub> ≈ E<sub>out</sub>，否則機器根本不會學習，所以前一篇都在亂講，PLA根本無法學習，等一下，先沉住氣，聽我解釋一下，你就會明白PLA還是可以做到機器學習的。</p>
<h5><u>VC Generalization Bound</u></h5>
<p><img alt="image" src="https://dl.dropbox.com/s/5p6sz6y53oh58xe/MachineLearningFoundations.007.jpeg"></p>
<p>問題出在這裡，我們在Multi-Bin Hoeffding’s Inequality中採用了一個假設，就是假設每組Hypotheses的Bad Data彼此間都沒有重疊，所以在M→∞的情況下，當然會有一個無限大的上限值，但如果考慮了Bad Data重疊的情形，縱使M→∞的情況下還是有機會把Bad Data的出現機率壓在一個有限的定值之下。</p>
<p>我們回到二元分類問題，看一下上圖中左側的圖例，如果今天在二維平面上做二元分類，當n=1時，就算你的Hypotheses有無限多組，對於一組Data來說就只有兩種而已，再來看n=2的情況，一樣的無限多組的Hypotheses也只能分類成4種。</p>
<p>因此Hypotheses彼此之間因為Data數量的關係，而出現重疊的狀況。但聰明的你一定想到，如果今天n的數量不斷的增加，則Hypotheses被分類的數量就會增加，Hypotheses彼此之間的重疊就會漸漸減少，我們還是無法限制住Bad Data出現的機率。</p>
<p>我們繼續看下去，當n=3，沒有意外的Hypotheses會被分類為8種，那接下來n=4時，你就會發現一個有趣的現象，開始有一些情況是不會出現在這一組Hypothesis Set的，因此我們擔心因為Data數量增加而造成Hypotheses的種類暴增的情形被排除了，有一些狀況是不會出現的。</p>
<p>剛剛所提到的分類方式的數量又稱為Dichotomy。在n=1、n=2到n=3的情形，所有列得出來的方式都可被完整分類開來，我們稱這情形為Shatter，但是到了n=4的時候，有些不可能被分類的情形出現了，稱為不可被Shatter，另外又稱此情形開始發生的那點為Break Point，這邊注意一下喔! 會不會有Break Point取決於你的Hypothesis Set長怎麼樣，現在是因為線性二元分類的Hypothesis Set，所以Break Point才會在n=4，其他的Hypothesis Set就不一定了。</p>
<p>Break Point的出現非常重要，他所代表的是Bad Data的出現機率不會無所限制的大下去，因此把這概念帶入Multi-Bin Hoeffding’s Inequality，經過繁複的計算，就可以得到上圖右側的公式，原本的M消失了，取而代之的是Growth Function，Growth Function與Data數量N有關，這就是我們剛剛解說的，決定Hypothesis Set的種類的其實是 Data的數量N。</p>
<p>那麼Growth Function要怎麼和Break Point連結起來呢？</p>
<p>先定義一下VC Dimension：d<sub>VC</sub>= Break Point-1，Break Point代表首次出現不Shatter的情況，那比它小一級代表的正是最大可以Shatter的點，上面的例子中d<sub>VC</sub>=3。而這個VC Dimension就可以和我們在意的Growth Function連接起來，經過數學推倒可以得到上圖右側下方的關係式。</p>
<p>所以我們就知道啦！<strong>只要有Break Point存在，VC Dimension就是一個有限的值，也因此Growth Function是一個有限的值，VC Bound就產生了，就可以確保Bad Data出現的機率被壓在一個定值之下，所以一樣的只要資料量N夠多就可以確保E<sub>in</sub> ≈ E<sub>out</sub>，機器將可以學習。</strong></p>
<p>另外一件重要的事，VC Dimension在數學上是有意義的，<strong>d<sub>VC</sub> ≈ 可調控變數的個數</strong>，像是上述的二維二元分類問題，它的可調控變數有w0, w1 和 w2，總共3個，所以d<sub>VC</sub>=3。<strong>也就是說Hypothesis Set的可調變參數如果是有限，大部分都可以做機器學習。</strong></p>
<h5><u>機器要能學習的三要素</u></h5>
<p>前面拉哩拉雜的講了一堆，終於要推出我們的結論了! 所以如果剛剛的數學讓你感到很挫敗，沒關係，讀懂這段那就足夠了。</p>
<p>從VC Generalization Bound，我們可以知道機器學習是可能的，只要它具備三點要素：</p>
<ol>
<li><strong>Good Hypothesis Set: Hypothesis Set 必須有Break Point的存在，也意味著VC Dimension是有限的，而且越小越好，在意義上代表可以調控的變數不要太多。</strong></li>
<li><strong>Good Data: 數據量越大越好，可以壓低VC Generalization Bound</strong></li>
<li><strong>Good Learning Algorithm: 以上兩點可以確定的是E<sub>in</sub> ≈ E<sub>out</sub>，接下來好的Learning Algorithm要有能力找到E<sub>in</sub> 最小的參數。很直觀的，當我們可以調控的變數越多，我們的選擇就越多，也就是我們可以找到更小E<sub>in</sub> 的機會變多了，所以可以調控的變數不可以太少。</strong></li>
</ol>
<p>眼尖的你有沒有發現矛盾啊! 可以調控的變數很少，我們能確保E<sub>in</sub> ≈ E<sub>out</sub>，但是如果我想要找到更小的E<sub>in</sub> 又必須有更多的調控變數，這個矛盾是機器學習上一個重要的課題，<strong>解法是我們必須要能找到適當的調控變數數量，也就是適當大小的d<sub>VC</sub> </strong>。</p>
<p><img alt="" src="https://dl.dropbox.com/s/0dxzdyi0r8ourz6/MachineLearningFoundations.000.02.jpeg"></p>
<p>from: <a href="https://d396qusza40orc.cloudfront.net/ntumlone/lecture_slides/07_handout.pdf">https://d396qusza40orc.cloudfront.net/ntumlone/lecture_slides/07_handout.pdf</a></p>
<p>上圖中，我們把VC Generalization Bound公式帶入Growth Function和d<sub>VC</sub>的關係式，並且設δ 為最大可以容忍的Bad Data出現機率，把它帶入取代掉ε，整理一下，就可以推出上圖的公式，後面帶根號的紅字稱為Model Complexity，這一項代表的是Hypothesis Set造成的模型複雜度，我們可以看到它隨著d<sub>VC</sub>增加而增加。Model Complexity越大代表Bad Data更容易出現，所以E<sub>in</sub>和E<sub>out</sub>開始被帶開了。</p>
<p><strong>這個現象有一個很常見的名字叫做Overfitting，指的是使用非常複雜的Model來Fitting，雖然可以把手頭上的數據Fit的很漂亮，但是拿到其他的數據來看就會發現這Model的預測性非常的差，原因就是因為Model Complexity造成E<sub>in</sub>和E<sub>out</sub>脫鉤了，所以選擇一個複雜度適中的Model是很重要的。</strong></p>
<h5><u>機器學習架構一般化</u></h5>
<p><img alt="image" src="https://dl.dropbox.com/s/h8qabqhjjaew5gs/MachineLearningFoundations.008.jpeg"></p>
<p>最後我們來總結一下機器學習的流程，上圖中是之前提到的機器學習的架構，額外的我們還需要考慮到一些真實情形，</p>
<ol>
<li>每筆Data出現的機會不一定，同樣的採樣結果也是會受機率的影響，所以上圖中標示為P(x)，這個修改並不會影響機器學習的流程和結果。</li>
<li>Data可能會受到Noise的影響，所以給定X<sub>n</sub>並不一定會百分之一百得到y<sub>n</sub>，他存在著可能會出錯，上圖標示為P(y|x)，我們可以增大我們採樣的數量N來減少Noise的影響。</li>
<li>我們是採用E<sub>in</sub>來當作選擇Model參數的指標，因此我們需要訂出Error的評估方式，常見的有Squared Error = (y<sub>n</sub> - y<sub>prediction</sub>)<sup>2</sup>。</li>
</ol>
<p>跟著架構我們就有一套機器學習的<strong>標準流程</strong>，</p>
<ol>
<li><strong>準備好足夠的數據</strong></li>
<li><strong>把Model建立好，d<sub>VC</sub>必須要是有限的，而且大小要適中</strong></li>
<li><strong>定義好評估E<sub>in</sub>的Error Measurement</strong></li>
<li><strong>使用演算法找出最佳參數把E<sub>in</sub>降低</strong></li>
<li><strong>最後評估一下是否有Overfitting的狀況，確保E<sub>in</sub> ≈ E<sub>out</sub></strong>（未來會講怎麼做）</li>
</ol></dd>
              
            	<dt>2016 / 6月 06</dt>
            	<dd><a href="../ml-course-foundations_1.html">機器學習基石 學習筆記 (1)：何時可以使用機器學習?</a></dd>
              <dd style="border:0.5px solid rgb(200,200,200);padding:15px;margin-top:10px;margin-bottom:50px;max-height:60vh;overflow:hidden;pointer-events:none;background-color:rgb(250,250,250);"><h5><u>前言</u></h5>
<p>經過幾個月的努力，終於完成田神在Coursera上machine learning的兩門課中的第一門課—<a href="https://www.coursera.org/course/ntumlone">機器學習基石</a>，田神不愧為田神的名號，整門課上起來非常流暢，每個觀念講得非常得清晰，考究學理，但是又不會單單只有理論而已，課程中會舉很多實用的例子，讓你了解每個觀念如何實踐。因此，非常推薦大家去把Coursera上面的課程完整聽一次，應該會收益良多，接下來一系列的文章，我會摘要出《機器學習基石》之中主要的概念，適合對Machine Learning（ML）有興趣的初學者來一窺它的脈絡。</p>
<p>《機器學習基石》一共有16堂課，主要分為四個方向，第一個方向，<strong>何時可以使用機器學習(When Can Machines Learn? )</strong>，點出什麼是機器學習，適合在哪些情形下使用，並引入貫穿整個課程的二元分類問題，第二個方向，<strong>為什麼機器可以學習(Why Can Machines Learn?)</strong>，介紹學理上機器學習必須要有哪些條件才可行，這些理論是了解機器學習非常重要的內功，第三個方向，<strong>機器可以怎麼樣學習(How Can Machines Learn?)</strong>，學習完了學理，我們來看機器學習有哪些的使用方法，最後一個方向，<strong>機器可以怎麼樣學得更好(How Can Machines Learn Better?)</strong>，探討哪些問題會造成機器學不好，然後怎麼去改善。</p>
<h5><u>什麼是Machine Learning (ML)</u></h5>
<p>在了解機器學習之前，我們不妨來想想「你」從小是怎麼學習的，有人會說學習就是一個不斷記憶的過程，但這樣的說法顯然不夠全面，你總不會認為把考題的所有答案都背起來的學生就已經學會一門知識了吧！所以，考題只是表象，我們真正要學習的是它背後的觀念，可以拿來推敲未知的知識。</p>
<p>同樣的，ML的學習方式也有點類似於人類的學習，機器從Data中開始學習起，這些Data就像是一道一道的考題，而ML做的事正是去學習Data後面的觀念，而不是單純把Data給儲存起來，有了Data背後的觀念才能舉一反三，才算是真正的學會了。</p>
<p>所以，做ML有點像是手把手的造一顆大腦，並且訓練它學會Data背後的知識。那這個大腦要怎麼設計呢？這個大腦用我們學物理的人的說法就是建一個Model，而餵給它Data的過程就是Fit Model。</p>
<p>那什麼是Model呢？讓我來解釋一下，<strong>所謂的Model就是一個用來描述未知現象的架構</strong>，舉個例子，我們都知道力的公式是F=ma（力＝質量x加速度），但如果你今天拿一顆皮球來，你就會發現這個公式不那麼正確，因為皮球會形變，那怎麼辦呢？我們可以假設形變會把部份的力給抵消掉，所以式子改寫成(F-F1)=ma，在這邊F1就是那個抵消的力，這樣就是設計了一個Model來描述這個現象，而F1是一個未知的值，我們可以用實驗數據來推估F1，這就是所謂的Model Fitting。</p>
<p>物理上的Model通常是這樣做的，我們先觀察未知現象，然後從中猜測可能造成這現象的原因，總結這些原因來設計一個Model，Model中可能有一些參數還沒被決定，此時我們就可以用數據來決定它，這就是Model Fitting。</p>
<p><img alt="MachineLearningFoundations.001" src="https://dl.dropbox.com/s/rjxzcwyfabb02ae/MachineLearningFoundations.001.jpeg"></p>
<p>了解了Model的概念就相當好了解ML的架構，上圖是ML的基本架構，<strong>假設我們今天要讓機器學一樣技術，這個技術我們用一個函數來表示，稱之為Target Function，這個Target Function就是隱藏在Data後面的真正道理</strong>，每個變數X會有相應的正確答案Y。</p>
<p>今天我從Target Function中取出N組當作Data來給我的機器學習，那目標是什麼?<strong>目標當然是讓機器學習出這個Target Function啦！</strong>所以我們要先設計我們的Model，最終目的是決定Model裡的參數之後，這個被選擇的Model就是Target Function。</p>
<p>Model就是上圖中的Hypothesis Set，在Model參數還沒被決定之前，你可以想像它就像一個集合包含很多可以選擇的函數，而使用數據Model Fitting以後，選出一組最佳化的參數，就好像從這個集合中挑選一組函數一樣。</p>
<p>在這個找最佳化參數的過程，我們需要一個機制，這個機制可以評估Hypothesis Set中每組函數描述Data的好壞，並且找出描述Data最好的那組參數，這個機制就是上圖中的Learning Algorithm。</p>
<p><strong>建立Model，使用Data加上Learning Algorithm找出最佳參數，這就是ML的架構輪廓</strong>。當然這邊要補充一下，物理上的Model通常是建基在已知的知識之上，而常見的ML強大之處是不需要太多的人為的智慧，機器可以自行學習，所以我這裡指的Model是比物理上的Model更為廣義的。</p>
<h5><u>Machine Learning (ML)的使用時機</u></h5>
<p>剛剛帶大家初探了ML的架構，接下來帶大家了解什麼時候我們適合使用ML。</p>
<p>舉幾個例子，大家可能比較有感覺，譬如說Netflix曾辦過一場競賽，競賽的內容是利用客戶的影片評分紀錄，來預測未評分影片的得分，如果可以增進預測率10%，就可以獨得100萬美元獎金，這個問題就可以使用ML，Data是過去得評分紀錄，Target Function是用戶評分的規律，如此一來，機器學到了這個技術，未來就可以舉一反三的推出未評分影片的分數，和用戶喜歡的影片可能有哪些。</p>
<p>再多看幾個例子，例如設計火星勘查機，人類目前對火星的了解仍相當有限，所以我們沒辦法完全猜測勘查機在火星會遇到什麼問題，所以必須讓勘查機有ML的能力去學習各種問題的解決方法。</p>
<p>再來個例子，現在很夯的汽車自動駕駛也需要ML技術，機器去學習辨識交通號誌。</p>
<p>看了這麼多例子，我們會發現這些例子都很難以寫出簡單的規則，但是卻又存在著一種規律，這種情形正是適合用ML來做。</p>
<p>在以往電腦工程幾乎都是由工程師用嚴謹的邏輯去逐條的把規則一一的寫上，這樣的機器不具有學習能力，或稱得上人工智慧，因為它只是單純反應工程師的工人智慧而已，但如果遇到一些困難的問題，譬如告訴機器什麼是狗，這時候你就會發現很難用規則來描述，有尾巴，可是是怎樣的尾巴？有耳朵，那這耳朵怎麼和貓區分開來？此時Hard Coding就太困難了，我們不這麼做，反過來我們設計架構讓機器自己去從Data中學習。</p>
<p>總結一下，ML的使用時機有以下三種</p>
<ol>
<li><strong>你想要學習的技術存在一種模式</strong></li>
<li><strong>要學習的技術不容易簡單的Hard Coding</strong></li>
<li><strong>有可以代表這個要學習模式的Data</strong></li>
</ol>
<h5><u>二元分類問題</u></h5>
<p><img alt="img" src="https://dl.dropbox.com/s/4chm6lt80pnb684/MachineLearningFoundations.000.01.jpeg"></p>
<p>from: <a href="https://class.coursera.org/ntumlone-003/lecture/17">https://class.coursera.org/ntumlone-003/lecture/17</a></p>
<p>好! 大家現在應該對於機器學習有一些認識了，那接下來我們來實作一些例子來了解機器學習架構怎麼運作。像個小學生一樣，我們先從簡單的是非題來學起，雖然是非題看起來非常簡單，但它其實非常的powerful，是非題饒口一點的講法就是「二元分類問題」，這樣的問題將會貫穿整個16堂課程，相當重要!</p>
<p>舉個例子，今天有一家銀行想要開發一款ML的軟體，這個軟體可以根據過去信用卡核發用戶的資料，去判斷要不要核發信用卡給這個新的申請人，這些過去的資料可能包括：用戶年齡、用戶性別、用戶年薪等等，讓機器藉由這些資料去學習判斷要不要核發信用卡。把這樣的二元分類問題化作</p>
<p>Target Function：f: X =&gt; y</p>
<p>X有年齡、性別和年薪這些變數，而y則是個二元類別，不是y=1(核發)就是y= -1(不核發)。</p>
<p>那接下來，我們就要決定我們的Learning Model，也就是Hypothesis Set。</p>
<p><img alt="MachineLearningFoundations.002" src="https://dl.dropbox.com/s/r2vv0p2k097v6wb/MachineLearningFoundations.002.jpeg"></p>
<p>引入<strong>Perceptron(感知器) Hypothesis Set</strong>來當作我們的Hypothesis Set，如上圖，我們給予我們的輸入變數個別的權重，然後相加起來，並且看這個值是正還是負，來決定輸出值是+1或-1，sign函數的作用是假設輸入的值為正則輸出+1，反之則輸出-1。</p>
<p>對應核發信用卡這個例子，</p>
<p>x1 = 用戶年齡; x2 = 用戶性別; x3 = 用戶年薪，</p>
<p>在分別乘上weight w1, w2, w3，這個變數前面的weight代表這個變數對於答案Y有什麼影響，如果是正向影響，weight &gt; 0，如果沒有影響，weight = 0，如果負向影響，weight &lt; 0，舉個例子，高年薪也許可以提升核發信用卡的機會，那它前面的weight應該就是正的，也許性別並不影響核發信用卡的機會，則weight = 0，那麼考慮到這些input變數對結果影響的評估，我們會得到一個數值 w1*x1+w2*x2+....。</p>
<p>此時我們要用這個數值去做「二元分類」，也就是一分為二，怎麼做呢? 很簡單，給他一分水嶺，高於一個值(-w0)我就給他 y=+1，低於(-w0)我就給他 y=-1，用數學表示就是 sign(w0+w1*x1+w2*x2+...) ，w0可以看成是一個閥值。</p>
<p>上圖中的 s = w0+w1*x1+w2*x2+... 就像一個分數(score)一樣，高分 s&gt;0 的我就核發(+1)，低分 s &lt; 0 的我就不核發(-1)，其中權重 w0, w1, w2, ... 都可以由機器學習去調整，這些不同的weight就構成了Hypothesis Set，也就是Model，那接下來我們還需要Learning Algorithm來取出最佳參數，也就是決定一組最佳weight。</p>
<p><img alt="MachineLearningFoundations.003" src="https://dl.dropbox.com/s/51aipr85rfbylj3/MachineLearningFoundations.003.jpeg"></p>
<p>如上圖所示，<strong>Perceptron Learning Algorithm(PLA)</strong>是用於處理Perceptron Hypothesis Set的一種演算法。</p>
<p>它的作法簡單來講是，藉由一筆一筆的數據去逐步的更新它的weight使得Model可以描述這筆數據，直到不需要再更新為止，此時所有的Data都可以用這個Model表示，更新的方法是先判斷進來的這筆數據是否符合目前的Model，如果不符合，則朝變數向量Xn的方向，跨出或後退大小為Learning Rate的一步來更新weight，前進還是後退端看你的Data是y=-1或+1，y=+1就往前跨，y=-1就往後退。</p>
<p>因此，這個跨步更新的動作必須可以使Model接近正確答案，這麼神奇，真的假的？不太直覺，先從score來想起，假設有一筆資料為(Xn,yn)，則Score：s = Wt・Xn，在Wt和Wn向量彼此有同向分量的情況下，s &gt; 0，如果這個時候yn剛好為+1，則sign(s)=yn，這個時候Wt描述這個數據就很好啊，我們就不需要去更新它；如果相反yn=-1，這個Wt描述這個數據就不正確，也就是說Wt 和 Xn不應該同向，所以我們讓Wt加上-Xn(=yn*Xn)，把Wt從原本與Xn同向的狀態反向拉離開來。那如果在Wt和Xn向量彼此不同向的情況下，s &lt; 0，這個時候如果yn剛好為-1，則sign(s)=yn，很好我們不去更新它；如果相反yn=+1，這個Wt描述這個數據不正確，也就是說Wt 和 Xn不應該反向，所以我們讓Wt加上Xn(=yn*Xn)，把Wt拉到和Xn同向一點。這就是PLA找到更好Wt的機制。</p>
<p><img alt="MachineLearningFoundations.004" src="https://dl.dropbox.com/s/aq6n1491d91z906/MachineLearningFoundations.004.jpeg"></p>
<p>Seeing is believing，上面這張圖帶我們來看PLA如何運作，</p>
<ul>
<li>Initially: 在最一開始的時候，我們weight Wt先設成零向量</li>
<li>Update 1: PLA更新把零向量的Wt拉成W(t+1)</li>
<li>Update 2: 上一輪的W(t+1)已經是這一輪的Wt，也就是紅色的那個向量，Wt決定了一條壁壘分明的二元分類邊界，這條線的方程式其實就是 w0+w1x1+... = 0，如果你還記得高中數學的話，這條邊界必然會和Wt垂直，如圖所示，而Wt的方向是屬於y=+1的區域，這一輪剛剛好找到一個圈(y=+1)落在y=-1的區域，因此我們需要更新weight，做法是把Wt 和 yn*Xn(=Xn)相加成為新的weight Wt+1</li>
<li>...........以此類推</li>
</ul>
<p><strong>如果資料線性可分的話，PLA在迭代多次後，是可以用一條線完全區分兩種數據</strong>。但如果數據不是線性可分，不存在一條線來區分數據，此時最佳解就必須評估整體犯錯有多少，找出犯錯最少的那條直線就是最佳解，但可惜的是PLA方法並不會在迭代中趨向於犯錯最少的那條線，什麼時候該停止迭代是個世紀難解的NP-Hard問題（如果不了解這個名詞，<a href="http://www.ycc.idv.tw/YCNote/post/19">詳見</a>）。</p>
<p>因此要改變一下PLA，這個方法我們稱之為Pocket，當每次得到一組weight的時候，都拿它來評估它對所有Data的區分能力好或壞，而只留下一組最好的放進口袋裡，所以當迭代次數做多了，保留在口袋的這組解就可以看成是最佳解，就這麼簡單。</p>
<h5><u>多元學習</u></h5>
<p>機器學習和人類學習一樣，有各式各樣的學習型態。剛剛的<strong>「二元分類問題」</strong>就像考「是非題」一樣，答案要嘛是Yes不然就是No，表示為 <strong>y={-1, 1}</strong>，這就像是機器在小學時代的問題，較為簡單。</p>
<p>現在機器脫離國小來到了國中，考試題目開始出現「選擇題」，這和機器學習中的<strong>「多元分類問題」</strong>一樣，必須從兩個以上有限的答案中作選擇，表示為 <strong>y={1, 2, ... , k}</strong>。</p>
<p>另外機器還可能遇到傷透腦筋的「計算題」，在機器學習裏頭稱為<strong>「Regression 問題」</strong>，這個時候答案已經放寬到整個實數系了，表示為 <strong>y∈R</strong>，舉個例子，譬如利用過去天氣的數據去預測明日氣溫，或者利用歷史股價資料預測未來股價，都是Regression的應用。</p>
<p>此時，機器到了大學，開始碰到不那麼容易回答，甚至不存在單一答案的「申論題」，這在ML中像是<strong>「Structure Learning 問題」</strong>，答案的選擇換成了各種結構，表示為 <strong>y={structures}</strong>，舉個例子可能比較好理解，例如：自然語言，我們都希望有一天電腦可以理解我們的語言，我們可以不再需要以機器語言來和電腦溝通，而是用人類的語言直接和電腦溝通，聽起來很棒對吧! 這個部分的ML就需要Structure Learning來學習語言的文法結構。</p>
<p>我們教機器學習也有各種不同的教育方法。</p>
<p>有像是填鴨式教育的<strong>「Supervised Learning」(監督式學習）</strong>，直接告訴機器考題和答案，讓機器從中學習，這種情況下每筆資料Xn對應的yn都有明確Label，答案是人類直接告訴機器的。</p>
<p>有像是培養科學家教育方法一樣的<strong>「Unsupervised Learning」(非監督式學習）</strong>，此時每筆資料Xn對應的yn都沒有Label，所以機器要自己歸納整理，然後從中學到規律，通常用於分群問題，對資料做分類找出規律性。</p>
<p>那還有折衷於上述兩種方法的啟發式教育，<strong>「Semi-supervised Learning」(半監督式學習）</strong>，在這個情形下有部分資料yn是有Label的，機器可以藉由有Label的正確答案和資料的規律性來做更好的學習，一個有名的例子是Facebook的人臉辨識標記功能，有部分已經被用戶標記的照片，這屬於有Label的yn，但有更多沒有標記的照片，這些照片也可以幫助ML學習。</p>
<p>那還有像是訓練小狗的方法，當我跟小狗說坐下，如果牠真的坐下了，這個時候我就給牠獎勵，譬如說餵牠好吃的食物，久而久之牠就會學會聽從這個命令，<strong>「Reinforcement Learning」(強化式學習）</strong>就是不直接表明yn的Label，但是機器能知道yn結果的好壞，再從這個好壞當作回饋去優化它的學習。</p>
<p>Data給的方法也可以有很多種類。</p>
<p>剛剛舉的ML例子都是屬於<strong>「Batch Learning」</strong>，也就是一次給你所有的Data。另外一種給Data的方法叫做<strong>「Online Learning」</strong>，這個情形下Data會一個一個以序列的方式餵給機器，這麼方式下的Model可以隨時更新。最後一種方式是<strong>「Active Learning」</strong>，機器不僅是被動的接受 Data，而是會根據它自己的需求向使用者索取它想要的Data。</p>
<p>另外，除了有輸出值yn有多種種類之外，輸入的變數 Xn的來源也有很多種，我們稱之為Features。</p>
<p>如果具有物理意義的輸入變數，稱之為<strong>「Concrete Features」</strong>，這些變數建立在人類知識的預先處理。還有輸入變數並不具有物理含意的情形，這稱之為<strong>「Abstract Features」</strong>。那有些情形下直接採用不加以處理的原始數據，稱為<strong>「Raw Features」</strong>。</p>
<p>而使用工人智慧由人力從Raw Features中萃取出Concrete Features，這叫做Feature Engineering，而現在很夯的Deep Learning厲害的地方是他可以自行從Data中學習 Features。</p>
<p><strong>總結一下，機器學習有很多種型態，從Data的給予方式可分為Batch Learning、Online Learning和Active Learning。Data的表達形式由輸入變數 Xn和輸出值 yn所決定，從輸入變數 Xn的來源可分為Concrete Features、Raw Features和Abstract Features，從輸出值 yn的種類上可以分為二元分類、多元分類、Regression和Structured Learning 問題，從輸出值 yn的Label給予情況可分為Supervised Learning、Unsupervised Learning、Semi-supervised Learning 和 Reinforcement Learning。</strong></p>
<p>順道一提，這16堂課裡頭主要聚焦在探討Batch Supervised Learning with Concrete Features。</p>
<h5><u>後話</u></h5>
<p>這篇文章帶大家初探了一眼機器學習，介紹了機器學習的架構和種類，以及它的使用時機，還有介紹了整門課非常重要的二元分類問題。但是講這麼多，機器學習真的可能嗎? 那如果可以做到，會需要哪一些要素呢? 這就必須深入理論之中，才能找到答案，在下一篇文章裡，我將介紹這門課的第二個部分：Why Can Machines Learn? </p></dd>
              
            </dl>
        </div>
    </div>
        <!-- /Content --> 

        <!-- Footer -->
        <div class="footer gradient-2">
            <div class="container footer-container ">
                <div class="row">
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Sitemap</div>
                        <ul class="list-unstyled">
                            <li><a href="../archives.html">Archives</a></li>
                            <li><a href="../tags.html">Tags</a></li>
                            <li><a href="YCNote/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">Atom Feed</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                        <div class="footer-title">Contact Me</div>
                        <ul class="list-unstyled">
                            <li><a href="./about-me.html" target="_blank">About Me</a></li>
                            <li><a href="https://github.com/GitYCC" target="_blank">Github</a></li>
                            <li><a href="mailto:ycc.tw.email@gmail.com" target="_blank">Email</a></li>
                        </ul>
                    </div>
                    <div class="col-xs-4 col-sm-3 col-md-3 col-lg-3">
                    </div> 
                    <div class="col-xs-12 col-sm-3 col-md-3 col-lg-3">
                        <p class="pull-right text-right">
                            <small><em>Proudly powered by <a href="http://docs.getpelican.com/" target="_blank">pelican</a></em></small><br/>
                            <small><em>Theme and code by <a href="https://github.com/molivier" target="_blank">molivier</a></em></small><br/>
                            <small>&copy; YC Note 2018</small>
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <!-- /Footer -->
    </body>
</html>